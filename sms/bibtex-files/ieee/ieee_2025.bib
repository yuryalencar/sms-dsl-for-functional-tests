@INPROCEEDINGS{8449605,
  author={Dos Santos, Ernani César and Vilain, Patrícia and Hiura Longo, Douglas},
  booktitle={2018 IEEE/ACM 40th International Conference on Software Engineering: Companion (ICSE-Companion)}, 
  title={Poster: A Systematic Literature Review to Support the Selection of User Acceptance Testing Techniques}, 
  year={2018},
  volume={},
  number={},
  pages={418-419},
  abstract={User Acceptance Testing (UAT) aims to determine whether or not a software satisfies users acceptance criteria. Although some studies have used acceptance tests as software requirements, no previous study has collected information about available UAT techniques and established a comparison of them, to support an organization in the selection of one over another. This work presents a Systematic Literature Review on UAT to find out available techniques and compare their main features. We selected 80 studies and found out 21 UAT techniques. As result, we created a comparative table summarizing these techniques and their features.},
  keywords={Testing;Software;Natural languages;Tools;Bibliographies;Software engineering;Systematics;User acceptance testing;techniques;classification;features},
  doi={},
  ISSN={2574-1934},
  month={May},}
  
@INPROCEEDINGS{7133548,
  author={Rahman, Mazedur and Gao, Jerry},
  booktitle={2015 IEEE Symposium on Service-Oriented System Engineering}, 
  title={A Reusable Automated Acceptance Testing Architecture for Microservices in Behavior-Driven Development}, 
  year={2015},
  volume={},
  number={},
  pages={321-325},
  abstract={Cloud Computing and Mobile Cloud Computing are reshaping the way applications are being developed and deployed due to their unique needs such as massive scalability, guaranteed fault tolerance, near zero downtime, etc. and also daunting challenges such as security, reliability, continuous deployment and update capability. Microservices architecture, where application is composed of a set of independently deployable services, is increasingly becoming popular due to its capability to address most of these needs and challenges. In recent years, the Behavior-Driven Development (BDD) has become one of the most popular agile software development processes, and frequently used in microservices development. The key to success of BDD is the executable acceptance tests that describe the expected behavior of a feature and its acceptance criteria in the form of scenarios using simple and business people readable syntax. The reusability, auditability, and maintainability become some of the major concerns when BDD test framework is applied for each microservice repository and no previous research addresses these concerns. In this paper, we present a reusable automated acceptance testing architecture to address all these concerns.},
  keywords={Data structures;Boolean functions;Business;Testing;Software;Maintenance engineering;executable automated acceptance testing; Gherkin; functional testing; behavior-driven development; microservice},
  doi={10.1109/SOSE.2015.55},
  ISSN={},
  month={March},}
  
@INPROCEEDINGS{9799224,
  author={Abdalla, Zeinab and Andrews, Anneliese and Alhaddad, Ahmed},
  booktitle={2021 International Conference on Computational Science and Computational Intelligence (CSCI)}, 
  title={Regression Testing of Mobile Apps}, 
  year={2021},
  volume={},
  number={},
  pages={1912-1917},
  abstract={Because mobile applications, or apps, are becoming essential in our personal lives and at work, and mobile applications update frequently, it is important that developers perform regression testing to ensure their quality. In this paper we adapt the FSMWeb approach for selective regression testing for mobile applications. We apply rules to classify the original set of tests into obsolete, retestable, and reusable tests based on the types of changes to the model. New tests are added to cover portions the App that have not been tested.},
  keywords={Adaptation models;Scientific computing;Minimization;Mobile applications;Computational intelligence;Testing;Selective Regression Testing;FSM;Mobile Apps},
  doi={10.1109/CSCI54926.2021.00055},
  ISSN={},
  month={Dec},}
  
@INPROCEEDINGS{7570913,
  author={Pandit, Pallavi and Tahiliani, Swati and Sharma, Meena},
  booktitle={2016 Symposium on Colossal Data Analysis and Networking (CDAN)}, 
  title={Distributed agile: Component-based user acceptance testing}, 
  year={2016},
  volume={},
  number={},
  pages={1-9},
  abstract={Testing is conducted at multiple levels during the development of software. User Acceptance Testing conforms that the software meets user's criteria. In this paper, User Acceptance Testing is automatically conducted based on acceptance criteria. The acceptance criteria are written in the form of Given-When-Then Template. These acceptance criteria are broken down into steps and numbered. The dependencies among the steps are determined as Given->When->Then. Henceforth, the steps are arranged in a dependency graph. This graph further leads to the creation of a decision table in which the outcome of one step leads to the outcomes of its dependent steps. The decision table forms the basis of generation of a binary weighted dependency tree. This tree becomes the means to form test coverage (number of combinations to test) which forms the basis of generation of acceptance test cases.},
  keywords={Testing;Online banking;Data analysis;Credit cards;Software;Algorithm design and analysis},
  doi={10.1109/CDAN.2016.7570913},
  ISSN={},
  month={March},}
  
@INPROCEEDINGS{9793870,
  author={Elsner, Daniel and Wuersching, Roland and Schnappinger, Markus and Pretschner, Alexander and Graber, Maria and Dammer, René and Reimer, Silke},
  booktitle={2022 IEEE/ACM 44th International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP)}, 
  title={Build System Aware Multi-language Regression Test Selection in Continuous Integration}, 
  year={2022},
  volume={},
  number={},
  pages={87-96},
  abstract={At IVU Traffic Technologies, continuous integration (CI) pipelines build, analyze, and test the code for inadvertent effects before pull requests are merged. However, compiling the entire code base and executing all regression tests for each pull request is infeasible due to prohibitively long feedback times. Regression test selection (RTS) aims to reduce the testing effort. Yet, existing safe RTS techniques are not suitable, as they largely rely on language-specific program analysis. The IVU code base consists of more than 13 million lines of code in Java or C/C++ and contains thousands of non-code artifacts. Regression tests commonly operate across languages, using cross-language links, or read from non-code artifacts. In this paper, we describe our build system aware multi-language RTS approach, which selectively compiles and executes affected code modules and regression tests, respectively, for a pull request. We evaluate our RTS technique on 397 pull requests, covering roughly 2,700 commits. The results show that we are able to safely exclude up to 75% of tests on average (no undetected real failures slip into the target branches) and thereby save 72% of testing time, whereas end-to-end CI pipeline time is reduced by up to 63% on average.},
  keywords={Java;Codes;Runtime;Pipelines;Software;Performance analysis;Testing;Software testing;regression test selection;continuous integration},
  doi={10.1145/3510457.3513078},
  ISSN={},
  month={May},}
  
@INPROCEEDINGS{9529903,
  author={Xu, Jincheng and Du, Qingfeng and Li, Xiaojun},
  booktitle={2021 IEEE 45th Annual Computers, Software, and Applications Conference (COMPSAC)}, 
  title={A Requirement-based Regression Test Selection Technique in Behavior-Driven Development}, 
  year={2021},
  volume={},
  number={},
  pages={1303-1308},
  abstract={Regression testing is an essential software maintenance activity before the release of a new version implementing a bug fix or a new feature. A regression test selection (RTS) technique chooses a subset of existing test cases to ensure that the system will not be adversely affected by the latest modifications. With the rise of DevOps, behavior-driven development (BDD) is growing in popularity as it is in close alignment with agile practices, for example, continuous integration. Hence, it is necessary to propose a novel and effective RTS technique for BDD specifically to accelerate the development process while ensuring software quality. Since most existing techniques for RTS are code-based and thus subject to some limitations, we present a requirement-based technique which uses the requirements in BDD to select test cases in both high-level (acceptance testing) and low-level (unit testing). Our technique firstly illustrates the new requirement with a scenario, and subsequently computes the semantic similarity of the new scenario and all existing scenarios with the vector space model. According to the results, the modification-traversing regression test cases can be selected in a semi-automated way. We also conduct an experimental study to evaluate our technique in terms of inclusiveness, precision, efficiency and generality. The study shows that our technique is applicable for BDD and effective in practice.},
  keywords={Software maintenance;Codes;Computational modeling;Conferences;Semantics;Computer bugs;Software quality;regression test selection;behavior-driven development;requirement-based technique;vector space model},
  doi={10.1109/COMPSAC51774.2021.00182},
  ISSN={0730-3157},
  month={July},}
  
@INPROCEEDINGS{8859410,
  author={Ramler, Rudolf and Klammer, Claus},
  booktitle={2019 IEEE 19th International Conference on Software Quality, Reliability and Security Companion (QRS-C)}, 
  title={Enhancing Acceptance Test-Driven Development with Model-Based Test Generation}, 
  year={2019},
  volume={},
  number={},
  pages={503-504},
  abstract={Acceptance test-driven development is widely used in practice. However, writing and maintaining acceptance tests is a costly and time-consuming activity, in particular when a system is tested via the GUI. In model-based testing, the tests are automatically generated from a model of the system. In this paper, we report our experience from applying a combination of acceptance test-driven development and model-based testing in several real-world projects from industry. With the application of model-based testing, we increased test coverage and extend testing to usage scenarios not exercised by the existing acceptance tests. In the industry projects, MBT was used as an enhancement rather than a replacement for ATDD. By creating a layered test automation architecture, we were able to reuse the established automation for model-based testing and to apply both approaches simultaneously. This strategy also helped us to minimize the risks and to reduce the effort involved in introducing MBT in the projects.},
  keywords={Testing;Graphical user interfaces;Automation;Adaptation models;Tools;Software;Industries;acceptance testing;model based testing;BDD;GUI testing;end-to-end testing;test automation architecture},
  doi={10.1109/QRS-C.2019.00096},
  ISSN={},
  month={July},}
  
@INPROCEEDINGS{8990263,
  author={Bertolino, Antonia and De Angelis, Guglielmo and Lonetti, Francesca},
  booktitle={2019 IEEE International Symposium on Software Reliability Engineering Workshops (ISSREW)}, 
  title={Governing Regression Testing in Systems of Systems}, 
  year={2019},
  volume={},
  number={},
  pages={144-148},
  abstract={Great advances in network technology and software engineering have triggered the development and spread of Systems of Systems (SoSs). The dynamic and evolvable nature of SoSs poses important challenges on the validation of such systems and in particular on their regression testing, aiming at assessing that run-time changes and evolutions do not introduce regression in SoS behavior. This paper outlines issues and challenges of regression testing of SoSs, identifying the main kinds of evolution that can impact on their regression testing activity. Furthermore, it presents a conceptual framework for governing the regression testing of SoSs. The proposed framework leverages the concept of an orchestration graph that describes the flow of test cases and sketches a solution for deriving a regression test plan according to test cases dependencies.},
  keywords={System of Systems;Regression Testing;Governance;Test Cases Orchestration},
  doi={10.1109/ISSREW.2019.00064},
  ISSN={},
  month={Oct},}
  
@INPROCEEDINGS{9920124,
  author={von Olberg, Pauline and Strey, Lukas},
  booktitle={2022 IEEE 30th International Requirements Engineering Conference Workshops (REW)}, 
  title={Approach to Generating Functional Test Cases from BPMN Process Diagrams}, 
  year={2022},
  volume={},
  number={},
  pages={185-189},
  abstract={Business Process Model and Notation (BPMN) is a popular and widespread modelling language used to describe business processes. These BPMN business process models can serve as a foundation for functional software testing. Functional software testing is an important part of software development, which ensures that software works as expected and that it includes all the desired functionality, as defined in the process models. This position paper presents an approach and considers two different methods on how to automatically create functional test cases from BPMN business process models. The generated test cases shall be understandable for all stakeholders and abstracting from the technical implementation. To achieve this general understandability of the test cases, Gherkin is used as a test case definition language. The two proposed methods will be developed and evaluated in future work. This planned evaluation includes comparing the automatically created test cases with manually created ones.},
  keywords={Software testing;Conferences;Software;Stakeholders;Requirements engineering;Business;BPMN;functional software testing;Gherkin;business process modelling},
  doi={10.1109/REW56159.2022.00042},
  ISSN={2770-6834},
  month={Aug},}
  
@INPROCEEDINGS{9920080,
  author={Anderson, Jacob and Hekmatnejad, Mohammad and Fainekos, Georgios},
  booktitle={2022 IEEE 30th International Requirements Engineering Conference (RE)}, 
  title={PyFoReL: A Domain-Specific Language for Formal Requirements in Temporal Logic}, 
  year={2022},
  volume={},
  number={},
  pages={266-267},
  abstract={Temporal Logic (TL) bridges the gap between natural language and formal reasoning in the field of complex systems verification. However, in order to leverage the expressivity entailed by TL, the syntax and semantics must first be understood—a large task in itself. This significant knowledge gap leads to several issues: (1) the likelihood of adopting a TL-based verification method is decreased, and (2) the chance of poorly written and inaccurate requirements is increased. In this ongoing work, we present the Pythonic Formal Requirements Language (PyFoReL) tool: a Domain-Specific Language inspired by the programming language Python to simplify the elicitation of TL-based requirements for engineers and non-experts.},
  keywords={Semantics;Natural languages;Syntactics;Cognition;Requirements engineering;Task analysis;Complex systems;domain-specific language;temporal logic;formal requirements;requirements-based testing},
  doi={10.1109/RE54965.2022.00037},
  ISSN={2332-6441},
  month={Aug},}
  
@INPROCEEDINGS{10667978,
  author={Shrivastava, Ajita and Mitra, Arka Pratap and Dungdung, Vinita and Singh, Deep},
  booktitle={2024 IEEE Space, Aerospace and Defence Conference (SPACE)}, 
  title={Development of a Critical System Using a Domain Specific Language}, 
  year={2024},
  volume={},
  number={},
  pages={56-60},
  abstract={For enhanced software modularity and maintainability, while meeting the exceedingly complex system requirements, a Domain Specific Language (DSL) was developed for the safety related computer system– Emergency Core Cooling System Test Facility, in a nuclear power station. This conception facilitated accurate development of test sequences directly by domain experts possessing limited familiarity with computer programming aspects. This unique DSL-based programming approach and exhaustive closed-loop black box testing enabled accurate tuning of variabilities, speedier software development, and ease of future modification. Considering the stringent atomic energy regulatory requirements, this also duly saved on development effort and software qualification timeline.},
  keywords={Accuracy;Test facilities;Surveillance;Programming;Safety;DSL;Space stations;Domain Specific Language;Software Development;Nuclear Power Plant},
  doi={10.1109/SPACE63117.2024.10667978},
  ISSN={},
  month={July},}
  
@INPROCEEDINGS{9978261,
  author={Corradini, Davide and Zampieri, Amedeo and Pasqua, Michele and Ceccato, Mariano},
  booktitle={2022 IEEE International Conference on Software Maintenance and Evolution (ICSME)}, 
  title={RestTestGen: An Extensible Framework for Automated Black-box Testing of RESTful APIs}, 
  year={2022},
  volume={},
  number={},
  pages={504-508},
  abstract={Over the past few years, several novel black-box testing approaches targeting RESTful APIs have been proposed. In order to assess their effectiveness, such testing strategies had to be implemented as a prototype tool and validated on empirical data. However, developing a testing tool is a time-consuming task, and reimplementing from scratch the same common basic features represents a waste of resources that causes a remarkable overhead in the "time to market" of research results.In this paper, we present RestTestGen, an extensible framework for implementing new automated black-box testing strategies for RESTful APIs. The framework provides a collection of commonly used components, such as a robust OpenAPI specification parser, dictionaries, input value generators, mutation operators, oracles, and others. Many of the provided components are customizable and extensible, enabling researchers and practitioners to quickly prototype, deploy, and evaluate their novel ideas. Additionally, the framework facilitates the development of novel black-box testing strategies by guiding researchers, by means of abstract components that explicitly identify those parts of the framework requiring a concrete implementation.As an adoption example, we show how we can implement nominal and error black-box testing strategies for RESTful APIs, by reusing primitives and features provided by the framework, and by concretely extending very few abstract components.RestTestGen is open-source, actively maintained, and publicly available on GitHub at https://github.com/SeUniVr/RestTestGen},
  keywords={Software maintenance;Dictionaries;Closed box;Restful API;Prototypes;Time to market;Generators;REST API;Test case generation;Black-box testing},
  doi={10.1109/ICSME55016.2022.00068},
  ISSN={2576-3148},
  month={Oct},}
  
@INPROCEEDINGS{9161781,
  author={Popic, Srdjan and Komadina, Vanja and Arsenovic, Ranka and Stepanovic, Mia},
  booktitle={2020 Zooming Innovation in Consumer Technologies Conference (ZINC)}, 
  title={Implementation of the simple domain-specific language for system testing in V-Model development lifecycle}, 
  year={2020},
  volume={},
  number={},
  pages={290-294},
  abstract={This paper presents easy to use domain-specific language for system testing in V-model development lifecycle. The systematic approach offered by the domain-specific language for system testing eliminates miscommunications between testers and requirement engineers making the testing closer to the requirement engineers. This concept enables automation in the generation of the tests based on given System Requirements in the future. As many would argue on V-Model's difficulty to align system requirements and system tests, this approach enables better mapping between those two parts of the V-diagram. This will make no functional requirement missing its counterpart in testing and vice-versa.},
  keywords={DSL;System testing;Task analysis;XML;Tools;Software;domain-specific language;V-model;system testing},
  doi={10.1109/ZINC50678.2020.9161781},
  ISSN={},
  month={May},}
  
@INPROCEEDINGS{10406875,
  author={Gigerl, B. and Zhao, Y. and Raminger, J. and Bakakeu, J. and Kern, R. and Thalmann, S.},
  booktitle={2023 IEEE International Conference on Industrial Engineering and Engineering Management (IEEM)}, 
  title={Predicting Partial Discharges of Transformers: Decision Support System for Factory Acceptance Test}, 
  year={2023},
  volume={},
  number={},
  pages={1687-1691},
  abstract={Partial discharges, mainly caused by an insufficient drying processes or different types of contamination, reduce the lifetime of a transformer, and thus lead to expensive rework costs. Herein, a decision support system for partial discharges of transformers occurring in oven and filling processes is introduced. Based on machine learning (ML), partial discharge results are evaluated in dependency of various manufacturing parameters and an automated prediction tool to guide the production with preventive actions is developed. The required data, obtained from sensors and manufacturing sources, is used to train supervised learning algorithms that aim to predict and classify partial discharges. To achieve adequate accuracy and reliability, multiple ML and data mining techniques are applied, including feature engineering, clustering, and final evaluation of performance by cost factors. The evaluation results show that the introduced ML models effectively detect and classify early test failures in oven and filling processes, resulting in a successful identification of key factors and consequently to more efficient action derivation. Overall, the potential of decision support systems as a valuable tool in the field of transformers is emphasized.},
  keywords={Partial discharges;Decision support systems;Costs;Ovens;Transformers;Prediction algorithms;Production facilities;Decision Support System;Predictive Quality Control;Machine Learning;Partial Discharges},
  doi={10.1109/IEEM58616.2023.10406875},
  ISSN={},
  month={Dec},}
  
@INPROCEEDINGS{6767211,
  author={Wacht, P. and Trick, U. and Fuhrmann, W. and Ghita, B.},
  booktitle={Second International Conference on Future Generation Communication Technologies (FGCT 2013)}, 
  title={A new service description for communication services as basis for automated functional testing}, 
  year={2013},
  volume={},
  number={},
  pages={59-64},
  abstract={The advances in the telecommunication domain to support complex communication services has resulted in a need for a new approach to automatically verify that the communication services meet the demands of the customers. This paper presents a concept for automated functional testing by means of a novel test framework. Within the framework, the tests are automatically derived from a proposed new sort of requirements specification for communication services, the Service Description, and afterwards generated by means of predefined test modules. Finally, the test cases are executed against the System under Test, the communication service.},
  keywords={Unified modeling language;Testing;Protocols;Standards;Syntactics;Analytical models;Tunneling magnetoresistance;automated functional testing;communication services;requirements specification;test framework;testing methodology},
  doi={10.1109/FGCT.2013.6767211},
  ISSN={2377-2638},
  month={Nov},}
  
@INPROCEEDINGS{8387650,
  author={Pinkevich, Vasiliy and Platunov, Alexey},
  booktitle={2018 IEEE Industrial Cyber-Physical Systems (ICPS)}, 
  title={Model-driven functional testing of cyber-physical systems using deterministic replay techniques}, 
  year={2018},
  volume={},
  number={},
  pages={141-146},
  abstract={Specialized embedded computer systems are one of the core technologies of modern industrial cyber-physical systems. They implement application algorithms and perform data acquisition, processing and transfer. The article presents the original approach to functional testing of embedded computer systems to overcome a number of restrictions imposed by specifics of the process of their design and development.},
  keywords={Embedded systems;Computational modeling;Data acquisition;Computer architecture;Cyber-physical systems;Testing;cyber-physical systems;embedded systems;record and deterministic replay;computer architecture;high-level modeling;functional testing;verification;debug},
  doi={10.1109/ICPHYS.2018.8387650},
  ISSN={},
  month={May},}
  
@INPROCEEDINGS{4464015,
  author={Diaz, Jessica and Yague, Agustin and Alarcon, Pedro P. and Garbajosa, Juan},
  booktitle={Seventh International Conference on Composition-Based Software Systems (ICCBSS 2008)}, 
  title={A Generic Gateway for Testing Heterogeneous Components in Acceptance Testing Tools}, 
  year={2008},
  volume={},
  number={},
  pages={110-119},
  abstract={Acceptance testing tools and Systems Under Test (SUT) require a gateway that will set up the communication link between them. Nevertheless, SUTs are often large systems composed of heterogeneous components that are executed in heterogeneous networks and platforms. Therefore, a non trivial communication problem between testing tools and these SUT heterogeneous components arises. A significant effort is invested in designing and implementing gateways for each specific component interface to cope with heterogeneity. This problem may be addressed through the use of middleware technologies that hide heterogeneity. However, this solution is too specific for each SUT domain. It may require a noteworthy effort to support the wide range of currently available interface standards that are provided by the different platforms and networks. An approach for testing heterogeneous components based on a generic gateway is presented in this paper. The generic gateway implements a service-oriented middleware named OSGi (Open Service Gateway initiative). OSGi helps to solve the heterogeneity problem and reduces the impact of designing a gateway for each specific SUT domain. The solution has been validated using the acceptance testing tool TOPEN (Test and Operation ENvironment) in a home automation scenario.},
  keywords={System testing;Automatic testing;Middleware;Software systems;Software testing;Home automation;Embedded system;Intelligent sensors;Computer architecture;Software tools;test automation;middleware;complex systems testing;acceptance testing tools;OSGI;TOPEN;gateway;validation},
  doi={10.1109/ICCBSS.2008.31},
  ISSN={},
  month={Feb},}
  
@ARTICLE{10015143,
  author={de Oliveira, Lívia Fernanda and Rodrigues, Cássio Leonardo and Bulcão-Neto, Renato de Freitas},
  journal={IEEE Latin America Transactions}, 
  title={Characterizing the Software Acceptance Testing and the Inclusion of People with Disabilities by Means of a Systematic Mapping}, 
  year={2023},
  volume={21},
  number={1},
  pages={35-46},
  abstract={Acceptance testing is a test technique where the final user evaluates the actual use of the software. In this test, software must meet the acceptance criteria defined on the requirement engineering phase to be approved. This paper describes the participation of the person with a disability in acceptance testing in terms of testing approaches, tools and user feedback. By performing a systematic mapping, we analyzed 609 articles, of which 58 were considered to answer four research questions. We identified that acceptance tests have no standard and present a dispersed variety of approaches and tools. We also few acceptance tests studies applied to testers with disabilities, although this approach has been rising in recent years. Considering accessibility for all users when including them in acceptance tests can ensure greater reach of users to systems},
  keywords={Software;Testing;Systematics;IEEE transactions;Libraries;Statistics;Standards;Software Testing;Acceptance Testing;Disabled User;Systematic Mapping;Secondary Study},
  doi={10.1109/TLA.2023.10015143},
  ISSN={1548-0992},
  month={Jan},}
  
@INPROCEEDINGS{5069055,
  author={Talby, David},
  booktitle={2009 ICSE Workshop on Automation of Software Test}, 
  title={The perceived value of authoring and automating acceptance tests using a model driven development toolset}, 
  year={2009},
  volume={},
  number={},
  pages={154-157},
  abstract={One approach to applying keyword driven testing in a model-driven development environment is by defining a domain specific language for test cases. The toolset then provides test editors, versioning, validation, reporting and hyperlinks across models - in addition to enabling automated test execution. This case study evaluates the effectiveness of such a solution as perceived by two teams of professional testers, who used it to test several products over a two year period. The results suggest that in addition to the expected benefits of automation, the solution reduces the time and effort required to write tests, maintain tests and plan the test authoring and execution efforts - at the expense of requiring longer training and a higher bar for recruiting testers.},
  keywords={Automatic testing;DSL;Logic testing;Domain specific languages;Application software;Automation;Recruitment;Programming;Metamodeling;Context modeling},
  doi={10.1109/IWAST.2009.5069055},
  ISSN={},
  month={May},}
  
@INPROCEEDINGS{10758052,
  author={Gatherer, Alan and Sengupta, Chaitali and Sen, Sudipta and Reed, Jeffery H.},
  booktitle={2024 IEEE 100th Vehicular Technology Conference (VTC2024-Fall)}, 
  title={Directed Testing of ORAN using a Partially Specified Declarative Digital Twin}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  abstract={Real Time performance testing can be divided into two distinct parts: system test and algorithm test. System test checks that the right functions operate on the right data within power, latency, and other constraints under all conditions. Major RAN OEMs, put as much effort into system test and debug as they do into algorithm test, to ensure a competitive product. An algorithm tester will provide little insight into real time and hardware-software (HW-SW) capacity as it is unaware of the system implementation. In this paper we present an innovative Digital Twin technology, which we call Declarative Digital Twin (DDT). A DDT can describe the system requirements of the RAN such that critical corner cases can be found via automation, that would normally be missed by conventional testing. This is possible even when the RAN requirements are only partially specified. We present a Domain Specific Language (DSL) for declarative description of the RAN and show results from an automated solver that demonstrate how potential HW-SW implementation related corner cases can be identified from the DDT of an ORAN DU.},
  keywords={Vehicular and wireless technologies;Programming;Real-time systems;Vectors;Digital twins;Servers;DSL;System implementation;Testing;Domain specific languages;DSL;RAN construction;Software Defined Radio;Automation;5G;6G;Open RAN;O-RAN;Functional Testing},
  doi={10.1109/VTC2024-Fall63153.2024.10758052},
  ISSN={2577-2465},
  month={Oct},}
  
@INPROCEEDINGS{5540750,
  author={Vanderbauwhede, W. and Margala, M. and Chalamalasetti, S. R. and Purohit, S.},
  booktitle={ASAP 2010 - 21st IEEE International Conference on Application-specific Systems, Architectures and Processors}, 
  title={A C++-embedded Domain-Specific Language for programming the MORA soft processor array}, 
  year={2010},
  volume={},
  number={},
  pages={141-148},
  abstract={MORA is a novel platform for high-level FPGA programming of streaming vector and matrix operations, aimed at multimedia applications. It consists of soft array of pipelined low-complexity SIMD processors-in-memory (PIM). We present a Domain-Specific Language (DSL) for high-level programming of the MORA soft processor array. The DSL is embedded in C++, providing designers with a familiar language framework and the ability to compile designs using a standard compiler for functional testing before generating the FPGA bitstream using the MORA toolchain. The paper discusses the MORA-C++ DSL and the compilation route into the assembly for the MORA machine and provides examples to illustrate the programming model and performance.},
  keywords={Domain specific languages;Field programmable gate arrays;Parallel processing;DSL;Streaming media;Application specific integrated circuits;Parallel programming;Concurrent computing;Algorithm design and analysis;Programming profession;Reconfigurable Processor;Soft Processor Array;Multimedia Processing;Domain-Specific Language},
  doi={10.1109/ASAP.2010.5540750},
  ISSN={1063-6862},
  month={July},}
  
@INPROCEEDINGS{9610682,
  author={Corradini, Davide and Zampieri, Amedeo and Pasqua, Michele and Ceccato, Mariano},
  booktitle={2021 IEEE 21st International Working Conference on Source Code Analysis and Manipulation (SCAM)}, 
  title={Empirical Comparison of Black-box Test Case Generation Tools for RESTful APIs}, 
  year={2021},
  volume={},
  number={},
  pages={226-236},
  abstract={In literature, we can find research tools to automatically generate test cases for RESTful APIs, addressing the specificity of this particular programming domain. However, no direct comparison of these tools is available to guide developers in deciding which tool best fits their REST API project.In this paper, we present the results of an empirical comparison of automated black-box test case generation approaches for REST APIs. We surveyed the available black-box testing tools that have been proposed in recent literature, finding four usable prototypes: RestTestGen, RESTler, bBOXRT and RESTest. We used these tools to generate test cases for 14 real-world REST services. Then, testing results have been analyzed and compared in terms of robustness (i.e., success rate) and test coverage.Among the considered tools, RESTler appears to be the most solid, able to successfully test all case studies (the other tools experienced crashes). Conversely, test cases generated by RestTestGen scored the highest coverage, suggesting that its testing strategy is the most effective in testing REST APIs.},
  keywords={Codes;Restful API;Tools;Programming;Solids;Robustness;Computer crashes;REST API;Test coverage;Black-box testing;Automated software testing;Experimental comparison},
  doi={10.1109/SCAM52516.2021.00035},
  ISSN={2470-6892},
  month={Sep.},}
  
@INPROCEEDINGS{8728933,
  author={Aichernig, Bernhard K. and Maderbacher, Benedikt and Tiran, Stefan},
  booktitle={2019 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)}, 
  title={Programming Behavioral Test Models for SMT Solving in Scala}, 
  year={2019},
  volume={},
  number={},
  pages={52-60},
  abstract={We present a novel approach for modeling cyber-physical systems for analysis and test purposes. Instead of creating a new expressive specification language with sophisticated semantics and complex compilers, we rely on a lightweight version of Back's Action Systems, for which we provide a simple bounded model checker using the SMT solver Z3. In order to model industrial-sized embedded systems, we extend our simple specification language by using the powerful capa-bilities of the modern programming language Scala for creating Domain Specific Languages (DSL). This enables us to use the features of an expressive, object-oriented and functional generalpurpose language without the need to increase the complexity of the model checker. We demonstrate how to model a railway interlocking system with a configurable track layout and sketch the application to model-based testing.},
  keywords={Object oriented modeling;Unified modeling language;Semantics;DSL;Syntactics;Testing;Computational modeling;model-based testing;test case generation;modelling;domain specific language;Scala},
  doi={10.1109/ICSTW.2019.00032},
  ISSN={},
  month={April},}
  
@INPROCEEDINGS{6200141,
  author={Borjesson, Emil},
  booktitle={2012 IEEE Fifth International Conference on Software Testing, Verification and Validation}, 
  title={Industrial Applicability of Visual GUI Testing for System and Acceptance Test Automation}, 
  year={2012},
  volume={},
  number={},
  pages={475-478},
  abstract={The software market is becoming more challenging as demands for faster time-to-market and higher software quality continue to grow. These challenges are embedded in all areas of Software Engineering, including Verification and Validation where they are proposed as solvable with automated testing. However, most automated testing techniques focus on low system level testing and are not suitable for high level tests, i.e. System and Acceptance tests, leaving industrial needs for test automation unfulfilled. In this paper we present a research plan to evaluate a novel automated testing technique, called visual GUI testing, based on image recognition algorithms and scripts that interact through the system GUI to automate complex scenario based tests. The technique has been evaluated at the company Saab AB where industrial, safety critical, scenario based, test cases were automated showing the industrial applicability of the technique. However, many factors are still unknown regarding the techniques industrial applicability, i.e. script maintenance costs, usability and learn ability, etc. Our research aims to uncover these unknown factors with the final research goal to show that visual GUI testing is a viable and cost-effective technique that will fill the gap in industry for a cost-effective, simple, robust, high-level test automation technique.},
  keywords={Testing;Graphical user interfaces;Visualization;Industries;Automation;Robustness;Maintenance engineering;V&V;Automated testing;Visual GUI testing;Image recognition;Scripted testing},
  doi={10.1109/ICST.2012.129},
  ISSN={2159-4848},
  month={April},}
  
@INPROCEEDINGS{4299957,
  author={Kim, Dae-Woo and Lim, Hyun-Min and Lee, Sang-Kon},
  booktitle={International Conference on Software Engineering Advances (ICSEA 2007)}, 
  title={A Case Study on Testing Activites for KT-OSS Maintenance}, 
  year={2007},
  volume={},
  number={},
  pages={77-77},
  abstract={This paper describes the testing activities for the maintenance of the KT-OSS (Korea Telecom Operations Support System). Since the KT-OSS is a large software, it is essential to continuously perform maintenance activities such as the addition of new services from business departments and new functions requested by users and operators, performance improvement of existing functions, correction of the errors found during operation of the system, and so on. To ensure the successful maintenance of the KT-OSS without any effect on the existing functions and performance, we performed various tests related to functionality, efficiency and others before the added and modified parts were applied to the KT-OSS. In this paper, we show the maintenance process, the various tests related to it, the test organization, and the test environment for controlling the quality of the KT-OSS maintenance. Through these testing activities, we were able to successfully maintain the KT-OSS.},
  keywords={Software maintenance;Error correction;Telecommunications;Preventive maintenance;System testing;Information management;Quality management;DSL;Paper technology;Research and development},
  doi={10.1109/ICSEA.2007.1},
  ISSN={},
  month={Aug},}
  
@INPROCEEDINGS{9039968,
  author={Nuriddinov, Askhat and Tavernier, Wouter and Colle, Didier and Pickavet, Mario and Peustery, Manuel and Schneidery, Stefan},
  booktitle={2019 IEEE Conference on Network Function Virtualization and Software Defined Networks (NFV-SDN)}, 
  title={Reproducible Functional Tests for Multi-scale Network Services}, 
  year={2019},
  volume={},
  number={},
  pages={1-6},
  abstract={Network functions virtualization (NFV) is developed to take advantage of virtualization technologies to separate network functions from the underlying hardware appliances. This approach brings a new level of flexibility in network services deployment for rapid composition, migration and scaling based on changing traffic load and current demand, leading to a higher QoS. Moreover, it allows introducing DevOps approach in the development process resulting in reduced life-cycle and shorter time-to-market. However, in order to realize the anticipated benefits of NFV, developers and network operators need mechanisms to adequately test Virtualized Network Functions (VNF) against different deployment scenarios and in different scales. To this end, we introduce a new library for light-weight automated functional testing of VNFs. It helps the developers to write functional tests in Python and run them on different platforms. Using the library, test developers choose which infrastructure to use, which network services and test VNFs to launch, how to interconnect them, how to trigger the test process and inspect, verify and validate the output against the expected values and conditions. Test developers can use network service packages to deploy services or can compose them in a test code using different parameters to simulate various deployment scenarios. Finally, the library can be used to set automated testing in CI/CD environments or can be used locally during the development phase. In this paper, we describe the architecture of the library, the basic workflow and give an example of testing multiple flavors of a network service using the same code.},
  keywords={Testing;Libraries;Python;Network function virtualization;Computer architecture;Emulation;Conferences;5GTANGO;NFV;network functions virtualization;VNF;virtualized network functions;functional testing},
  doi={10.1109/NFV-SDN47374.2019.9039968},
  ISSN={},
  month={Nov},}
  
@INPROCEEDINGS{9626192,
  author={Schneid, Konrad and Stapper, Leon and Thöne, Sebastian and Kuchen, Herbert},
  booktitle={2021 IEEE 25th International Enterprise Distributed Object Computing Conference (EDOC)}, 
  title={Automated Regression Tests: A No-Code Approach for BPMN-based Process-Driven Applications}, 
  year={2021},
  volume={},
  number={},
  pages={31-40},
  abstract={BPMN-based Process-Driven Applications (PDA) require less coding since they are not only based on source code, but also on executable process models. Automated testing of such model-driven applications gains growing relevance, and it becomes a key enabler if we want to found their development on continuous integration (CI) techniques.While process analysts are typically responsible for test case specifications from a business perspective, technically skilled process engineers take the responsibility for implementing the required test code. This is time-consuming and, due to their often different skills and backgrounds, might result in communication problems such as information losses and misunderstandings. This paper presents a new approach which enables an analyst to generate executable tests for PDAs without the need for manual coding. It consists of a sophisticated model analysis, a wizard-based specification of test cases, and a subsequent code generation. The resulting tests can easily be integrated into CI pipelines.The concept is underpinned by a user-friendly tool which has been evaluated in case studies and in real-world implementation projects from different industry sectors. During the evaluation, the prototype proved a more efficient test creation process and a higher test quality.},
  keywords={Industries;Analytical models;Codes;Handheld computers;Conferences;Prototypes;Manuals;Model-Based Testing;BPMN;No-Code;Process-Driven Application},
  doi={10.1109/EDOC52215.2021.00014},
  ISSN={2325-6362},
  month={Oct},}
  
@ARTICLE{10701278,
  author={Cedillo, Priscila and Valdez-Solis, Wilson and Erazo-Garzón, Lenin and Cardenas-Delgado, Paul},
  journal={IEEE Access}, 
  title={FOGAAL: A Domain-Specific Language for Fog Computing in Ambient Assisted Living Environments}, 
  year={2024},
  volume={12},
  number={},
  pages={143058-143073},
  abstract={The Internet of Things (IoT) has revolutionized numerous sectors, with healthcare being a prominent beneficiary. One key area of advancement is Ambient Assisted Living (AAL), which leverages Information and Communication Technologies (ICT) like IoT to improve the quality of life for vulnerable populations, particularly seniors. However, despite efforts to tailor solutions to the needs of these environments, the prevailing approach often becomes overly focused on intricate implementation details rather than the core characteristics of the problem domain. This narrow focus inhibits generalization and leads to unsustainable solutions due to these environments’ diverse, dynamic, and scalable nature. In response, emerging technology paradigms, such as Fog Computing (FC), have emerged to enhance performance and other quality aspects. FC enables the deployment of distributed, latency-aware applications and services. Therefore, this paper introduces FOGAAL, a Domain-Specific Language (DSL) tool crafted to design AAL architectures, including the FC advantages. To demonstrate FOGAAL’s practical applicability, this contribution uses FOGAAL to build an AAL architecture for a real scenario; in addition, it has an empirical evaluation employing the Technology Acceptance Model (TAM) to gauge user perceptions while utilizing the tool for modeling AAL Architectures. The evaluation, conducted as a quasi-experiment, underscore the tool’s alignment with requirements and its potential for adoption, as evidenced by software engineers who have expressed their intention to use FOGAAL when creating architecture for AAL environments.},
  keywords={DSL;Internet of Things;Ambient assisted living;Cloud computing;Ontologies;Domain specific languages;Computer architecture;Edge computing;Medical services;Computational modeling;Ambient assisted living;cloud computing;domain specific language;fog computing;Internet of Things},
  doi={10.1109/ACCESS.2024.3471412},
  ISSN={2169-3536},
  month={},}

@INPROCEEDINGS{7128889,
  author={Visic, Niksa and Fill, Hans-Georg and Buchmann, Robert Andrei and Karagiannis, Dimitris},
  booktitle={2015 IEEE 9th International Conference on Research Challenges in Information Science (RCIS)}, 
  title={A domain-specific language for modeling method definition: From requirements to grammar}, 
  year={2015},
  volume={},
  number={},
  pages={286-297},
  abstract={The core process a modeling method engineer needs to accomplish starts with the acquisition of domain knowledge and requirements, and ends with the deployment of a usable modeling tool. In between, a key intermediate deliverable of this process is the modeling method specification which, ideally, should be platform independent. On one hand, it takes input from a structured understanding of the application domain and scenarios; on the other hand, it provides sufficiently structured input to support the implementation of tool support for modeling activities. It is quite common that such modeling methods are domain-specific, in the sense that they provide concepts from the domain as “first-class modeling citizens”. However, for the purposes of this paper, we raise the level of abstraction for “domain specificity” and consider “modeling method engineering” as the application domain. Consequently, we raise several research questions - whether a domain-specific language can support this domain, and what would be its requirements, properties, constructs and grammar. We propose an initial draft of such a language - one that abstracts away from meta-modeling platforms by establishing a meta2 layer of abstraction where a modeling method can be defined in a declarative manner, then the final modeling tool is generated by automated compilation of the method definition for the meta-modeling environment of choice.},
  keywords={Unified modeling language;Analytical models;DSL;Metamodeling;Semantics;Computational modeling;Domain specific languages;domain-specific language;modeling method;meta-modeling;modeling tool},
  doi={10.1109/RCIS.2015.7128889},
  ISSN={2151-1357},
  month={May},}
  
@INPROCEEDINGS{6228998,
  author={Hallenberg, Niels and Carlsen, Philip Lykke},
  booktitle={2012 7th International Workshop on Automation of Software Test (AST)}, 
  title={Declarative automated test}, 
  year={2012},
  volume={},
  number={},
  pages={96-102},
  abstract={Automated tests at the business level can be expensive to develop and maintain. One common approach is to have a domain expert instruct a QA developer to implement what she would do manually in the application. Though there exist record-replay tools specifically developed for this, these tend to scale poorly for more complicated test scenarios. We present a different solution: An Embedded Domain Specific Language (EDSL) in F#, containing the means to model the user interface, and the various manipulations of it. We hope that this DSL will bridge the gap between the business domain and technical domain of applications to such a degree that domain experts may be able to construct automatic tests without depending on QA developers, and that these tests will prove more maintainable.},
  keywords={Testing;DSL;Documentation;Phantoms;Engines;Business;User interfaces;Functional Testing;Automated Testing;Domain Specific Language;F#},
  doi={10.1109/IWAST.2012.6228998},
  ISSN={},
  month={June},}

@INPROCEEDINGS{9287916,
  author={Breitenhuber, Guido},
  booktitle={2020 Fourth IEEE International Conference on Robotic Computing (IRC)}, 
  title={Towards application level testing of ROS networks}, 
  year={2020},
  volume={},
  number={},
  pages={436-442},
  abstract={Robotics is a highly interdisciplinary field where applications of complex architectures are built to solve challenging problems. Integration of multiple components in this environment continues to be a time-consuming, tedious and error-prone activity. Typical approaches in parallel component development or composition from re-usable components focus on structural and static aspects like ROS message and topic definitions. However, the behavioural part of robot software components is hardly ever specified in sufficient detail. In this work, we tackle this issue by proposing an application-level testing framework for robot software applications that uses a fluent API to describe the expected behaviour of an application or its components. This is a first step towards test-first based development of robot software in order to increase robot software quality.},
  keywords={Conferences;Software quality;Computer architecture;Robots;Testing;Software testing;ROS testing;Software quality;Automated testing;Testing frameworks;ROS},
  doi={10.1109/IRC.2020.00081},
  ISSN={},
  month={Nov},}

@INPROCEEDINGS{8725698,
  author={Nuriddinov, Askhat and Tavernier, Wouter and Colle, Didier and Pickavet, Mario},
  booktitle={2018 IEEE Conference on Network Function Virtualization and Software Defined Networks (NFV-SDN)}, 
  title={A framework for functional testing of VNFs}, 
  year={2018},
  volume={},
  number={},
  pages={1-2},
  abstract={Network Function Virtualization (NFV) is a promising technology which can significantly boost innovations in the area of telecommunication networks. However, to realize the anticipated benefits of NFV, network operators need to solve several challenges which include performance and functional testing of Virtualized Network Functions (VNF). Furthermore, the development process of VNFs is very complex and error-prone, therefore the developers also need to do functional testing of their VNFs.To this end, we introduce a new open-source framework for functional testing of VNFs. It allows to write tests in Python and use its simplicity to write tests with minimal effort. The framework has integration with virtual infrastructure to make the test process seamless and less time consuming.},
  keywords={Testing;Conferences;Python;Network function virtualization;Acceleration;Software defined networking},
  doi={10.1109/NFV-SDN.2018.8725698},
  ISSN={},
  month={Nov},}

@INPROCEEDINGS{5463692,
  author={Chatley, Robert and Ayres, John and White, Tom},
  booktitle={2010 Third International Conference on Software Testing, Verification, and Validation Workshops}, 
  title={LiFT: Driving Development Using a Business-Readable DSL for Web Testing}, 
  year={2010},
  volume={},
  number={},
  pages={460-468},
  abstract={This paper describes the development and evolution of LiFT, a framework for writing automated tests in a style that makes them very readable, even for non-programmers. We call this style 'literate testing'. By creating a domain-specific language embedded within Java, we were able to write automated tests that read almost like natural language, allowing business requirements to be expressed very clearly. This allows development to be driven from tests that are created by developers and customers together, helping give all stakeholders confidence that the right things are being tested and hence a correct system being built. We discuss the experiences of a team using these tools and techniques in a large commercial project, and the lessons learned from the experience.},
  keywords={DSL;Automatic testing;System testing;Writing;Software testing;Domain specific languages;Java;Natural languages;Business communication;Formal specifications;TDD;acceptance testing;DSL},
  doi={10.1109/ICSTW.2010.12},
  ISSN={},
  month={April},}

@INPROCEEDINGS{4055046,
  author={Kim, Dae-woo and Lim, Hyun-min and Lee, Sang-kon},
  booktitle={2006 Canadian Conference on Electrical and Computer Engineering}, 
  title={Testing Activities for KT-OSS Development}, 
  year={2006},
  volume={},
  number={},
  pages={2397-2400},
  abstract={This paper describes the testing activities for the development of the KTOSS (Korea Telecom Operations Support System). In this paper, we show the test phases for performing the verification and validation activities for the development and maintenance of KT-OSS. They are based on the general software development lifecycle, with an operational test added to it as an additional phase. To ensure the successful development of the KT-OSS, we performed various tests related to functionality, efficiency and others. Also, the tests were performed for maintenance after the field release. We also show the criteria for them and deal with the test organizations and the test-bed for managing and controlling the quality of the KT-OSS in this paper. Through these testing activities, we were able to successfully develop and release the KT-OSS},
  keywords={Life testing;Software testing;Programming;System testing;Laboratories;Performance evaluation;Quality management;DSL;Telecommunications;ISO standards;Test;Operations Support System;Verification;Validation;Software Development Lifecycle},
  doi={10.1109/CCECE.2006.277822},
  ISSN={0840-7789},
  month={May},}

@INPROCEEDINGS{5261058,
  author={McMahon, Chris},
  booktitle={2009 Agile Conference}, 
  title={History of a Large Test Automation Project Using Selenium}, 
  year={2009},
  volume={},
  number={},
  pages={363-368},
  abstract={In 2007 I started work as a tester for a company called Socialtext. When I joined the company there was already a Selenium-based test framework in place, but there were only a couple of automated test cases created; we had about 400 test steps, or individual assertions about the behavior of the application. When I left Socialtext two years later, we had just surpassed 10,000 test steps in the main set of regression tests. We also had browser-specific test sets in place, an automated test case for visually checking the application, and a Continuous-Integration-like script that ran all day and all night against the latest version of the code. At about 4000 test steps, regression bugs released to production dropped essentially to zero. The other 6000 test steps covered ongoing new features in the project, and more robust testing of the older application functions. This report discusses how I helped grow this system, and the things we learned along the way that helped it be such a successful ongoing project. The report covers initial conditions and test design; discusses issues in application feature coverage; how and when to grow the system quickly; a couple of test design smells that caused us problems along the way; how we treat Continuous Integration in a system like this; and how we coped when significant parts of the User Interface were completely re-engineered.},
  keywords={History;Automatic testing;System testing;Quality management;Failure analysis;User interfaces;Engineering profession;Home automation;Radio access networks;Computer bugs;UI automation;Selenium;test design;experience},
  doi={10.1109/AGILE.2009.9},
  ISSN={},
  month={Aug},}

@INPROCEEDINGS{9000047,
  author={Gafurov, Davrondzhon and Hurum, Arne Erik and Markman, Martin},
  booktitle={2018 33rd IEEE/ACM International Conference on Automated Software Engineering (ASE)}, 
  title={Achieving Test Automation with Testers without Coding Skills: An Industrial Report}, 
  year={2018},
  volume={},
  number={},
  pages={749-756},
  abstract={We present a process driven test automation solution which enables delegating (part of) automation tasks from test automation engineer (expensive resource) to test analyst (non-developer, less expensive). In our approach, a test automation engineer implements test steps (or actions) which are executed automatically. Such automated test steps represent user actions in the system under test and specified by a natural language which is understandable by a non-technical person. Then, a test analyst with a domain knowledge organizes automated steps combined with test input to create an automated test case. It should be emphasized that the test analyst does not need to possess programming skills to create, modify or execute automated test cases. We refine benchmark test automation architecture to be better suitable for an effective separation and sharing of responsibilities between the test automation engineer (with coding skills) and test analyst (with a domain knowledge). In addition, we propose a metric to empirically estimate cooperation between test automation engineer and test analyst's works. The proposed automation solution has been defined based on our experience in the development and maintenance of Helsenorge, the national electronic health services in Norway which has had over one million of visits per month past year, and we still use it to automate the execution of regression tests.},
  keywords={Knowledge engineering;Measurement;Automation;Natural languages;Organizations;Manuals;Programming;Encoding;Maintenance;Software engineering;Test automation;process-driven test automation;keyword-driven test automation;DSL for test automation;Helsenorge},
  doi={10.1145/3238147.3240463},
  ISSN={2643-1572},
  month={Sep.},}

@INPROCEEDINGS{6984109,
  author={Häser, Florian and Felderer, Michael and Breu, Ruth},
  booktitle={2014 9th International Conference on the Quality of Information and Communications Technology}, 
  title={Test Process Improvement with Documentation Driven Integration Testing}, 
  year={2014},
  volume={},
  number={},
  pages={156-161},
  abstract={Improving the maturity of the test process in an organization, especially but not limited to integration testing, involves obstacles and risks, such as the additional work overhead of the new process. In addition, integration testing descriptions are often too technical not addressing the language needs of the domain. In research cooperations with companies from the insurance and banking domain it turned out that test descriptions and reports are one of the most useful testing artifacts, while doing adhoc testing. This paper presents a bottom up testing approach, which first helps the integration tester in producing a semi-formal test description and report, up to be an enabler for automatic model-based testing in the very end. The presented approach is based on a textual domain specific language that is able to evolve over time. This is done by analyzing the test descriptions and reports automatically with machine learning techniques as well as manually by integration testers. Often recurring test steps or used components are integrated into the test language, making it specially tailored for a specific organization. For each test step implementations can be attached, preparing it for the next iteration. In this paper the methodology and architecture of our integration testing approach are presented together with the underlying language concepts.},
  keywords={Testing;Documentation;Unified modeling language;DSL;Insurance;Companies;Model-Based Integration Testing;Test Process Improvement;Regression Testing},
  doi={10.1109/QUATIC.2014.29},
  ISSN={},
  month={Sep.},}
  
@INPROCEEDINGS{5626417,
  author={Strasser, T and Peters, T and Jägle, H. and Zrenner, E. and Wilke, R.},
  booktitle={2010 Annual International Conference of the IEEE Engineering in Medicine and Biology}, 
  title={An integrated domain specific language for post-processing and visualizing electrophysiological signals in Java}, 
  year={2010},
  volume={},
  number={},
  pages={4687-4690},
  abstract={Electrophysiology of vision - especially the electroretinogram (ERG) - is used as a non-invasive way for functional testing of the visual system. The ERG is a combined electrical response generated by neural and non-neuronal cells in the retina in response to light stimulation. This response can be recorded and used for diagnosis of numerous disorders. For both clinical practice and clinical trials it is important to process those signals in an accurate and fast way and to provide the results as structured, consistent reports. Therefore, we developed a freely available and open-source framework in Java (http://www.eye.uni-tuebingen.de/project/idsI4sigproc). The framework is focused on an easy integration with existing applications. By leveraging well-established software patterns like pipes-and-filters and fluent interfaces as well as by designing the application programming interfaces (API) as an integrated domain specific language (DSL) the overall framework provides a smooth learning curve. Additionally, it already contains several processing methods and visualization features and can be extended easily by implementing the provided interfaces. In this way, not only can new processing methods be added but the framework can also be adopted for other areas of signal processing. This article describes in detail the structure and implementation of the framework and demonstrate its application through the software package used in clinical practice and clinical trials at the University Eye Hospital Tuebingen one of the largest departments in the field of visual electrophysiology in Europe.},
  keywords={Lead;Java;Artificial neural networks;Hospitals;HTML;Visualization},
  doi={10.1109/IEMBS.2010.5626417},
  ISSN={1558-4615},
  month={Aug},}

@INPROCEEDINGS{7102616,
  author={Haser, Florian and Breu, Ruth},
  booktitle={2015 IEEE 8th International Conference on Software Testing, Verification and Validation (ICST)}, 
  title={Non-Intrusive Documentation-Driven Integration Testing}, 
  year={2015},
  volume={},
  number={},
  pages={1-2},
  abstract={Powerful development frameworks and adoption of agile development methods are continuously increasing release frequency, thus compress test cycles. Test automation, often relying on model based approaches, helps to reduce test time, however the introduction of related heavy weight processes is often quite challenging. In order to tackle this problem, we propose a bottom up testing approach, which in a nutshell, in the initial phase supports the integration tester in creating a semi-formal test case description and report. The approach, a textual domain specific framework, will guide the test expert in evolving the base language, in order to be tailored for a domain language of an organization. The evolved language can be linked to executable code, which enables in the long run (semi-)automated model based regression testing.},
  keywords={Testing;Automation;Context;Unified modeling language;Business;Writing;Measurement},
  doi={10.1109/ICST.2015.7102616},
  ISSN={2159-4848},
  month={April},}

@INPROCEEDINGS{6569742,
  author={Di Nardo, Daniel and Alshahwan, Nadia and Briand, Lionel and Labiche, Yvan},
  booktitle={2013 IEEE Sixth International Conference on Software Testing, Verification and Validation}, 
  title={Coverage-Based Test Case Prioritisation: An Industrial Case Study}, 
  year={2013},
  volume={},
  number={},
  pages={302-311},
  abstract={This paper presents an industrial case study of coverage-based prioritisation techniques on a real world system with real regression faults. The study evaluates four common and different test case prioritisation techniques and examines the effects of using various coverage criteria on the fault detection rates of the prioritised test suites. The results show that prioritisation techniques that are based on additional coverage with finer grained coverage criteria perform significantly better in fault detection rates. The study also reveals that using modification information does not significantly enhance fault detection rates.},
  keywords={Testing;Fault detection;Measurement;Software;Computer aided software engineering;Minimization;Data collection;regression testing;industrial case study;test case prioritisation},
  doi={10.1109/ICST.2013.27},
  ISSN={2159-4848},
  month={March},}

@INPROCEEDINGS{6569751,
  author={Törsel, Arne-Michael},
  booktitle={2013 IEEE Sixth International Conference on Software Testing, Verification and Validation}, 
  title={A Testing Tool for Web Applications Using a Domain-Specific Modelling Language and the NuSMV Model Checker}, 
  year={2013},
  volume={},
  number={},
  pages={383-390},
  abstract={Test case generation from formal models using model checking software is an established method. This paper presents a model-based testing approach for web applications based on a domain-specific language model. It is shown how the domain-specific language is transformed into the input language of the NuSMV model checker and how the resulting traces are converted into executable test scripts for various test automation tools. The presented approach has been implemented with comprehensive automation in a research tool which architecture is outlined.},
  keywords={DSL;Automation;Adaptation models;Software;Web pages;Model checking;web applications;model-based testing;model checking;test automation},
  doi={10.1109/ICST.2013.54},
  ISSN={2159-4848},
  month={March},}

@ARTICLE{4052587,
  author={Yilmaz, Cemal and Porter, Adam and Krishna, Arvind S. and Memon, Atif M. and Schmidt, Douglas C. and Gokhale, Aniruddha S. and Natarajan, Balachandran},
  journal={IEEE Transactions on Software Engineering}, 
  title={Reliable Effects Screening: A Distributed Continuous Quality Assurance Process for Monitoring Performance Degradation in Evolving Software Systems}, 
  year={2007},
  volume={33},
  number={2},
  pages={124-141},
  abstract={Developers of highly configurable performance-intensive software systems often use in-house performance-oriented "regression testing" to ensure that their modifications do not adversely affect their software's performance across its large configuration space. Unfortunately, time and resource constraints can limit in-house testing to a relatively small number of possible configurations, followed by unreliable extrapolation from these results to the entire configuration space. As a result, many performance bottlenecks escape detection until systems are fielded. In our earlier work, we improved the situation outlined above by developing an initial quality assurance process called "main effects screening". This process 1) executes formally designed experiments to identify an appropriate subset of configurations on which to base the performance-oriented regression testing, 2) executes benchmarks on this subset whenever the software changes, and 3) provides tool support for executing these actions on in-the-field and in-house computing resources. Our initial process had several limitations, however, since it was manually configured (which was tedious and error-prone) and relied on strong and untested assumptions for its accuracy (which made its use unacceptably risky in practice). This paper presents a new quality assurance process called "reliable effects screening" that provides three significant improvements to our earlier work. First, it allows developers to economically verify key assumptions during process execution. Second, it integrates several model-driven engineering tools to make process configuration and execution much easier and less error prone. Third, we evaluate this process via several feasibility studies of three large, widely used performance-intensive software frameworks. Our results indicate that reliable effects screening can detect performance degradation in large-scale systems more reliably and with significantly less resources than conventional techniques},
  keywords={Quality assurance;Monitoring;Degradation;Software systems;Performance evaluation;Software performance;Software testing;System testing;Time factors;Extrapolation;Distributed continuous quality assurance;performance-o-ri-ented regression testing;design-of-experiments theory.},
  doi={10.1109/TSE.2007.20},
  ISSN={1939-3520},
 month={Feb},}

@INPROCEEDINGS{7928002,
  author={Dwarakanath, Anurag and Era, Dipin and Priyadarshi, Aditya and Dubash, Neville and Podder, Sanjay},
  booktitle={2017 IEEE International Conference on Software Testing, Verification and Validation (ICST)}, 
  title={Accelerating Test Automation through a Domain Specific Language}, 
  year={2017},
  volume={},
  number={},
  pages={460-467},
  abstract={Test automation involves the automatic execution of test scripts instead of being manually run. This significantly reduces the amount of manual effort needed and thus is of great interest to the software testing industry. There are two key problems in the existing tools & methods for test automation - a) Creating an automation test script is essentially a code development task, which most testers are not trained on, and b) the automation test script is seldom readable, making the task of maintenance an effort intensive process. We present the Accelerating Test Automation Platform (ATAP) which is aimed at making test automation accessible to non-programmers. ATAP allows the creation of an automation test script through a domain specific language based on English. The English-like test scripts are automatically converted to machine executable code using Selenium WebDriver. ATAP's English-like test script makes it easy for non-programmers to author. The functional flow of an ATAP script is easy to understand as well thus making maintenance simpler (you can understand the flow of the test script when you revisit it many months later). ATAP has been built around the Eclipse ecosystem and has been used in a real-life testing project. We present the details of the implementation of ATAP and the results from its usage in practice.},
  keywords={Automation;DSL;Tools;Selenium;Natural languages;Java;Programming;Test automation;Selenium;Xtext;DSL},
  doi={10.1109/ICST.2017.52},
  ISSN={},
  month={March},}
  
@INPROCEEDINGS{10151185,
  author={Waitchasarn, Kritchawat and Thongtak, Arthit and Suwannasart, Taratip},
  booktitle={2023 8th International Conference on Computer and Communication Systems (ICCCS)}, 
  title={Generating Robot Framework Test Scripts from User Stories and Scenarios for Web Application Testing}, 
  year={2023},
  volume={},
  number={},
  pages={795-801},
  abstract={The Scrum Framework combined with Behavior-Driven Development (BDD) has gained popularity as an agile development process. In this approach, user requirements are captured and expressed through user stories and scenarios, which serve as examples and acceptance criteria. Employing small iterations, this process reduces both time and cost of software development. Nonetheless, frequent small iterations pose a significant challenge for testing, especially regression testing. To mitigate the problem, automation testing has been adopted; however, manually composing test scripts can be error-prone and inefficient. This can result in the essential regression tests being deferred to later iterations, rendering it too late to identify errors. This paper proposes an approach to address this issue by generating Robot test scripts automatically from user stories and scenarios. Using XML to lay out the UI structures of the web page, test scripts can be created during the design phase. In addition to the examples provided in scenarios, the XML Schema Definition (XSD) is utilized to produce augmented test datasets automatically. Parameterizing test scripts also enables multiple test scripts with shared test steps but different test datasets, to be consolidated into a single script, thus drastically reducing maintenance effort.},
  keywords={Costs;Communication systems;XML;Web pages;Maintenance engineering;Rendering (computer graphics);Software;software testing;test automation;data-driven testing;robot framework;test script;user story;scenario},
  doi={10.1109/ICCCS57501.2023.10151185},
  ISSN={},
  month={April},}

@INPROCEEDINGS{10336270,
  author={Duque-Torres, Alejandra and Pfahl, Dietmar},
  booktitle={2023 IEEE International Conference on Software Maintenance and Evolution (ICSME)}, 
  title={Towards a Complete Metamorphic Testing Pipeline}, 
  year={2023},
  volume={},
  number={},
  pages={606-610},
  abstract={Metamorphic Testing (MT) addresses the test oracle problem by examining the relationships between input-output pairs in consecutive executions of the System Under Test (SUT). These relations, known as Metamorphic Relations (MRs), specify the expected output changes resulting from specific input changes. However, achieving full automation in generating, selecting, and understanding MR violations poses challenges. Our research aims to develop methods and tools that assist testers in generating MRs, defining constraints, and providing explainability for MR outcomes. In the MR generation phase, we explore automated techniques that utilise a domain-specific language to generate and describe MRs. The MR constraint definition focuses on capturing the nuances of MR applicability by defining constraints. These constraints help identify the specific conditions under which MRs are expected to hold. The evaluation and validation involve conducting empirical studies to assess the effectiveness of the developed methods and validate their applicability in real-world regression testing scenarios. Through this research, we aim to advance the automation of MR generation, enhance the understanding of MR violations, and facilitate their effective application in regression testing.},
  keywords={Software maintenance;Automation;Pipelines;Testing;Domain specific languages;Metamorphic Testing;Metamorphic Relations;Automation;Regression Testing},
  doi={10.1109/ICSME58846.2023.00081},
  ISSN={2576-3148},
  month={Oct},}

@ARTICLE{8405633,
  author={Garousi, Vahid and Felderer, Michael and Karapıçak, Çağrı Murat and Yılmaz, Uğur},
  journal={IEEE Software}, 
  title={What We Know about Testing Embedded Software}, 
  year={2018},
  volume={35},
  number={4},
  pages={62-69},
  abstract={To cost-effectively test embedded software, practitioners and researchers have proposed many test techniques, approaches, tools, and frameworks. However, obtaining an overview of the state of the art and state of the practice in this area is challenging for practitioners or new researchers. In addition, owing to an inadequate overview of what already exists in this area, some companies often reinvent the wheel by designing a test approach that’s new to them but already exists. To address these problems, the authors conducted a systematic literature review of this area that covered the testing topics, testing activities, test artifacts, and industries on which the studies focused. The results can benefit both practitioners and researchers by serving as an index to the vast body of knowledge in this important, fast-growing area.},
  keywords={Testing;Unified modeling language;Automation;Automotive engineering;Embedded software;software testing;embedded systems;embedded software;systematic literature mapping;systematic literature review;software engineering;software development},
  doi={10.1109/MS.2018.2801541},
  ISSN={1937-4194},
  month={July},}

@INPROCEEDINGS{9583706,
  author={Marksteiner, Stefan and Bronfman, Slava and Wolf, Markus and Lazebnik, Eddie},
  booktitle={2021 IEEE European Symposium on Security and Privacy Workshops (EuroS&PW)}, 
  title={Using Cyber Digital Twins for Automated Automotive Cybersecurity Testing}, 
  year={2021},
  volume={},
  number={},
  pages={123-128},
  abstract={Cybersecurity testing of automotive systems has become a practical necessity, with the wide adoption of advanced driving assistance functions and vehicular communications. These functionalities require the integration of information and communication technologies that not only allow for a plethora of on-the-fly configuration abilities, but also provide a huge surface for attacks. Theses circumstances have also been recognized by standardization and regulation bodies, making the need for not only proper cybersecurity engineering but also proving the effectiveness of security measures by verification and validation through testing also a formal necessity. In order to keep pace with the rapidly growing demand of neutral-party security testing of vehicular systems, novel approaches are needed. This paper therefore presents a methodology to create and execute cybersecurity test cases on the fly in a black box setting by using pattern matching-based binary analysis and translation mechanisms to formal attack descriptions as well as model-checking techniques. The approach is intended to generate meaningful attack vectors on a system with next-to-zero a priori knowledge.},
  keywords={Analytical models;Digital twin;Tools;Regulation;DSL;Computer security;Vehicle dynamics;automotive;cybersecurity;testing;digital twin;model-based testing},
  doi={10.1109/EuroSPW54576.2021.00020},
  ISSN={2768-0657},
  month={Sep.},}

@INPROCEEDINGS{6058739,
  author={Headrick, William J and Bodkin, Michael A and Fox, Robert R and Davis, Timothy W and Dusch, Kevin and Wolfe, Dan},
  booktitle={2011 IEEE AUTOTESTCON}, 
  title={Signal Based Domain Specific Language (SBDSL) a proposal for a next generation test}, 
  year={2011},
  volume={},
  number={},
  pages={240-244},
  abstract={Signal Based Domain Specific Language (SBDSL) is a domain specific language which combines the use of ATLAS Signal statements with high-level programming language constructs. The goals of this new language are: facilitate the writing of concurrent test programs, provide a language that is easy to extend with new constructs, maintain backwards compatibility with ATLAS Family of languages, enable interoperability between test stations, and enable engineers' fresh out of college to quickly become productive with a test programming language. This paper will cover how the design of the SBDSL language, SBDSL Integrated Development Environment (IDE) and runtime executable will accomplish these goals and present results from the technology demonstration developed.},
  keywords={Hardware;Instruments;Visualization;Syntactics;Debugging;Programming;Libraries},
  doi={10.1109/AUTEST.2011.6058739},
  ISSN={1558-4550},
  month={Sep.},}
  
@ARTICLE{9801672,
  author={Yaraghi, Ahmadreza Saboor and Bagherzadeh, Mojtaba and Kahani, Nafiseh and Briand, Lionel C.},
  journal={IEEE Transactions on Software Engineering}, 
  title={Scalable and Accurate Test Case Prioritization in Continuous Integration Contexts}, 
  year={2023},
  volume={49},
  number={4},
  pages={1615-1639},
  abstract={Continuous Integration (CI) requires efficient regression testing to ensure software quality without significantly delaying its CI builds. This warrants the need for techniques to reduce regression testing time, such as Test Case Prioritization (TCP) techniques that prioritize the execution of test cases to detect faults as early as possible. Many recent TCP studies employ various Machine Learning (ML) techniques to deal with the dynamic and complex nature of CI. However, most of them use a limited number of features for training ML models and evaluate the models on subjects for which the application of TCP makes little practical sense, due to their small regression testing time and low number of failed builds. In this work, we first define, at a conceptual level, a data model that captures data sources and their relations in a typical CI environment. Second, based on this data model, we define a comprehensive set of features that covers all features previously used by related studies. Third, we develop methods and tools to collect the defined features for 25 open-source software systems with enough failed builds and whose regression testing takes at least five minutes. Fourth, relying on the collected dataset containing a comprehensive feature set, we answer four research questions concerning data collection time, the effectiveness of ML-based TCP, the impact of the features on effectiveness, the decay of ML-based TCP models over time, and the trade-off between data collection time and the effectiveness of ML-based TCP techniques.},
  keywords={Feature extraction;Codes;Testing;History;Training;Data collection;Computational modeling;Machine learning;software testing;test case prioritization;test case selection;continuous integration},
  doi={10.1109/TSE.2022.3184842},
  ISSN={1939-3520},
  month={April},}

@INPROCEEDINGS{5254116,
  author={Miller, Anne and Kumar, Balaji and Singhal, Anukul},
  booktitle={2009 33rd Annual IEEE International Computer Software and Applications Conference}, 
  title={Photon: A Domain-Specific Language for Testing Converged Applications}, 
  year={2009},
  volume={2},
  number={},
  pages={269-274},
  abstract={Automated testing of converged applications can be complex, as it is rare for a single testing tool to provide a single solution for all access points which a given application supports. As such, testing teams often create customized testing frameworks, which integrate several different testing tools, and a myriad of programming languages and scripting tools. When an applicationpsilas unique set of access points changes, or a new testing tool comes to market which offers a competitive advantage over existing test tools, the cost of updating these customized frameworks can be difficult to justify. This paper provides a solution to this problem by introducing ldquoPhotonese,rdquo a domain-specific language which testers can use to compose automation scripts which are independent of the test tool used for automation. In this way, the tester creates reusable testing assets in a framework which is reusable across multiple projects.},
  keywords={Domain specific languages;Automatic testing;Automation;Software testing;Life testing;Telephony;Books;Application software;Displays;Computer applications;Software quality;Software reusability;Software testing},
  doi={10.1109/COMPSAC.2009.143},
  ISSN={0730-3157},
  month={July},}

@INPROCEEDINGS{8990249,
  author={Janes, Andrea and Russo, Barbara},
  booktitle={2019 IEEE International Symposium on Software Reliability Engineering Workshops (ISSREW)}, 
  title={Automatic Performance Monitoring and Regression Testing During the Transition from Monolith to Microservices}, 
  year={2019},
  volume={},
  number={},
  pages={163-168},
  abstract={The transition from monolith to microservices poses several challenges, like how to redistribute the features of system over different microservices. During the transition, developers may also redesign or rethink system services significantly, which can have a strong impact on various quality aspects of the resulting system. Thus, the new system may be more or less performing depending on the ability of the developers to design microservices and the capability of the microservice architecture to represent the system. Overall, a transition to microservices may or may not end up with the same or a better performing system. One way to control the migration to microservices is to continuously monitor a system by continuously collecting performance data and feeding the resulting data analysis back in the transition process. In DevOps, such continuous feedback can be exploited to re-tune the development and deployment of system's builds. In this paper, we present PPTAM+, a tool to continuously assess the degradation of a system during a transition to microservices. In an in-production system, the tool can continuously monitor each microservice and provide indications of lost performance and overall degradation. The system is designed to be integrated in a DevOps process. The tool automates the whole process from collecting data for building the reference operational profile to streamline performance data and automatically adapt and regress performance tests on each build based the analysis' feedback obtained from tests of the previous build.},
  keywords={Microservices;DevOps;Testing},
  doi={10.1109/ISSREW.2019.00067},
  ISSN={},
  month={Oct},}

@INPROCEEDINGS{5069041,
  author={Clark, Tony},
  booktitle={2009 ICSE Workshop on Automation of Software Test}, 
  title={Model based functional testing using pattern directed filmstrips}, 
  year={2009},
  volume={},
  number={},
  pages={53-61},
  abstract={Model driven functional system testing generates test scenarios from behavioural and structural models. In order to autmatically generate tests, conditions such as invariants and pre-/post-conditions must be precisely defined. UML provides the Object Constraint Language (OCL) for this purpose; however OCL expressions can become very complex. This paper describes an approach that allows many commonly found OCL patterns to be expressed as snapshot patterns that correspond directly to the information model diagrams. Behaviour is constructed as chains of snapshots, or filmstrips. Snapshots and filmstrips are as expressive as UML behaviour models and OCL but it is argued that they are more accessible and more modular.},
  keywords={System testing;Unified modeling language;Logic testing;Software testing;Context modeling;Software engineering;Test pattern generators;Process design;Concrete;Graphical user interfaces},
  doi={10.1109/IWAST.2009.5069041},
  ISSN={},
  month={May},}

@INPROCEEDINGS{6976605,
  author={Jiménez, Miguel A. and Gómez, Ángela Villota and Villegas, Norha M. and Tamura, Gabriel and Duchien, Laurence},
  booktitle={2014 IEEE 8th International Symposium on the Maintenance and Evolution of Service-Oriented and Cloud-Based Systems}, 
  title={A Framework for Automated and Composable Testing of Component-Based Services}, 
  year={2014},
  volume={},
  number={},
  pages={1-10},
  abstract={The vision of service-oriented computing has been largely developed on the fundamental principle of building systems by composing and orchestrating services in their control flow. Nowadays, software development is notably influenced by service-oriented architectures (SOAs), in which the quality of software systems is determined by the quality of the involved services and their actual composition. Despite the efforts on improving their individual quality, adding or replacing services in an evolving system can introduce failures, thus compromising the satisfaction of the system's functional and extra-functional requirements. These failures erode the trust in the SOA vision. Thus, a key issue for the industrial adoption of SOA is providing service providers, integrators, and consumers the means to build confidence that services behave according to the contracted quality conditions. In this paper we present a first version of PA SCA NI, a framework for specifying and executing test specifications for service-oriented systems. From a test specification, PA SCA NI generates a configuration of testing services compliant with the Service Component Architecture (SCA) specification, which can be composed to integrate different testing strategies, being these tests traceable in an automated way. Our evaluation results show the applicability of the framework and a substantial gain in the tester's effort for developing tests.},
  keywords={Testing;Service-oriented architecture;Computer architecture;DSL;Software systems;Software service testing;SOA testing;composable tests;SCA testing},
  doi={10.1109/MESOCA.2014.9},
  ISSN={2326-6937},
  month={Sep.},}

@INPROCEEDINGS{10063766,
  author={Maldonado, Sergio David Romero and García, José Joaquín Bocanegra},
  booktitle={2022 Third International Conference on Information Systems and Software Technologies (ICI2ST)}, 
  title={Towards a Domain-Specific Language for Provisioning Multiple Cloud Testing Environments for Mobile Applications}, 
  year={2022},
  volume={},
  number={},
  pages={178-184},
  abstract={Towards a Domain-Specific LanguagTowards a Domain-Specific Languag Provisioning testing environments for mobile applications is one of the most significant challenges within the software industry. Due to this high complexity that exists when provisioning test environments within the multiple available cloud platforms, it is necessary to make a significant investment in human resources, like time and effort for the implementation and execution of testing. There is additional complexity: testing software in a single environment is no longer sufficient. Today's mobile industry is constantly growing, and execution environments tend to be always different; the hardware configuration is usually different and sometimes exceeds the software barrier. It is challenging to execute testing on each of the existing devices, as this requires a long task of human intervention. Today some platforms provide testing services in different environments. However, not all providers have the complete set of environments that one would like to have, and specific knowledge is mandatory for using each available tool. It is a task that requires expertise and time. This work seeks to mitigate the impact on time and the learning curve through a high-level tool developed using a model-oriented approach, thus reducing the time needed for setting up each required platform for organizations. As a solution, we propose a Domain-Specific Language for provisioning multiple cloud testing environments for mobile applications. The configuration of the environment is done with the Domain Specific Language to make the usage easier by the final user. The necessary code is generated through transformations to set up an environment on cloud platforms such as Amazon Web Services (AWS) and Google Cloud Platform (GCP). This usage of this platform results in fewer code lines written and less time learning about the specific knowledge for each platform.},
  keywords={Industries;Codes;Web services;Software;Mobile applications;Complexity theory;Task analysis;DSL;testing;DevOPS;cloud;mobile},
  doi={10.1109/ICI2ST57350.2022.00033},
  ISSN={},
  month={Nov},}
  
@INPROCEEDINGS{7005185,
  author={Peltola, Jukka and Sierla, Seppo and Vyatkin, Valeriy},
  booktitle={Proceedings of the 2014 IEEE Emerging Technology and Factory Automation (ETFA)}, 
  title={Adapting Keyword driven test automation framework to IEC 61131-3 industrial control applications using PLCopen XML}, 
  year={2014},
  volume={},
  number={},
  pages={1-8},
  abstract={Factory Acceptance Testing should involve customer's experts and knowledge in defining, reading and validating tests, while keeping labor costs at moderate level. This involvement requires a testing approach, which hides implementation details and emphasizes domain terminology. Keyword driven testing is seen a viable test automation solution to reduce cost and enable customer involvement in acceptance testing. We propose an approach for adaptation of Keyword driven testing framework to IEC 61131-3 industrial process control applications. It utilizes importing of application elements, presented with PLCopen XML, and transforming them to proxy objects to be used as variables in test code, with domain specific names. Benefits include simplification of test and keyword specifications and hiding of implementation details from testers.},
  keywords={Testing;Libraries;XML;Automation;Process control;IEC standards;Radio frequency;Industrial Process Control System;IEC 61131-3;PLCopen XML;Factory Acceptance Testing;Test Automation;Keyword Driven Testing;Test Framework},
  doi={10.1109/ETFA.2014.7005185},
  ISSN={1946-0759},
  month={Sep.},}

@INPROCEEDINGS{5770632,
  author={Navarro, Pedro Luis Mateo and Pérez, Gregorio Martínez and Ruiz, Diego Sevilla},
  booktitle={2011 Fourth IEEE International Conference on Software Testing, Verification and Validation}, 
  title={Towards Software Quality and User Satisfaction through User Interfaces}, 
  year={2011},
  volume={},
  number={},
  pages={415-418},
  abstract={With this PhD we expect to provide the community and the industry with a solid basis for the development, integration, and deployment of software testing tools. As a solid basis we mean, on one hand, a set of guidelines, recommendations, and clues to better comprehend, analyze, and perform software testing processes, and on the other hand, a set of robust software frameworks that serve as a starting point for the development of future testing tools.},
  keywords={Graphical user interfaces;Software testing;Usability;Open source software;Computer architecture;software testing;GUI testing;automatic test case generation;usability evaluation;user experience evaluation;GUI-data verification},
  doi={10.1109/ICST.2011.13},
  ISSN={2159-4848},
  month={March},}

@INPROCEEDINGS{4221614,
  author={Bertolino, Antonia},
  booktitle={Future of Software Engineering (FOSE '07)}, 
  title={Software Testing Research: Achievements, Challenges, Dreams}, 
  year={2007},
  volume={},
  number={},
  pages={85-103},
  abstract={Software engineering comprehends several disciplines devoted to prevent and remedy malfunctions and to warrant adequate behaviour. Testing, the subject of this paper, is a widespread validation approach in industry, but it is still largely ad hoc, expensive, and unpredictably effective. Indeed, software testing is a broad term encompassing a variety of activities along the development cycle and beyond, aimed at different goals. Hence, software testing research faces a collection of challenges. A consistent roadmap of the most relevant challenges to be addressed is here proposed. In it, the starting point is constituted by some important past achievements, while the destination consists of four identified goals to which research ultimately tends, but which remain as unreachable as dreams. The routes from the achievements to the dreams are paved by the outstanding research challenges, which are discussed in the paper along with interesting ongoing work.},
  keywords={Software testing;Software engineering;Laboratories;Software systems;Software quality;Councils;Computer industry;Quality assurance;Feedback;State estimation},
  doi={10.1109/FOSE.2007.25},
  ISSN={},
  month={May},}

@INPROCEEDINGS{10846301,
  author={Luo, Li and Tie, Junbo and Zhang, Ying and He, Hongjun and Pan, Guoteng and Zhou, Li},
  booktitle={2024 6th International Conference on Circuits and Systems (ICCS)}, 
  title={An Agile Verification for Multi-core Processor Off-chip Memory System}, 
  year={2024},
  volume={},
  number={},
  pages={268-273},
  abstract={Multi-core processors utilize off-chip DDR memory to provide high-bandwidth and large-capacity memory access. The off-chip memory system is crucial to processor chip research and development. Seventy percent of the research and development cycle is the verification time, because DDR memory design parameters and working modes are diverse, the verification is time-consuming and resource-consuming, and its verification often becomes the bottleneck of the entire research and development cycle. This paper presents an agile verification platform for off-chip memory systems for multi-core processors and introduces key technologies for improving the efficiency of agile verification. The proposed verification platform includes high-quality test programs and an optimized verification procedure. The test program uses directed and random tests based on prior knowledge and NSA (The Negative Selection Algorithm, NSA), respectively, and three acceleration strategies are designed for the initialization, debugging, and running phase of the verification procedure. To validate the effectiveness of the self-designed 16-core processor, the RTL code of an actual off-chip memory system is used as the verification object. The experimental results demonstrate that when the random test program case is reduced by 40%, there is a 29% improvement in the functional coverage and a 3 to 20 times improvement in verification efficiency. Additionally, a total of 26 design bugs are discovered across four categories. The verification results clearly indicate that the proposed method enhances verification efficiency and is ideal for full-chip functional test and regression test.},
  keywords={Machine learning algorithms;Codes;Multicore processing;Computer bugs;Memory management;Life estimation;Machine learning;Debugging;Research and development;Optimization;agile verification;SoC design;DDR memory;NSA (The Negative Selection Algorithm;NSA)},
  doi={10.1109/ICCS62517.2024.10846301},
  ISSN={},
  month={Sep.},}

@BOOK{9100404,
  author={Takanen, Ari and Demott, Jared and Miller, Charles and Kettunen, Atte},
  booktitle={Fuzzing for Software Security Testing and Quality Assurance, Second Edition},
  year={2018},
  volume={},
  number={},
  pages={},
  abstract={This newly revised and expanded second edition of the popular Artech House title, Fuzzing for Software Security Testing and Quality Assurance, provides practical and professional guidance on how and why to integrate fuzzing into the software development lifecycle. This edition introduces fuzzing as a process, goes through commercial tools, and explains what the customer requirements are for fuzzing. The advancement of evolutionary fuzzing tools, including American Fuzzy Lop (AFL) and the emerging full fuzz test automation systems are explored in this edition. Traditional software programmers and testers will learn how to make fuzzing a standard practice that integrates seamlessly with all development activities. It surveys all popular commercial fuzzing tools and explains how to select the right one for software development projects. This book is a powerful new tool to build secure, high-quality software taking a weapon from the malicious hacker’s arsenal. This practical resource helps engineers find and patch flaws in software before harmful viruses, worms, and Trojans can use these vulnerabilities to rampage systems. The book shows how to make fuzzing a standard practice that integrates seamlessly with all development activities.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Artech},
  isbn={9781630815196},
  url={https://ieeexplore.ieee.org/document/9100404},}
  
@INPROCEEDINGS{6470771,
  author={Mayrhofer, Dieter and Huemer, Christian},
  booktitle={2012 IEEE 14th International Conference on Commerce and Enterprise Computing}, 
  title={REA-DSL: Business Model Driven Data-Engineering}, 
  year={2012},
  volume={},
  number={},
  pages={9-16},
  abstract={An accounting information system (AIS) manages data about a company's financial and economic status. The contribution of this paper is closing the gap between the languages used by business domain experts and IT-experts in analyzing the relevant data. A well accepted approach for an accountability infrastructure is the Resource-Event-Agent (REA) ontology. Although REA has been based on well-established concepts of the accounting theory, its representation has not been intuitive to domain experts. In previous work, we developed the REA-DSL, a dedicated and easy-to-understand graphical domain specific modeling language for the REA ontology. Evidently, a model-driven approach requires to transform the REA-DSL artifacts to code. In this paper we present the transformation of the REA-DSL to a relational database for AIS. This approach offers the advantage that a domain expert verifies the relevant data in an "accounting language", whereas the IT expert is able to work with traditional data base structures.},
  keywords={Economics;Marine animals;Business;Ontologies;Unified modeling language;Marketing and sales;Analytical models;REA;domain-specific language;business models;relational schema},
  doi={10.1109/CEC.2012.12},
  ISSN={2378-1971},
  month={Sep.},}

@ARTICLE{8440671,
  author={Mirza, Aamir Mehmood and Khan, Muhammad Naeem Ahmed},
  journal={IEEE Access}, 
  title={An Automated Functional Testing Framework for Context-Aware Applications}, 
  year={2018},
  volume={6},
  number={},
  pages={46568-46583},
  abstract={In the modern era of mobile computing, context-aware computing is an emerging paradigm due to its widespread applications. Context-aware applications are gaining increasing popularity in our daily lives since these applications can determine and react according to the situational context and help users to enhance usability experience. However, testing these applications is not straightforward since it poses several challenges, such as generating test data, designing context-coupled test cases, and so on. However, the testing process can be automated to a greater extent by employing model-based testing technique for context-aware applications. To achieve this goal, it is necessary to automate model transformation, test data generation, and test case execution processes. In this paper, we propose an approach for behavior modeling of context-aware application by extending the UML activity diagram. We also propose an automated model transformation approach to transform the development model, i.e., extended UML activity diagram into the testing model in the form of function nets. The objective of this paper is to automate the context-coupled test case generation and execution. We propose a functional testing framework for automated execution of keyword-based test cases. Our functional testing framework can reduce the testing time and cost, thus enabling the test engineers to execute more testing cycles to attain a higher degree of test coverage.},
  keywords={Unified modeling language;Testing;Petri nets;Context-aware services;Sensors;Context modeling;Context-aware applications;model based testing;function net;petri net;model transformation},
  doi={10.1109/ACCESS.2018.2865213},
  ISSN={2169-3536},
  month={},}

@INPROCEEDINGS{10554858,
  author={Duque-Torres, Alejandra},
  booktitle={2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings (ICSE-Companion)}, 
  title={Selecting and Constraining Metamorphic Relations}, 
  year={2024},
  volume={},
  number={},
  pages={212-216},
  abstract={Software testing is a critical aspect of ensuring the reliability and quality of software systems. However, it often poses challenges, particularly in determining the expected output of a System Under Test (SUT) for a given set of inputs, a problem commonly referred to as the test oracle problem. Metamorphic Testing (MT) offers a promising solution to the test oracle problem by examining the relations between input-output pairs in consecutive executions of the SUT. These relations, referred to as Metamorphic Relations (MRs), define the expected changes in the output when specific changes are made to the input. Our research is focused on developing methods and tools to assist testers in the selection of MRs, the definition of constraints, and providing explanations for MR outcomes. The research is divided in three parts. The first part focuses on MR collection and description, entailing the creation of a comprehensive repository of MRs from various sources. A standardised MR representation is devised to promote machine-readability and wide-ranging applicability. The second part introduces MetraTrimmer, a test-data-driven approach for systematically selecting and constraining MRs. This approach acknowledges that MRs may not be universally applicable to all test data space. The final part, evaluation and validation, encompasses empirical studies aimed at assessing the effectiveness of the developed methods and validating their suitability for real-world regression testing scenarios. Through this research, we aim to advance the automation of MR generation, enhance the understanding of MR violations, and facilitate their effective application in regression testing.},
  keywords={Software testing;Automation;Software systems;Software reliability;Data mining;Software engineering;Test Oracle;Metamorphic Testing;Metamorphic Relations;Test Data;Pattern Mining},
  doi={10.1145/3639478.3639781},
  ISSN={2574-1934},
  month={April},}

@INPROCEEDINGS{4293629,
  author={Kitiyakara, Narti and Graves, Joseph},
  booktitle={Agile 2007 (AGILE 2007)}, 
  title={Growing a Build Management System from Seed}, 
  year={2007},
  volume={},
  number={},
  pages={401-407},
  abstract={This paper describes the authors' experiences creating a full build management system from a simple version control system. We will explore how the XP values of simplicity, feedback, communication, courage and respect play into making a system that provides the developers, testers and customer with excellent value showing how various XP principles (like baby steps and mutual benefit) come into play. We will also demonstrate how we are able to remain true to our XP values while still achieving ISO 9001- 2001 and CMM Level II certifications. Finally, we compare the build management system with other systems we have encountered that were not developed in accordance with XP values.},
  keywords={System testing;Communication system control;Control systems;Feedback;Pediatrics;ISO standards;Coordinate measuring machines;Certification;Control system synthesis;Writing},
  doi={10.1109/AGILE.2007.32},
  ISSN={},
  month={Aug},}

@BOOK{10745290,
  author={Crocker, Nathan},
  booktitle={AI-Powered Developer: Build software with ChatGPT and Copilot},
  year={2024},
  volume={},
  number={},
  pages={},
  abstract={Use groundbreaking generative AI tools to increase your productivity, efficiency, and code quality. AI coding tools like ChatGPT and GitHub Copilot are changing the way we write code and build software. AI-Powered Developer reveals the practical best practices you need to deliver reliable results with AI. It cuts through the hype, showcasing real-world examples of how these tools ease and enhance your everyday tasks, and make you more creative. In AI-Powered Developer you’ll discover how to get the most out of AI:  Harness AI to help you design and plan software Use AI for code generation, debugging, and documentation Improve your code quality assessments with the help of AI Articulate complex problems to prompt an AI solution Develop a continuous learning mindset that keeps you up to date Adapt your development skills to almost any language  AI coding tools give you a smart and reliable junior developer that’s fast and keen to help out with your every task and query. AI-Powered Developer helps you put your new assistant to work. You’ll learn to use AI for everything from writing boilerplate, to testing and quality assessment, managing infrastructure, delivering security, and even assisting with software design.},
  keywords={Copilot;test;ChatGPT;solutions;software design;adaptive;CodeWhisperer;fast;code generation;debugging;documentation;quality assessment;hands-on},
  doi={},
  ISSN={},
  publisher={Manning},
  isbn={9781633437616},
url={https://ieeexplore.ieee.org/document/10745290},}

@BOOK{9106105,
  author={Demott, Jared and Miller, Charles and Takanen, Ari},
  booktitle={Fuzzing for Software Security Testing and Quality Assurance},
  year={2008},
  volume={},
  number={},
  pages={},
  abstract={Learn the code cracker's malicious mindset, so you can find worn-size holes in the software you are designing, testing, and building. Fuzzing for Software Security Testing and Quality Assurance takes a weapon from the black-hat arsenal to give you a powerful new tool to build secure, high-quality software. This practical resource helps you add extra protection without adding expense or time to already tight schedules and budgets. The book shows you how to make fuzzing a standard practice that integrates seamlessly with all development activities. This comprehensive reference goes through each phase of software development and points out where testing and auditing can tighten security. It surveys all popular commercial fuzzing tools and explains how to select the right one for a software development project. The book also identifies those cases where commercial tools fall short and when there is a need for building your own fuzzing tools.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Artech},
  isbn={9781596932159},
  url={https://ieeexplore.ieee.org/document/9106105},}
  
@INPROCEEDINGS{6233406,
  author={Landhäußer, 1Mathias and Genaid, Adrian},
  booktitle={2012 Third International Workshop on Recommendation Systems for Software Engineering (RSSE)}, 
  title={Connecting User Stories and code for test development}, 
  year={2012},
  volume={},
  number={},
  pages={33-37},
  abstract={User Stories are short feature descriptions from the user's point of view. Functional tests ensure that the feature described by a User Story is fully implemented. We present a tool that builds an ontology for code and links completed User Stories in natural language with the related code artifacts. The ontology also contains links to API components that were used to implement the functional tests. Preliminary results show that these links can be used to recommend reusable test steps for new User Stories.},
  keywords={Ontologies;Natural languages;Software;Data structures;Boolean functions;Compounds;Testing;code mining;functional testing;reasoning;traceability;ontology},
  doi={10.1109/RSSE.2012.6233406},
  ISSN={2327-0942},
  month={June},}

@INPROCEEDINGS{8665630,
  author={Crapo, Andrew W. and Moitra, Abha},
  booktitle={2019 IEEE 13th International Conference on Semantic Computing (ICSC)}, 
  title={Using OWL Ontologies as a Domain-Specific Language for Capturing Requirements for Formal Analysis and Test Case Generation}, 
  year={2019},
  volume={},
  number={},
  pages={361-366},
  abstract={Our experience at GE Research suggests that the use of a controlled-English grammar and a rich authoring environment can greatly facilitate subject matter experts' ability to understand, create, and collaboratively employ models. A domain ontology is an ideal foundation for many advanced capabilities. An example is extending our controlled-English grammar and authoring environment for OWL model generation to allow the capture of high-level requirements, assumptions, and assertions, enabling requirement engineers to create models of system capability and behavior amenable to formal methods analysis to detect incompleteness, conflict, and a variety of other issues. The same domain models and formal requirements can be used to automatically generate test cases and test procedures. Automated test generation represents a huge reduction in the time and effort required to create and validate critical software. In this paper we illustrate how ontologies enable the ASSERT™ tool suite to support the above capabilities through a small grounding use case.},
  keywords={Semantics;Ontologies;Valves;Grammar;OWL;Software;Tools;ontology;requirements;formal methods;automated test generation},
  doi={10.1109/ICOSC.2019.8665630},
  ISSN={2325-6516},
  month={Jan},}

@INPROCEEDINGS{10132273,
  author={Schneid, Konrad and Thöne, Sebastian and Kuchen, Herbert},
  booktitle={2023 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)}, 
  title={Modification-Impact based Test Prioritization for Process-Driven Applications}, 
  year={2023},
  volume={},
  number={},
  pages={365-372},
  abstract={Automated regression tests are essential, especially when practicing continuous software engineering techniques. In the case of Process-Driven Applications (PDA), the tests must consider executable process models typically using the BPMN notation and external software services as crucial parts of the application. The complexity of processes is constantly increasing and hence also the need for more tests to ensure the correctness of the PDA. Running all implemented tests in an arbitrary order is time-consuming and, causes high costs. It is more promising in terms of early fault detection to start with those tests that are more likely affected by the changes which have been implemented for the upcoming release under test. For this purpose, we have identified the different types of modifications at the control flow and activity level and evaluated their impact for the intended test prioritization. Furthermore, our concept considers the impact of adaptions at the data-flow level. The approach has been implemented prototypically for the Camunda BPM platform. Experimental results through various case studies proved an earlier detection of errors compared to traditional process coverage techniques.},
  keywords={Software testing;Costs;Fault detection;Conferences;Software;Complexity theory;Software engineering;Test Prioritization;Process-Driven Applications;BPMN},
  doi={10.1109/ICSTW58534.2023.00068},
  ISSN={2159-4848},
  month={April},}

@INPROCEEDINGS{8425200,
  author={Petruzza, Steve and Treichler, Sean and Pascucci, Valerio and Bremer, Peer-Timo},
  booktitle={2018 IEEE International Parallel and Distributed Processing Symposium (IPDPS)}, 
  title={BabelFlow: An Embedded Domain Specific Language for Parallel Analysis and Visualization}, 
  year={2018},
  volume={},
  number={},
  pages={463-473},
  abstract={The rapid growth in simulation data requires large-scale parallel implementations of scientific analysis and visualization algorithms, both to produce results within an acceptable timeframe and to enable in situ deployment. However, efficient and scalable implementations, especially of more complex analysis approaches, require not only advanced algorithms, but also an in-depth knowledge of the underlying runtime. Furthermore, different machine configurations and different applications may favor different runtimes, i.e., MPI vs Charm++ vs Legion, etc., and different hardware architectures. This diversity makes developing and maintaining a broadly applicable analysis software infrastructure challenging. We address some of these problems by explicitly separating the implementation of individual tasks of an algorithm from the dataflow connecting these tasks. In particular, we present an embedded domain specific language (EDSL) to describe algorithms using a new task graph abstraction. This task graph is then executed on top of one of several available runtimes (MPI, Charm++, Legion) using a thin layer of library calls. We demonstrate the flexibility and performance of this approach using three different large scale analysis and visualization use cases, i.e., topological analysis, rendering and compositing dataflow, and image registration of large microscopy scans. Despite the unavoidable overheads of a generic solution, our approach demonstrates performance portability at scale, and, in some cases, outperforms hand-optimized implementations.},
  keywords={Task analysis;Runtime;Software algorithms;Payloads;Software;Libraries;Rendering (computer graphics);Embedded DSL;user productivity;in situ analytics;Simulation runtime systems;programming models},
  doi={10.1109/IPDPS.2018.00056},
  ISSN={1530-2075},
  month={May},}

@INPROCEEDINGS{8754465,
  author={Tuglular, Tugkan and Şensülün, Sercan},
  booktitle={2019 IEEE 43rd Annual Computer Software and Applications Conference (COMPSAC)}, 
  title={SPL-AT Gherkin: A Gherkin Extension for Feature Oriented Testing of Software Product Lines}, 
  year={2019},
  volume={2},
  number={},
  pages={344-349},
  abstract={As cloud platforms turn into software product lines (SPLs), testing products composed of customer selected features becomes more and more important. In this paper, we propose a feature-oriented testing approach for platform-based SPLs through a novel extension to Gherkin called SPL-AT Gherkin and a novel automatic test method generation technique, which utilizes TestNG framework. We demonstrate the applicability of the proposed approach by a case study.},
  keywords={Testing;Mobile applications;Software product lines;Web pages;Software;Feature extraction;Gold;software product lines;feature-oriented testing;acceptance testing;Gherkin;automatic test generation},
  doi={10.1109/COMPSAC.2019.10230},
  ISSN={0730-3157},
  month={Jul},}

@INPROCEEDINGS{10174023,
  author={Lu, Chengjie and Yue, Tao and Ali, Shaukat},
  booktitle={2023 IEEE/ACM 20th International Conference on Mining Software Repositories (MSR)}, 
  title={DeepScenario: An Open Driving Scenario Dataset for Autonomous Driving System Testing}, 
  year={2023},
  volume={},
  number={},
  pages={52-56},
  abstract={With the rapid development of autonomous driving systems (ADSs), testing ADSs under various environmental conditions has become a key method to ensure the successful deployment of ADS in the real world. However, it is impossible to test all the scenarios due to the inherent complexity and uncertainty of ADSs and the driving tasks. Further, testing of ADSs is expensive regarding time and computational resources. Therefore, a large-scale driving scenario dataset consisting of various driving conditions is needed. To this end, we present an open driving scenario dataset DeepScenario, containing over 30K executable driving scenarios, which are collected by 2880 test executions of three driving scenario generation strategies. Each scenario in the dataset is labeled with six attributes characterizing test results. We further show the attribute statistics and distribution of driving scenarios. For example, there are 1050 collision scenarios, in 917 scenarios there were collisions with other vehicles, 105 and 28 with pedestrians and static obstacles, respectively. Target users include ADS developers who need to validate their systems under various environmental conditions.},
  keywords={System testing;Uncertainty;Pedestrians;Software;Complexity theory;Data mining;Task analysis;autonomous driving system testing;driving scenario;open source;dataset},
  doi={10.1109/MSR59073.2023.00020},
  ISSN={2574-3864},
  month={May},}

@INPROCEEDINGS{9796462,
  author={Elsner, Daniel and Wuersching, Roland and Schnappinger, Markus and Pretschner, Alexander},
  booktitle={2022 IEEE/ACM International Conference on Automation of Software Test (AST)}, 
  title={Probe-based Syscall Tracing for Efficient and Practical File-level Test Traces}, 
  year={2022},
  volume={},
  number={},
  pages={126-137},
  abstract={Efficiently collecting per-test execution traces is a common prerequisite of dynamic regression test optimization techniques. However, as these test traces are typically recorded through language-specific code instrumentation, non-code artifacts and multi-language source code are usually not included. In contrast, more complete test traces can be obtained by instrumenting operating system calls and thereby tracing all accessed files during a test’s execution. Yet, existing test optimization techniques that use syscall tracing are impractical as they either modify the Linux kernel or operate in user space, thus raising transferability, performance, and security concerns. Recent advances in operating system development provide versatile, lightweight, and safe kernel instrumentation frameworks: They allow to trace syscalls by instrumenting probes in the operating system kernel. Probe-based Syscall Tracing (ProST), our novel technique, harnesses this potential to collect file-level test traces that go beyond language boundaries and consider non-code artifacts. To evaluate ProST’s efficiency and the completeness of obtained test traces, we perform an empirical study on 25 multi-language open-source software projects and compare our approach to existing language-specific instrumentation techniques. Our results show that most studied projects use source files from multiple languages (22/25) or non-code artifacts during testing (22/25) that are missed by language-specific techniques. With the low execution time overhead of 4.6% compared to non-instrumented test execution, ProST is more efficient than language-specific instrumentation. Furthermore, it collects on average 89% more files on top of those collected by language-specific techniques. Consequently, ProST paves the way for efficiently extracting valuable information through dynamic analysis to better understand and optimize testing in multi-language software systems. CCS CONCEPTS • Software and its engineering → Software testing and debugging.},
  keywords={Software testing;Codes;Runtime;Instruments;Linux;Software systems;Security;software testing;dynamic program analysis;multi-language software;non-code artifacts},
  doi={10.1145/3524481.3527239},
  ISSN={},
  month={May},}
  
@INPROCEEDINGS{7515454,
  author={Enoiu, Eduard P. and Cauevic, Adnan and Sundmark, Daniel and Pettersson, Paul},
  booktitle={2016 IEEE International Conference on Software Testing, Verification and Validation (ICST)}, 
  title={A Controlled Experiment in Testing of Safety-Critical Embedded Software}, 
  year={2016},
  volume={},
  number={},
  pages={1-11},
  abstract={In engineering of safety critical systems, regulatory standards often put requirements on both traceable specification-based testing, and structural coverage on program units. Automated test generation techniques can be used to generate inputs to cover the structural aspects of a program. However, there is no conclusive evidence on how automated test generation compares to manual test design, or how testing based on the program implementation relates to specification-based testing. In this paper, we investigate specification -- and implementation-based testing of embedded software written in the IEC 61131-3 language, a programming standard used in many embedded safety critical software systems. Further, we measure the efficiency and effectiveness in terms of fault detection. For this purpose, a controlled experiment was conducted, comparing tests created by a total of twenty-three software engineering master students. The participants worked individually on manually designing and automatically generating tests for two IEC 61131-3 programs. Tests created by the participants in the experiment were collected and analyzed in terms of mutation score, decision coverage, number of tests, and testing duration. We found that, when compared to implementation-based testing, specification-based testing yields significantly more effective tests in terms of the number of faults detected. Specifically, specification-based tests more effectively detect comparison and value replacement type of faults, compared to implementation-based tests. On the other hand, implementation-based automated test generation leads to fewer tests (up to 85% improvement) created in shorter time than the ones manually created based on the specification.},
  keywords={Testing;IEC Standards;Manuals;Embedded software;Safety;automated test generation;controlled experiment;specification-based testing;manual testing;embedded software;safety-critical systems},
  doi={10.1109/ICST.2016.15},
  ISSN={},
  month={April},}

@INPROCEEDINGS{7437001,
  author={Hafidhoh, Nisa'ul and Liem, Inggriani and Azizah, Fazat Nur},
  booktitle={2015 International Conference on Data and Software Engineering (ICoDSE)}, 
  title={Source code generator for automating business rule implementation}, 
  year={2015},
  volume={},
  number={},
  pages={219-224},
  abstract={Business rules can be implemented on business processes, business behavior, people, or software in an organization. Aligned with software development, business rules are captured from requirement elicitation and analysis, then designed and implemented in the software. The changes of business environment may affect business rules. The changes of the business rules may bring impact in the software, so that the software needs to be redeveloped. In this paper, we present a source code generator to automate business rule implementation using business rule approach. We propose a Domain Specific Language (DSL) of business rule to help expressing business rules in a business-friendly language. We also develop a business rule generator to generate source codes based on a DSL script for expressing the business rules. The proposed solution has been tested in two case studies. It is shown that the generator can help the implementation of the business rules in source codes and that the generated code can be used in business applications.},
  keywords={DSL;Natural languages;Software;Generators;Organizations;Grammar;business rule;implementation;source code generator;DSL},
  doi={10.1109/ICODSE.2015.7437001},
  ISSN={},
  month={Nov},}

@ARTICLE{841114,
  author={Pietro, P.S. and Morzenti, A. and Morasca, S.},
  journal={IEEE Transactions on Software Engineering}, 
  title={Generation of execution sequences for modular time critical systems}, 
  year={2000},
  volume={26},
  number={2},
  pages={128-149},
  abstract={We define methods for generating execution sequences for time-critical systems based on their modularized formal specification. An execution sequence represents a behavior of a time critical system and can be used, before the final system is built, to validate the system specification against the user requirements (specification validation) and, after the final system is built, to verify whether the implementation satisfies the specification (functional testing). Our techniques generate execution sequences in the large, in that we focus on the connections among the abstract interfaces of the modules composing a modular specification. Execution sequences in the large are obtained by composing execution sequences in the small for the individual modules. We abstract from the specification languages used for the individual modules of the system, so our techniques can also be used when the modules composing the system are specified with different formalisms. We consider the cases in which connections give rise to either circular or noncircular dependencies among specification modules. We show that execution sequence generation can be carried out successfully under rather broad conditions and we define procedures for efficient construction of execution sequences. These procedures can be taken as the basis for the implementation of (semi)automated tools that provide substantial support to the activity of specification validation and functional testing for industrially-sized time critical systems.},
  keywords={System testing;Software testing;Animation;Formal specifications;Sequential analysis;Computer Society;Time factors;Construction industry;Electrical equipment industry;Software prototyping},
  doi={10.1109/32.841114},
  ISSN={1939-3520},
  month={Feb},}

@INPROCEEDINGS{8904507,
  author={Losvik, Daneil Steen and Rutle, Adrian},
  booktitle={2019 ACM/IEEE 22nd International Conference on Model Driven Engineering Languages and Systems Companion (MODELS-C)}, 
  title={A Domain-Specific Language for the Development of Heterogeneous Multi-robot Systems}, 
  year={2019},
  volume={},
  number={},
  pages={549-558},
  abstract={In this paper, we explore how model-driven software engineering can be used in the development of heterogeneous multi-robot systems where we have different robots with different capabilities. Multiple robots can achieve more complex tasks that are impossible to achieve for a single robot alone. We propose a framework where simple actions are used as building blocks to define larger tasks that require multiple robots with different capabilities to achieve. We show how task distribution can be performed in such a system and how the robot operating system can be utilized. We also show how a user interface can be used to define multiple different missions for a team of heterogeneous robots without the need for regeneration of code and redeployment on each robot.},
  keywords={model-driven;mdse;domain-specific;dsl;robotics;multi robot;heterogeneous multi-robot system;heterogeneous multi-robot;task definition robotics},
  doi={10.1109/MODELS-C.2019.00085},
  ISSN={},
  month={Sep.},}

@INPROCEEDINGS{10350786,
  author={Gómez-Abajo, Pablo and Cañizares, Pablo C. and Núñez, Alberto and Guerra, Esther and de Lara, Juan},
  booktitle={2023 ACM/IEEE International Conference on Model Driven Engineering Languages and Systems Companion (MODELS-C)}, 
  title={Gotten: A Model-Driven Solution to Engineer Domain-specific Metamorphic Testing Environments}, 
  year={2023},
  volume={},
  number={},
  pages={65-69},
  abstract={Testing is essential for assessing the correctness of software systems. Metamorphic testing (MT) is an approach especially suited when the system under test lacks oracles, or they are expensive to compute. However, creating an MT environment for a specific domain (e.g., cloud simulation, model transformation, machine learning) requires substantial effort. To alleviate these difficulties, we present a model-driven tool that automates the construction of MT environments. Starting from a meta-model with the domain concepts, and a description of the domain execution environment, our tool produces an MT environment featuring comprehensive support for the MT process. This includes the definition of domain-specific metamorphic relations, their evaluation, detailed reporting of the testing results, and the automated search-based generation of follow-up test cases. This paper illustrates the tool on a case-study in the domain of video streaming APIs. A video showcasing the tool is available at https://youtu.be/DeuIW6V4LaQ.},
  keywords={Computational modeling;Machine learning;Streaming media;Software systems;Model driven engineering;Testing;Metamorphic testing;Model-driven engineering;Domain-specific languages;Video streaming APIs},
  doi={10.1109/MODELS-C59198.2023.00021},
  ISSN={},
  month={Oct},}
  
@INPROCEEDINGS{6005491,
  author={Hellmann, Theodore D. and Maurer, Frank},
  booktitle={2011 Agile Conference}, 
  title={Rule-Based Exploratory Testing of Graphical User Interfaces}, 
  year={2011},
  volume={},
  number={},
  pages={107-116},
  abstract={This paper introduces rule-based exploratory testing, an approach to GUI testing that combines aspects of manual exploratory testing with rule-based test automation. This approach uses short, automated rules to increase the bug-detection capability of recorded exploratory test sessions. A preliminary evaluation found that this approach can be used to detect both general and application-specific bugs, but that rules for general bugs are easier to transfer between applications. Also, despite the advantages of keyword-based testing, it interferes with the transfer of rules between applications.},
  keywords={Testing;Graphical user interfaces;Computer bugs;Humans;Manuals;Security;Fires;GUI testing;rule-based testing;exploratory testing},
  doi={10.1109/AGILE.2011.23},
  ISSN={},
  month={Aug},}

@INPROCEEDINGS{7342393,
  author={McGinty, Stephen and Hadad, Daniel and Nappi, Chris and Caquelin, Brian},
  booktitle={2015 IEEE International Test Conference (ITC)}, 
  title={Developing a modern platform for test engineering — Introducing the origen semiconductor developer's kit}, 
  year={2015},
  volume={},
  number={},
  pages={1-10},
  abstract={Many of the tools used today in semiconductor test engineering are single-point solutions that are concerned with the mechanics of translating test IP between domains and formats. There is no cohesive standardized framework to bind them all together; and workflow and application architecture choices are largely left up to the individual engineer. Learning from the state-of-the-art in other software engineering domains, we have developed a modern framework for semiconductor engineering that favors a convention-based approach to application architectures. By following conventions, powerful abstractions can be created to enable truly modular test development within a unified environment for the creation of test patterns, test programs, and all other test collateral. The paper reviews the background and some of the main capabilities of the framework and discusses how it is being used in production today to replace many conventional pattern flows. This is also a formal announcement that Freescale Semiconductor is open-sourcing the Origen Semiconductor Developer's Kit (SDK) to enable future development to be done in collaboration with the global semiconductor engineering community.},
  keywords={IP networks;Companies;Complexity theory;Testing;Industries;Computer architecture;Silicon},
  doi={10.1109/TEST.2015.7342393},
  ISSN={},
  month={Oct},}
  
@INPROCEEDINGS{8411751,
  author={Pröll, Reinhard and Bauer, Bernhard},
  booktitle={2018 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)}, 
  title={A Model-Based Test Case Management Approach for Integrated Sets of Domain-Specific Models}, 
  year={2018},
  volume={},
  number={},
  pages={175-184},
  abstract={Due to rapid improvements in the area of embedded processing hardware, the complexity of developed systems constantly increases. In order to ensure a high quality level of such systems, related quality assurance concepts have to evolve. The introduction of Model-Based Testing (MBT) approaches has shown promising results by automating and abstracting multiple activities of the software testing life cycle. Nevertheless, there is a strong need for approaches supporting scoped test models, i.e. subsets of test cases, reflecting specific test purposes driven by risk-oriented development strategies. Therefore, we developed an integrated and model-based approach supporting test case management, which incorporates the beneficial aspects of abstract development methodologies with predominant research for test case management in non-model-based scenarios. Based on a new model artifact, the integration model, tasks like cross-domain information mapping and the integration of domain-specific KPIs derived by analyses favor the subsequently applied constraint-based mechanism for test case management. Further, a prototypical implementation of these concepts within the Architecture And Analysis Framework (A3F) is elaborated and further evaluated based on representative application scenarios. A comparative view on related work leads to a conclusive statement regarding our future work.},
  keywords={Analytical models;Data models;Context modeling;Software;Software testing;Model-Based Testing;Test Case Management;Test Selection;Test Prioritization;Test Suite Reduction;Test Model Scoping},
  doi={10.1109/ICSTW.2018.00048},
  ISSN={},
  month={April},}

@INPROCEEDINGS{7323087,
  author={Iber, Johannes and Kajtazović, Nermin and Höller, Andrea and Rauter, Tobias and Kreiner, Christian},
  booktitle={2015 3rd International Conference on Model-Driven Engineering and Software Development (MODELSWARD)}, 
  title={Ubtl UML testing profile based testing language}, 
  year={2015},
  volume={},
  number={},
  pages={1-12},
  abstract={The continuous increase of software complexity is one of the major problems associated with the development of today's complex technical systems. In particular, for safety-critical systems, which usually require to be thoroughly verified and validated, managing such a complexity is of high importance. To this end, industry is utilizing Model-Driven Development (MDD) in many aspects of systems engineering, including verification and validation activities. Until now many specifications and standards have been released by the MDD community to support those activities by putting models in focus. The general problem is, however, that applying those specifications is often difficult, since they comprise a broader scope than usually required to solve specific problems. In this paper we propose a domain-specific language (DSL) that allows to specify tests from the UML Testing Profile (UTP). The main contribution is that only particular aspects of UTP are captured, thereby allowing the MDD process to be narrowed to specific needs, such as supporting code generation facilities for certain types of tests or even specific statements in tests. In the end we show the application of the DSL using a simple example within a MDD process, and we report on performance of that process.},
  keywords={Unified modeling language;Testing;Generators;Biological system modeling;DSL;Software;Concrete;UML Testing Profile;UML;Textual Domain-Specific Language;Test Specification Language;Software Testing;Model-Driven Development},
  doi={},
  ISSN={},
  month={Feb},}
  
@INPROCEEDINGS{10148770,
  author={Pinto, Gustavo and Miranda, Breno and Dissanayake, Supun and D'Amorim, Marcelo and Treude, Christoph and Bertolino, Antonia},
  booktitle={2020 IEEE/ACM 17th International Conference on Mining Software Repositories (MSR)}, 
  title={What is the Vocabulary of Flaky Tests?}, 
  year={2020},
  volume={},
  number={},
  pages={492-502},
  abstract={Flaky tests are tests whose outcomes are non-deterministic. Despite the recent research activity on this topic, no effort has been made on understanding the vocabulary of flaky tests. This work proposes to automatically classify tests as flaky or not based on their vocabulary. Static classification of flaky tests is important, for example, to detect the introduction of flaky tests and to search for flaky tests after they are introduced in regression test suites. We evaluated performance of various machine learning algorithms to solve this problem. We constructed a data set of flaky and non-flaky tests by running every test case, in a set of 64k tests, 100 times (6.4 million test executions). We then used machine learning techniques on the resulting data set to predict which tests are flaky from their source code. Based on features, such as counting stemmed tokens extracted from source code identifiers, we achieved an F-measure of 0.95 for the identification of flaky tests. The best prediction performance was obtained when using Random Forest and Support Vector Machines. In terms of the code identifiers that are most strongly associated with test flakiness, we noted that job, action, and services are commonly associated with flaky tests. Overall, our results provides initial yet strong evidence that static detection of flaky tests is effective.},
  keywords={Support vector machines;Vocabulary;Java;Machine learning algorithms;Codes;Source coding;Programming;Test flakiness;Regression testing;Text classification},
  doi={10.1145/3379597.3387482},
  ISSN={2574-3864},
  month={May},}
  
@INPROCEEDINGS{1541840,
  author={Kajko-Mattsson, M. and Meyer, P.},
  booktitle={2005 International Symposium on Empirical Software Engineering, 2005.}, 
  title={Evaluating the acceptor side of EM/sup 3/: release management at SAS}, 
  year={2005},
  volume={},
  number={},
  pages={10 pp.-},
  abstract={Today, there are no detailed standard process models encompassing the overall release management activities. To remedy this, we have created an individual release management process model, called EM/sup 3/: release management. In this paper, we evaluate its acceptor side against an industrial release management process performed at Scandinavian Airline Systems (SAS). We have observed some similarities and differences. Some of the observed differences provide feedback for the improvement and further extension of EM/sup 3/: release management.},
  keywords={Synthetic aperture sonar;Technology management;Control systems;Software maintenance;Laboratories;Software standards;Performance evaluation;Feedback;Software systems;Lead},
  doi={10.1109/ISESE.2005.1541840},
  ISSN={},
  month={Nov},}
  
@INPROCEEDINGS{10669832,
  author={Alshahwan, Nadia and Harman, Mark and Harper, Inna and Marginean, Alexandru and Sengupta, Shubho and Wang, Eddy},
  booktitle={2024 IEEE/ACM 2nd International Workshop on Interpretability, Robustness, and Benchmarking in Neural Software Engineering (InteNSE)}, 
  title={Assured LLM-Based Software Engineering}, 
  year={2024},
  volume={},
  number={},
  pages={7-12},
  abstract={In this paper we address the following question: How can we use Large Language Models (LLMs) to improve code independently of a human, while ensuring that the improved code(1)does not regress the properties of the original code ?(2)improves the original in a verifiable and measurable way ?To address this question, we advocate Assured LLM-Based Software Engineering; a generate-and-test approach, inspired by Genetic Improvement. Assured LLMSE applies a series of semantic filters that discard code that fails to meet these twin guarantees. This overcomes the potential problem of LLM’s propensity to hallucinate. It allows us to generate code using LLMs, independently of any human. The human plays the role only of final code reviewer, as they would do with code generated by other human engineers.This paper is an outline of the content of the keynote by Mark Harman at the International Workshop on Interpretability, Robustness, and Benchmarking in Neural Software Engineering, Monday 15th April 2024, Lisbon, Portugal.},
  keywords={Codes;Filters;Large language models;Conferences;Semantics;Benchmark testing;Genetics;Large Language Models (LLMs);Genetic Improvement (GI);Search Based Software Engineering (SBSE);Llama;CodeLlama;Automated Code Generation},
  doi={},
  ISSN={},
  month={April},}
  
@ARTICLE{8306153,
  author={Bures, Miroslav and Frajtak, Karel and Ahmed, Bestoun S.},
  journal={IEEE Transactions on Reliability}, 
  title={Tapir: Automation Support of Exploratory Testing Using Model Reconstruction of the System Under Test}, 
  year={2018},
  volume={67},
  number={2},
  pages={557-580},
  abstract={For a considerable number of software projects, the creation of effective test cases is hindered by design documentation that is either lacking, incomplete, or obsolete. The exploratory testing approach can serve as a sound method in such situations. However, the efficiency of this testing approach strongly depends on the method, the documentation of explored parts of a system, the organization and distribution of work among individual testers on a team, and the minimization of potential (very probable) duplicities in performed tests. In this paper, we present a framework for replacing and automating a portion of these tasks. A screen-flow-based model of the tested system is incrementally reconstructed during the exploratory testing process by tracking testers' activities. With additional metadata, the model serves for an automated navigation process for a tester. Compared with the exploratory testing approach, which is manually performed in two case studies, the proposed framework allows the testers to explore a greater extent of the tested system and enables greater detection of the defects present in the system. The results show that the time efficiency of the testing process improved with the framework support. This efficiency can be increased by team-based navigational strategies that are implemented within the proposed framework, which is documented by another case study presented in this paper.},
  keywords={Testing;Navigation;Lead;Documentation;Browsers;Web pages;Automation;Functional testing;generation of test cases from model;model reengineering;model-based testing (MBT);system under test (SUT) model;web applications testing},
  doi={10.1109/TR.2018.2799957},
  ISSN={1558-1721},
  month={June},}
  
@INPROCEEDINGS{9172680,
  author={Lawler, Christopher R. and Ridenhour, Forrest L. and Khan, Shaheer A. and Rossomando, Nicholas M. and Rothstein-Dowden, Ansel},
  booktitle={2020 IEEE Aerospace Conference}, 
  title={Blackbird: Object-Oriented Planning, Simulation, and Sequencing Framework Used by Multiple Missions}, 
  year={2020},
  volume={},
  number={},
  pages={1-20},
  abstract={Every JPL flight mission relies on activity planning and sequence generation software to perform operations. Most such tools in use at JPL and elsewhere use attribute-based schemas or domain-specific languages (DSLs) to define activities. This reliance poses user training, software maintenance, performance, and other challenges. To solve this problem for future missions, a new software called Blackbird was developed which allows engineers to specify behavior in standard Java. The new code base has over an order of magnitude fewer lines of code than other JPL planning software, since no DSL or schema interpreter is needed. The use of Java for defining activities also allows mission adapters to debug their code in an integrated development environment, seamlessly call external libraries, and set up truly multi-mission models. These efficiency gains have significantly reduced the amount of development effort required to support the software. This paper discusses Blackbird's design, principles, and use cases. Within a year of its completion, six projects have begun using Blackbird. The Mars 2020 mission is using Blackbird to generate command sequences for cruise and Mars approach. By using multi-mission models, the Mars 2020 cruise adaptation was created in fewer than three months by three engineers at less than half time each. Work has begun to use Blackbird for communications planning during Mars 2020 surface operations. The Psyche mission uses Blackbird to generate its reference mission plans in development. Full simulations with 123,000 activities and 4.7 million resource value changes complete in about one minute. Psyche is also working towards using Blackbird in operations to support integrated activity planning and generate sequences. The InSight project is using Blackbird for mission planning in operations, replacing error-prone manual processes. For the NISAR mission, Blackbird evaluates threats to the commissioning phase timeline. The Europa Lander pre-project used Blackbird to perform a trade study. The ASTERIA mission is automating sequence generation in Blackbird. Going forward, more interested projects are likely to begin using Blackbird, and the capabilities of the core and multi-mission models will keep growing.},
  keywords={Training;Adaptation models;Java;Codes;Object oriented modeling;Planning;DSL},
  doi={10.1109/AERO47225.2020.9172680},
  ISSN={1095-323X},
  month={March},}
  
@INPROCEEDINGS{8369563,
  author={Glock, Thomas and Sillman, Björn and Kobold, Max and Rebmann, Sebatian and Sax, Eric},
  booktitle={2018 Annual IEEE International Systems Conference (SysCon)}, 
  title={Model-based validation and testing of industry 4.0 plants}, 
  year={2018},
  volume={},
  number={},
  pages={1-8},
  abstract={One of the major technical aspects of Industry 4.0 (I4.0) is the decentralized task and function distribution based on a service-oriented approach. This leads to new challenges by using the existing technologies and planning methods of I4.0 industrial plants. The validation of such dynamic service­oriented architectures requires new validation methods to ensure the correct functionality of the I4.0 plant. The interactions of distributed functions and the integration of functions on several devices play an important role in planning, configuration, and validation of such plants. This work presents a new approach of a continuous model-based validation and testing method of I4.0 plant service-oriented architectures. The method allows to reuse existing functional tests of services and to adjust them with the help of the model-based information of a plant architecture model.},
  keywords={Tools;Unified modeling language;Testing;Computer architecture;Sensors;Industries;Planning;Functional Testing of plants;industry 4.0;service-oriented;distributed systems;reuse of tests;validation of I4.0 plant systems},
  doi={10.1109/SYSCON.2018.8369563},
  ISSN={2472-9647},
  month={April},}
  
@INPROCEEDINGS{8877042,
  author={España, Sergio and Bik, Niels and Overbeek, Sietse},
  booktitle={2019 13th International Conference on Research Challenges in Information Science (RCIS)}, 
  title={Model-driven engineering support for social and environmental accounting}, 
  year={2019},
  volume={},
  number={},
  pages={1-12},
  abstract={Social and environmental accounting (SEA) is becoming commonplace in organisations that want to assess and report on their business ethics and sustainability performance. While there is an abundance of methods and standards that define how to perform such practice, tool support is much more limited and does not fully match the needs of organisations. In this paper, we present a model-driven engineering approach to support SEA. We are engineering a domain-specific language that allows creating models of SEA methods and we are implementing a tool that interprets such models at runtime, effectively supporting the accounting, and providing basic model management operations. This paper presents the preliminary results and, as a running example, we use UniSAF, a SEA method to assess the sustainability of universities. The results are promising, according to two SEA experts we have consulted, and have triggered the interest of the Fair Trade Software Foundation.},
  keywords={Tools;DSL;Sustainable development;Companies;Unified modeling language;ISO Standards;social and environmental accounting;sustainability reporting;model-driven engineering;domain-specific language;method engineering;software for responsibility;ICT for sustainability},
  doi={10.1109/RCIS.2019.8877042},
  ISSN={2151-1357},
  month={May},}
  
@INPROCEEDINGS{7862407,
  author={Wienke, Johannes and Wrede, Sebastian},
  booktitle={2016 IEEE International Conference on Simulation, Modeling, and Programming for Autonomous Robots (SIMPAR)}, 
  title={Continuous regression testing for component resource utilization}, 
  year={2016},
  volume={},
  number={},
  pages={273-280},
  abstract={Unintended changes in the utilization of resources like CPU and memory can lead to severe problems for the operation of robotics and intelligent systems. Still, systematic testing for such performance regressions has largely been ignored in this domain. We present a method to specify and execute performance tests for individual components of component-based robotics systems based on their component interfaces. The method includes an automatic analysis of each component revision against previous ones that reports potential changes to the resource usage characteristics. This informs developers about the impact of their changes. We describe the design of the framework and present evaluation results for the automatic detection of performance changes based on tests for a variety of robotics components.},
  keywords={Robots;Testing;Middleware;Intelligent systems;Radiation detectors;Systematics},
  doi={10.1109/SIMPAR.2016.7862407},
  ISSN={},
  month={Dec},}
  
@INPROCEEDINGS{687914,
  author={Bernard, D.E. and Dorais, G.A. and Fry, C. and Gamble, E.B. and Kanefsky, B. and Kurien, J. and Millar, W. and Muscettola, N. and Nayak, P.P. and Pell, B. and Rajan, K. and Rouquette, N. and Smith, B. and Williams, B.C.},
  booktitle={1998 IEEE Aerospace Conference Proceedings (Cat. No.98TH8339)}, 
  title={Design of the Remote Agent experiment for spacecraft autonomy}, 
  year={1998},
  volume={2},
  number={},
  pages={259-281 vol.2},
  abstract={This paper describes the Remote Agent flight experiment for spacecraft commanding and control. In the Remote Agent approach, the operational rules and constraints are encoded in the flight software. The software may be considered to be an autonomous "remote agent" of the spacecraft operators in the sense that the operators rely on the agent to achieve particular goals. The experiment will be executed during the flight of NASA's Deep Space One technology validation mission. During the experiment, the spacecraft will not be given the usual detailed sequence of commands to execute. Instead, the spacecraft will be given a list of goals to achieve during the experiment. In flight, the Remote Agent flight software will generate a plan to accomplish the goals and then execute the plan in a robust manner while keeping track of how well the plan is being accomplished. During plan execution, the Remote Agent stays on the lookout for any hardware faults that might require recovery actions or replanning. In addition to describing the design of the remote agent, this paper discusses technology-insertion challenges and the approach used in the Remote Agent approach to address these challenges. The experiment integrates several spacecraft autonomy technologies developed at NASA Ames and the Jet Propulsion Laboratory: on-board planning, a robust multi threaded executive, and model-based failure diagnosis and recovery.},
  keywords={Space vehicles;Space technology;Robustness;NASA;Humans;Propulsion;Laboratories;Space missions;Orbital robotics;Paper technology},
  doi={10.1109/AERO.1998.687914},
  ISSN={1095-323X},
  month={March},}
  
@INPROCEEDINGS{8882769,
  author={Butler, Michael and Dghaym, Dana and Hoang, Thai Son and Omitola, Tope and Snook, Colin and Fellner, Andreas and Schlick, Rupert and Tarrach, Thorsten and Fischer, Tomas and Tummeltshammer, Peter},
  booktitle={2019 24th International Conference on Engineering of Complex Computer Systems (ICECCS)}, 
  title={Behaviour-Driven Formal Model Development of the ETCS Hybrid Level 3}, 
  year={2019},
  volume={},
  number={},
  pages={97-106},
  abstract={Behaviour driven formal model development (BDFMD) enables domain engineers to influence and validate mathematically precise and verified specifications. In previous work we proposed a process where manually authored scenarios are used initially to support the requirements and help the modeller. The same scenarios are used to verify behavioural properties of the model. The model is then mutated to automatically generate scenarios that have a more complete coverage than the manual ones. These automatically generated scenarios are used to animate the model in a final acceptance stage. In this paper, we discuss lessons learned from applying this BDFMD process to a real-life specification: The European Train Control Systems (ETCS) Hybrid Level 3. During the case study, we have developed our understanding of the process, modifying the way we do some stages and developing improved tool support to make the process more efficient. We discuss (1) the need for abstract scenarios during incremental model development and verification, (2) tools and techniques developed to make the running of scenarios more efficient, and (3) improvements to tools that generate new test cases to improve coverage.},
  keywords={Unified modeling language;Object oriented modeling;Tools;Visualization;Mathematical model;Testing;Computational modeling;Event-B, UML-B, MoMuT, BDFMD, Scenario, ETCS Hybrid Level 3},
  doi={10.1109/ICECCS.2019.00018},
  ISSN={},
  month={Nov},}
  
@INPROCEEDINGS{8241120,
  author={Jumagaliyev, Assylbek and Whittle, Jon and Elkhatib, Yehia},
  booktitle={2017 IEEE International Conference on Cloud Computing Technology and Science (CloudCom)}, 
  title={Using DSML for Handling Multi-tenant Evolution in Cloud Applications}, 
  year={2017},
  volume={},
  number={},
  pages={272-279},
  abstract={Multi-tenancy is sharing a single application's resources to serve more than a single group of users (i.e. tenant). Cloud application providers are encouraged to adopt multi-tenancy as it facilitates increased resource utilization and ease of maintenance, translating into lower operational and energy costs. However, introducing multi-tenancy to a single-tenant application requires significant changes in its structure to ensure tenant isolation, configurability and extensibility. In this paper, we analyse and address the different challenges associated with evolving an application's architecture to a multi-tenant cloud deployment. We focus specifically on multi-tenant data architectures, commonly the prime candidate for consolidation and multi-tenancy. We present a Domain-Specific Modeling language (DSML) to model a multi-tenant data architecture, and automatically generate source code that handles the evolution of the application's data layer. We apply the DSML on a representative case study of a single-tenant application evolving to become a multi-tenant cloud application under two resource sharing scenarios. We evaluate the costs associated with using this DSML against the state of the art and against manual evolution, reporting specifically on the gained benefits in terms of development effort and reliability.},
  keywords={Databases;Data models;Load modeling;Computer architecture;Cloud computing;Software as a service;Business},
  doi={10.1109/CloudCom.2017.31},
  ISSN={2330-2186},
  month={Dec},}
  
@INPROCEEDINGS{4839232,
  author={Díaz, Jessica and Yagüe, Agustín and Garbajosa, Juan},
  booktitle={2009 16th Annual IEEE International Conference and Workshop on the Engineering of Computer Based Systems}, 
  title={A Systematic Process for Implementing Gateways for Test Tools}, 
  year={2009},
  volume={},
  number={},
  pages={58-66},
  abstract={Test automation is facing a new challenge because tools, as well as having to provide conventional test functionalities, must be capable to interact with ever more heterogeneous complex systems under test (SUT). The number of existing software interfaces to access these systems is also a growing number. The problem cannot be analyzed only from a technical or engineering perspective; the economic perspective is as important. This paper presents a process to systematically implement gateways which support the communication between test tools and SUTs with a reduced cost. The proposed solution does not preclude any  interface protocol at the SUT side. This process is supported using a generic architecture of a  gateway defined on top of  OSGi. Any test tool can communicate with the gateway through a unique defined interface. To communicate the gateway and the SUT, basically, the driver corresponding to the SUT software interface has to be loaded.},
  keywords={System testing;Automatic testing;Costs;Service oriented architecture;Home automation;Internet;Radiofrequency identification;Intelligent sensors;Software testing;Web services;test automation;gateway;complex systems testing;acceptance testing tools;OSGI;TOPEN},
  doi={10.1109/ECBS.2009.40},
  ISSN={},
  month={April},}
  
@INPROCEEDINGS{5967121,
  author={Piho, Gunnar and Tepandi, Jaak and Parman, Marko and Puusep, Viljam and Roost, Mart},
  booktitle={2011 Proceedings of the 34th International Convention MIPRO}, 
  title={Test Driven domain modelling}, 
  year={2011},
  volume={},
  number={},
  pages={576-581},
  abstract={To write software we have to know requirements; to know requirements we have to know domain; to know the domain we have to analyze and model one. We propose a methodology for applying Test Driven Modelling in engineering of domains, requirements and software. We will restrict ourselves here to enterprise information systems and therefore to business domains. As common for Software Factories, domain models (as well as all other models) are software artefacts, not only documentation artefacts. In our approach Test Driven Modelling utilizes Test Driven Development for domain modelling. Domain models engineered in this way are used as Domain Specific Language for specifying software requirements. The hypothesis is that such domain models can be used for validation of requirements and verification of software, lead developments towards Software Factories, and increase dependability of software.},
  keywords={Software;Measurement units;Time division multiplexing;Analytical models;Business;Silicon;Production facilities;domain analysis and engineering;domain model and domain modelling;software engineering;software factory;software testing;test driven development;test driven modelling;verification and validation},
  doi={},
  ISSN={},
  month={May},}

@INPROCEEDINGS{8491143,
  author={Moitra, Abha and Siu, Kit and Crapo, Andrew and Chamarthi, Harsh and Durling, Michael and Li, Meng and Yu, Han and Manolios, Panagiotis and Meiners, Michael},
  booktitle={2018 IEEE 26th International Requirements Engineering Conference (RE)}, 
  title={Towards Development of Complete and Conflict-Free Requirements}, 
  year={2018},
  volume={},
  number={},
  pages={286-296},
  abstract={Writing requirements is no easy task. Common problems include ambiguity in statements, specifications at the wrong level of abstraction, statements with inconsistent references to types, conflicting requirements, and incomplete requirements. These pitfalls lead to errors being introduced early in the design process. The longer the gap between error introduction and error discovery, the higher the cost associated with the error. To address the growing cost of system development, we introduce a tool called ASSERT" (Analysis of Semantic Specifications and Efficient generation of Requirements-based Tests) for capturing requirements, backed by a formal requirements analysis engine. ASSERT" also automatically generates a complete set of requirements-based test cases. Capturing requirements in an unambiguous way and then formally analyzing them with an automated theorem prover eliminates errors as soon as requirements are written. It also addresses the historical problem that analysis engines are hard to use for someone without formal methods expertise and analysis results are often difficult for the end-user to understand and make actionable. ASSERT"'s major contribution is to bring powerful requirements capture and analysis capability to the domain of the end-user. We provide explainable and automated formal analysis, something we found important for a tool's adoptability in industry.},
  keywords={Tools;Aircraft propulsion;Ontologies;Semantics;Engines;Software;Aerospace electronics;Requirements Formalization, Formal Analysis of Requirements, Ontology, Automated Requirements Based Test Generation, Requirements Engineering, Formal Methods},
  doi={10.1109/RE.2018.00036},
  ISSN={2332-6441},
  month={Aug},}

@INPROCEEDINGS{802127,
  author={Reyes, A.A. and Richardson, D.},
  booktitle={14th IEEE International Conference on Automated Software Engineering}, 
  title={Siddhartha: a method for developing domain-specific test driver generators}, 
  year={1999},
  volume={},
  number={},
  pages={81-90},
  abstract={Siddhartha applies the domain-specific language (DSL) paradigm to solve difficult problems in specification-based testing (SBT). Domain-specific test case data specifications (TestSpecs) and difficult-to-test program design styles engender difficult SBT problems, which are the essential phenomena of interest to Siddhartha. Difficult-to-test program design styles are explicitly represented by domain-specific, unit test driver reference designs that accommodate the problematic program design styles. DSLs are developed to represent both TestSpecs and Driver reference designs. A DSL language processing tool (a translator) is developed that maps TestSpecs into Drivers. We developed a prototype implementation of Siddhartha via Reasoning SDK (formerly known as Software Refinery) and developed two domain-specific TestSpec/spl rarr/Driver translators. Each translator generated Drivers that revealed new failures in a real-world digital flight control application program.},
  keywords={Automatic testing;Application software;DSL;Aerospace control;Computer science;Aerospace electronics;Domain specific languages;Prototypes;Formal specifications;Automatic control},
  doi={10.1109/ASE.1999.802127},
  ISSN={},
  month={Oct},}

@INPROCEEDINGS{4600272,
  author={Sadri, S.M.R. and Harandi, Y. N. and Pirhadi, M. and Waskasi, M. Yaghoubi and Tabrizipoor, A. Iravani and Mirzabaghi, M.},
  booktitle={2007 International Symposium on High Capacity Optical Networks and Enabling Technologies}, 
  title={Test strategy for DSL broadband IP access services}, 
  year={2007},
  volume={},
  number={},
  pages={1-7},
  abstract={In this paper the test methodology has been expanded for evaluation of DSL broadband services. This is a new strategy for testing and considering different aspects of test. This strategy can precisely test various features of DSL broadband equipment and services which can be delivered by them in terms of different aspects of testing such as functionality, performance, conformance, etc. It was practically executed over a designed testbed in Iran Telecommunications Research Center (ITRC) where a lab named NGN Pilot exists. The introduced strategy has a hierarchical structure from the topmost level of testing including different categories to the bottommost level including detailed test topics and test cases. This paper elaborates how the strategy was designed and how it can be applied to a desired environment to compare different DSL scenarios with each other. The strategy can then be used by a service provider to help deciding which solutions can achieve the best outcome.},
  keywords={Modems;Broadband communication;IP networks;Multiprotocol label switching;Throughput;Quality of service;Next generation networking;DSL;Broadband Access;Test Strategy;Testbed;Pilot},
  doi={10.1109/HONET.2007.4600272},
  ISSN={1949-4106},
  month={Nov},}

@INPROCEEDINGS{4626839,
  author={Magro, Bélen and Garbajosa, Juan and Pérez, Jennifer},
  booktitle={2008 12th International Software Product Line Conference}, 
  title={A Software Product Line Definition for Validation Environments}, 
  year={2008},
  volume={},
  number={},
  pages={45-54},
  abstract={Functional requirements must be tested to check if the system executes as the end user expects. Validation environments must be able to test multiple kinds of applications that belong to different domains and technologies. Since this wide validation spectrum is very difficult to cope with, validation environments are usually specialized in domains, programming languages, technologies, etc. However, it is possible to identify that the validation processes for different systems share a set of commonalities and variability points. This is a perfect framework to apply the software product line approach to develop domain specific validation environments for testing specific products. In this paper we present our experience of applying software product lines to support the variability of validation environments. We illustrate our product-line experience of developing two domain-specific validation environments for two different case studies: digital TV and slots machines.},
  keywords={Graphical user interfaces;Software;Engines;Testing;Computer architecture;Computer languages;Databases;tools;CASE;acceptance testing;software product lines;validation environments;validation tools;testing tools;software engineering environments;software development environments},
  doi={10.1109/SPLC.2008.35},
  ISSN={},
  month={Sep.},}

@ARTICLE{10409619,
  author={Payandeh, Amirreza and Sharbaf, Mohammadreza and Rahimi, Shekoufeh Kolahdouz},
  journal={IEEE Transactions on Games}, 
  title={A Systematic Review of Model-Driven Game Development Studies}, 
  year={2025},
  volume={17},
  number={1},
  pages={1-12},
  abstract={Model-driven game development (MDGD) leverages the concept of model-driven engineering and game development. The focus of MDGD is to automate the game development process by emphasizing a higher level of abstraction, which will make game development faster and easier. In recent years, researchers in the MDGD community have developed several approaches in this domain. The goal of this article is to survey and classify existing works in MDGD, identify the challenges in this domain, and provide promising future research directions. To achieve this, we conducted a systematic review by selecting 43 articles from a set of 849. The results show that MDE techniques are used to develop games in various genres. 42% of the investigated studies proposed a graphical concrete syntax for game specification and 56% of them used different target environment tools, such as Unity Engine. Moreover, our suggestions include taking advantage of tooling environments and focusing on game components rather than a complete game.},
  keywords={Games;Computational modeling;Systematics;Software;Planning;Codes;Video games;Domain-specific language (DSL);game development;game feature model;model-driven software engineering},
  doi={10.1109/TG.2024.3356408},
  ISSN={2475-1510},
  month={March},}

@INPROCEEDINGS{5630332,
  author={Agaram, Mukundan K. and Laird, Brenda},
  booktitle={2010 14th IEEE International Enterprise Distributed Object Computing Conference}, 
  title={A Componentized Architecture for Externalized Business Rules}, 
  year={2010},
  volume={},
  number={},
  pages={175-183},
  abstract={Delta Dental is the leading provider of Dental benefits in the Midwest, serving nearly 5.1 million members. Delta Dental of Michigan uses Business Rules to articulate complex claims adjudication rules, from legacy COBOL programs. These rules are entirely authored, managed, tested and governed by Subject Matter Experts with no programming background. The Business Rules Architecture incorporates a componentized approach to authoring and organizing Business Rules which promotes extensive rule reuse at several levels through layered components. To mitigate complexity, the framework provides features for sandbox testing and regression testing of the rules by the Business Users. It also provides a rich reporting and trace ability back to the Policy charter. As a result of this framework a legacy adjudication COBOL program of 50,000+ lines were reduced to 30+ reusable rules. Nearly 94 % of the claims processed are automatically adjudicated by the rules engine. The purpose of this paper is to describe in detail the insights gained from the architecture and the measurable productivity gains accomplished.},
  keywords={Business;Dentistry;Vocabulary;Testing;Production;Runtime;Humans;Architecture;Business Rules;BRIDE;Component},
  doi={10.1109/EDOC.2010.26},
  ISSN={1541-7719},
  month={Oct},}

@INPROCEEDINGS{11029969,
  author={Guan, Kevin and Legunsen, Owolabi},
  booktitle={2025 IEEE/ACM 47th International Conference on Software Engineering (ICSE)}, 
  title={Instrumentation-Driven Evolution-Aware Runtime Verification}, 
  year={2025},
  volume={},
  number={},
  pages={103-115},
  abstract={Runtime verification (RV) found hundreds of bugs by monitoring passing tests against formal specifications (specs). RV first instruments a program to obtain relevant events, e.g., method calls, to monitor. A hindrance to RV adoption, especially in continuous integration, is its high overhead. So, prior work proposed spec-driven evolution-aware techniques to speed up RV. They use complex analysis to re-monitor a subset of specs related to code impacted by changes. But, these techniques assume that RV overhead is dominated by monitoring time, and their designs often sacrifice safety (ability to find all new violations) for speed. We present IMOP, the first instrumentation-driven evolution-aware RV framework. IMOP leverages a recent observation that RV overhead during testing is often dominated by instrumentation, not monitoring. IMOP embodies a family of 14 techniques that aim to safely speed up RV by simply re-instrumenting only changed code. Instrumentation from the old revision is re-used for unchanged code, and all specs are re-monitored in the new revision. We implement IMOP as a Maven plugin and evaluate it on 2,028 revisions of 66 projects, using 160 specs of correct JDK API usage. IMOP is safe by design. It is up to 40.2x faster than re-running RV from scratch after each change, and 17.8x and 6.7x faster than safe and unsafe spec-driven techniques, respectively. IMOP is faster than just applying regression test selection to RV.},
  keywords={Codes;Runtime;Instruments;Diversity reception;Computer bugs;Continuous integration;Safety;Formal specifications;Monitoring;Testing},
  doi={10.1109/ICSE55347.2025.00099},
  ISSN={1558-1225},
  month={April},}

@INPROCEEDINGS{9463040,
  author={Arrieta, Aitor and Ayerdi, Jon and Illarramendi, Miren and Agirre, Aitor and Sagardui, Goiuria and Arratibel, Maite},
  booktitle={2021 IEEE/ACM International Conference on Automation of Software Test (AST)}, 
  title={Using Machine Learning to Build Test Oracles: an Industrial Case Study on Elevators Dispatching Algorithms}, 
  year={2021},
  volume={},
  number={},
  pages={30-39},
  abstract={The software of elevators requires maintenance over several years to deal with new functionality, correction of bugs or legislation changes. To automatically validate this software, test oracles are necessary. A typical approach in industry is to use regression oracles. These oracles have to execute the test input both, in the software version under test and in a previous software version. This practice has several issues when using simulation to test elevators dispatching algorithms at system level. These issues include a long test execution time and the impossibility of re-using test oracles both at different test levels and in operation. To deal with these issues, we propose DARIO, a test oracle that relies on regression learning algorithms to predict the Qualify of Service of the system. The regression learning algorithms of this oracle are trained by using data from previously tested versions. An empirical evaluation with an industrial case study demonstrates the feasibility of using our approach in practice. A total of five regression learning algorithms were validated, showing that the regression tree algorithm performed best. For the regression tree algorithm, the accuracy when predicting verdicts by DARIO ranged between 79 to 87%.},
  keywords={Machine learning algorithms;Software algorithms;Legislation;Machine learning;Maintenance engineering;Prediction algorithms;Software;Test Oracle;Regression Testing;Machine-Learning},
  doi={10.1109/AST52587.2021.00012},
  ISSN={},
  month={May},}

@BOOK{10163131,
  author={Hsu, Min-Yih},
  booktitle={LLVM Techniques, Tips, and Best Practices Clang and Middle-End Libraries: Design powerful and reliable compilers using the latest libraries and tools from LLVM},
  year={2021},
  volume={},
  number={},
  pages={},
  abstract={Learn how you can build the next big programming language, compiler, or source code analyzer using LLVM and ClangKey FeaturesExplore Clang, LLVM’s middle-end and backend, in a pragmatic wayDevelop your LLVM skillset and get to grips with a variety of common use casesEngage with real-world LLVM development through various coding examplesBook DescriptionEvery programmer or engineer, at some point in their career, works with compilers to optimize their applications. Compilers convert a high-level programming language into low-level machine-executable code. LLVM provides the infrastructure, reusable libraries, and tools needed for developers to build their own compilers. With LLVM’s extensive set of tooling, you can effectively generate code for different backends as well as optimize them. In this book, you’ll explore the LLVM compiler infrastructure and understand how to use it to solve different problems. You’ll start by looking at the structure and design philosophy of important components of LLVM and gradually move on to using Clang libraries to build tools that help you analyze high-level source code. As you advance, the book will show you how to process LLVM IR – a powerful way to transform and optimize the source program for various purposes. Equipped with this knowledge, you’ll be able to leverage LLVM and Clang to create a wide range of useful programming language tools, including compilers, interpreters, IDEs, and source code analyzers. By the end of this LLVM book, you’ll have developed the skills to create powerful tools using the LLVM framework to overcome different real-world challenges.What you will learnFind out how LLVM’s build system works and how to reduce the building resourceGet to grips with running custom testing with LLVM’s LIT frameworkBuild different types of plugins and extensions for ClangCustomize Clang’s toolchain and compiler flagsWrite LLVM passes for the new PassManagerDiscover how to inspect and modify LLVM IRUnderstand how to use LLVM’s profile-guided optimizations (PGO) frameworkCreate custom compiler sanitizersWho this book is forThis book is for software engineers of all experience levels who work with LLVM. If you are an academic researcher, this book will help you learn useful LLVM skills in a short time and enable you to build your prototypes and projects quickly. Programming language enthusiasts will also find this book useful for building a new programming language with the help of LLVM.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781838829728},
  url={https://ieeexplore.ieee.org/document/10163131},}

@INPROCEEDINGS{7323106,
  author={Beckmann, Kai},
  booktitle={2015 3rd International Conference on Model-Driven Engineering and Software Development (MODELSWARD)}, 
  title={Integrating existing proprietary system models into a model-driven test process for an industrial automation scenario}, 
  year={2015},
  volume={},
  number={},
  pages={255-262},
  abstract={The introduction of modern model-driven software development methodologies into the industrial practise still proves to be a challenge. Especially small or medium-sized enterprises (SMEs) need an incremental and continuous modernisation process, which incorporates existing projects, is customised and cost-effective. Particularly, suitable solutions for model-based or -driven testing with test automation to increase the efficiency are in demand. This paper presents an approach for integrating existing proprietary system models of an SME partner for describing industrial automation processes into a model-driven test process, utilising a domain-specific language for the test specification. The test objectives focuses on the correct implementation of the communication and synchronisation of distributed state machines. The presented approach is integrated into a test framework, which is based on the Eclipse Modelling Framework (EMF) and the Eclipse Test and Performance Tools Platform Project (TPTP) framework. To separate the possibly changeable system and DSL-specific models from the implementation of the test framework, a stable and more generic test meta model was defined.},
  keywords={Unified modeling language;Adaptation models;DSL;Object oriented modeling;Biological system modeling;Software;Automation;MDSD;DSL;Metamodelling;Testing;MDT;Model-driven Testing},
  doi={},
  ISSN={},
  month={Feb},}

@INPROCEEDINGS{4547343,
  author={Syed, Tariq and Das, Sunil R. and Biswas, Satyendra N. and Petriu, Emil M.},
  booktitle={2008 IEEE Instrumentation and Measurement Technology Conference}, 
  title={Developing Automated Test System for ADSL Equipment}, 
  year={2008},
  volume={},
  number={},
  pages={1833-1838},
  abstract={The requirement for an automated test system has immensely increased due to the realization that manual testing is associated with additional resources and staffing constraints. In order to achieve a competitive edge, reduced development cost, timely product delivery, and product quality are mandatory in today's organization. Manual testing requires skilled operators that increase cost, time, and product delivery. The low cost computer based automated system helps to get an edge by fulfilling these organizational demands. In this paper, an automated testing system has been developed to support functional testing of Nortel Network's modem system (1-Meg SUT). The modem is an inherently complex asymmetric digital subscriber line (ADSL) product and its testing is far more complex than just verification of process faults. The complexity of ADSL system renders automated test system an important and imperative part of ADSL testing. The subject paper demonstrates the indispensable need of automated test system for ADSL testing and its advantages in providing a competitive edge for the organization.},
  keywords={Automatic testing;System testing;Automation;Modems;Costs;DSL;Information technology;Internet;Graphical user interfaces;Software testing;Asymmetric digital subscriber line (ADSL);automated testing;hardware platform;software platform},
  doi={10.1109/IMTC.2008.4547343},
  ISSN={1091-5281},
  month={May},}

@INPROCEEDINGS{7809817,
  author={Ratiu, Daniel and Voelter, Markus},
  booktitle={2016 IEEE/ACM 11th International Workshop in Automation of Software Test (AST)}, 
  title={Automated Testing of DSL Implementations - Experiences from Building mbeddr}, 
  year={2016},
  volume={},
  number={},
  pages={15-21},
  abstract={Domain specific languages promise to improve productivity and quality of software by providing problem-adequate abstractions to developers. Projectional language workbenches like JetBrains MPS allow the definition of modular and extensible domain specific languages, generators and development environments. While recent advances in language engineering have enabled the definition of DSLs and tooling in a modular and cost-effective manner, the quality assurance of their implementation is still challenging. In this paper we present our work on testing the implementation of domain specific languages and associated tools, and discuss different approaches to increase the automation of language testing. We illustrate this based on MPS and our experience with testing mbeddr, a set of domain specific languages and tools on top of C tailored to embedded software development.},
  keywords={Testing;DSL;Generators;C languages;Software;Semantics;Automation;domain specific languages;quality assurance;automated testing},
  doi={},
  ISSN={},
  month={May},}

@INPROCEEDINGS{9155827,
  author={Arrieta, Aitor and Agirre, Joseba A. and Sagardui, Goiuria},
  booktitle={2020 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)}, 
  title={A Tool for the Automatic Generation of Test Cases and Oracles for Simulation Models Based on Functional Requirements}, 
  year={2020},
  volume={},
  number={},
  pages={1-5},
  abstract={Simulation models are frequently used to model, simulate and test complex systems (e.g., Cyber-Physical Systems (CPSs)). To allow full test automation, test cases and test oracles are required. Safety standards (e.g., the ISO 26262) highly recommend that the test cases of systems like CPSs are associated to requirements. As a result, typically, test cases that need to cover specific requirements are manually generated in the context of simulation models. This is, of course, a time-consuming and non-systematic process. However, the current practice lacks tools that generate test cases by considering functional requirements for simulation-based testing. In this short paper we propose a Domain-Specific Language (DSL) for specifying requirements for simulation-based testing in an easy manner. These files are later parsed by an automatic test generation algorithm, which generates test cases that follow the ASAM-XiL standard. The tool was integrated with two professional tools: (1) SYNECT from dSPACE and (2) xMOD from FEV. An initial validation was also performed with an industrial simulation model from YASA motors.},
  keywords={Tools;DSL;Test pattern generators;Generators;Standards;Subspace constraints;Simulation-based Testing;Functional Requirements;Test Case Generation},
  doi={10.1109/ICSTW50294.2020.00018},
  ISSN={},
  month={Oct},}

@INPROCEEDINGS{8101621,
  author={Olajubu, Oyindamola and Ajit, Suraj and Johnson, Mark and Thomson, Scott and Edwards, Mark and Turner, Scott},
  booktitle={2017 9th Computer Science and Electronic Engineering (CEEC)}, 
  title={Automated test case generation from high-level logic requirements using model transformation techniques}, 
  year={2017},
  volume={},
  number={},
  pages={178-182},
  abstract={It is not uncommon for industries to use natural language to represent high-level software requirement specifications. It is also not uncommon for these requirement specifications to be translated into design and used further for implementation and generation of test cases in the software engineering life-cycle. These requirements are often ambiguous, incorrect, and incomplete. Finding them late in the development lifecycle proves very expensive and lowers the productivity. This paper reports on the experience of applying model-based technologies from academia to a real-world problem domain in the aviation industry to improve the productivity. The paper focuses on the application of a model-based technique to automatically generate test cases to satisfy Modified Condition/Decision Coverage (MC/DC) from high-level logic requirements expressed in a Domain Specific Language (DSL).},
  keywords={Legged locomotion;DSL;Software;Testing;Industries;Productivity;Natural languages},
  doi={10.1109/CEEC.2017.8101621},
  ISSN={},
  month={Sep.},}

@BOOK{10162439,
  author={Leszko, Rafał},
  booktitle={Continuous Delivery with Docker and Jenkins: Create secure applications by building complete CI/CD pipelines},
  year={2022},
  volume={},
  number={},
  pages={},
  abstract={Create a complete continuous delivery process using modern DevOps tools such as Docker, Jenkins, Kubernetes, Ansible, Terraform, and many moreKey FeaturesBuild reliable and secure applications using Docker containersCreate a highly available environment to scale Jenkins and your services using KubernetesAutomate your release process end-to-endBook DescriptionThis updated third edition of Continuous Delivery with Docker and Jenkins will explain the advantages of combining Jenkins and Docker to improve the continuous integration and delivery process of app development. You’ll start by setting up a Docker server and configuring Jenkins on it. Next, you’ll discover steps for building applications and microservices on Dockerfiles and integrating them with Jenkins using continuous delivery processes such as continuous integration, automated acceptance testing, configuration management, and Infrastructure as Code. Moving ahead, you'll learn how to ensure quick application deployment with Docker containers, along with scaling Jenkins using Kubernetes. Later, you’ll explore how to deploy applications using Docker images and test them with Jenkins. Toward the concluding chapters, the book will focus on missing parts of the CD pipeline, such as the environments and infrastructure, application versioning, and non-functional testing. By the end of this continuous integration and continuous delivery book, you’ll have gained the skills you need to enhance the DevOps workflow by integrating the functionalities of Docker and Jenkins.What you will learnGrasp Docker fundamentals and dockerize applications for the CD processUnderstand how to use Jenkins on-premises and in the cloudScale a pool of Docker servers using KubernetesWrite acceptance tests using CucumberRun tests in the Docker ecosystem using JenkinsProvision your servers and infrastructure using Ansible and TerraformPublish a built Docker image to a Docker registryDeploy cycles of Jenkins pipelines using community best practicesWho this book is forThe book is for DevOps engineers, system administrators, Docker professionals, or anyone who wants to explore the power of working with Docker and Jenkins together. No prior knowledge of DevOps is required to get started.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781803245300},
  url={https://ieeexplore.ieee.org/document/10162439},}

@INPROCEEDINGS{9643783,
  author={Patkar, Nitish and Chiş, Andrei and Stulova, Nataliia and Nierstrasz, Oscar},
  booktitle={2021 ACM/IEEE International Conference on Model Driven Engineering Languages and Systems Companion (MODELS-C)}, 
  title={Interactive Behavior-driven Development: a Low-code Perspective}, 
  year={2021},
  volume={},
  number={},
  pages={128-137},
  abstract={Within behavior-driven development (BDD), different types of stakeholders collaborate in creating scenarios that specify application behavior. The current workflow for BDD expects non-technical stakeholders to use an integrated development environment (IDE) to write textual scenarios in the Gherkin language and verify application behavior using test passed/failed reports. Research to date shows that this approach leads non-technical stakeholders to perceive BDD as an overhead in addition to the testing. In this vision paper, we propose an alternative approach to specify and verify application behavior visually, interactively, and collaboratively within an IDE. Instead of writing textual scenarios, non-technical stakeholders compose, edit, and save scenarios by using tailored graphical interfaces that allow them to manipulate involved domain objects. Upon executing such interactively composed scenarios, all stakeholders verify the application behavior by inspecting domain-specific representations of run-time domain objects instead of a test run report. Such a low code approach to BDD has the potential to enable nontechnical stakeholders to engage more harmoniously in behavior specification and validation together with technical stakeholders within an IDE. There are two main contributions of this work: (i) we present an analysis of the features of 13 BDD tools, (ii) we describe a prototype implementation of our approach, and (iii) we outline our plan to conduct a large-scale developer survey to evaluate our approach to highlight the perceived benefits over the existing approach.},
  keywords={Codes;Prototypes;Writing;Programming;Model driven engineering;Stakeholders;Testing;bdd;behavior-driven development;collaborative development;acceptance testing;visual programming;end-user programming},
  doi={10.1109/MODELS-C53483.2021.00024},
  ISSN={},
  month={Oct},}

@INPROCEEDINGS{9143910,
  author={Wolde, Behailu Getachew and Boltana, Abiot Sinamo},
  booktitle={2020 Seventh International Conference on Software Defined Systems (SDS)}, 
  title={Behavior-Driven Re-engineering for Testing the Cloud}, 
  year={2020},
  volume={},
  number={},
  pages={75-82},
  abstract={Cloud is an emerging technology in software development by its capability using orchestration to provide composite services through implementations somewhere on the Internet. Once it is composed the services can be easily usable by diversified clients. In practice, however, each client relies on the presentation of high level features to utilize the required Service Level Agreements. On top of this, test engineer knows only these features while the implementation and the infrastructure remain hidden. Testing becomes challenging the classical and legacy testing procedure. The paper describes the importance of cloud challenges and elaborates the appropriate way like a behavior- driven model-based re-engineering to design and validate test instances with a sample scenario for testing the cloud. It also illustrates this scenario with GraphWalker model which generates test paths through a Finite State Machine through which it enables instances of domain specific language. The generated paths include forwarding inputs and assertions to validate quality of test instances. As evaluation, the tester uses an Open Source Web Application implemented in Eclipse environment with REST assured libraries. The web application setup is configured on the cloud before a testing operation is done. Then, the implemented service is deployed on Azure platform as-a Service using Microsoft Azure plugin toolkit for Eclipse to execute the test plans on specified service.},
  keywords={Testing;Cloud computing;Software;Object oriented modeling;Data models;Java;Measurement;REST Service;Behavior-Driven Model;Test Path Generation;Domain Specific Language},
  doi={10.1109/SDS49854.2020.9143910},
  ISSN={},
  month={April},}

@ARTICLE{8317991,
  author={Johanson, Arne and Hasselbring, Wilhelm},
  journal={Computing in Science & Engineering}, 
  title={Software Engineering for Computational Science: Past, Present, Future}, 
  year={2018},
  volume={20},
  number={2},
  pages={90-109},
  abstract={Despite the increasing importance of in silico experiments to the scientific discovery process, state-of-the-art software engineering practices are rarely adopted in computational science. To understand the underlying causes for this situation and to identify ways to improve it, the authors conducted a literature survey on software engineering practices in computational science. They identified 13 recurring key characteristics of scientific software development that are the result of the nature of scientific challenges, the limitations of computers, and the cultural environment of scientific software development. Their findings allow them to point out shortcomings of existing approaches for bridging the gap between software engineering and computational science and to provide an outlook on promising research directions that could contribute to improving the current situation.},
  keywords={Scientific computing;Software engineering;Computational modeling;Software development management;survey;software engineering;computational science;software development;history of computing},
  doi={10.1109/MCSE.2018.021651343},
  ISSN={1558-366X},
  month={Mar},}

@INPROCEEDINGS{7237259,
  author={Lavrishcheva, Ekaterina},
  booktitle={2015 Science and Information Conference (SAI)}, 
  title={Ontological approach to the formal specification of the standard life cycle}, 
  year={2015},
  volume={},
  number={},
  pages={965-972},
  abstract={Approach is offered to the formal specification of Standard Life Cycle (LC) of the program systems (PS) by the ontology facilities with purpose automation and generation of the variants LC for making the appropriate kinds process for development different PS. Ontological approach to presentation LC model of the standard ISO/IEC 12207-2007 is included the specification of general, organizational and support processes. These processes are presented in the subject-oriented DSL, which than transformed to XML for realization. One of the processes, the testing process is given in terms of Protégé systems. An eventual result of this system Protégé got generally at accepted to the XML, suitable for implementation tasks testing PS on computer.},
  keywords={DSL;Ontologies;XML;ISO Standards;IEC Standards;Testing;ontology;the life cycle standard;model of life cycle;processes;actions;task;testing;DSL;description;Protégé;XML},
  doi={10.1109/SAI.2015.7237259},
  ISSN={},
  month={July},}

@INPROCEEDINGS{6130706,
  author={Svendsen, Andreas and Haugen, Øystein and Moller-Pedersen, Birger},
  booktitle={2011 18th Asia-Pacific Software Engineering Conference}, 
  title={Using Variability Models to Reduce Verification Effort of Train Station Models}, 
  year={2011},
  volume={},
  number={},
  pages={348-356},
  abstract={We show how the effort needed to verify a transformed base model can be reduced by analyzing the definition of the modification. The Common Variability Language (CVL) is a generic language for modeling variability, where a CVL model describes the increment from one base model to another (transformed) base model. Assuming that a property of the base model has been verified, we use the CVL model to reduce the effort needed to verify the property of the transformed model. Based on the CVL model, we narrow down the set of traces required to be verified, including the increment and the cascading effects. We apply CVL to several models of the Train Control Language (TCL) to illustrate how the effort of verifying safety properties of transformed train station models can be reduced.},
  keywords={Safety;Biological system modeling;Analytical models;Metals;Switches;Semantics;Mathematical model;analysis;variability;safety property;Common Variability Language;Train Control Language},
  doi={10.1109/APSEC.2011.21},
  ISSN={1530-1362},
  month={Dec},}

@INPROCEEDINGS{10219353,
  author={Wang, Ning and Zhang, Kegang},
  booktitle={2023 China Semiconductor Technology International Conference (CSTIC)}, 
  title={Fabrication and Characterization of a Novel Embedded Mirror Gate Sonos}, 
  year={2023},
  volume={},
  number={},
  pages={1-3},
  abstract={A novel embedded mirror gate SONOS is fabricated and its electrical performance is observed. Compared with traditional 2T SONOS, the mirror gate SONOS with two split-gate SONOS symmetrically distributed on sides of a selective gate can save over 25% of cell area. The fabrication of the embedded mirror gate SONOS is quite simple, and no additional mask is added as compared to 2T cells. Dedicated source line and shared source line array structure of the mirror gate SONOS are investigated. The E/P curve of both arrays reveal good Vt window. Furthermore, the shared source line array applied in a differential IP passes the functional test in a wide operating temperature of -40~85°C.},
  keywords={SONOS devices;Fabrication;Voltage;Logic gates;Split gate flash memory cells;Mirrors;DSL},
  doi={10.1109/CSTIC58779.2023.10219353},
  ISSN={},
  month={June},}

@INPROCEEDINGS{5464138,
  author={Noubissi, Agnes C. and Iguchi-Cartigny, Julien and Lanet, Jean-Louis},
  booktitle={2010 Fifth International Conference on Systems}, 
  title={Incremental Dynamic Update for Java-Based Smart Cards}, 
  year={2010},
  volume={},
  number={},
  pages={110-113},
  abstract={One of the most appealing feature for multi-application smart cards is their ability to dynamically download or delete applications once the card has been issued. Applications can be updated by deleting old versions and loading the new ones. Nevertheless, for system components, the update is sligthly more complex because the systems never stop. Indeed, for smart cards based on Java called JavaCard, the virtual machine has a life cycle similar to the card because persistent objects are preserved after the communication sessions with the reader have expired. We present in this paper, our research in dynamic system components updating of JavaCard. Our technique requires a lot of off-card and on-card mechanisms. Our approach uses control flow graph to determine change between versions, a domain specific language to represent the change for minimization of the download overhead throughout the communication link with the card.},
  keywords={Java;Smart cards;Virtual machining;Runtime;Application software;Domain specific languages;Aerodynamics;Embedded software;DSL;Operating systems;Smart Card;HotSwUp;Java Card;Dynamic update;e-passport},
  doi={10.1109/ICONS.2010.27},
  ISSN={},
  month={April},}

@INPROCEEDINGS{5967120,
  author={Piho, Gunnar and Tepandi, Jaak and Roost, Mart and Parman, Marko and Puusep, Viljam},
  booktitle={2011 Proceedings of the 34th International Convention MIPRO}, 
  title={From archetypes based domain model via requirements to software: Exemplified by LIMS Software Factory}, 
  year={2011},
  volume={},
  number={},
  pages={570-575},
  abstract={The Archetypes Based Development (ABD) proceeds from archetypes based domain model via requirements to software. We give an overview of ABD and exemplify its application on Laboratory Information Management Systems (LIMS) Software Factory development. ABD is guided by Zachman Framework and utilizes software engineering triptych together with archetypes and archetype patterns. For modelling of domains the Test Driven Modelling (TDM) techniques are used. TDM utilizes test driven development techniques in domain engineering. The resultant domain models serve as the Domain Specific Language for prescribing requirements. Implementation and testing of the LIMS Software Factory proves feasibility of archetypes based techniques in real life systems. ABD helps developers to better understand business requirements, to design cost effective enterprise applications through systematic reuse of archetypal components, as well as to validate and verify requirements resulting in higher quality software.},
  keywords={Software;Production facilities;Laboratories;Capability maturity model;DSL;Organizations;Archetypes;archetype patterns;domain analysis;domain model;domain modelling;software engineering;software factory;laboratory information management system (LIMS);laboratory domain model},
  doi={},
  ISSN={},
  month={May},}

@INPROCEEDINGS{6908678,
  author={Granda, Maria Fernanda and Condori-Fernández, Nelly and Vos, Tanja E.J. and Pastor, Oscar},
  booktitle={2014 IEEE 1st International Workshop on Requirements Engineering and Testing (RET)}, 
  title={Towards the automated generation of abstract test cases from requirements models}, 
  year={2014},
  volume={},
  number={},
  pages={39-46},
  abstract={In a testing process, the design, selection, creation and execution of test cases is a very time-consuming and error-prone task when done manually, since suitable and effective test cases must be obtained from the requirements. This paper presents a model-driven testing approach for conceptual schemas that automatically generates a set of abstract test cases, from requirements models. In this way, tests and requirements are linked together to find defects as soon as possible, which can considerably reduce the risk of defects and project reworking. The authors propose a generation strategy which consists of: two meta-models, a set of transformations rules which are used to generate a Test Model, and the Abstract Test Cases from an existing approach to communication-oriented Requirements Engineering; and an algorithm based on Breadth-First Search. A practical application of our approach is included.},
  keywords={Unified modeling language;Testing;Abstracts;Object oriented modeling;Analytical models;Concrete;Business;Requirements-based testing;Communication Analysis;Model-driven testing;Conceptual Schema Testing;Test Model Generation;Test Case Generation},
  doi={10.1109/RET.2014.6908678},
  ISSN={},
  month={Aug},}

@INPROCEEDINGS{1226344,
  author={Caouras, N. and Freda, M. and Monfet, F. and Aldea, V.S. and Naeem, O. and Tho Le-Ngoc and Champagne, B.},
  booktitle={CCECE 2003 - Canadian Conference on Electrical and Computer Engineering. Toward a Caring and Humane Technology (Cat. No.03CH37436)}, 
  title={Performance evaluation platform for xDSL deployment in a complex multi-segment environment}, 
  year={2003},
  volume={1},
  number={},
  pages={61-64 vol.1},
  abstract={This paper presents a highly flexible simulation platform catering to the easy and rapid evaluation of existing and future digital subscriber line (DSL) deployments as well as DSL modem performance prediction using practical modem implementations in a complex multi-segment environment. The paper outlines the methodology employed to architect and develop the core software, followed by a description of the performance prediction hooks for a variety of current and future DSL modem technologies. The graphical user interface (GUI) abstracting the core software for the user is described in terms of the various configuration options and the quick and easy graphical design of typical and complex deployment scenarios. The proposed simulator's calculations, notably theoretical SNR margin, maximum theoretical capacity and reach, plus performance evaluation using user-designed modem models, are also outlined. To support the accuracy of the new simulator, results for some example scenarios are presented and compared against other available simulators.},
  keywords={DSL;Crosstalk;Modems;Communication cables;Background noise;Electromagnetic coupling;Copper;Telephony;Frequency;Colored noise},
  doi={10.1109/CCECE.2003.1226344},
  ISSN={0840-7789},
  month={May},}

@INPROCEEDINGS{7985680,
  author={Rolim, Reudismam and Soares, Gustavo and D'Antoni, Loris and Polozov, Oleksandr and Gulwani, Sumit and Gheyi, Rohit and Suzuki, Ryo and Hartmann, Björn},
  booktitle={2017 IEEE/ACM 39th International Conference on Software Engineering (ICSE)}, 
  title={Learning Syntactic Program Transformations from Examples}, 
  year={2017},
  volume={},
  number={},
  pages={404-415},
  abstract={Automatic program transformation tools can be valuable for programmers to help them with refactoring tasks, and for Computer Science students in the form of tutoring systems that suggest repairs to programming assignments. However, manually creating catalogs of transformations is complex and time-consuming. In this paper, we present REFAZER, a technique for automatically learning program transformations. REFAZER builds on the observation that code edits performed by developers can be used as input-output examples for learning program transformations. Example edits may share the same structure but involve different variables and subexpressions, which must be generalized in a transformation at the right level of abstraction. To learn transformations, REFAZER leverages state-of-the-art programming-by-example methodology using the following key components: (a) a novel domain-specific language (DSL) for describing program transformations, (b) domain-specific deductive algorithms for efficiently synthesizing transformations in the DSL, and (c) functions for ranking the synthesized transformations. We instantiate and evaluate REFAZER in two domains. First, given examples of code edits used by students to fix incorrect programming assignment submissions, we learn program transformations that can fix other students' submissions with similar faults. In our evaluation conducted on 4 programming tasks performed by 720 students, our technique helped to fix incorrect submissions for 87% of the students. In the second domain, we use repetitive code edits applied by developers to the same project to synthesize a program transformation that applies these edits to other locations in the code. In our evaluation conducted on 56 scenarios of repetitive edits taken from three large C# open-source projects, REFAZER learns the intended program transformation in 84% of the cases using only 2.9 examples on average.},
  keywords={DSL;Programming profession;Tools;C# languages;Pattern matching;Open source software;Program transformation;program synthesis;tutoring systems;refactoring},
  doi={10.1109/ICSE.2017.44},
  ISSN={1558-1225},
  month={May},}

@INPROCEEDINGS{6139107,
  author={Belli, Fevzi and Endo, Andre Takeshi and Linschulte, Michael and Simao, Adenilso},
  booktitle={Proceedings of 2011 IEEE 6th International Symposium on Service Oriented System (SOSE)}, 
  title={Model-based testing of web service compositions}, 
  year={2011},
  volume={},
  number={},
  pages={181-192},
  abstract={The use of web services integrated in different applications, especially the composition of services, brings challenges for testing due to their complex interactions. In this paper, we propose an event-based approach to test web service compositions. The approach is based on event sequence graphs which we extend by facilities to consider the specific features of web service compositions. An enterprise service bus component supports the test case execution. A case study, based on a commercial web application, demonstrates the feasibility of the approach and analyzes its characteristics. The results of empirical work suggest that the approach is a promising candidate to reach a high level of confidence and reliability.},
  keywords={Testing;Service oriented architecture;Business;Data models;Monitoring;enterprise service bus;event sequence graphs;model-based testing;service composition testing},
  doi={10.1109/SOSE.2011.6139107},
  ISSN={},
  month={Dec},}

@INPROCEEDINGS{9004946,
  author={Reinhardt, Oliver and Uhrmacher, Adelinde M. and Hinsch, Martin and Bijak, Jakub},
  booktitle={2019 Winter Simulation Conference (WSC)}, 
  title={Developing Agent-Based Migration Models in Pairs}, 
  year={2019},
  volume={},
  number={},
  pages={2713-2724},
  abstract={Developing a realistic agent-based model of human migration requires particular care. Committing too early to a specific model architecture, design, or language environment can later become costly in terms of the revisions required. To examine specifically the impact of differences in implementation, we have developed two instances of the same model in parallel. One model is realized in the programming language Julia, the underlying execution semantics is of a discrete stepwise stochastic process. The other is realized in an external domain-specific language ML3, based on a continuous-time Markov chain (CTMC) semantics. By developing models in pairs in different approaches, important properties of the target model can be more effectively revealed. In addition, the realization within a programming language and an external domain-specific modeling language respectively, helped identifying crucial features and trade-offs for the future implementation of the model and the design of the domain-specific modeling language.},
  keywords={Urban areas;Biological system modeling;Adaptation models;Computational modeling;Semantics;Data models;Stochastic processes},
  doi={10.1109/WSC40007.2019.9004946},
  ISSN={1558-4305},
  month={Dec},}

@ARTICLE{6312844,
  author={Baker, F. Terry},
  journal={IEEE Transactions on Software Engineering}, 
  title={Structured programming in a production programming environment}, 
  year={1975},
  volume={SE-1},
  number={2},
  pages={241-252},
  abstract={Discusses how structured programming methodology has been introduced into a large production programming organization using an integrated but flexible approach. It next analyzes the advantages and disadvantages of each component of the methodology and presents some quantitative results on its use. It concludes with recommendations based on this generally successful experience, which could be useful to other organizations interested in improving reliability and productivity.},
  keywords={Programming;DSL;Libraries;Organizations;Encoding;Standards;Guidelines;Chief programmer teams (CPT's);development support libraries (DSL's);structured coding;structured programming;top-down development;top-down programming},
  doi={10.1109/TSE.1975.6312844},
  ISSN={1939-3520},
  month={June},}

@INPROCEEDINGS{6498461,
  author={Duclos, Etienne and Le Digabel, Sébastien and Guéhéneuc, Yann-Gaël and Adams, Bram},
  booktitle={2013 17th European Conference on Software Maintenance and Reengineering}, 
  title={ACRE: An Automated Aspect Creator for Testing C++ Applications}, 
  year={2013},
  volume={},
  number={},
  pages={121-130},
  abstract={We present ACRE, an Automated aspect creator, to use aspect-oriented programming (AOP) to perform memory, invariant and interferences testing for software programs written in C++. ACRE allows developers without knowledge in AOP to use aspects to test their programs without modifying the behavior of their source code. ACRE uses a domain-specific language (DSL), which statements testers insert into the source code like comments to describe the aspects to be used. The presence of DSL statements in the code does not modify the program's compilation and behavior. ACRE parses the DSL statements and automatically generates appropriate aspects that are then weaved into the source code to identify bugs due to memory leaks, incorrect algorithm implementation, or interference among threads. Thanks to the use of aspects and ACRE, testers can add or remove tests easily. Using an aspect generated by ACRE, we find a memory leak in a complex C++ software program, NOMAD, used in both industry and research. We also verify a crucial mathematical point of the algorithm behind NOMAD and collect data to find possible interference bugs, in NOMAD.},
  keywords={Testing;DSL;Interference;Computer bugs;Radiation detectors;Java;Weaving;AOP;C++;NOMAD;interference bug pattern;memory testing;invariant testing},
  doi={10.1109/CSMR.2013.22},
  ISSN={1534-5351},
  month={March},}

@INPROCEEDINGS{7095983,
  author={Kayama, Mizue and Ogata, Shinpei and Nagai, Takashi and Yokoka, Hiroaki and Masumoto, Kento and Hashimoto, Masami},
  booktitle={2015 IEEE Global Engineering Education Conference (EDUCON)}, 
  title={Effectiveness of Model-Driven Development in conceptual modeling education for university freshmen}, 
  year={2015},
  volume={},
  number={},
  pages={274-282},
  abstract={The purpose of this study is to explore educational methods for conceptual modeling for novices. In this research, the subjects are mainly freshmen in university. Model driven development (MDD) and a domain specific language (DSL) are key factors in this study. By using MDD, learners are expected to be able to evaluate their own model by observing the target device's behavior. By using a DSL, teachers can control the difficulty of the problems given to their learners. In this paper, we describe our research approach using MDD and a DSL, then, show our experiment design and results. We also discuss the effectiveness of MDD in university freshmen courses with proposed educational methodology.},
  keywords={Unified modeling language;Object oriented modeling;DSL;Computational modeling;Robots;Conferences;Analytical models;model driven development;university freshmen;state machine diagram;model quality;achievment level},
  doi={10.1109/EDUCON.2015.7095983},
  ISSN={2165-9567},
  month={March},}

@INPROCEEDINGS{9163526,
  author={Ammon, Lorenz and Deutschmann, Jörg and Hielscher, Kai-Steffen and German, Reinhard},
  booktitle={2020 43rd International Conference on Telecommunications and Signal Processing (TSP)}, 
  title={One-Way Delay and Goodput Measurements with a VDSL, DOCSIS, and MPTCP Internet Access}, 
  year={2020},
  volume={},
  number={},
  pages={563-568},
  abstract={Different Internet access technologies have both advantages and disadvantages. Two common wire-based standards are Very high speed Digital Subscriber Line (VDSL) and Data Over Cable Service Interface Specification (DOCSIS). The simultaneous use of multiple Internet access links can be achieved with Multipath TCP (MPTCP). This paper presents one-way delay and bulk data transfer measurements of a typical end-user DSL as well as DOCSIS Internet access. In addition, MPTCP is used for bandwidth aggregation of both Internet access links. The results show that the DSL Internet access has a more stable performance than the DOCSIS Internet access. MPTCP can provide bandwidth aggregation unless the link rates are too different. The observed fluctuations indicate that Internet access link models with simple impairment assumptions need to be extended. However, some effects cannot be explained without having access to the operator and backbone infrastructure.},
  keywords={DSL;Internet;Delays;Downlink;Uplink;Data transfer;Bandwidth;DOCSIS;Goodput;Hybrid Access;Multipath TCP;Network Measurements;One-Way Delay;Throughput;VDSL},
  doi={10.1109/TSP49548.2020.9163526},
  ISSN={},
  month={July},}

@ARTICLE{469456,
  author={Krishnamurthy, B. and Rosenblum, D.S.},
  journal={IEEE Transactions on Software Engineering}, 
  title={Yeast: a general purpose event-action system}, 
  year={1995},
  volume={21},
  number={10},
  pages={845-857},
  abstract={Distributed networks of personal workstations are becoming the dominant computing environment for software development organizations. Many cooperative activities that are carried out in such environments are particularly well suited for automated support. Taking the point of view that such activities are modeled most naturally as the occurrence of events requiring actions to be performed, we developed a system called Yeast (Yet another Event Action Specification Tool). Yeast is a client server system in which distributed clients register event action specifications with a centralized server, which performs event detection and specification management. Each specification submitted by a client defines a pattern of events that is of interest to the client's application plus an action that is to be executed in response to an occurrence of the event pattern; the server triggers the action of a specification once it has detected an occurrence of the associated event pattern. Yeast provides a global space of events that is visible to and shared by all users. In particular, events generated by one user can trigger specifications registered by another user. Higher level applications are built as collections of Yeast specifications. We use Yeast on a daily basis for a variety of applications, from deadline notification to software process automation. The paper presents an in depth description of Yeast and an example application of Yeast, in which Yeast specifications are used to automate a software distribution process involving several interdependent software tools.<>},
  keywords={Fungi;Application software;Event detection;Software tools;Workstations;Computer networks;Distributed computing;Programming;Client server systems;Automation},
  doi={10.1109/32.469456},
  ISSN={1939-3520},
  month={Oct},}

@INPROCEEDINGS{10638618,
  author={Le, Tri and Tran, Thien and Cao, Duy and Le, Vy and Nguyen, Tien N. and Nguyen, Vu},
  booktitle={2024 IEEE Conference on Software Testing, Verification and Validation (ICST)}, 
  title={KAT: Dependency-Aware Automated API Testing with Large Language Models}, 
  year={2024},
  volume={},
  number={},
  pages={82-92},
  abstract={API testing has increasing demands for software companies. Prior API testing tools were aware of certain types of dependencies that needed to be concise between operations and parameters. However, their approaches, which are mostly done manually or using heuristic-based algorithms, have limitations due to the complexity of these dependencies. In this paper, we present KAT (Katalon API Testing), a novel AI -driven approach that leverages the large language model GPT in conjunction with advanced prompting techniques to autonomously generate test cases to validate RESTful APIs. Our comprehensive strategy encompasses various processes to construct an operation depen-dency graph from an OpenAPI specification and to generate test scripts, constraint validation scripts, test cases, and test data. Our evaluation of KAT using 12 real-world RESTful services shows that it can improve test coverage, detect more undocumented status codes, and reduce false positives in these services in comparison with a state-of-the-art automated test generation tool. These results indicate the effectiveness of using the large language model for generating test scripts and data for API testing.},
  keywords={Software testing;Codes;Large language models;Heuristic algorithms;Software algorithms;Restful API;Companies;REST API;Black-box testing;API testing;Large language models for testing},
  doi={10.1109/ICST60714.2024.00017},
  ISSN={2159-4848},
  month={May},}

@INPROCEEDINGS{10229466,
  author={He, Yejun and Razi, Muslim and Gao, Jerry and Tao, Chuanqi},
  booktitle={2023 IEEE International Conference On Artificial Intelligence Testing (AITest)}, 
  title={A Framework for Autonomous Vehicle Testing Using Semantic Models}, 
  year={2023},
  volume={},
  number={},
  pages={66-73},
  abstract={With the rapid development of big data and artificial intelligence, autonomous vehicles (AV) have achieved great success in diverse application domains. Although the current development of autonomous driving technology has been greatly improved, the accident rate of driverless vehicles still rises. Therefore, how to ensure the quality of AVs using different testing technologies has become an important issue. Traditional testing approaches such as decision tables and state charts struggle with diversified test inputs. Hence, there is a strong demand for new test models for AV testing. In this paper, we present a model-based testing framework that utilizes semantic trees and 3-dimensional (3D) test tables to model driving scenarios into individual test cases. Using the proposed framework, we generate fine-grained examples corresponding to the real world. Also, we evaluate the behavior of the Apollo AV system in 30 scenarios in SVL simulator. The results show that the scenarios generated by our framework can cover most conditions occurring, thus addressing the challenge of testing AVs for safety and reliability.},
  keywords={Solid modeling;Three-dimensional displays;Semantics;Closed box;Big Data;Safety;Reliability;model-based testing;autonomous vehicle testing;test scenario generation;black-box test},
  doi={10.1109/AITest58265.2023.00020},
  ISSN={2835-3560},
  month={July},}

@INPROCEEDINGS{9787819,
  author={Bachmann, Tobias and van der Wal, Djurre and van der Bijl, Machiel and van der Meij, Daan and Oprescu, Ana},
  booktitle={2022 IEEE Conference on Software Testing, Verification and Validation (ICST)}, 
  title={Translating EULYNX SysML Models into Symbolic Transition Systems for Model-Based Testing of Railway Signaling Systems}, 
  year={2022},
  volume={},
  number={},
  pages={355-364},
  abstract={The EULYNX Consortium is a European initiative by 13 infrastructure managers to standardize interfaces of railway signaling systems. The consortium creates specifications based on semi-formal SysML models. In this paper, we research the feasibility of using the EULYNX SysML models for Model-Based Testing (MBT), which contributes to higher quality specifications and more efficient conformance testing of system implementations. MBT promotes safety, which is one of the most important aspects of railway signaling systems. Our approach is to translate EULYNX SysML models to Symbolic Transition Systems (STS) and to use the STS for MBT. We utilize the Axini Modeling Language (AML) and the Axini Modeling Platform (AMP), which are used by ProRail. As our System Under Test (SUT) we use a software simulation of the EULYNX point subsystem, used by EULYNX developers. We revealed several non-conforming behaviors of the SUT which shows that interface specifications like EULYNX benefit from the application of MBT. However, we observe that better advantage of MBT can be taken if EULYNX SysML specifications abstracted from implementation details, which is currently not the case.},
  keywords={Communication system signaling;Mathematical models;Rail transportation;Behavioral sciences;Safety;Complexity theory;Formal specifications;Axini;Axini Modeling Platform;AMP;Axini Modeling Language;AML;EULYNX;SysML;Model Based Testing;MBT;Signaling Systems},
  doi={10.1109/ICST53961.2022.00044},
  ISSN={2159-4848},
  month={April},}

@INPROCEEDINGS{10685191,
  author={Grube, Nicolas and Massah, Mozhdeh and Tebbe, Michael and Wancura, Paul and Wiesbrock, Hans-Werner and Grossmann, Jürgen and Kharma, Sami},
  booktitle={2024 IEEE International Conference on Artificial Intelligence Testing (AITest)}, 
  title={On a Systematic Test of ML-Based Systems: Experiments on Test Statistics}, 
  year={2024},
  volume={},
  number={},
  pages={11-20},
  abstract={Machine learning (ML)-based systems are becoming increasingly ubiquitous even in safety critical environments. The strength of ML systems, to solve complex problems with a stochastic model, leads to challenges in the testing domain. This motivates us to introduce a rigorous testing method for ML-models and their application environment akin to classical software testing, which is independent of the training process and considers the probabilistic nature of ML. The approach is based on the concept of the Probabilistically Extended ONtology (PEON). In brief, PEON is a an ontology modeling the designated Operational Design Domain (ODD), which is extended by assigning probability distributions to classes and their individual attributes, as well as probabilistic dependencies between these attributes. The relevant statistical key figures like accuracy depend not only on the ML-based model but also strongly on the statistics of the test data set, which we refer to by quality assurance (QA) data set, to emphasize its independence from the test data set in the training process. This implies that we have to consider the statistical properties of the QA data in order to evaluate an ML-based system. In this paper we present first experimental results comparing established test selection methods e.g. N-wise, with a new approach the PEON. Our findings strongly suggest, that the underlying statistical properties of the QA data significantly influence the test results of ML-based systems. In this respect, careful attention must be paid to the statistical independence and balance of the QA data. The PEON provides a good basis for the composition of QA data sets, which are not only independent of the development process but also statistically representative and balanced with respect to the modeled ODD.},
  keywords={Training;Software testing;Systematics;Quality assurance;Training data;Stochastic processes;Ontologies;Testing AI Systems;Black Box Test for AI Systems;Systematic Evaluation of Training data sets;Probabilistic Modeling},
  doi={10.1109/AITest62860.2024.00010},
  ISSN={2835-3560},
  month={July},}

@INPROCEEDINGS{8673038,
  author={Liu, Xiao and Jiang, Yufei and Wu, Dinghao},
  booktitle={2019 IEEE 19th International Symposium on High Assurance Systems Engineering (HASE)}, 
  title={A Lightweight Framework for Regular Expression Verification}, 
  year={2019},
  volume={},
  number={},
  pages={1-8},
  abstract={Regular expressions and finite state automata have been widely used in programs for pattern searching and string matching. Unfortunately, despite the popularity, regular expressions are difficult to understand and verify even for experienced programmers. Conventional testing techniques remain a challenge as large regular expressions are constantly used for security purposes such as input validation and network intrusion detection. In this paper, we present a lightweight verification framework for regular expressions. In this framework, instead of a large number of test cases, it takes in requirements in natural language descriptions to automatically synthesize formal specifications. By checking the equivalence between the synthesized specifications and target regular expressions, errors will be detected and counterexamples will be reported. We have built a web application prototype and demonstrated its usability with two case studies.},
  keywords={Automata;Testing;Software;Tools;Security;Natural language processing;regular expression;verification;natural language;formal specification;domain-specific language},
  doi={10.1109/HASE.2019.00011},
  ISSN={2640-7507},
  month={Jan},}

@BOOK{10745328,
  author={Van Overmeire, Sam},
  booktitle={Write Powerful Rust Macros},
  year={2024},
  volume={},
  number={},
  pages={},
  abstract={An example-driven, step-by-step guide to success with Rust macros. In Write Powerful Rust Macros you’ll learn how to use these amazing metaprogramming tools to push Rust to its full potential. This hands-on guide takes you from the absolute basics to advanced macro techniques, exploring Rust macros through interesting and engaging examples. Inside Write Powerful Rust Macros you’ll discover:  Writing declarative macros Procedural macros Reading and debugging macro code Improving the type system with newtypes and zero-sized types How common Rust libraries use macros  Write Powerful Rust Macros teaches you how to write, test, debug, and publish macros for Rust. It’s perfect for Rust practitioners who want to master this powerful development technique. Build your knowledge chapter-by-chapter. You’ll start with declarative macros before diving into the real power: procedural macros that can generate code, augment data structures, and even create domain-specific languages.},
  keywords={macros;meta programming;declarative;procedural;newtypes;zero-sized types;safety;domain-specific language;reliable;advanced;share;compile-time;mini-DSLs;instructions;debug;crates;public fields;integration},
  doi={},
  ISSN={},
  publisher={Manning},
  isbn={9781633437494},
  url={https://ieeexplore.ieee.org/document/10745328},}

@INPROCEEDINGS{7102598,
  author={Alegroth, Emil and Bache, Geoffrey and Bache, Emily},
  booktitle={2015 IEEE 8th International Conference on Software Testing, Verification and Validation (ICST)}, 
  title={On the Industrial Applicability of TextTest: An Empirical Case Study}, 
  year={2015},
  volume={},
  number={},
  pages={1-10},
  abstract={Software systems are becoming more complex, not least in their Graphical User Interfaces (GUIs), which presents challenges for existing testing practices. Pressure to reduce time to market leaves less time for manual testing and increases the importance of test automation. Previous research has identified several generations of automated GUI-based test approaches with different cost-benefit tradeoffs. Whilst test automation provides fast quality feedback it can be associated with high costs and inability to identify defects not explicitly anticipated by the test designer. TextTest is a capture-replay tool for GUI-based testing with a novel approach that overcomes several of the challenges experienced with previous approaches. Firstly the tool supports Approval Testing, an approach where ASCII-art representations of the GUI's visual state are used to verify correct application behavior at the system level. Secondly it records and replays test scripts in a user defined domain specific language (DSL) that is readable by all stakeholders. In this paper we present a three phase industrial case study that aims to identify TextTest's applicability in industrial practice. The paper reports that the tool is associated with (1) low script development costs due to recording functionality, (2) low maintenance costs, on average 7 minutes per test case, (3) better defect finding ability than manual system testing, (4) high test case execution performance (In this case 500 test cases in 20 minutes), (5) high script readability due to DSL defined scripts, and (6) test suites that are robust to change (In this case 93 percent per iteration). However, the tool requires a higher degree of technical skill for customization work, test maintainers need skills in designing regular expressions and the tool's applicability is currently restricted to Java and Python based applications.},
  keywords={Graphical user interfaces;Testing;Maintenance engineering;DSL;Companies;Manuals;Data collection},
  doi={10.1109/ICST.2015.7102598},
  ISSN={2159-4848},
  month={April},}

@INPROCEEDINGS{1158072,
  author={Saha, A. and Mishra, V. and Saxena, P. and Mundhada, R. and Katiyar, K. and Kumar, S.},
  booktitle={15th Annual IEEE International ASIC/SOC Conference}, 
  title={Design of broadband controller for residential gateway applications}, 
  year={2002},
  volume={},
  number={},
  pages={283-287},
  abstract={As Internet technology becomes more pervasive, homes are getting connected to cable or DSL. The increasing user demands for "always on" service along with multiple connectivity for voice and data is gradually making the presence of a residential gateway in every household a reality. A residential gateway should have routing and bridging capabilities along with seamless connectivity to contemporary premise networking technologies. Integration of all these features on a single device essentially requires a rich architecture, smart design techniques and thorough verification. This paper describes the architecture and design of a broadband network controller which is a critical component of TI's voice and data centric residential gateway solution.},
  keywords={Internet;DSL;Broadband communication;Ethernet networks;Universal Serial Bus;Control systems;Instruments;Routing;Home automation;Transceivers},
  doi={10.1109/ASIC.2002.1158072},
  ISSN={},
  month={Sep.},}

@INPROCEEDINGS{8411756,
  author={Elodie, Bernard and Fabrice, Ambert and Bruno, Legeard and Arnaud, Bouzy},
  booktitle={2018 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)}, 
  title={Lightweight Model-Based Testing for Enterprise IT}, 
  year={2018},
  volume={},
  number={},
  pages={224-230},
  abstract={Model-Based Testing (MBT) popularity in IT is growing at a very slow pace. A recent survey stated that no more than 14% of respondents use MBT in their projects. Our experience, presented in this paper, demonstrates that the complexity in use of the current MBT approaches for the average tester is the main reason for this low dissemination. Then we introduce a lightweight MBT approach and a tool, called Yest, dedicated to business process-based testing of enterprise information systems. This tool uses a workflow-based graphical representation linked with decision tables to be used by functional testers without requiring any kind of modeling skill (such as UML for example). These approach and tool are dedicated to a particular class of applications (i.e. enterprise IT applications such as ERP and bespoke business applications). This focus strongly helps to simplify the approach and to adapt the tooling to the targeted users (namely IT functional testers). Finally, we discuss the way MBT may support emerging Acceptance Test Driven Development practices in agile.},
  keywords={Tools;Testing;Unified modeling language;Task analysis;Password;Electronic mail;Model-based-testing;Lightweight MBT;MBT tool;Test cases generation;Business process-based testing},
  doi={10.1109/ICSTW.2018.00053},
  ISSN={},
  month={April},}

@INPROCEEDINGS{7102636,
  author={Herbold, Steffen and De Francesco, Alberto and Grabowski, Jens and Harms, Patrick and Hillah, Lom M. and Kordon, Fabrice and Maesano, Ariele-Paolo and Maesano, Libero and Di Napoli, Claudia and De Rosa, Fabio and Schneider, Martin A. and Tonellotto, Nicola and Wendland, Marc-Florian and Wuillemin, Pierre-Henri},
  booktitle={2015 IEEE 8th International Conference on Software Testing, Verification and Validation (ICST)}, 
  title={The MIDAS Cloud Platform for Testing SOA Applications}, 
  year={2015},
  volume={},
  number={},
  pages={1-8},
  abstract={While Service Oriented Architectures (SOAs) are for many parts deployed online, and today often in a cloud, the testing of the systems still happens mostly locally. In this paper, we want to present the MIDAS Testing as a Service (TaaS), a cloud platform for the testing of SOAs. We focus on the testing of whole SOA orchestrations, a complex task due to the number of potential service interactions and the increasing complexity with each service that joins an orchestration. Since traditional testing does not scale well with such a complex setup, we employ a Model-based Testing (MBT) approach based on the Unified Modeling Language (UML) and the UML Testing Profile (UTP) within MIDAS. Through this, we provide methods for functional testing, security testing, and usage-based testing of service orchestrations. Through harnessing the computational power of the cloud, MIDAS is able to generate and execute complex test scenarios which would be infeasible to run in a local environment.},
  keywords={Testing;Unified modeling language;DSL;Cloud computing;Service-oriented architecture;Monitoring},
  doi={10.1109/ICST.2015.7102636},
  ISSN={2159-4848},
  month={April},}

@INPROCEEDINGS{5974321,
  author={Prasetya, I.S.W.B. and Amorim, J. and Vos, T.E.J. and Baars, A.},
  booktitle={6th Iberian Conference on Information Systems and Technologies (CISTI 2011)}, 
  title={Using Haskell to script combinatoric testing of Web Services}, 
  year={2011},
  volume={},
  number={},
  pages={1-6},
  abstract={The Classification Tree Method (CTM) is a popular approach in functional testing as it allows the testers to systematically partition the input domain of an SUT, and specifies the combinations they want. We have implemented the approach as a small domain specific language (DSL) embedded in the functional language Haskell. Such an embedding leads to clean syntax and moreover we can natively access Haskell's full features. This paper will explain the approach, and how it is applied for testing Web Services.},
  keywords={Cities and towns;Strips;automated testing;combinatoric testing},
  doi={},
  ISSN={2166-0735},
  month={June},}

@INPROCEEDINGS{9153386,
  author={Cotroneo, Domenico and De Simone, Luigi and Liguori, Pietro and Natella, Roberto},
  booktitle={2020 50th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN)}, 
  title={ProFIPy: Programmable Software Fault Injection as-a-Service}, 
  year={2020},
  volume={},
  number={},
  pages={364-372},
  abstract={In this paper, we present a new fault injection tool (ProFIPy) for Python software. The tool is designed to be programmable, in order to enable users to specify their software fault model, using a domain-specific language (DSL) for fault injection. Moreover, to achieve better usability, ProFIPy is provided as software-as-a-service and supports the user through the configuration of the faultload and workload, failure data analysis, and full automation of the experiments using container- based virtualization and parallelization.},
  keywords={Tools;Computer bugs;Python;DSL;Pattern matching;Open source software;Software Fault Injection;Python;Software-as-a-Service;Bug Pattern},
  doi={10.1109/DSN48063.2020.00052},
  ISSN={1530-0889},
  month={June},}

@ARTICLE{10460136,
  author={Hwang, Sungjae and Lee, Sungho and Ryu, Sukyoung},
  journal={IEEE Transactions on Software Engineering}, 
  title={An Empirical Study of JVMs’ Behaviors on Erroneous JNI Interoperations}, 
  year={2024},
  volume={50},
  number={4},
  pages={979-994},
  abstract={Java Native Interface (JNI) allows Java applications to access native libraries, but it is challenging to develop correct JNI programs. By leveraging native code, the JNI enables Java developers to implement efficient applications and reuse code written in other programming languages such as C and C++. The core Java libraries use the JNI to provide system features like graphical user interfaces, and mainstream Java Virtual Machines (JVMs) support the JNI. However, implementing correct JNI programs is not trivial due to the complex interoperation semantics between different programming languages. While JVMs do not validate JNI interoperations by default because of the performance overhead, they provide two methods. First, JVMs report the interoperation failures defined in the JNI specification at runtime. Second, they support a debug option, which validates JNI interoperations, degrading the runtime performance. To the best of our knowledge, literature has not thoroughly studied the quality of JVMs’ methods, even though erroneous JNI interoperations may result in incorrect behaviors. In this paper, we empirically study the behaviors of JVMs on erroneous JNI interoperations. For a systematic study, we propose JUSTGen, a semi-automatic tool that generates JNI test programs incurring erroneous interoperations from the JNI specification. JUSTGen receives the JNI specification written in our domain-specific language (DSL) and automatically discovers cases that may lead to runtime errors on interoperations using an SMT solver. It then generates test programs that trigger the behaviors on the erroneous cases. Using the generated tests, we empirically evaluate JVM's failure handling mechanisms and the debug option capabilities on erroneous JNI interoperations. Our experiment results show that there exist erroneous cases in which JVMs do not handle failures or handle them differently from the specification. We also found that the JNI debug option does not validate thousands of erroneous cases, which can cause critical runtime errors such as memory corruption and violation of the Java type system. We reported 18 erroneous cases of which JVMs do not handle failures correctly to their respective vendors. Among them, 16 cases have been resolved.},
  keywords={Java;Codes;Behavioral sciences;Semantics;Runtime;Virtual machining;DSL;Java native interface;Java virtual machine;testing;empirical study;debugging},
  doi={10.1109/TSE.2024.3373239},
  ISSN={1939-3520},
  month={April},}

@ARTICLE{10998949,
  author={Deng, Yao and Tu, Zhi and Yao, Jiaohong and Zhang, Mengshi and Zhang, Tianyi and Zheng, Xi},
  journal={IEEE Transactions on Software Engineering}, 
  title={TARGET: Traffic Rule-Based Test Generation for Autonomous Driving via Validated LLM-Guided Knowledge Extraction}, 
  year={2025},
  volume={51},
  number={7},
  pages={1950-1968},
  abstract={Recent incidents with autonomous vehicles highlight the need for rigorous testing to ensure safety and robustness. Constructing test scenarios for autonomous driving systems (ADSs), however, is labor-intensive. We propose TARGET, an end-to-end framework that automatically generates test scenarios from traffic rules. To address complexity, we leverage a Large Language Model (LLM) to extract knowledge from traffic rules. To mitigate hallucinations caused by large context during input processing, we introduce a domain-specific language (DSL) designed to be syntactically simple and compositional. This design allows the LLM to learn and generate test scenarios in a modular manner while enabling syntactic and semantic validation for each component. Based on these validated representations, TARGET synthesizes executable scripts to render scenarios in simulation. Evaluated seven ADSs with 284 scenarios derived from 54 traffic rules, TARGET uncovered 610 rule violations, collisions, and other issues. For each violation, TARGET generates scenario recordings and detailed logs, aiding root cause analysis. Two identified issues were confirmed by ADS developers: one linked to an existing bug report and the other to limited ADS functionality.},
  keywords={DSL;Testing;Roads;Syntactics;Scenario generation;Autonomous vehicles;Test pattern generators;Meteorology;Data mining;Semantics;Test generation;autonomous driving system testing;scenario-based testing;LLM},
  doi={10.1109/TSE.2025.3569086},
  ISSN={1939-3520},
  month={July},}

@INPROCEEDINGS{7101658,
  author={McCormick, Patrick and Sweeney, Christine and Moss, Nick and Prichard, Dean and Gutierrez, Samuel K. and Davis, Kei and Mohd-Yusof, Jamaludin},
  booktitle={2014 Fourth International Workshop on Domain-Specific Languages and High-Level Frameworks for High Performance Computing}, 
  title={Exploring the Construction of a Domain-Aware Toolchain for High-Performance Computing}, 
  year={2014},
  volume={},
  number={},
  pages={1-10},
  abstract={The push towards exascale computing has sparked a new set of explorations for providing new productive programming environments. While many efforts are focusing on the design and development of domain-specific languages (DSLs), few have addressed the need for providing a fully domain-aware toolchain. Without such domain awareness critical features for achieving acceptance and adoption, such as debugger support, pose a long-term risk to the overall success of the DSL approach. In this paper we explore the use of language extensions to design and implement the Scout DSL and a supporting toolchain infrastructure. We highlight how language features and the software design methodologies used within the toolchain play a significant role in providing a suitable environment for DSL development.},
  keywords={DSL;Runtime;Computer architecture;Syntactics;Graphics processing units;Image color analysis},
  doi={10.1109/WOLFHPC.2014.9},
  ISSN={},
  month={Nov},}

@INPROCEEDINGS{7321521,
  author={Schuts, Mathijs and Hooman, Jozef},
  booktitle={2015 Federated Conference on Computer Science and Information Systems (FedCSIS)}, 
  title={Using Domain Specific Languages to improve the development of a power control unit}, 
  year={2015},
  volume={},
  number={},
  pages={781-788},
  abstract={To improve the design of a power control unit at Philips, two Domain Specific Languages (DSLs) have been used. The first DSL provides a concise and readable notation for the essential state transitions. It is used to generate both configuration files and analysis models. In addition, we also generate instances of a second DSL which represents test traces. This second DSL is used to generate test cases for the power control unit. The use of DSLs not only improved productivity, but also the quality of the configuration files and the test set.},
  keywords={DSL;Power control;Hardware;Generators;X-ray imaging;Software;Voltage control},
  doi={10.15439/2015F46},
  ISSN={},
  month={Sep.},}

@ARTICLE{1519613,
  author={Young-Jae Cho and Seung-Hoon Lee},
  journal={IEEE Transactions on Circuits and Systems I: Regular Papers}, 
  title={An 11b 70-MHz 1.2-mm/sup 2/ 49-mW 0.18-/spl mu/m CMOS ADC with on-chip current/voltage references}, 
  year={2005},
  volume={52},
  number={10},
  pages={1989-1995},
  abstract={This work proposes an 11b 70-MHz CMOS pipelined analog-digital converter (ADC) as one of core circuit blocks for very high speed digital subscriber line system applications. The proposed ADC for the internal use has the strictly limited number of externally connected I/O pins while the ADC employs on-chip CMOS current/voltage references and a merged-capacitor switching technique to improve ADC performances. The ADC implemented in a 0.18-/spl mu/m 1P4M CMOS technology shows the maximum signal-to-noise distortion ratio (SNDR) of 60 dB at 70 MSample/s. The ADC maintains the SNDR of 58 dB and the spurious-free dynamic resistance of 68 dB for input frequencies up to the Nyquist rate at 60 MSample/s. The measured differential and integral nonlinearities of the ADC are within /spl plusmn/0.63 and /spl plusmn/1.21 LSB, respectively. The active chip area is 1.2 mm/sup 2/ and the ADC consumes 49 mW at 70 MSample/s at 1.8 V.},
  keywords={Analog-digital conversion;CMOS technology;CMOS digital integrated circuits;CMOS analog integrated circuits;DSL;Pins;Voltage;Frequency;Distortion measurement;Electrical resistance measurement;Analog–digital converter (ADC);CMOS;low power;on-chip references},
  doi={10.1109/TCSI.2005.853251},
  ISSN={1558-0806},
  month={Oct},}

@INPROCEEDINGS{7203010,
  author={Abreu, Rui and Erdogmus, Hakan and Perez, Alexandre},
  booktitle={2015 IEEE/ACM 37th IEEE International Conference on Software Engineering}, 
  title={CodeAware: Sensor-Based Fine-Grained Monitoring and Management of Software Artifacts}, 
  year={2015},
  volume={2},
  number={},
  pages={551-554},
  abstract={Current continuous integration (CI) tools, although extensible, can be limiting in terms of flexibility. In particular, artifact analysis capabilities available through plug in mechanisms are both coarse-grained and centralized. To address this limitation, this paper introduces a new paradigm, Code Aware, for distributed and fine-grained artifact analysis. Code Aware is an ecosystem inspired by sensor networks, consisting of monitors and actuators, aimed at improving code quality and team productivity. Code ware's vision entails (a) the ability to probe software artifacts of any granularity and localization, from variables to classes or files to entire systems, (b) the ability to perform both static and dynamic analyses on these artifacts, and (c) the ability to describe targeted remediation actions, for example to notify interested developers, through automated actuators. We provide motivational examples for the use of Code Aware that leverage current CI solutions, sketch the architecture of its underlying ecosystem, and outline research challenges.},
  keywords={Probes;Software;Ecosystems;Software engineering;Monitoring;Electronic mail;DSL},
  doi={10.1109/ICSE.2015.192},
  ISSN={1558-1225},
  month={May},}

@INPROCEEDINGS{1253823,
  author={Brini, S. and Benjelloun, D. and Castanier, F.},
  booktitle={2003 Design, Automation and Test in Europe Conference and Exhibition}, 
  title={A flexible virtual platform for computational and communication architecture exploration of DMT VDSL modems}, 
  year={2003},
  volume={},
  number={},
  pages={164-169},
  abstract={},
  keywords={Computer architecture;OFDM modulation;Modems;Transceivers;DSL;System-level design;Digital signal processing;Quality of service;Testing;Libraries},
  doi={10.1109/DATE.2003.1253823},
  ISSN={1530-1591},
  month={March},}

@INPROCEEDINGS{8804458,
  author={Giorgi, Fabio and Paulisch, Frances},
  booktitle={2019 IEEE/ACM 41st International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP)}, 
  title={Transition Towards Continuous Delivery in the Healthcare Domain}, 
  year={2019},
  volume={},
  number={},
  pages={253-254},
  abstract={Continuous Delivery is meanwhile well-established in many parts of the software industry. In a transition towards continuous delivery in the healthcare domain, there are a number of additional challenges that should be addressed. We present how we have addressed some of these challenges and highlight some potential research topics that could be addressed in this space to make further progress in this important area. Although our focus is on the healthcare domain, the approach and the research topics are applicable also to a broad range of other application domains.},
  keywords={Software;Medical services;Automation;Industries;Documentation;Organizations;DSL;continuous delivery, agile, test-driven development, behavior-driven development, domain-driven design, test automation, pair-programming, deployment pipeline},
  doi={10.1109/ICSE-SEIP.2019.00035},
  ISSN={},
  month={May},}

@INPROCEEDINGS{1186689,
  author={Brini, S. and Benjelloun, D. and Castanier, F.},
  booktitle={2003 Design, Automation and Test in Europe Conference and Exhibition}, 
  title={A flexible virtual platform for computational and communication architecture exploration of DMT VDSL modems}, 
  year={2003},
  volume={},
  number={},
  pages={164-169 suppl.},
  abstract={In this paper a high-level SoC architecture exploration of DMT (Discrete Multitone) VDSL transceivers (Very high speed Digital Subscriber Line) is presented. A flexible and complete virtual platform was developed for the purpose, exploiting the paradigm of "orthogonalization of concerns" (functionality independent from architecture) in the framework of Cadence VCC system level design tool. An accurate processor model, obtained through the back-annotation of profiling results on a target DSP core, allowed the exploration of different HW/SW partitioning and the study of the computational units required. A transaction-accurate VCC bus model was developed for the investigation of the on-chip bus architecture and its relevant parameters dimensioning.},
  keywords={Computer architecture;OFDM modulation;Modems;Transceivers;DSL;System-level design;Digital signal processing;Quality of service;Testing;Libraries},
  doi={10.1109/DATE.2003.1186689},
  ISSN={1530-1591},
  month={March},}

@INPROCEEDINGS{6104710,
  author={Liu, Jianqing and Liu, Guangyong},
  booktitle={2011 4th International Conference on Intelligent Networks and Intelligent Systems}, 
  title={Research and Implementation of SNMP-Based Network Management System}, 
  year={2011},
  volume={},
  number={},
  pages={129-132},
  abstract={After the brief introduction of the current situation of matured system framework based on SNMP, the basic problems of network management framework design are presented in detail. This paper focuses on the construction of SNMP-based platform in terms of software framework. Finally, by the use of the medium-sized data exchange network, it achieves the function of discovering the topology.},
  keywords={Topology;Software;Programming;Testing;Synchronization;DSL;Scalability;Network management framework;Auto-Topology control;Scalability;Generality},
  doi={10.1109/ICINIS.2011.39},
  ISSN={},
  month={Nov},}

@INPROCEEDINGS{766632,
  author={Blome, F. and Hamich, A.},
  booktitle={International Symposium on Switching}, 
  title={24 months of commercial isdn experience}, 
  year={1990},
  volume={2},
  number={},
  pages={159-164},
  abstract={},
  keywords={ISDN;Switches;Telephony;Packet switching;DSL;Telecommunications;Circuits;Roads;Testing;Cities and towns},
  doi={10.1109/ISS.1990.766632},
  ISSN={},
  month={May},}

@INPROCEEDINGS{410732,
  author={Kelly, D. and Hartmann, Q. and Gude, W.},
  booktitle={Sixth Annual IEEE International ASIC Conference and Exhibit}, 
  title={A multi-FPGA prototype of a DS1/HDSL synchronizer and desynchronizer prior to ASIC fabrication}, 
  year={1993},
  volume={},
  number={},
  pages={332-335},
  abstract={As the speed and complexity of today's ASICs continues to grow, conventional prototyping techniques for algorithm verification begin to break down. A novel implementation of DS1/HDSL synchronizer and desynchronizer utilizing an array of four FPGA devices to verify algorithm performances prior to ASIC fabrication is described. The utilization of FPGA devices for ASIC prototyping can significantly reduce the risk, cost, and time-to-market involved with complex ASIC devices.<>},
  keywords={Prototypes;Application specific integrated circuits;Fabrication;Timing;Field programmable gate arrays;Jitter;Frequency synchronization;Costs;DSL;Testing},
  doi={10.1109/ASIC.1993.410732},
  ISSN={},
  month={Sep.},}

@INPROCEEDINGS{713476,
  author={Daou, F.H.},
  booktitle={1998 IEEE AUTOTESTCON Proceedings. IEEE Systems Readiness Technology Conference. Test Technology for the 21st Century (Cat. No.98CH36179)}, 
  title={Overview of ADSL test requirement towards conformance, performance and interoperability}, 
  year={1998},
  volume={},
  number={},
  pages={413-420},
  abstract={The enormous installed base of copper in the access network with the right transmission techniques present a huge potential for delivering broadband services to bandwidth hungry customers. Various Digital Subscriber line technologies (xDSL) employ various transmission methods and efficiently utilize the last available bandwidth on existing copper wires. Asymmetric Digital Subscriber Line (ADSL) delivers up to 6 Mbps to the user. This transmission of 6 Mbps is achieved using sophisticated modulation and compression techniques in a spectrum up to 1.1 MHz, pushing the physical limit on the usable bandwidth in the copper. This technology co-exists with impairments, noise intrusions, bridge taps, and other non-spectrally compatible transmissions. This paper presents an overview of xDSL Technology (which includes ADSL), the test challenges facing ADSL technology, and outlines the three areas of ADSL tests needed to ensure product conformance and cross vendor interoperability.},
  keywords={Testing;Copper;Bandwidth;Bit rate;OFDM modulation;Quadrature amplitude modulation;DSL;Modulation coding;Working environment noise;Phase modulation},
  doi={10.1109/AUTEST.1998.713476},
  ISSN={1088-7725},
  month={Aug},}

@INPROCEEDINGS{1689533,
  author={Dae-Woo Kim and Hyun-Min Lim and Sang-Kon Lee},
  booktitle={2006 IEEE International Symposium on Consumer Electronics}, 
  title={A Case Study on Testing and Evaluation in the KT-OSS Development}, 
  year={2006},
  volume={},
  number={},
  pages={1-6},
  abstract={This paper describes the test and evaluation activities for the development of the KT-OSS (Korea Telecom Operations Support System). In this paper, we show the test and evaluation phases for the development and maintenance of the KT-OSS. To ensure the successful development of the KT-OSS, we performed various tests related to functionality, efficiency and others. We also show the criteria for them and deal with the test organizations and the test-bed for managing and controlling the quality of the KT-OSS. And we describe our experiences in performing these tests. Through these test and evaluation activities, we were able to successfully develop and release the KT-OSS},
  keywords={Computer aided software engineering;Life testing;Software testing;Programming;System testing;Performance evaluation;Quality management;Telecommunications;Information management;DSL;Test;Evaluation;Operations Support System;Software Development Lifecycle},
  doi={10.1109/ISCE.2006.1689533},
  ISSN={2159-1423},
  month={June},}

@INPROCEEDINGS{5479285,
  author={Guo-Ming Sung and Yen-Tang Chang and Wen-Huei Chen and Hsiang-Yuan Hsieh},
  booktitle={2010 International Conference on Networking and Digital Society}, 
  title={A new architecture of broadband network system suitable for asymmetric digital subscriber line application}, 
  year={2010},
  volume={1},
  number={},
  pages={624-628},
  abstract={This paper presents the design and implementation on a new architecture of broadband network system which suitable for asymmetric digital subscriber line (ADSL) application. The main design skill is based on the cell-based digital IC design process, and is implemented in 0.18 μm 1P6M CMOS process. The main function of this chip is to build a bridge between Ethernet and ATM which is used to substitute for RISC processor, leading to enhance the broadband network switching ability and stability. Furthermore, the clock management system is adopted to manage the packages. By this technique, a small size and low cost chip will be obtained.},
  keywords={Broadband communication;DSL;CMOS integrated circuits;CMOS digital integrated circuits;Process design;CMOS process;Bridges;Ethernet networks;Asynchronous transfer mode;Reduced instruction set computing;component;Asynchronous transfer mode (ATM);asymmetric digital subscriber line (ADSL);Broadband Network},
  doi={10.1109/ICNDS.2010.5479285},
  ISSN={},
  month={May},}

@INPROCEEDINGS{916745,
  author={Lightfoot, R.S.},
  booktitle={Proceedings of the 2000 IEEE International Conference on Management of Innovation and Technology. ICMIT 2000. 'Management in the 21st Century' (Cat. No.00EX457)}, 
  title={Establishing long-term viability in the information economy: the strength of the integrated systems engineering process}, 
  year={2000},
  volume={2},
  number={},
  pages={526-529 vol.2},
  abstract={Information technology companies are under increasing pressure to develop high technology products and services at an accelerated pace. They are often rewarded for such efforts through increases in their stock prices only to see these gains drop due to poor quality, failed technology, and poor customer support. In considering these events the question that comes to mind is, "What is the formula for providing long term viability in the high-tech information economy?" This paper presents recommendations that can be applied to the integrated systems/software engineering process which, when implemented, will provide companies with the formula for long-term viability in this rapidly changing fast-paced environment.},
  keywords={Information technology;Companies;Systems engineering and theory;Stock markets;IEEE news;Educational institutions;Acceleration;Software engineering;DSL;Modems},
  doi={10.1109/ICMIT.2000.916745},
  ISSN={},
  month={Nov},}

@INPROCEEDINGS{739853,
  author={Pingree, P.J.},
  booktitle={17th DASC. AIAA/IEEE/SAE. Digital Avionics Systems Conference. Proceedings (Cat. No.98CH36267)}, 
  title={Deep space one integration and test challenges: getting to the launch pad in the faster, better, cheaper world}, 
  year={1998},
  volume={2},
  number={},
  pages={H21/1-H21/8 vol.2},
  abstract={This paper describes the integration and test challenges of verifying and validating the avionics hardware and flight software which have been experienced in meeting the New Millennium Program Deep Space One (DS1) project's faster, better, cheaper requirements. This paper gives a high level overview of the development and application of the two flight system testbeds (DSI Hotbench), the testbed activities supported the DSI Spacecraft Assembly, Test and Launch Operations (ATLO), and the testing performed to prepare for and support post-launch mission operations. In the "Faster, Better, Cheaper" environment of the New Millennium Program, DSI Integration and Test (I&T) has defined new methods and decision criteria to meet our requirements and goals while assessing and minimizing the risk in the paths we have taken. Our successes and failures are largely yet to be seen as we approach our July 1 launch date. This paper describes the challenges that have been faced and some that have been overcome during the DSI I&T phase. It presents the risks that have been accepted in our attempts to test completely the DSI Avionics system in preparation for launch and mission operations.},
  keywords={Atherosclerosis;Aerospace electronics;Space technology;Electronic equipment testing;Software testing;System testing;Hardware;Space vehicles;DSL;Space missions},
  doi={10.1109/DASC.1998.739853},
  ISSN={},
  month={Oct},}

@INPROCEEDINGS{6210038,
  author={Bianchin, Carlos Gabriel and Demonti, Rogers and de Almeida, André Rubens and da Silva Filho, Matheus T. and da Silva Pinto, Cleverson L.},
  booktitle={2012 IEEE International Conference on Industrial Technology}, 
  title={Development of static switch with high speed algorithm to fault detection}, 
  year={2012},
  volume={},
  number={},
  pages={808-814},
  abstract={This paper presents the updated results on research for development of a static switch for operation in medium voltage. It shows the results of the algorithm that detects an outage and enables the digital processing to control the static switch. This algorithm was based on the Clarke and Park transforms. The operation of the switch consists of transferring the power supply from the main source, where an outage occurred, to an alternative power source (backup). The load stays off for a time of milliseconds. A low voltage prototype was built, to allow the evaluation of the algorithm. Then a prototype of the static switch was built in voltage of 13,8kV. The first tests - feeding resistive load - are presented in voltage of 13.8 kV.},
  keywords={DSL;Switches;algorithm;digital processing;series-connected thyristors;medium voltage},
  doi={10.1109/ICIT.2012.6210038},
  ISSN={},
  month={March},}

@INPROCEEDINGS{9576969,
  author={Ngwenya, Sikhumbuzo and Shebeshi, Zelalem and Terzoli, Alfredo},
  booktitle={2021 IST-Africa Conference (IST-Africa)}, 
  title={Implementing a Content-Based Routing Framework for Application Integration on to Teleweaver Application Server}, 
  year={2021},
  volume={},
  number={},
  pages={1-9},
  abstract={This paper presents an architectural overview of content-based dynamic routing for integrating applications on to an application server named TeleWeaver, a middleware platform developed within Siyakhula Living Lab (SLL). SLL is an ICT4D project in the Eastern Cape Province of South Africa. TeleWeaver was created as a mediation layer between software systems developed for use by beneficiaries of the Siyakhula Living Lab. The main challenge with these disparate systems was that they had unnecessary, redundant components; TeleWeaver acts as a common platform that suits the development of many services such as eGovernment, eHealth, and eJudiciary.},
  keywords={Java;XML;Routing;Software systems;Routing protocols;Electronic healthcare;DSL;Application Integration;Dynamic Routing;Teleweaver.},
  doi={},
  ISSN={2576-8581},
  month={May},}

@ARTICLE{10942340,
  author={Marian Pasca, Emil and Delinschi, Daniela and Erdei, Rudolf and Matei, Oliviu},
  journal={IEEE Access}, 
  title={LLM-Driven, Self-Improving Framework for Security Test Automation: Leveraging Karate DSL for Augmented API Resilience}, 
  year={2025},
  volume={13},
  number={},
  pages={56861-56886},
  abstract={Modern software architectures heavily rely on APIs, yet face significant security challenges, particularly with Broken Object Level Authorization (BOLA) vulnerabilities, which remain the most critical API security risk according to OWASP. This paper introduces Karate-BOLA-Guard, an innovative framework leveraging Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) techniques to automate security-focused test case generation for APIs. Our approach integrates vector databases for context retrieval, multiple LLM models for test generation, and observability tools for process monitoring. Initial experiments were carried out on three deliberately vulnerable APIs (VAmPI, Crapi, and OWASP Juice Shop), with subsequent validation on fifteen additional production APIs spanning diverse domains including social media, version control systems, financial services, and transportation services. Our evaluation metrics show Llama 3 8B achieving consistent performance (Accuracy: 3.1-3.4, Interoperability: 3.7-4.3) with an average processing time of 143.76 seconds on GPU. Performance analysis revealed significant GPU acceleration benefits, with 20-25x improvement over CPU processing times. Smaller models demonstrated efficient processing, with Phi-3 Mini averaging 69.58 seconds and Mistral 72.14 seconds, while maintaining acceptable accuracy scores. Token utilization patterns showed Llama 3 8B using an average of 36,591 tokens per session, compared to Mistral’s 25,225 and Phi-3 Mini’s 31,007. Our framework’s effectiveness varied across APIs, with notably strong performance in complex platforms (Instagram: A = 4.3, I = 4.4) while maintaining consistent functionality in simpler implementations (VAmPI: A = 3.6, I = 4.3). The iterative refinement process, evaluated through comprehensive metrics including Accuracy (A), Complexity (C), and Interoperability (I), represents a significant advancement in automated API security testing, offering an efficient, accurate, and adaptable approach to detecting BOLA vulnerabilities across diverse API architectures.},
  keywords={Security;Testing;Retrieval augmented generation;Test pattern generators;Application programming interfaces;Accuracy;Software testing;Automation;Systematics;Computer architecture;API security;automation testing tools;cybersecurity;restful API;software testing},
  doi={10.1109/ACCESS.2025.3554960},
  ISSN={2169-3536},
  month={},}

@ARTICLE{5441292,
  author={Amatriain, Xavier and Arumi, Pau},
  journal={IEEE Transactions on Software Engineering}, 
  title={Frameworks Generate Domain-Specific Languages: A Case Study in the Multimedia Domain}, 
  year={2011},
  volume={37},
  number={4},
  pages={544-558},
  abstract={We present an approach to software framework development that includes the generation of domain-specific languages (DSLs) and pattern languages as goals for the process. Our model is made of three workflows-framework, metamodel, and patterns-and three phases-inception, construction, and formalization. The main conclusion is that when developing a framework, we can produce with minimal overhead-almost as a side effect-a metamodel with an associated DSL and a pattern language. Both outputs will not only help the framework evolve in the right direction, but will also be valuable in themselves. In order to illustrate these ideas, we present a case study in the multimedia domain. For several years, we have been developing a multimedia framework. The process has produced a full-fledged domain-specific metamodel for the multimedia domain, with an associated DSL and a pattern language.},
  keywords={Domain specific languages;DSL;Unified modeling language;Vocabulary;Concrete;Software engineering;Computer aided software engineering;Natural languages;Metamodeling;Best practices;Domain-specific architectures;visual programming;life cycle;CASE.},
  doi={10.1109/TSE.2010.48},
  ISSN={1939-3520},
  month={July},}

@ARTICLE{10089522,
  author={Chaleshtari, Nazanin Bayati and Pastore, Fabrizio and Goknil, Arda and Briand, Lionel C.},
  journal={IEEE Transactions on Software Engineering}, 
  title={Metamorphic Testing for Web System Security}, 
  year={2023},
  volume={49},
  number={6},
  pages={3430-3471},
  abstract={Security testing aims at verifying that the software meets its security properties. In modern Web systems, however, this often entails the verification of the outputs generated when exercising the system with a very large set of inputs. Full automation is thus required to lower costs and increase the effectiveness of security testing. Unfortunately, to achieve such automation, in addition to strategies for automatically deriving test inputs, we need to address the oracle problem, which refers to the challenge, given an input for a system, of distinguishing correct from incorrect behavior (e.g., the response to be received after a specific HTTP GET request). In this paper, we propose Metamorphic Security Testing for Web-interactions (MST-wi), a metamorphic testing approach that integrates test input generation strategies inspired by mutational fuzzing and alleviates the oracle problem in security testing. It enables engineers to specify metamorphic relations (MRs) that capture many security properties of Web systems. To facilitate the specification of such MRs, we provide a domain-specific language accompanied by an Eclipse editor. MST-wi automatically collects the input data and transforms the MRs into executable Java code to automatically perform security testing. It automatically tests Web systems to detect vulnerabilities based on the relations and collected data. We provide a catalog of 76 system-agnostic MRs to automate security testing in Web systems. It covers 39% of the OWASP security testing activities not automated by state-of-the-art techniques; further, our MRs can automatically discover 102 different types of vulnerabilities, which correspond to 45% of the vulnerabilities due to violations of security design principles according to the MITRE CWE database. We also define guidelines that enable test engineers to improve the testability of the system under test with respect to our approach. We evaluated MST-wi effectiveness and scalability with two well-known Web systems (i.e., Jenkins and Joomla). It automatically detected 85% of their vulnerabilities and showed a high specificity (99.81% of the generated inputs do not lead to a false positive); our findings include a new security vulnerability detected in Jenkins. Finally, our results demonstrate that the approach scale, thus enabling automated security testing overnight.},
  keywords={Security;Testing;Uniform resource locators;Graphical user interfaces;DSL;Automation;Transforms;System security testing;metamorphic testing;domain-specific languages},
  doi={10.1109/TSE.2023.3256322},
  ISSN={1939-3520},
  month={June},}

@INPROCEEDINGS{7000030,
  author={Albuquerque, Marco Túlio C. F. and Ramalho, Geber Lisboa and Corruble, Vincent and Santos, André Luís Medeiros and Freitas, Fred},
  booktitle={2014 Brazilian Symposium on Computer Games and Digital Entertainment}, 
  title={Helping Developers to Look Deeper inside Game Sessions}, 
  year={2014},
  volume={},
  number={},
  pages={31-40},
  abstract={Game design and development activities are increasingly relying on the analysis of gamer's behavior and preferences data. Various tools are available to the developers to track and analyze general data concerning acquisition, retention and monetization aspects of game commercialization. This is good enough to give hints on where problems are, but not to enable a precise diagnosis, which demands fine-grained data. For this kind of data, there is not enough support or guidance to decide which data to capture, to write the code to capture it, to choose the best representation of it and to allow an adequate retrieval and presentation of it. This paper introduces GameGuts (GG), a framework devoted to give further assistance to developers in choosing, representing, accessing and presenting game sessions fine-grained data. As a case study, GG recorded sessions of a game platform with over a hundred thousand users. The logs were analyzed using a Visual Domain Specific Language (as a query language) and an ensemble of rules (as a compliance test). The results are encouraging, since we could - among other results - find bugs and catch cheaters, as well as spot design flaws.},
  keywords={Games;Ontologies;Visualization;Servers;DSL;Database languages;Measurement;game analytics;knowledge representation;game data mining},
  doi={10.1109/SBGAMES.2014.28},
  ISSN={2159-6662},
  month={Nov},}

@INPROCEEDINGS{7962331,
  author={Chiw, Charisee and Kindlmann, Gordon and Reppy, John},
  booktitle={2017 IEEE/ACM 12th International Workshop on Automation of Software Testing (AST)}, 
  title={DATm: Diderot's Automated Testing Model}, 
  year={2017},
  volume={},
  number={},
  pages={45-51},
  abstract={Diderot is a parallel domain-specific language forthe analysis and visualization of multidimensional scientific images, such as those produced by CT and MRI scanners. Diderot is designed to support algorithms that are based on differential tensor calculus and produces a higher-order mathematical model which allows direct manipulation of tensor fields. One of the main challenges of the Diderot implementation is bridging this semantic gap by effectively translating high-level mathematical notation of tensor calculus into efficient low-level code in the target language. A key question for a high-level language, such as Diderot, is how do we know that the implementation is correct. We have previously presented and defended a core set of rewriting rules, but the full translation from source to executable requires much more work. In this paper, we present DATm, Diderot's automated testing model to check the correctness of the core operations in the programming language. DATm can automatically create test programs, and predict what the outcome should be. We measure the accuracy of the computations written in the Diderot language, based on how accurately the output of the program represents the mathematical equivalent of the computations. This paper describes a model for testing a high-level language based on correctness. It introduces the pipeline for DATm, a tool that can automatically create and test tens of thousands of Diderot test programs and that has found numerous bugs. We make a case for the necessity of extensive testing by describing bugs that are deep in the compiler, and only could be found with a unique application of operations. Lastly, we demonstrate that the model can be used to create other types of tests by visual verification.},
  keywords={Testing;Tensile stress;Kernel;Shape;Computational modeling;Generators;Calculus;domain specific testing;Diderot;DSL;tensor calc;visual verificaiton},
  doi={10.1109/AST.2017.5},
  ISSN={},
  month={May},}

@INPROCEEDINGS{9103781,
  author={Liu, Huabin},
  booktitle={2020 International Conference on Computer Engineering and Application (ICCEA)}, 
  title={A Universal Automated Test Solution for Trunking Communication System}, 
  year={2020},
  volume={},
  number={},
  pages={47-51},
  abstract={In order to improve the portability of the automated test of the trunking communication system, this paper proposed a universal automated test solution for the trunking communication system. It's based on the general architecture design of the trunking communication system, providing a replaceable communication protocol codec module. With the DSL-defined test script language describing the test cases, and efficient scheduling schemes for the test task, the automated functional test and performance test of the trunking communication system are realized. Theoretical analysis and experimental results show that the minimum load test task scheduling scheme based on user operation load prediction has lower response time and lower load balancing effect compared with the traditional static task scheduling scheme.},
  keywords={Task analysis;Testing;Protocols;Codecs;Media;Computer architecture;component;trunking communication;automated test;test task scheduling},
  doi={10.1109/ICCEA50009.2020.00017},
  ISSN={},
  month={March},}

@ARTICLE{1094910,
  author={Ahamed, S. and Bohn, P. and Gottfried, N.},
  journal={IEEE Transactions on Communications}, 
  title={A Tutorial on Two-Wire Digital Transmission in the Loop Plant}, 
  year={1981},
  volume={29},
  number={11},
  pages={1554-1564},
  abstract={This paper explores the constraints on the design of twowire repeaterless digital subscriber loop (DSL) systems. Broadly categorized, the design depends on the technical feasibility of the approach used to achieve two-wire transmission, constraints related to compatibility with other systems sharing the same cable, and immunity to central office noise. Each of these varies With the choice of system parameters including the transmission rate, transmit power, choice of line codes, etc. Technical feasibility is evaluated by computer simulation studies. Compatibility with other systems is explored by crosstalk calculations. Noise immunity considerations, as they translate into digital line power levels, are also explored.},
  keywords={Tutorial;Crosstalk;DSL;Echo cancellers;Wire;Repeaters;Central office;Noise cancellation;Computer simulation;Noise level},
  doi={10.1109/TCOM.1981.1094910},
  ISSN={1558-0857},
  month={November},}

@INPROCEEDINGS{8906733,
  author={Juhnke, Katharina and Tichy, Matthias},
  booktitle={2019 45th Euromicro Conference on Software Engineering and Advanced Applications (SEAA)}, 
  title={A Tailored Domain Analysis Method for the Development of System-Specific Testing DSLs Enabling Their Smooth Introduction in Automotive Practice}, 
  year={2019},
  volume={},
  number={},
  pages={10-18},
  abstract={Automotive Test Case Specifications (TestSpecs) are a fundamental part of a structured test process in the automotive domain. For system and integration tests, acceptance and customer experience test cases are executed manually by human testers in a prototype vehicle. To ensure that these test cases are understood by humans, they are usually described in natural language, which often leads to ambiguities, misunderstandings, or incomplete test cases. In addition, the description of test cases vary significantly depending on the system to be tested and the respective test level. Test Designers want individual assistance in documenting their test cases with respect to system-specific characteristics, instead of using programming languages or standardized languages such as UML. Thus, Domain Specific Languages (DSLs) are a possible solution to satisfy this demand and to improve the quality of test cases, for example in terms of preciseness, uniformity, and completeness. The contribution of this paper is a systematic approach to support the development of system-specific automotive Testing DSLs that achieve high acceptance by test designers and testers. Therefore, we focus on the analysis phase in the DSL development process. We adapted domain analysis activities and defined a domain analysis method tailored to the analysis of automotive TestSpecs. We demonstrate the applicability of our method by means of five different automotive systems. Our evaluation shows that the derived system-specific Testing DSLs cover between 70% and 95% of the test steps contained in TestSpecs with only 11 to 35 conceptual templates. Moreover, a usability study with practitioners revealed a good usability of the Testing DSLs and the corresponding tool as well as that this eases the specification of test cases.},
  keywords={DSL;Testing;Automotive engineering;Natural languages;Switches;Unified modeling language;Usability;automotive software testing;natural language test cases;domain analysis;domain specific languages},
  doi={10.1109/SEAA.2019.00011},
  ISSN={},
  month={Aug},}

@INPROCEEDINGS{8115712,
  author={Meftah, Lakhdar and Gomez, Maria and Rouvoy, Romain and Chrisment, Isabelle},
  booktitle={2017 32nd IEEE/ACM International Conference on Automated Software Engineering (ASE)}, 
  title={ANDROFLEET: Testing WiFi peer-to-peer mobile apps in the large}, 
  year={2017},
  volume={},
  number={},
  pages={961-966},
  abstract={WiFi P2P allows mobile apps to connect to each other via WiFi without an intermediate access point. This communication mode is widely used by mobile apps to support interactions with one or more devices simultaneously. However, testing such P2P apps remains a challenge for app developers as i) existing testing frameworks lack support for WiFi P2P, and ii) WiFi P2P testing fails to scale when considering a deployment on more than two devices. In this paper, we therefore propose an acceptance testing framework, named Androfleet, to automate testing of WiFi P2P mobile apps at scale. Beyond the capability of testing point-to-point interactions under various conditions, An-drofleet supports the deployment and the emulation of a fleet of mobile devices as part of an alpha testing phase in order to assess the robustness of a WiFi P2P app once deployed in the field. To validate Androfleet, we demonstrate the detection of failing black-box acceptance tests for WiFi P2P apps and we capture the conditions under which such a mobile app can correctly work in the field. The demo video of Androfleet is made available from https://youtu.be/gJ5_Ed7XL04.},
  keywords={Wireless fidelity;Peer-to-peer computing;Testing;Mobile communication;Androids;Humanoid robots;Mobile handsets},
  doi={10.1109/ASE.2017.8115712},
  ISSN={},
  month={Oct},}

@INPROCEEDINGS{4696109,
  author={Khoshbakhtian, Masoumeh and HezareMoghadam, Nasrin and Mazoochi, Mojtaba},
  booktitle={2008 Third International Conference on Broadband Communications, Information Technology & Biomedical Applications}, 
  title={Identification of Various Problems in an Environment of Multi Vendor Equipments for PSTN Services in NGN}, 
  year={2008},
  volume={},
  number={},
  pages={194-201},
  abstract={This paper focuses on Iran next generation network (NGN) Pilot, the purpose of which is to achieve appropriate knowledge for a desired move through current telecommunication network of Iran to NGN. On the base of this purpose, obtaining a proper knowledge about capabilities and weaknesses of vendors' NGN equipments is necessary as well as verifying the functionalities of these equipments in a multi-vendor environment. Accordingly, a test bed has been designed with the capability of executing various tests scenarios related to NGN and processing the results. At these stage two types of tests, i.e. Functional tests and Interoperability tests have been done. Since Functional tests are performed in a single-vendor environment; the proper interoperation among multi-vendor equipments can not be guaranteed. Consequently, other types of tests called Interoperability tests have been designed. These tests are performed in a multi-vendor environment and classified into two groups which process interoperation of two Call Servers or a Call Server and Gateway from different vendors, respectively. The following paper represents the results obtained from the mentioned tests as well as the precise assessment of them.},
  keywords={Next generation networking;Testing;Cities and towns;Network servers;File servers;Databases;Performance evaluation;Modems;Access protocols;Switching circuits;interoperability;Megaco;NGN;SIP},
  doi={10.1109/BROADCOM.2008.14},
  ISSN={},
  month={Nov},}

@INPROCEEDINGS{11101432,
  author={Akarte, Harishchandra A and Yadav, Dharmendra K},
  booktitle={2025 International Conference on Electronics, AI and Computing (EAIC)}, 
  title={Performance Testing and Validation of Data Plane Code Written in P4 for SDN}, 
  year={2025},
  volume={},
  number={},
  pages={1-5},
  abstract={Software-defined networking (SDN) enables programmable data plane behavior using languages like P4, making meticulous testing and validation essential to ensure correctness, security, and performance. In this paper, we explore three key methodologies for testing P4-based data plane applications: functional testing, performance evaluation, and formal verification. Functional testing validates packet forwarding logic using Mininet, BMv2, and Scapy, ensuring expected packet processing. Performance testing measures throughput, latency, and packet loss using tools like iperf to assess network efficiency. Formal verification applies mathematical proofs using tools such as p4v to check correctness, reachability, and loop freedom. By integrating these methodologies, we achieved robust validation of P4 programs, which ensured their reliability in SDN environments. Our study focuses on the importance of a multiple-layer approach to testing, combining experimental testing with formal verification for comprehensive validation of programmable data planes of software-defined networks. This article explores various performance evaluation techniques for P4-based SDN data planes, presents a testing methodology, and discusses experimental results.},
  keywords={Performance evaluation;Codes;Pipelines;Writing;Throughput;Software;Security;Software defined networking;Testing;Formal verification;Data Plane;P4 Program;Testing;Validation;SDN},
  doi={10.1109/EAIC66483.2025.11101432},
  ISSN={},
  month={June},}

@INPROCEEDINGS{386480,
  author={Rao, S.R. and Bi-Yu Pan and Armstrong, J.R.},
  booktitle={1993 European Conference on Design Automation with the European Event in ASIC Design}, 
  title={Hierarchical test generation for VHDL behavioral models}, 
  year={1993},
  volume={},
  number={},
  pages={175-182},
  abstract={In this method, the VHDL model to be tested is represented by its process model graph (PMG). Test sets for individual processes of the model are precomputed and stored in the design library. The Hierarchical Behavioral Test Generator (HBTG) algorithm accepts the PMG and the precomputed tests as inputs, from which it hierarchically constructs a test sequence that tests the functionality of the model. Such an automatic test generation process relieves the modeler of the time-consuming task of developing test-benches. The test sequence generated by HBTG is then used for simulation of the model. Experimental results indicate that the tests generated exercise the model thoroughly.<>},
  keywords={Circuit testing;Automatic testing;Circuit faults;Libraries;Computational modeling;Circuit simulation;Computer simulation;Computer industry;Hardware;Design engineering},
  doi={10.1109/EDAC.1993.386480},
  ISSN={},
  month={Feb},}

@ARTICLE{6784505,
  author={Vara, Juan Manuel and Bollati, Verónica A. and Jiménez, Álvaro and Marcos, Esperanza},
  journal={IEEE Transactions on Software Engineering}, 
  title={Dealing with Traceability in the MDDof Model Transformations}, 
  year={2014},
  volume={40},
  number={6},
  pages={555-583},
  abstract={Traceability has always been acknowledged as a relevant topic in Software Engineering. However, keeping track of the relationships between the different assets involved in a development process is a complex and tedious task. The fact that the main assets handled in any model-driven engineering project are models and model transformations eases the task. In order to take advantage of this scenario, which has not been appropriately capitalized on by the most widely adopted model transformation languages before, this work presents MeTAGeM-Trace, a methodological and technical proposal with which to support the model-driven development of model transformations that include trace generation. The underlying idea is to start from a high-level specification of the transformation which is subsequently refined into lower-level transformation models in terms of a set of DSLs until the source code that implements the transformation can be generated. Running this transformation produces not only the corresponding target models, but also a trace model between the elements of the source and target models. As part of the proposal, an EMF-based toolkit has been developed to support the development of ATL and ETL model transformations. This toolkit has been empirically validated by conducting a set of case studies following a systematic research methodology.},
  keywords={Proposals;Object oriented modeling;Software;DSL;Complexity theory;Data models;Software engineering;Model-driven engineering;model transformations;traceability},
  doi={10.1109/TSE.2014.2316132},
  ISSN={1939-3520},
  month={June},}

@ARTICLE{7270333,
  author={Mellegård, Niklas and Ferwerda, Adry and Lind, Kenneth and Heldal, Rogardt and Chaudron, Michel R. V.},
  journal={IEEE Transactions on Software Engineering}, 
  title={Impact of Introducing Domain-Specific Modelling in Software Maintenance: An Industrial Case Study}, 
  year={2016},
  volume={42},
  number={3},
  pages={245-260},
  abstract={Domain-specific modelling (DSM) is a modern software development technology that aims at enhancing productivity. One of the claimed advantages of DSM is increased maintainability of software. However, current empirical evidence supporting this claim is lacking. In this paper, we contribute evidence from a case study conducted at a software development company. We study how the introduction of DSM affected the maintenance of a legacy system. We collected data about the maintenance phase of a system that was initially developed using manual programming, but which was gradually replaced by DSM development. We performed statistical analyses of the relation between the use of DSM and the time needed to resolve defects, the defect density, and the phase in which defects were detected. The results show that after introducing DSM the defect density is lower, that defects are found earlier, but resolving defects takes longer. Other observed benefits are that the number of developers and the number of person-hours needed for maintaining the system decreased, and the portability to new platforms increased. Our findings are useful for organizations that consider introducing DSM and would like to know which benefits can be realized in software maintenance.},
  keywords={DSL;Maintenance engineering;Unified modeling language;Business;Software maintenance;Productivity;Empirical investigation;software maintenance;maintenance measurement;process measurement;productivity;Empirical investigation;software maintenance;maintenance measurement;process measurement;productivity},
  doi={10.1109/TSE.2015.2479221},
  ISSN={1939-3520},
  month={March},}

@INPROCEEDINGS{7577380,
  author={Kapre, Nachiket and Bayliss, Samuel},
  booktitle={2016 26th International Conference on Field Programmable Logic and Applications (FPL)}, 
  title={Survey of domain-specific languages for FPGA computing}, 
  year={2016},
  volume={},
  number={},
  pages={1-12},
  abstract={High-performance FPGA programming has typically been the exclusive domain of a small band of specialized hardware developers. They are capable of reasoning about implementation concerns at the register-transfer level (RTL) which is analogous to assembly-level programming in software. Sometimes these developers are required to push further down to manage even lower levels of abstraction closer to physical aspects of the design such as detailed layout to meet critical design constraints. In contrast, software programmers have long since moved away from textual assembly-level programming towards relying on graphical integrated development environments (IDEs), high-level compilers, smart static analysis tools and runtime systems that optimize, manage and assist the program development tasks. Domain-specific languages (DSLs) can bridge this productivity gap by providing higher levels of abstraction in environments close to the domain of application expert. DSLs carefully limit the set of programming constructs to minimize programmer mistakes while also enabling a rich set of domain-specific optimizations and program transformations. With a large number of DSLs to choose from, an inexperienced FPGA user may be confused about how to select an appropriate one for the intended domain. In this paper, we review a combination of legacy and state-of-the-art DSLs available for FPGA development and provide a taxonomy and classification to guide selection and correct use of the framework.},
  keywords={Field programmable gate arrays;Hardware;DSL;Hardware design languages;Software;Programming;Productivity},
  doi={10.1109/FPL.2016.7577380},
  ISSN={1946-1488},
  month={Aug},}

@INPROCEEDINGS{8080500,
  author={Mueller, Peter and Belschner, Tim and Reichel, Reinhard},
  booktitle={2017 IEEE AUTOTESTCON}, 
  title={Automated test artifact generation for a distributed avionics platform utilizing abstract state machines}, 
  year={2017},
  volume={},
  number={},
  pages={1-8},
  abstract={The development of complex and highly safety-critical avionics systems, such as fly-by-wire, is typically linked with high efforts, risks and thus costs. Especially with regard to certification the testing activities during verification are playing a major role. This paper introduces the automatization complex of the testing artifact generation by use of Abstract State Machines (ASM), which allows a unified approach for system and software testing. The baseline is the Flexible Platform technology (a platform based development approach) currently under development by the Institute of Aircraft Systems (ILS) of the University of Stuttgart. The remaining automatization complex is the automated generation of certification relevant documentation, i.e. the requirements. These three complexes establish the AAA-Process which lays the foundation for an effective total system capability for complex avionics systems while simultaneously mitigating risks and costs. The actual test artifact generation is strictly aligned to development standards used in the aviation industry. Requirements exist as classes in a textual representation as well as in a specification model, represented by ASMs. The functional behavior, as described by the models, serves as a test oracle for test case generation. For this the model is translated into a graph system, instrumented by selectable testing methods and executed. The resulting trace data is used to automatically derive test procedures under consideration of the corresponding test environment as scripts, which are directly executable within our testing infrastructure consisting of a HiL simulation. Furthermore this includes the automatic generation of the associated traceability data and test specification documentation. An initial framework has been defined to support exchangeability of individual tasks in the generation tool-chain. The feasibility of the approach has been demonstrated by testing the complete heterogeneous signal communication of an exemplary avionics system, resp. platform instance, at system level as well as at software high-level.},
  keywords={Aerospace electronics;Software;Hardware;Tools;Aircraft;DSL;Testing},
  doi={10.1109/AUTEST.2017.8080500},
  ISSN={1558-4550},
  month={Sep.},}

@ARTICLE{10485249,
  author={Queiroz, Rodrigo and Sharma, Divit and Caldas, Ricardo and Czarnecki, Krzysztof and García, Sergio and Berger, Thorsten and Pelliccione, Patrizio},
  journal={IEEE Transactions on Intelligent Transportation Systems}, 
  title={A Driver-Vehicle Model for ADS Scenario-Based Testing}, 
  year={2024},
  volume={25},
  number={8},
  pages={8641-8654},
  abstract={Scenario-based testing for automated driving systems (ADS) must be able to simulate traffic scenarios that rely on interactions with other vehicles. Although many languages for high-level scenario modelling have been proposed, they lack the features to precisely and reliably control the required micro-simulation, while also supporting behavior reuse and test reproducibility for a wide range of interactive scenarios. To fill this gap between scenario design and execution, we propose the Simulated Driver-Vehicle (SDV) model to represent and simulate vehicles as dynamic entities with their behavior being constrained by scenario design and goals set by testers. The model combines driver and vehicle as a single entity. It is based on human-like driving and the mechanical limitations of real vehicles for realistic simulation. The model leverages behavior trees to express high-level behaviors in terms of lower-level maneuvers, affording multiple driving styles and reuse. Furthermore, optimization-based maneuver planners guide the simulated vehicles towards the desired behavior. Our extensive evaluation shows the model’s design effectiveness using NHTSA pre-crash scenarios, its motion realism in comparison to naturalistic urban traffic, and its scalability with traffic density. Finally, we show the applicability of our SDV model to test a real ADS and to identify crash scenarios, which are impractical to represent using predefined vehicle trajectories. The SDV model instances can be injected into existing simulation environments via co-simulation.},
  keywords={Testing;Roads;Trajectory;Vehicles;Vehicle dynamics;Scalability;DSL;Intelligent vehicles;autonomous vehicles;autonomous driving;system testing;simulation;road traffic},
  doi={10.1109/TITS.2024.3373531},
  ISSN={1558-0016},
  month={Aug},}

@INPROCEEDINGS{8327149,
  author={Binamungu, Leonard Peter and Embury, Suzanne M. and Konstantinou, Nikolaos},
  booktitle={2018 IEEE Workshop on Validation, Analysis and Evolution of Software Tests (VST)}, 
  title={Detecting duplicate examples in behaviour driven development specifications}, 
  year={2018},
  volume={},
  number={},
  pages={6-10},
  abstract={In Behaviour-Driven Development (BDD), the behaviour of the software to be built is specified as a set of example interactions with the system, expressed using a “Given-When-Then” structure. The examples are written using customer language, and are readable by end-users. They are also executable, and act as tests that determine whether the implementation matches the desired behaviour or not. This approach can be effective in building a common understanding of the requirements, but it can also face problems. When the suites of examples grow large, they can be difficult and expensive to change. Duplication can creep in, and can be challenging to detect manually. Current tools for detecting duplication in code are also not effective for BDD examples. Moreover, human concerns of readability and clarity can rise. We present an approach for detecting duplication in BDD suites that is based around dynamic tracing, and describe an evaluation based on three open source systems.},
  keywords={Tools;Production;Software;Syntactics;Semantics;Cloning;DSL;behaviour-driven development;duplication detection;dynamic tracing},
  doi={10.1109/VST.2018.8327149},
  ISSN={},
  month={March},}

@INPROCEEDINGS{10350772,
  author={Fuksa, Mario and Speth, Sandro and Becker, Steffen},
  booktitle={2023 ACM/IEEE International Conference on Model Driven Engineering Languages and Systems Companion (MODELS-C)}, 
  title={Applicability of the ViMoTest Approach for Automated GUI Testing: A Field Study}, 
  year={2023},
  volume={},
  number={},
  pages={821-830},
  abstract={Automated GUI testing is widely used in practice to ensure high software quality. While current tools and methods support automatic GUI testing, open challenges exist, like specification efforts, fragility problems, and maintainability difficulties. To tackle those challenges, we have proposed the complementary approach ViMoTest as a preliminary concept. The ViMoTest approach aims to automate GUI tests with behavior-oriented test specifications based on ViewModel-abstractions and projectional DSLs to address those open challenges. However, no empirical ev-idence shows the applicability of automated GUI testing with the combination of ViewModels and projectional DSLs. Therefore, we develop a prototypical implementation using JetBrains MPS, extending our preliminary ViMoTest concept as proof of concept. Through a field study for which we applied our prototype to five real-world subject applications, we could show the applicability of our approach with 17 exemplary ViewModels. We successfully modeled 11 standard GUI widgets as first-class constructs and used them in 38 test scenarios in our projectional DSLs. Our field study indicates that the ViMoTest approach is applicable to support test developers in testing the view logic independent of specific GUI frameworks and rendering aspects.},
  keywords={Industries;Prototypes;Software quality;Rendering (computer graphics);Model driven engineering;Behavioral sciences;DSL;MDSD;ViMoTest;ViewModel;BDD;GUI Testing;JetBrains MPS},
  doi={10.1109/MODELS-C59198.2023.00131},
  ISSN={},
  month={Oct},}

@ARTICLE{1341380,
  author={Varsamou, M. and Antonakopoulos, T. and Papandreou, N.},
  journal={IEEE Design & Test of Computers}, 
  title={From protocol models to their implementation: a versatile testing methodology}, 
  year={2004},
  volume={21},
  number={5},
  pages={416-428},
  abstract={The design and test of communication protocols relies extensively on formal description languages. In this protocol design and verification scheme, high-level models serve in generating simulation sequences for low-level models, and all simulation is based on directed testing. The methodology is versatile and flexible, and difficult to set up the first time.},
  keywords={Protocols;Object oriented modeling;System testing;Mathematical model;Design methodology;Computational modeling;Appropriate technology;Process design;DSL;Automata},
  doi={10.1109/MDT.2004.61},
  ISSN={1558-1918},
  month={Sep.},}

@INPROCEEDINGS{10740201,
  author={Batten, Christopher and Pinckney, Nathaniel and Liu, Mingjie and Ren, Haoxing and Khailany, Brucek},
  booktitle={2024 ACM/IEEE 6th Symposium on Machine Learning for CAD (MLCAD)}, 
  title={PyHDL-Eval: An LLM Evaluation Framework for Hardware Design Using Python-Embedded DSLs}, 
  year={2024},
  volume={},
  number={},
  pages={1-17},
  abstract={Embedding hardware design frameworks within Python is a promising technique to improve the productivity of hardware engineers. At the same time, there is significant interest in using large-language models (LLMs) to improve key chip design tasks. This paper describes PyHDL-Eval, a new framework for evaluating LLMs on specification-to-RTL tasks in the context of Python-embedded domainspecific languages (DSLs). The framework includes 168 problems, Verilog reference solutions, Verilog test benches, Python test scripts, and workflow orchestration scripts. We use the framework to conduct a detailed case study comparing five LLMs (CodeGemma 7B, Llama3 8B/70B, GPT4, and GPT4 Turbo) targeting Verilog and five Python-embedded DSLs (PyMTL3, PyRTL, MyHDL, Migen, and Amaranth). Our results demonstrate the promise of in-context learning when applied to smaller models (e.g.,pass rate for CodeGemma 7B improves from 14.9% to 32.7% on Verilog) and Python-embedded DSLs (e.g., pass rate for LLama3 70B improves from 0.6% to 33.0% on PyMTL3). We find LLMs perform better when targeting Verilog as compared to Python-embedded DSLs (e.g., pass rate for GPT4 Turbo is 72.2% on Verilog and 29.8–62.0% on the Python-embedded DSLs) despite using a popular general-purpose host language. PyHDL-Eval will serve as a useful framework for future research at the intersection of Python-embedded DSLs and LLMs. CCS Concepts • Hardware → Hardware description languages and compilation; • Computing methodologies → Machine learning.},
  keywords={Productivity;Solid modeling;Machine learning;Hardware;DSL;Hardware design languages;Chip scale packaging;hardware description languages;Python-embedded domain-specific languages;large language models},
  doi={10.1109/MLCAD62225.2024.10740201},
  ISSN={},
  month={Sep.},}

@INPROCEEDINGS{8227296,
  author={Tse, Kit Sum and Johnson, Peter C.},
  booktitle={2017 IEEE Security and Privacy Workshops (SPW)}, 
  title={A Framework for Validating Session Protocols}, 
  year={2017},
  volume={},
  number={},
  pages={110-119},
  abstract={Communication protocols are complex, their implementations are difficult, causing many unintended (and severe) vulnerabilities in protocol parsing. While the problem of packet parsing is solved, session parsing remains challenging. Building on existing systems that reliably parse individual messages, we present our four-component framework for implementing protocol session parsers with the goal to improve security of protocol parsing: specification of a protocol message, description of a protocol state machine, testing routines to validate implementations against fake and real data, and graph generation to visualize implementations. This framework enables the creation of a session parser, which validates individual protocol messages in the context of other messages in the same conversation. This is helpful because more secure parsers lead to more secure communication.},
  keywords={Protocols;Data structures;Security;Testing;DSL;Semantics;Language-theoretic security;protocol state machine;protocol parsing;session parsing},
  doi={10.1109/SPW.2017.35},
  ISSN={},
  month={May},}

@INPROCEEDINGS{8428788,
  author={Bussenot, Robin and Leblanc, Hervé and Percebois, Christian},
  booktitle={2018 13th Annual Conference on System of Systems Engineering (SoSE)}, 
  title={Orchestration of Domain Specific Test Languages with a Behavior Driven Development approach}, 
  year={2018},
  volume={},
  number={},
  pages={431-437},
  abstract={An airplane is composed by many complexes and embedded systems. During the integration testing phase, the design office produces requirements of the targeted system, and the test center produces concrete test procedures to be executed on a test bench. In this context, integration tests are mostly written in natural language and manually executed step by step by a tester. In order to formalize integration tests procedures dedicated to each system with domain specific languages approved by testers, and in order to automatize integration tests, we have introduced agile practices in the integration testing phase. We have chosen a Behavior Driven Development (BDD) approach to orchestrate Domain Specific Test Languages produced for the ACOVAS FUI project.},
  keywords={Testing;Software;DSL;Natural languages;Hardware;Aerospace electronics;Aircraft},
  doi={10.1109/SYSOSE.2018.8428788},
  ISSN={},
  month={June},}

@INPROCEEDINGS{7073229,
  author={Mahmoudi, Charif and Mourlin, Fabrice},
  booktitle={2014 IEEE/ACS 11th International Conference on Computer Systems and Applications (AICCSA)}, 
  title={Business Process Management with mobile routes}, 
  year={2014},
  volume={},
  number={},
  pages={420-427},
  abstract={Business processes are milestone of the information system of any companies. Their availability is a crucial aspect. We provide a solution for the high level of availability of business processes by the use of cluster of enterprise service buses (ESB). Our approach is based on the dynamic creation of the route between the business services and the migration of a runtime context from one ESB to another one. So, we insure the management of business processes over a cluster and measure the impact of such incident. Through the use of log, we also report these events which allow the administrator for preparing updates of the information system. With the use of open source software, we guarantee the reuse of our case study with other kinds of enterprise service bus, which respect open standard exchanges like XML language and REST API.},
  keywords={Business;Containers;Context;DSL;XML;Servers;Routing;SOA architecture;orchestration;cluster of bus;message routing;web service},
  doi={10.1109/AICCSA.2014.7073229},
  ISSN={2161-5330},
  month={Nov},}

@INPROCEEDINGS{5959781,
  author={Miksovic, Christoph and Zimmermann, Olaf},
  booktitle={2011 Ninth Working IEEE/IFIP Conference on Software Architecture}, 
  title={Architecturally Significant Requirements, Reference Architecture, and Metamodel for Knowledge Management in Information Technology Services}, 
  year={2011},
  volume={},
  number={},
  pages={270-279},
  abstract={Capturing and sharing design knowledge such as architectural decisions is becoming increasingly important in professional Information Technology (IT) services firms. Methods, models, and tools supporting explicit knowledge management strategies have been proposed in recent years. In this paper, we extend previous work in the architectural knowledge management community to satisfy the requirements of an additional user group: the designers of IT infrastructure solutions that are outsourced from one company to another. Such strategic outsourcing solutions require complex, contractually relevant design decisions concerning many different resources such as IT infrastructures, people, and real estate. In this paper, we present a reference architecture and a decision process-oriented knowledge metamodel that we synthesized from the domain-specific functional requirements and quality attributes. We also present a tool implementation of these decision modeling concepts and discuss user feedback.},
  keywords={Proposals;Knowledge engineering;Knowledge based systems;Computer architecture;Communities;Engines;DSL;knowledge management;outsourcing;workflow},
  doi={10.1109/WICSA.2011.43},
  ISSN={},
  month={June},}

@INPROCEEDINGS{9287702,
  author={Chu, Minh-Hue and Dang, Duc-Hanh},
  booktitle={2020 12th International Conference on Knowledge and Systems Engineering (KSE)}, 
  title={Automatic Extraction of Analysis Class Diagrams from Use Cases}, 
  year={2020},
  volume={},
  number={},
  pages={109-114},
  abstract={At the early phase of software development, functional requirements of the software often need to be represented in the developer's language, resulting in a so-called analysis model. Current works in literature aim to increase automation in software development by either generating automatically the analysis model from a use case specification or transforming the analysis model to a design model. However, up to now, to precisely specify use cases is still a challenge, preventing us from realizing this aim. This paper proposes a method to extract analysis classes from a use case specification. Within our method, use cases are represented using our domain-specific modeling language named USL. We then define algorithms with transformation rules as a representation of analysis patterns in order to extract analysis classes from the USL use case model. We develop a support tool for our method in which transformation rules are realized using the ATL model-to-model transformation technique.},
  keywords={Knowledge engineering;Analytical models;Buildings;Transforms;Tools;Software;Generators;Use Case Specification;Model Transformation;Analysis Model;UML/OCL},
  doi={10.1109/KSE50997.2020.9287702},
  ISSN={2164-2508},
  month={Nov},}

@INPROCEEDINGS{9282667,
  author={Makedonski, Philip and Gheorghe-Pop, Ilie-Daniel and Rennoch, Axel and Kristoffersen, Finn and Pintar, Boštjan and Ulrich, Andreas},
  booktitle={2020 IEEE 20th International Conference on Software Quality, Reliability and Security Companion (QRS-C)}, 
  title={Using TDL for Standardised Test Purpose Definitions}, 
  year={2020},
  volume={},
  number={},
  pages={514-521},
  abstract={This article reports on experiences from the use of the ETSI Test Description Language (TDL) and its extension for structured test objective specification (TDL-TO) for the definition of functional and non-functional test purposes in the Internet of Things (IoT) domain. The experiences are based on results from different working groups at ETSI TC MTS and the ETSI Specialist Task Force (STF) 574, focusing on the definition of test purposes for functional, security, and performance testing of the CoAP and MQTT protocols as well as VxLTeinteroperability testing.},
  keywords={Protocols;Software quality;Software reliability;Security;Internet of Things;Task analysis;Testing;Test description;test purposes;security;performance;interoperability;CoAP;MQTT;IoT;VxLTE},
  doi={10.1109/QRS-C51114.2020.00091},
  ISSN={},
  month={Dec},}

@INPROCEEDINGS{11107217,
  author={Ghazavi, Sanaz and Abdulrazzaq, Ali Kareem and Mayer, Franziska and Schott, Christian and Markert, Erik and Heinkel, Ulrich},
  booktitle={2025 Smart Systems Integration Conference and Exhibition (SSI)}, 
  title={Towards Model-Driven Circuit Test Development: SysMLv2-Based Test Modeling and Assisted Workflow}, 
  year={2025},
  volume={},
  number={},
  pages={1-7},
  abstract={The final test of produced Integrated Circuits (ICs) is one of the most time-consuming stages of the circuit production lifecycle. It consists of the specification-based test development and the actual test execution to identify manufacturing defects. As the development depends heavily on the used Automatic Test Equipment (ATE) for executing the tests, it needs to be repeated if the test system or its software version changes. Therefore, this paper presents an approach for Model-Driven Circuit Test Development and its integration in existing processes. A SysMLv2 profile is introduced to model essential elements for the setup and execution of production tests for manufactured devices. Model-Based Systems Engineering (MBSE) is widely used for the development of complex systems, but has not been applied within circuit testing yet. An instantiation, either manually by the user or automatically from design sources or verification results, enables the generation of test programs in the target language of the used test system and its software platform. The combined approach reduces development effort and supports consistent, reusable test modeling across multiple abstraction levels.},
  keywords={Couplings;VHDL;Systematics;Production;Software;Timing;Test pattern generators;Integrated circuit modeling;Circuit testing;Standards;Circuit Testing;Test Program Development;Model-Based Systems Engineering;SysMLv2;Profile;V-Model Integration},
  doi={10.1109/SSI65953.2025.11107217},
  ISSN={},
  month={April},}

@INPROCEEDINGS{9755390,
  author={Tang, Weijian and Wang, Keming},
  booktitle={2021 16th International Conference on Intelligent Systems and Knowledge Engineering (ISKE)}, 
  title={Automatic Generation of Test Cases of Multi-Agent Systems Based on Model Checking}, 
  year={2021},
  volume={},
  number={},
  pages={24-29},
  abstract={The reliability of a train control system depends on the consistency between the system under the test cases and the design specification. It is vital to ensure consistency by verifying that the functions of the train control system meet the design specifications. This paper introduces a novel and formal approach of test cases generation that guarantees the reliability of a train control system. The proposed methodology is effectively applied to a reactive, concurrent and complex case study of the train control system, namely the level transition, and uses multi-agent systems to describe each system component as an intelligent agent. We use the model check tool, NuSMV, that supports formal specifications, modelling the communication among agents. Furthermore, this paper combines computation tree logic with modified condition/decision coverage to design the trap properties. Based on the model checking technology, the test sequence is automatically generated by verifying the trap properties. Lastly, a set of functional test cases that satisfy the transition coverage are obtained by revising the test sequence. The research results show that the method in this paper can be used to generate test cases that are executed to verify the consistency between the system under testing and the design specification.},
  keywords={Knowledge engineering;Computational modeling;Model checking;Control systems;Reliability engineering;Intelligent agents;Formal specifications;model checking;automatic generation;test cases;level transition;multi-agent;NuSMV},
  doi={10.1109/ISKE54062.2021.9755390},
  ISSN={},
  month={Nov},}

@INPROCEEDINGS{7102627,
  author={Krenn, Willibald and Schlick, Rupert and Tiran, Stefan and Aichernig, Bernhard and Jobstl, Elisabeth and Brandl, Harald},
  booktitle={2015 IEEE 8th International Conference on Software Testing, Verification and Validation (ICST)}, 
  title={MoMut::UML Model-Based Mutation Testing for UML}, 
  year={2015},
  volume={},
  number={},
  pages={1-8},
  abstract={Model-based mutation testing (MBMT) is a promising testing methodology that relies on a model of the system under test (SUT) to create test cases. Hence, MBMT is a so-called black-box testing approach. It also is fault based, as it creates test cases that are guaranteed to reveal certain faults: after inserting a fault into the model of the SUT, it looks for a test case revealing this fault. This turns MBMT into one of the most powerful and versatile test case generation approaches available as its tests are able to demonstrate the absence of certain faults, can achieve both, control-flow and data-flow coverage of model elements, and also may include information about the behaviour in the failure case. The latter becomes handy whenever the test execution framework is bound in the number of observations it can make and - as a consequence - has to restrict them. However, this versatility comes at a price: MBMT is computationally expensive. The tool MoMuT::UML (https://www.momut.org) is the result of a multi-year research effort to bring MBMT from the academic drawing board to industrial use. In this paper we present the current stable version, share the lessons learnt when applying two generations of MoMuT::UML in an industrial setting, and give an outlook on the upcoming, third,generation.},
  keywords={Unified modeling language;Testing;Computational modeling;Integrated circuit modeling;Circuit faults;Object oriented modeling;Semantics},
  doi={10.1109/ICST.2015.7102627},
  ISSN={2159-4848},
  month={April},}

@INPROCEEDINGS{8004372,
  author={Ahmad, Amro Al-Said and Brereton, Pearl and Andras, Peter},
  booktitle={2017 IEEE International Conference on Software Quality, Reliability and Security Companion (QRS-C)}, 
  title={A Systematic Mapping Study of Empirical Studies on Software Cloud Testing Methods}, 
  year={2017},
  volume={},
  number={},
  pages={555-562},
  abstract={Context: Software has become more complicated, dynamic, and asynchronous than ever, making testing more challenging. With the increasing interest in the development of cloud computing, and increasing demand for cloud-based services, it has become essential to systematically review the research in the area of software testing in the context of cloud environments. Objective: The purpose of this systematic mapping study is to provide an overview of the empirical research in the area of software cloud-based testing, in order to build a classification scheme. We investigate functional and non-functional testing methods, the application of these methods, and the purpose of testing using these methods. Method: We searched for electronically available papers in order to find relevant literature and to extract and analyze data about the methods used. Result: We identified 69 primary studies reported in 75 research papers published in academic journals, conferences, and edited books. Conclusion: We found that only a minority of the studies combine rigorous statistical analysis with quantitative results. The majority of the considered studies present early results, using a single experiment to evaluate their proposed solution.},
  keywords={Cloud computing;Security;Software testing;Reliability;Systematics;systematic mapping study;cloud software testing methods;software testing;empirical studies},
  doi={10.1109/QRS-C.2017.94},
  ISSN={},
  month={July},}

@INPROCEEDINGS{7880820,
  author={Guo, Xiaolong and Dutta, Raj Gautam and Mishra, Prabhat and Jin, Yier},
  booktitle={2016 17th International Workshop on Microprocessor and SOC Test and Verification (MTV)}, 
  title={Automatic RTL-to-Formal Code Converter for IP Security Formal Verification}, 
  year={2016},
  volume={},
  number={},
  pages={35-38},
  abstract={The wide usage of hardware intellectual property (IP) cores from untrusted vendors has raised security concerns in the integrated circuit (IC) industry. Existing testing methods are designed to validate the functionality of the hardware IP cores. These methods often fall short in detecting unspecified (often malicious) logic. Formal methods like Proof-Carrying Hardware (PCH), on the other hand, can help eliminate hardware Trojans and/or design backdoors by formally proving security properties on soft IP cores despite the high proof development cost. One of the causes to the high cost is the manual conversion of the hardware design from RTL code to a domain-specific language prior to verification. To mitigate this issue and to lower the overall cost of PCH framework, we propose an automatic code converter for translating VHDL to Formal-HDL, a domain specific language for representing hardware designs in Coq language. Our code converter provides support to wide variety of hardware designs. Towards the goal of speeding up the verification procedure in our PCH framework, the code converter is the important first step. The applicability of the tool is demonstrated by converting soft IP cores of AES to its Coq equivalent code.},
  keywords={Security;Hardware;IP networks;Hardware design languages;Trojan horses;Syntactics;Hardware Security;Hardware IP Protection;Formal Verification},
  doi={10.1109/MTV.2016.23},
  ISSN={2332-5674},
  month={Dec},}

@INPROCEEDINGS{8536162,
  author={Ed-douibi, Hamza and Cánovas Izquierdo, Javier Luis and Cabot, Jordi},
  booktitle={2018 IEEE 22nd International Enterprise Distributed Object Computing Conference (EDOC)}, 
  title={Automatic Generation of Test Cases for REST APIs: A Specification-Based Approach}, 
  year={2018},
  volume={},
  number={},
  pages={181-190},
  abstract={The REpresentation State Transfer (REST) has gained momentum as the preferred technique to design Web APIs. REST allows building loosely coupled systems by relying on HTTP and the Web-friendly format JSON. However, REST is not backed by any standard or specification to describe how to create/consume REST APIs, thus creating new challenges for their integration, testing and verification. To face this situation, several specification formats have been proposed (e.g., OpenAPI, RAML, and API Blueprint), which can help automate tasks in REST API development (e.g., testing) and consumption (e.g., SDKs generation). In this paper we focus on automated REST API testing relying on API specifications, and particularly the OpenAPI one. We propose an approach to generate specification-based test cases for REST APIs to make sure that such APIs meet the requirements defined in their specifications. We provide a proof-of-concept tool implementing our approach, which we have validated with 91 OpenAPI definitions. Our experiments show that the generated test cases cover on average 76.5% of the elements included in the OpenAPI definitions. Furthermore, our experiments also reveal that 40% of the tested APIs fail.},
  keywords={Testing;Tools;Task analysis;Reliability;Internet;Standards;Face;API testing;REST APIs;OpenAPI},
  doi={10.1109/EDOC.2018.00031},
  ISSN={2325-6362},
  month={Oct},}

@INPROCEEDINGS{8456399,
  author={Quenum, José G. and Aknine, Samir},
  booktitle={2018 IEEE International Conference on Services Computing (SCC)}, 
  title={Towards Executable Specifications for Microservices}, 
  year={2018},
  volume={},
  number={},
  pages={41-48},
  abstract={This paper presents an empirical approach for microservice automated testing. With the rise of the agile methodology, automated testing has gained momentum in software development, including using microservices as an architectural style. However, the tests are not always related to the core specifications of the system being developed. In this paper, we discuss an approach to derive the tests, especially the acceptance tests, from the specifications of the systems. To avoid any ambiguity in the specifications, we focus on the formal specifications of the system. To this end, we introduce intelligent agents as a conceptual unit to encapsulate the formal specifications of services. Indeed, a comparison of microservice tenets and the general characterization of agents reveals that both can be thought of as autonomous software entities, driven by goals and evolving within a distributed environment and communicating with one another. Using a real-world application we show how agent formal specifications can be linked to microservice automated testing.},
  keywords={Testing;Service-oriented architecture;Intelligent agents;Business;Computer architecture;Curriculum development;Services;Testing;Intelligent Agents;Formal Specifications},
  doi={10.1109/SCC.2018.00013},
  ISSN={2474-2473},
  month={July},}

@INPROCEEDINGS{786779,
  author={Nelson, G.},
  booktitle={IEEE ATM Workshop '99 Proceedings (Cat. No. 99TH8462)}, 
  title={Testing techniques for next-generation IP networks}, 
  year={1999},
  volume={},
  number={},
  pages={63-68},
  abstract={In this paper, we look at some examples of how "IP meets ATM" and then discuss some of the recent advances in IP standards. For the remainder of the paper, we examine testing techniques used in three different scenarios: functional testing of a layer 2/layer 3 switching device; class of service (CoS) contract verification in an IP network; interworking testing of an IP/ATM access device.},
  keywords={Testing;Next generation networking;IP networks;Asynchronous transfer mode;Multiprotocol label switching;Routing;Telecommunication traffic;Protocols;Packet switching;Proposals},
  doi={10.1109/ATM.1999.786779},
  ISSN={},
  month={May},}

@INPROCEEDINGS{7999652,
  author={de Moura, Jéssica Lasch and Charão, Andrea Schwertner and Lima, João Carlos Damasceno and de Oliveira Stein, Benhur},
  booktitle={2017 17th International Conference on Computational Science and Its Applications (ICCSA)}, 
  title={Test case generation from BPMN models for automated testing of Web-based BPM applications}, 
  year={2017},
  volume={},
  number={},
  pages={1-7},
  abstract={This article proposes an approach to generate test cases from BPMN models, for automated testing of Web applications implemented with the support of BPM suites. The work is primarily focused on functional testing and has the following objectives: (i) identify execution paths from the flow analysis in the BPMN model and (ii) generate the initial code of test scripts to be run on a given Web application testing tool. Throughout the article, we describe the design and implementation of a solution to achieve these goals, targeting automated tests using Selenium and Cucumber as tools. The approach was applied to processes from a public repository and was able to generate test scenarios from different BPMN models.},
  keywords={Testing;Tools;Logic gates;Selenium;Process control;XML;Monitoring;Business Process Management;BPMN;automatic software testing;test case generation},
  doi={10.1109/ICCSA.2017.7999652},
  ISSN={},
  month={July},}

@INPROCEEDINGS{7140371,
  author={Gebert, Steffen and Schwartz, Christian and Zinner, Thomas and Tran-Gia, Phuoc},
  booktitle={2015 IFIP/IEEE International Symposium on Integrated Network Management (IM)}, 
  title={Continuously delivering your network}, 
  year={2015},
  volume={},
  number={},
  pages={766-769},
  abstract={Softwarization and cloudification of networks through software defined networking and network functions virtualisation promise a new degree of flexibility and agility. By moving logic from device firmware into software applications and applying software development mechanisms, innovation can be introduced with less effort. Concrete ways how to operate and orchestrate such systems are not yet defined. The process of making changes to a controller software or a virtualized network function in a production network without the risk of network disruption is not covered by literature. Complexity of systems brings the risk of unexpected side-effects and has so long been a show-stopper for administrators applying changes to networking devices. This paper suggests the adaption of the successful concept of continuous delivery into the software defined networking world. Test-driven development and automatic acceptance tests demonstrate that the software engineering community already found ways to ensure that changes do not break. Applied to network engineering, the adaption of continuous delivery can be seen as an enabler for risk-free and frequent changes in production infrastructure through push button deployments.},
  keywords={Software;Pipelines;Production;Servers;Measurement;Technological innovation},
  doi={10.1109/INM.2015.7140371},
  ISSN={1573-0077},
  month={May},}

@INPROCEEDINGS{7980397,
  author={Contan, Andrei and Dehelean, Catalin and Miclea, Liviu},
  booktitle={2017 14th International Conference on Engineering of Modern Electric Systems (EMES)}, 
  title={Applying coding systems in the process of testing software applications}, 
  year={2017},
  volume={},
  number={},
  pages={127-131},
  abstract={The challenges met during the software projects fall into any number of categories. The development and the technical solutions bring about technical challenges, but the situations one is confronted with, may also be sociological, psychological or managerial in nature. Without any knowledge in the field of social sciences, the programmers, testers and managers might interpret the social aspects of the project improperly, and such interpretations lead to the inability to fully understand the problem and, ultimately, to inefficiency in the decision-making process. Furthermore, solid knowledge of theories in the area of the social sciences is required for a better understanding of both the context in which the application runs and of the final users who will use the developed project. The understanding of and the involvement in a software acceptance testing (SAT) project, requires the combination of multiple theories and principles from different disciplines.},
  keywords={Encoding;Testing;Software quality;Software engineering;Computational modeling;Psychology;coding models;software testing;software testing;social science},
  doi={10.1109/EMES.2017.7980397},
  ISSN={},
  month={June},}

@INPROCEEDINGS{1609829,
  author={Prashant Gandhi and Haugen, N.C. and Hill, M. and Watt, R.},
  booktitle={Agile Development Conference (ADC'05)}, 
  title={Creating a living specification using FIT documents}, 
  year={2005},
  volume={},
  number={},
  pages={253-258},
  abstract={Using FIT for automated acceptance testing supports a process in which developers and customers collaborate on a single executable specification for each story, i.e. the FIT documents. By collaborating closely on the FIT documents, the developers and customers reach a shared understanding of the domain and develop the ubiquitous language of the application. Our experience with this process was ultimately successful but not completely pain free. In this experience report we highlight the benefits and pitfalls and share techniques for achieving successful developer and customer collaboration in specifying executable FIT documents.},
  keywords={Collaboration;Automatic testing;Writing;Pain;Fixtures;Collaborative tools;Encoding;System testing;Automation},
  doi={10.1109/ADC.2005.19},
  ISSN={},
  month={July},}

@INPROCEEDINGS{6462664,
  author={Combemale, Benoît and Crégut, Xavier and Pantel, Marc},
  booktitle={2012 19th Asia-Pacific Software Engineering Conference}, 
  title={A Design Pattern to Build Executable DSMLs and Associated V&V Tools}, 
  year={2012},
  volume={1},
  number={},
  pages={282-287},
  abstract={Model executability is now a key concern in model-driven engineering, mainly to support early validation and verification (V&V). Some approaches allow to weave executability into metamodels, defining executable domain-specific modeling languages (DSMLs). Model validation can then be achieved by simulation and graphical animation through direct interpretation of the conforming models. Other approaches address model executability by model compilation, allowing to reuse the virtual machines or V&V tools existing in the target domain. Nevertheless, systematic methods are currently not available to help the language designer in the definition of such an execution semantics and related tools. For instance, simulators are mostly hand-crafted in a tool specific manner for each DSML. In this paper, we propose to reify the elements commonly used to support state-based execution in a DSML. We infer a design pattern (called Executable DSML pattern) providing a general reusable solution for the expression of the executability concerns in DSMLs. It favors flexibility and improves reusability in the definition of semantics-based tools for DSMLs. We illustrate how this pattern can be applied to ease the development of V&V tools.},
  keywords={Semantics;Unified modeling language;Runtime;Computational modeling;Abstracts;Animation;Concrete;Model Driven Engineering;Software Language Engineering;Validation & Verification},
  doi={10.1109/APSEC.2012.79},
  ISSN={1530-1362},
  month={Dec},}

@INPROCEEDINGS{10292050,
  author={Faturahman, Muhammad Azhar and Widyani, Yani},
  booktitle={2023 IEEE International Conference on Data and Software Engineering (ICoDSE)}, 
  title={Integration of Method Editor EssenceBoard with Method Base Management System}, 
  year={2023},
  volume={},
  number={},
  pages={138-143},
  abstract={In this research, a tool called EssenceBoard and Method Base Management System (MBMS) has been integrated. EssenceBoard and MBMS are two related tools that complement each other. EssenceBoard is responsible for creating and modifying method chunks, while MBMS is responsible for storing and managing method chunks. However, the existing EssenceBoard is unable to retrieve and modify method chunks stored in MBMS. Therefore, an integration between EssenceBoard and MBMS is needed to enable EssenceBoard to function as a method editor for MBMS. The integration is accomplished using a remote procedure invocation (RPI) approach, which was selected based on the analysis of integration criteria, including application coupling, integration simplicity, integration technology, data format, data timeliness, data or functionality, and asynchronicity. Additionally, a design has been made for the method chunk format exchanged between EssenceBoard and MBMS, referred to as the intermediate form of method chunks. The design is carried out by adding the visualization attribute to the intermediate form of method chunks. In EssenceBoard, a converter component is developed to transform the format of method chunks in EssenceBoard into an intermediate form of method chunks. An authenticator component is also developed in EssenceBoard to authenticate MBMS users. Through integration, EssenceBoard is now able to retrieve method chunks stored in MBMS, modify them, and save them back to MBMS. Based on the functional testing results, EssenceBoard and MBMS have been successfully integrated using the remote procedure invocation, allowing them to exchange method chunks with each other.},
  keywords={Couplings;Economic indicators;Data visualization;Transforms;Software;Testing;Software engineering;Software Integration;Method Base Management System;Method Editor;EssenceBoard},
  doi={10.1109/ICoDSE59534.2023.10292050},
  ISSN={2640-0227},
  month={Sep.},}

@INPROCEEDINGS{1541161,
  author={Hongyu Zhang and Bradbury, J.S. and Cordy, J.R. and Dingel, J.},
  booktitle={Fifth IEEE International Workshop on Source Code Analysis and Manipulation (SCAM'05)}, 
  title={Implementation and verification of implicit-invocation systems using source transformation}, 
  year={2005},
  volume={},
  number={},
  pages={87-96},
  abstract={In this paper we present a source transformation-based framework to support uniform testing and model checking of implicit-invocation software systems. The framework includes a new domain-specific programming language, the Implicit-Invocation Language (IIL), explicitly designed for directly expressing implicit-invocation software systems, and a set of formal rule-based source transformation tools that allow automatic generation of both executable and formal verification artifacts. We provide details of these transformation tools, evaluate the framework in practice, and discuss the benefits of formal automatic transformation in this context. Our approach is designed not only to advance the state-of-the-art in validating implicit-invocation systems, but also to further explore the use of automated source transformation as a uniform vehicle to assist in the implementation, validation and verification of programming languages and software systems in general.},
  keywords={Software systems;System testing;Software testing;Computer languages;Formal verification;Mathematical model;XML;Automatic testing;Electronic mail;Vehicles},
  doi={10.1109/SCAM.2005.15},
  ISSN={},
  month={Sep.},}

@ARTICLE{4749742,
  author={Scully, Padraig and Skehill, Ronan and McGrath, Sean},
  journal={IEEE Wireless Communications}, 
  title={Mobility in an RF-isolated test platform}, 
  year={2008},
  volume={15},
  number={6},
  pages={8-15},
  abstract={Understanding the practical impact of mobility in a wireless network is essential for the wireless networks of tomorrow. Mobility influences the network performance, behavior, and ability to provide seamless service across a wide area. Testing and evaluating the real impact of mobility is difficult in the field with so many variables such as interference, fading, and so on. Experimental wireless evaluation in a test environment must correspond to an actual deployment. Furthermore, it is important to achieve repeatability without sacrificing realism. This study presents a functional test platform that uses real IEEE 802.11 equipment, providing repeatability and reliability. The platform is used to test the practical impact of device movement in a WLAN cell while voice and data applications are running. The mobility characteristics of wireless devices are based on individual models used by researchers with the addition of real aspects of mobility from empirical studies to improve realism.},
  keywords={Testing;Fading;Wireless networks;Wireless LAN;Radio transmitters;Interference;GSM;3G mobile communication;Receivers;Wireless communication},
  doi={10.1109/MWC.2008.4749742},
  ISSN={1558-0687},
  month={December},}

@INPROCEEDINGS{9649736,
  author={Wanninger, Constantin and Rossi, Sebastian and Sch&#x00F6;rner, Martin and Hoffmann, Alwin and Poeppel, Alexander and Eymueller, Christian and Reif, Wolfgang},
  booktitle={2021 21st International Conference on Control, Automation and Systems (ICCAS)}, 
  title={ROSSi A Graphical Programming Interface for ROS 2}, 
  year={2021},
  volume={},
  number={},
  pages={255-262},
  abstract={The Robot Operating System (ROS) offers developers a large number of ready-made packages for developing robot programs. The multitude of packages and the different interfaces or adapters is also the reason why ROS projects often tend to become confusing. Concepts of model-driven software development using a domain-specific modeling language could counteract this and at the same time speed up the development process of such projects. This is investigated in this paper by transferring the core concepts from ROS 2 into a graphical programming interface. Elements of established graphical programming tools are compared and approaches from modeling languages such as UML are used to create a novel approach for graphical development of ROS projects. The resulting interface is evaluated through the development of a project built on ROS, and the approach shows promise towards facilitating work with the Robot Operating System.},
  keywords={Adaptation models;Visualization;Runtime;Operating systems;Unified modeling language;Programming;Software;robot operating system;ros;unmaned aerial vehicle;uav;model driven development;semantic plug and play},
  doi={10.23919/ICCAS52745.2021.9649736},
  ISSN={2642-3901},
  month={Oct},}

@INPROCEEDINGS{9137306,
  author={D’Alvia, Livio and Pittella, Erika and Fioriello, Francesca and Maugeri, Andrea and Rizzuto, Emanuele and Piuzzi, Emanuele and Sogos, Carla and Del Prete, Zaccaria},
  booktitle={2020 IEEE International Symposium on Medical Measurements and Applications (MeMeA)}, 
  title={Heart rate monitoring under stress condition during behavioral analysis in children with neurodevelopmental disorders}, 
  year={2020},
  volume={},
  number={},
  pages={1-6},
  abstract={Monitoring physiological parameters under stress conditions – i.e., heart rate, breath frequency or heart rate variability - through non-invasive and comfortable wearable devices was a research topic of great interest in many medical fields. In the last decade, several wearable devices and methods have been developed for stress monitoring, showing suitable performance in estimating the stress indicator. In spite of the interest in the field, the development of wearable solutions suitable for child neuropsychiatry applications was still an open challenge. In this study, we evaluate the cardiac activity in children - with and without neurodevelopmental disorders - through a novel wearable solution in order to compare the stress response in different structured activities and games. Each subject was equipped with a 3-lead electrocardiograph device and a piezoelectric respiratory sensor embedded into a thoracic belt. Subjects were asked to carry out five different activities previously chosen from a subset of specific behavioral tests (three different free-games and two structured activities). All experimental sessions were video-recorded. Results highlighted how wearable devices could help in estimating stress indicators for long-tests (over twenty-five minutes). The clinical application was conducted on a cohort of 32 children so divided: 13 with Specific Language Disorder, 15 with Autism Spectrum Disorder, and 4 control children without neurodevelopmental disorders. Statistical differences were observed between populations in the heart rate. Moreover, all subjects experienced stress effects evidenced by variation of heart rate and standard deviation, which are supported by the video analysis.},
  keywords={Wireless communication;Pediatrics;Wireless sensor networks;Frequency modulation;Wearable computers;Games;Biomedical monitoring;wearable devices;cardiac activity monitoring;heartbeat monitoring;heartbeat variability;specific language disorder;stress response indicator;autism spectrum disorder},
  doi={10.1109/MeMeA49120.2020.9137306},
  ISSN={},
  month={June},}

@INPROCEEDINGS{9609160,
  author={Kirinuki, Hiroyuki and Matsumoto, Shinsuke and Higo, Yoshiki and Kusumoto, Shinji},
  booktitle={2021 IEEE International Conference on Software Maintenance and Evolution (ICSME)}, 
  title={NLP-assisted Web Element Identification Toward Script-free Testing}, 
  year={2021},
  volume={},
  number={},
  pages={639-643},
  abstract={End-to-end test automation is important in modern web application development. However, existing test automation techniques have challenges in implementing and maintaining test scripts. It is difficult to keep correct locators, which test scripts require to identify web elements on web pages. The reason is that locators depend on the metadata in web elements or the structure of each web page. One efficient way to solve the problem of locators is to make test cases written in natural language executable without test scripts. As the first step of script-free testing, we propose a technique to identify web elements to be operated and to determine test procedures by interpreting test cases. The test cases are written in a domain-specific language without relying on the metadata of web elements or the structural information of web pages. We leverage natural language processing techniques to understand the semantics of web elements. We also create heuristic search algorithms to find promising test procedures. To evaluate our proposed technique, we applied it to two open-source web applications. The experimental results show that our technique successfully identified 94% of web elements to be operated in the test cases.},
  keywords={Software maintenance;Automation;Heuristic algorithms;Semantics;Web pages;Metadata;Natural language processing;Script-free Testing;Web Testing;Locator},
  doi={10.1109/ICSME52107.2021.00072},
  ISSN={2576-3148},
  month={Sep.},}

@INPROCEEDINGS{9270318,
  author={Mai, Phu X. and Goknil, Arda and Pastore, Fabrizio and Briand, Lionel C.},
  booktitle={2020 IEEE/ACM 42nd International Conference on Software Engineering: Companion Proceedings (ICSE-Companion)}, 
  title={SMRL: A Metamorphic Security Testing Tool for Web Systems}, 
  year={2020},
  volume={},
  number={},
  pages={9-12},
  abstract={We present a metamorphic testing tool that alleviates the oracle problem in security testing. The tool enables engineers to specify metamorphic relations that capture security properties of Web systems. It automatically tests Web systems to detect vulnerabilities based on those relations. We provide a domain-specific language accompanied by an Eclipse editor to facilitate the specification of metamorphic relations. The tool automatically collects the input data and transforms the metamorphic relations into executable Java code in order to automatically perform security testing based on the collected data. The tool has been successfully evaluated on a commercial system and a leading open source system (Jenkins). Demo video: https://youtu.be/9kx6u9LsGxs.},
  keywords={Tools;Testing;Security;Software engineering;Java;Europe;Manuals;Software and application security;Software verification and validation;Metamorphic Security Testing},
  doi={},
  ISSN={2574-1926},
  month={Oct},}

@INPROCEEDINGS{7785748,
  author={Carter, J. and Gardner, W. B.},
  booktitle={2016 IEEE 17th International Conference on Information Reuse and Integration (IRI)}, 
  title={BHive: Towards Behaviour-Driven Development Supported by B-Method}, 
  year={2016},
  volume={},
  number={},
  pages={249-256},
  abstract={Behaviour-Driven Development (BDD) is an "outside-in" approach to software development built upon semi-formal mediums for specifying the behaviour of a system as it would be observed externally. Through the representation of a system as a collection of user stories and scenarios using BDD's notation, practitioners automate acceptance tests using examples of desired behaviour for the envisioned system. A formal model created in concert with BDD tests would provide valuable insight into test validity and enhance the visibility of the problem domain. This work called BHive builds upon the formal underpinnings of BDD scenarios by mapping their "Given," "When," and "Then" statements to "Precondition," "Command," and "Postcondition" constructs as introduced by Floyd-Hoare logic. We posit that this mapping allows for a B-Method representation to be created and that such a model is useful for exploring system behaviour and exposing gaps in requirements. We also outline extensions to BDD tooling required for the described integration and present benefits of the BHive approach to integrating formalism within a BDD project.},
  keywords={Software;Testing;Stakeholders;Shape;Documentation;Conferences;BDD;Behaviour-Driven Development;B-Method;Agile},
  doi={10.1109/IRI.2016.39},
  ISSN={},
  month={July},}

@INPROCEEDINGS{7302512,
  author={Vinogradov, Sergey and Ozhigin, Artem and Ratiu, Daniel},
  booktitle={2015 IEEE International Symposium on Systems Engineering (ISSE)}, 
  title={Modern model-based development approach for embedded systems practical experience}, 
  year={2015},
  volume={},
  number={},
  pages={56-59},
  abstract={Control functionality of modern rail vehicles is getting more and more complex. It contains several modules such as the traction control unit or the central control unit, as well as input and output stations, such as driver's cab terminals and process I/Os. A plethora of devices are connected to the vehicle and train bus and are able to communicate. The functions of the vehicle control and traction systems are configured by using function blocks from which loadable programs are generated. The languages used to program the control units are well established in the field. However, one-size-fits-all approach cannot adequately address the increased complexity of the software in modern trains. In this paper we describe our preliminary experience with using the multi-paradigm modeling tool “mbeddr” in the railway domain. The following aspects have been in focus during the work: (a) matching the application requirements and domain specific language used for implementation; (b) integration of model-based approach into traditional product lifecycle; (c) reengineering existing functionality using modeling and code generation capabilities of mbeddr. The system example we chose was the application logic of automated train driving system implemented in development environment of Siemens process automation framework.},
  keywords={Software;Mathematical model;Complexity theory;Control systems;Domain specific languages;Formal verification;Rail transportation;model based development;language engineering},
  doi={10.1109/SysEng.2015.7302512},
  ISSN={},
  month={Sep.},}

@INPROCEEDINGS{10453777,
  author={Tang, Yazhe and Xu, Zixin and Han, Jie},
  booktitle={2023 7th International Conference on Communication and Information Systems (ICCIS)}, 
  title={Encryption of Sensitive Data Flow Information in Hybrid Networks with P4}, 
  year={2023},
  volume={},
  number={},
  pages={96-101},
  abstract={Encrypting sensitive information such as IP address and port number of data stream can improve the detection resistance of data during network transmission and reduce security threats. The challenge is that the process of achieving sensitive information obfuscation is complex in traditional network environments with limited network node capabilities. Existing work is mostly based on the principles of traditional cryptography to attain secure end-to-end encrypted authentication and transmission. However, it suffers from high time and space costs, complex and inefficient terminal systems, and the inability to encrypt the aforementioned sensitive information. In this paper, we propose to use the P4(Programming Protocol-independent Packet Processors Procedure, one domain-specific language for data plane) to empower network nodes with encryption and decryption of sensitive identification information in a programmable network environment, combined with a control plane to embed traffic security efforts inside the network and ensure the process is transparent to the end system. Experimental results demonstrate that the P4-based encryption and obfuscation mechanism can ensure data transmission in a hybrid network environment of traditional and programmable networks, obfuscating sensitive information without causing performance burden in terms of transmission delay and throughput.},
  keywords={Resistance;Program processors;Authentication;Process control;TCPIP;Throughput;Encryption;Encryption;Information security;SDN;P4},
  doi={10.1109/ICCIS59958.2023.10453777},
  ISSN={2837-4924},
  month={Oct},}

@INPROCEEDINGS{8648600,
  author={Koehler, Wolfgang and Jing, Yanguo},
  booktitle={2018 11th International Conference on Developments in eSystems Engineering (DeSE)}, 
  title={A Novel Block-Based Programming Framework for Non-programmers to Validate PLC Based Machine Tools for Automotive Manufacturing Facilities}, 
  year={2018},
  volume={},
  number={},
  pages={202-207},
  abstract={An automotive manufacturing cell typically consists of multiple stations, controlled by a single industrial programmable controller. Design flaws or assembly mistakes are normally discovered during the highly time-constrained integration phase, which leads to time loss and inefficiency. This paper presents a novel domain-specific 'language' to eliminate the PLC experts from the testing process, to minimize input from operators and to reduce cost significantly. The proposed 'language' was inspired by widely available educational robotic toys, built on a block based programming environment, which allows for intuitive interaction with novice users. A comparison and evaluation study has been carried out to compare the new framework to the traditional process of building equipment for an automotive manufacturing cell. The study has shown that the proposed 'language' not only eliminates the need for PLC experts, in the testing process, but also reduces the time needed for setup and testing by 90%. In addition, the high level of abstraction decreased the potential for programming errors by 95%.},
  keywords={Programming;Testing;Solenoids;XML;Machine tools;Automotive engineering;Sensors},
  doi={10.1109/DeSE.2018.00046},
  ISSN={2161-1351},
  month={Sep.},}

@INPROCEEDINGS{10564512,
  author={Schlamelcher, Jan and Goodfellow, Thomas and Kebianyor, Bewoayia and Gruettner, Kim},
  booktitle={MBMV 2024; 27. Workshop}, 
  title={Extending Clang/LLVM with Custom Instructions using TableGen – An Experience Report}, 
  year={2024},
  volume={},
  number={},
  pages={204-213},
  abstract={The extensibility of the RISC-V ISA by adding instructions allows for the rapid creation of custom processor cores. For that reason, it must be assured that the software tooling for this hardware does not become a bottleneck in this process. In this paper, we address this by describing an approach for automatically augmenting a compiler from a description of the instruction set extension. Our approach is based on Clang/LLVM with the custom instructions (RISC-V ISA-extensions) being described in a domain-specific language (DSL) called CoreDSL. These CoreDSL definitions are automatically translated into corresponding Clang/LLVM updates (TableGen and C++) with the goal of avoiding invasive changes to the compiler, while enabling free use of the custom instructions. Despite various challenges we encountered in the process, we have successfully automated the modification of Clang/LLVM to support custom instructions throughout the whole software toolchain (compiler, linker and debugger) and present our leanings and proposed next steps to further apply our proposed concept in a stable tool environment. The presented concept is not limited to RISC-V cores, but could also be adopted for other platforms with custom instruction support.},
  keywords={},
  doi={},
  ISSN={},
  month={Feb},}

@INPROCEEDINGS{10521374,
  author={Khan, Shaheer and Lawler, Christopher and Nicholas, Austin and Ortiz, Luis and Mukherji, Shubhodeep and Nowicki, Robert and Redfield, Benjamin and Voskanian, Vicken and Mak, Carter and Ruderman, Evan and Mallamaci, Michael},
  booktitle={2024 IEEE Aerospace Conference}, 
  title={Psyche: Innovations in Development of Planning and Sequencing Systems}, 
  year={2024},
  volume={},
  number={},
  pages={1-17},
  abstract={NASA Jet Propulsion Laboratory’s (JPL) mission Psyche will be the first of its kind in many ways: the first to explore a metal asteroid, the first to fly a Hall Effect Thruster (HET) in deep space, and the first to demonstrate Deep Space Optical Communication (DSOC). Challenges presented by firsts of these nature require powerful, yet agile ground and onboard systems to be addressed in a timely and effective manner. This paper examines the ways that innovations in Psyche’s planning and sequencing systems address both common challenges faced by many missions and unique challenges arising from first ever-in-flight activities. Psyche will be the first mission to operate using JPL’s new common core flight software Flight Core Product Line (FCPL) and make use of capabilities such as real-time telemetry reactivity, multi-level logic, and complex math operations which enable new levels of onboard autonomy. Augmentation of FCPL’s new capabilities through a user-friendly abstraction layer in ground software, an effort undertaken in conjunction with NASA’s Europa Clipper mission, enables even more complexity in reusable sequences, as well as streamlines and simplifies the ground operation and review process. The combination of increased onboard capability in the reusable sequences with Psyche’s agile planning and scheduling ground software allows Psyche to quickly develop new solutions to complex problems and represents an advancement from the approach of previous missions. An example of one of these solutions that takes full advantage of ground and onboard capabilities is presented: a unique momentum management strategy that addresses the challenge of the "swirl torque" that arises from the use of HETs in space. In order to address the challenges of testing these advanced reusable sequences, Psyche developed a robust and comprehensive automated regression testing framework and an intelligent sequence branch analysis tool that enables quick and reliable reusable sequence re-testing against new Flight Rules and constraints that are identified, resulting in significant risk reduction and cost-saving over manual approaches taken by previous missions. Analysis of these tools will prove useful to future missions that develop blocks, reusable sequences, or other onboard behaviors. A multi-mission process to parse command dictionaries into a set of Python command classes allows Psyche’s sequencers to develop sequences directly in Python, a significant improvement over the previous approach of sequencing in Domain Specific Languages (DSLs) This approach of developing Python command classes provides the ability to develop sequences in any Integrated Development Environment of their choice, and results in significant cost savings by avoiding the need to develop sequence editing graphical interfaces, as have been developed for missions in the past. Benefits, drawbacks, and work to go for each of these innovations are discussed and lessons from Psyche’s development phase are provided.},
  keywords={Technological innovation;Sequential analysis;Costs;Torque;Dictionaries;Software;Planning},
  doi={10.1109/AERO58975.2024.10521374},
  ISSN={1095-323X},
  month={March},}

@INPROCEEDINGS{6605922,
  author={Sobernig, Stefan and Hoisl, Bernhard and Strembeck, Mark},
  booktitle={2013 13th International Conference on Quality Software}, 
  title={Requirements-Driven Testing of Domain-Specific Core Language Models Using Scenarios}, 
  year={2013},
  volume={},
  number={},
  pages={163-172},
  abstract={In this paper, we present an approach for the scenario-based testing of the core language models of domain-specific modeling languages (DSML). The core language model is a crucial artifact in DSML development, because it captures all relevant domain abstractions and specifies the relations between these abstractions. In software engineering, scenarios are used to explore and to define (actual or intended) system behavior as well as to specify user requirements. The different steps in a requirements-level scenario can then be refined through detailed scenarios. In our approach, we use scenarios as a primary design artifact. Non-executable, human-understandable scenario descriptions can be refined into executable test scenarios. To demonstrate the applicability of our approach, we implemented a scenario-based testing framework based on the Eclipse Modeling Framework (EMF) and the Epsilon model-management toolkit.},
  keywords={Biological system modeling;Testing;Unified modeling language;Software;Prototypes;Metamodeling;domain-specific modeling;scenario-based testing;language engineering;metamodel testing},
  doi={10.1109/QSIC.2013.56},
  ISSN={2332-662X},
  month={July},}

@INPROCEEDINGS{10407894,
  author={Godfrey, Thomas and Zschaler, Steffen and Batra, Rahul and Douthwaite, Sam and Edgeworth, Jonathan and Edwards, Matthew and Miles, Simon},
  booktitle={2023 Winter Simulation Conference (WSC)}, 
  title={Supporting Emergency Department Risk Mitigation with a Modular and Reusable Agent-Based Simulation Infrastructure}, 
  year={2023},
  volume={},
  number={},
  pages={162-173},
  abstract={For emergency departments (EDs) to maintain sustainable care of patients, hospital management must continually explore potential interventions to clinical practice. Agent-based modelling (ABM) can be a valuable tool to support this planning in a controlled environment. Existing approaches to ABM development are best suited for one-off models. However, conditions in EDs can change frequently, making the use of one-off models infeasible. Decision-makers must be able to trust simulations appropriately for them to be effective in intervention exploration. Domain-specific modelling languages (DSMLs) can address these challenges by offering a reusable library of appropriately abstract, domain-familiar, modelling concepts across case studies and automatic translation of these concepts into executable models. In this paper, we present a DSML to support repeated modelling exercises in the ED domain and illustrate the use and reuse of this DSML across two concrete case studies in London-based NHS emergency departments.},
  keywords={Hospitals;Libraries;Planning;Risk mitigation},
  doi={10.1109/WSC60868.2023.10407894},
  ISSN={1558-4305},
  month={Dec},}

@INPROCEEDINGS{7018484,
  author={Hois, Bernhard and Sobernig, Stefan and Strembeck, Mark},
  booktitle={2014 2nd International Conference on Model-Driven Engineering and Software Development (MODELSWARD)}, 
  title={Natural-language scenario descriptions for testing core language models of domain-specific languages}, 
  year={2014},
  volume={},
  number={},
  pages={356-367},
  abstract={The core language model is a central artifact of domain-specific modeling languages (DSMLs) as it captures all relevant domain abstractions and their relations. Natural-language scenarios are a means to capture requirements in a way that can be understood by technical as well as non-technical stakeholders. In this paper, we use scenarios for the testing of structural properties of DSML core language models. In our approach, domain experts and DSML engineers specify requirements via structured natural-language scenarios. These scenario descriptions are then automatically transformed into executable test scenarios providing forward and backward traceability of domain requirements. To demonstrate the feasibility of our approach, we used Eclipse Xtext to implement a requirements language for the definition of semi-structured scenarios. Transformation specifications generate executable test scenarios that run in our test platform which is built on the Eclipse Modeling Framework and the Epsilon language family.},
  keywords={Testing;Unified modeling language;Vocabulary;Natural languages;Collaboration;Abstracts;Syntactics;Domain-Specific Modeling;Natural-Language Requirement;Scenario-based Testing;Metamodel Testing;Eclipse Modeling Framework;Xtext;Epsilon;EUnit},
  doi={},
  ISSN={},
  month={Jan},}

@INPROCEEDINGS{7552103,
  author={Poon, Chung Keung and Wong, Tak-Lam and Yu, Y. T. and Lee, Victor C. S. and Tang, Chung Man},
  booktitle={2016 IEEE 40th Annual Computer Software and Applications Conference (COMPSAC)}, 
  title={Toward More Robust Automatic Analysis of Student Program Outputs for Assessment and Learning}, 
  year={2016},
  volume={1},
  number={},
  pages={780-785},
  abstract={Automated analysis and assessment of students' programs, typically implemented in automated program assessment systems (APASs), are very helpful to both students and instructors in modern day computer programming classes. The mainstream of APASs employs a black-box testing approach which compares students' program outputs with instructor-prepared outputs. A common weakness of existing APASs is their inflexibility and limited capability to deal with admissible output variants, that is, outputs produced by acceptable correct programs that differ from the instructor's. This paper proposes a more robust framework for automatically modelling and analysing student program output variations based on a novel hierarchical program output structure called HiPOS. Our framework assesses student programs by means of a set of matching rules tagged to the HiPOS, which produces a better verdict of correctness. We also demonstrate the capability of our framework by means of a pilot case study using real student programs.},
  keywords={Robustness;Programming profession;Education;Computers;Testing;Natural language processing;automated assessment technology;computer science education;learning computer programming;program output variant;student program analysis},
  doi={10.1109/COMPSAC.2016.208},
  ISSN={0730-3157},
  month={June},}

@INPROCEEDINGS{9568383,
  author={Deantoni, Julien and Cambeiro, João and Bateni, Soroush and Lin, Shaokai and Lohstroh, Marten},
  booktitle={2021 Forum on specification & Design Languages (FDL)}, 
  title={Debugging and Verification Tools for Lingua Franca in Gemoc Studio}, 
  year={2021},
  volume={},
  number={},
  pages={01-08},
  abstract={LINGUA Franca (lf) is a polyglot coordination language designed for the composition of concurrent, time-sensitive, and potentially distributed reactive components called reactors. The LF coordination layer facilitates the use of target languages (e.g., C, C++, Python, TypeScript) to realize the program logic, where each target language requires a separate runtime implementation that must correctly implement the reactor semantics. Verifying the correctness of runtime implementations is not a trivial task, and is currently done on the basis of regression testing. To provide a more formal verification tool for existing and future target runtimes, as well as to help verify properties of LF programs, we recruit the use of GemocStudio-an Eclipse-based workbench for the development, integration, and use of heterogeneous executable modeling languages. We present an operational model for LF, realized in GEmocStudio, that is primed to interact with a rich set of analysis and verification tools. Our instrumentation provides the ability to navigate the execution of LF programs using an omniscient debugger with graphical model animation; to check assertions in particular execution runs, or exhaustively, using a model checker; and to validate or debug traces obtained from arbitrary LF runtime environments.},
  keywords={Runtime environment;Graphical models;Navigation;Instruments;Semantics;Tools;Inductors},
  doi={10.1109/FDL53530.2021.9568383},
  ISSN={1636-9874},
  month={Sep.},}

@ARTICLE{10884665,
  author={Ebert, Christof and Gallardo, Gorka and Hernantes, Josune and Serrano, Nicolás},
  journal={IEEE Software}, 
  title={DevOps 2.0}, 
  year={2025},
  volume={42},
  number={2},
  pages={24-32},
  abstract={DevOps is increasingly used. It evolves along new needs and lessons learned into DevOps 2.0. This article provides current industry experiences such as DevOps with AI, coping with regression tests, latest tools for DevOps cycles and more.},
  keywords={Industries;DevOps;Artificial intelligence;Information technology;Software development management;Digital arithmetic;Java;Codes},
  doi={10.1109/MS.2025.3525768},
  ISSN={1937-4194},
  month={March},}

@INPROCEEDINGS{6890825,
  author={Wanderley, Fernando and Silva, António and Araujo, João and Silveira, Denis S.},
  booktitle={2014 IEEE 4th International Model-Driven Requirements Engineering Workshop (MoDRE)}, 
  title={SnapMind: A framework to support consistency and validation of model-based requirements in agile development}, 
  year={2014},
  volume={},
  number={},
  pages={47-56},
  abstract={Two fundamental principles and values of agile methods are customer satisfaction by rapid delivery of useful software and the improvement of the communication process by continuous stakeholders' involvement. But, how to deal with customers' satisfaction and find a better visualization model at the requirements level (which stakeholders can understand and be involved) in an agile development context? Also, how this visualization model enhancement can guarantee consistency between agile requirements artefacts (e.g., user stories and domain models)? Thus, to answer these questions, this paper presents the SnapMind framework. This framework aims to make the requirements modelling process more user-centered, through the definition of a visual requirements language, based on mind maps, model-driven and domain specific language techniques. Moreover, through these techniques, the SnapMind framework focuses on support for consistency between user stories and the domain models using a model animation technique called snapshots. The framework was applied to an industrial case study to investigate its feasibility.},
  keywords={Visualization;Software;Syntactics;Unified modeling language;Adaptation models;Business;Semantics;Agile Software Requirements;Model-Driven Engineering;Domain-Specific Languages;Mind Map;Snapshots},
  doi={10.1109/MoDRE.2014.6890825},
  ISSN={},
  month={Aug},}

@INPROCEEDINGS{6062146,
  author={Groce, Alex and Havelund, Klaus and Smith, Margaret},
  booktitle={2010 ACM/IEEE 32nd International Conference on Software Engineering}, 
  title={From scripts to specifications: the evolution of a flight software testing effort}, 
  year={2010},
  volume={2},
  number={},
  pages={129-138},
  abstract={This paper describes the evolution of a software testing effort during a critical period for the flagship Mars Science Laboratory rover project at the Jet Propulsion Laboratory. Formal specification for post-run analysis of log files, using a domain-specific language, LogScope, replaced scripted real-time analysis. Log analysis addresses the key problems of on-the-fly approaches and cleanly separates specification and execution. Mining the test repository suggested the inadequacy of the scripted approach, and encouraged a partly engineer-driven development. LogScope development should hold insights for others facing the tight deadlines and reactionary nature of testing for critical projects. LogScope received a JPL Mariner Award for "improving productivity and quality of the MSL Flight Software" and has been discussed as an approach for other flight missions. We note LogScope features that most contributed to ease of adoption and effectiveness. LogScope is general and can be applied to any software producing logs.},
  keywords={Software;Telemetry;Space vehicles;Laboratories;Libraries;Semantics;Python;development practices;logs;runtime verification;space flight software;temporal logic;test infrastructure;testing},
  doi={10.1145/1810295.1810314},
  ISSN={1558-1225},
  month={May},}

@ARTICLE{10114402,
  author={Pidó, Sara and Pinoli, Pietro and Crovari, Pietro and Ieva, Francesca and Garzotto, Franca and Ceri, Stefano},
  journal={IEEE Access}, 
  title={Ask Your Data—Supporting Data Science Processes by Combining AutoML and Conversational Interfaces}, 
  year={2023},
  volume={11},
  number={},
  pages={45972-45988},
  abstract={Data Science is increasingly applied for solving real-life problems, in industry and in academic research, but mastering Data Science requires an interdisciplinary education that is still scarce on the market. Thus, there is a growing need for user-friendly tools that allow domain experts to directly apply data analysis methods to their datasets, without involving a Data Science expert. In this scenario, we present DSBot, an assistant that can analyze the user data and produce answers by mastering several Data Science techniques. DSBot understands the research question with the help of conversation interaction, produces a data science pipeline and automatically executes the pipeline in order to generate analysis. The strength of DSBot lies in the design of a rich domain specific language for modeling data analysis pipelines, the use of a suitable neural network for machine translation of research questions, the availability of a vast dictionary of pipelines for matching the translation output, and the use of natural language technology provided by a conversational agent. We empirically evaluated the translation capabilities and the autoML performances of the system. In the translation task, it obtains a BLEU score of 0.8. In prediction tasks, DSBot outperforms TPOT, an autoML tool, in 19 datasets out of 30.},
  keywords={Data science;Pipelines;Machine learning;Data analysis;Task analysis;Codes;Oral communication;Automated machine learning;data science;human-computer interaction;intelligent systems;natural language understanding;pipeline optimization;python},
  doi={10.1109/ACCESS.2023.3272503},
  ISSN={2169-3536},
  month={},}

@INPROCEEDINGS{8632429,
  author={Warnke, Tom and Uhrmacher, Adelinde M.},
  booktitle={2018 Winter Simulation Conference (WSC)}, 
  title={COMPLEX SIMULATION EXPERIMENTS MADE EASY}, 
  year={2018},
  volume={},
  number={},
  pages={410-424},
  abstract={Diverse methods for complex simulation experiments can contribute to developing and gaining insights into simulation models, for example simulation-based optimization, sensitivity analysis, or statistical model-checking. An effective tool for conducting simulation experiments must be highly flexible to support such a broad range of experimental methods. Furthermore, to facilitate reproducibility and communication of simulation experiments, an effective tool for simulation experimentation must yield experiment descriptions that are easily portable, executable, and human-readable. In this tutorial we introduce SESSL, a domain-specific language for setting up simulation experiments. SESSL is flexible and extensible, and experiment descriptions are executable, often succinct, and can be executed reproducibly across machines and operating systems. Based on a few examples, we demonstrate how SESSL can be leveraged to easily conduct complex simulation experiments while reusing existing software and methods, and how SESSL's capabilities can be extended and combined with arbitrary simulation software via bindings.},
  keywords={Analytical models;Software;Tools;Object oriented modeling;Computational modeling;Optimization;Sensitivity analysis},
  doi={10.1109/WSC.2018.8632429},
  ISSN={1558-4305},
  month={Dec},}

@INPROCEEDINGS{9825814,
  author={Kirinuki, Hiroyuki and Matsumoto, Shinsuke and Higo, Yoshiki and Kusumoto, Shinji},
  booktitle={2022 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER)}, 
  title={Web Element Identification by Combining NLP and Heuristic Search for Web Testing}, 
  year={2022},
  volume={},
  number={},
  pages={1055-1065},
  abstract={End-to-end test automation is critical in modern web application development. However, test automation techniques used in industry face challenges in implementing and maintaining test scripts. It is difficult to determine and maintain the locators needed by test scripts to identify web elements on web pages. The reason is that locators depend on the metadata of web elements and the structure of each web page. One effective way to solve such a problem of locators is to allow test cases written in natural language to be executed without test scripts. In this study, we propose a technique to identify web elements that should be operated on a web page by interpreting natural-language-like test cases. The test cases are written in a domain-specific language that independents on the metadata of web elements and the structural information of web pages. We leverage natural language processing techniques to understand the semantics of web elements. We also create heuristic search algorithms to explore web pages and find promising test procedures. To evaluate the proposed technique, we applied it to test cases for two open-source web applications. The experimental results show that our technique was able to successfully identify about 94% of web elements to be operated in the test cases. Our approach also succeeded in identifying all the web elements that were operated in 68% of the test cases.},
  keywords={Industries;Automation;Heuristic algorithms;Semantics;Web pages;Metadata;Natural language processing;test automation;test script;NLP;web testing},
  doi={10.1109/SANER53432.2022.00123},
  ISSN={1534-5351},
  month={March},}

@INPROCEEDINGS{7928008,
  author={Ukai, Hiroshi and Qu, Xiao},
  booktitle={2017 IEEE International Conference on Software Testing, Verification and Validation (ICST)}, 
  title={Test Design as Code: JCUnit}, 
  year={2017},
  volume={},
  number={},
  pages={508-515},
  abstract={In a development process where testing is highly automated, there is a major challenge to cope with issues such as huge test size and test stability. In this paper, we propose a model-based testing (MBT) tool called JCUnit, which generates a test suite from a model given as a Java class. Unlike other tools, it is designed to generate small and stable test suites and supports various popular models. With this tool, developers can apply MBT approach to their products without learning domain-specific language of proprietary MBT tools. Moreover, features such as portability and pluggability make it useful in a wide range of phases from unit testing to system testing. As a result, the efforts required in practical software testing will be reduced.},
  keywords={Tools;Java;Software;Graphical user interfaces;Pipelines;Software testing;automated testing;FSM spec;model-based testing;combinatorial interaction testing},
  doi={10.1109/ICST.2017.58},
  ISSN={},
  month={March},}

@ARTICLE{10767865,
  author={Li, Xiaochen and Zhu, Youcheng and Guo, Shikai and Jiang, He},
  journal={IEEE Transactions on Reliability}, 
  title={What Causes Bugs in Numerical Simulation Software? An Empirical Study}, 
  year={2024},
  volume={},
  number={},
  pages={1-15},
  abstract={Numerical simulation (NS) software is widely used in safety-critical domains (e.g., aerospace design) to simulate actual physical processes of real-world entities on computers. However, NS software is error-prone, whose bugs lead to incorrect simulation, and may even cause disastrous flaws in safety-critical applications. Although many studies investigate the bug characteristics of computation-centered software, such as machine learning systems, the characteristics of NS software bugs have not been fully studied: what are the root causes and symptoms; how are they different from other computation-centered software; and why the difference occurs. To bridge this gap, we present a systematic study of NS software bugs by analyzing 352 bugs in three popular NS projects (i.e., FDS, SU2, and Kratos) for different domains. We summarize seven root causes (with 18 subcategories) and five symptoms. We find that the correctness, completeness, compatibility, and parallelization to implement NS algorithms (i.e., models) are error-prone. Many root causes (e.g., incorrect model and incorrect initialization) require physical and chemical knowledge to avoid bugs, which may not be mastered by typical software developers. These findings motivate new challenges and opportunities for future NS software development, such as designing domain specific language systems for NS model.},
  keywords={Software;Computer bugs;Mathematical models;Computational modeling;Atmospheric modeling;Libraries;Data models;Software development management;Numerical models;Annotations;Bug characteristics;empirical study;high performance computing;numerical bugs;numerical simulation (NS)},
  doi={10.1109/TR.2024.3492380},
  ISSN={1558-1721},
  month={},}

@INPROCEEDINGS{7886903,
  author={Menendez, David and Nagarakatte, Santosh},
  booktitle={2016 IEEE/ACM 38th International Conference on Software Engineering (ICSE)}, 
  title={Termination-Checking for LLVM Peephole Optimizations}, 
  year={2016},
  volume={},
  number={},
  pages={191-202},
  abstract={Mainstream compilers contain a large number of peephole optimizations, which perform algebraic simplification of the input program with local rewriting of the code. These optimizations are a persistent source of bugs. Our recent research on Alive, a domain-specific language for expressing peephole optimizations in LLVM, addresses a part of the problem by automatically verifying the correctness of these optimizations and generating C++ code for use with LLVM. This paper identifies a class of non-termination bugs that arise when a suite of peephole optimizations is executed until a fixed point. An optimization can undo the effect of another optimization in the suite, which results in non-terminating compilation. This paper (1) proposes a methodology to detect non-termination bugs with a suite of peephole optimizations, (2) identifies the necessary condition to ensure termination while composing peephole optimizations, and (3) provides debugging support by generating concrete input programs that cause non-terminating compilation. We have discovered 184 optimization sequences, involving 38 optimizations, that cause non-terminating compilation in LLVM with Alive-generated C++ code.},
  keywords={Optimization;Computer bugs;C++ languages;Semantics;Concrete;Toxicology;Pattern matching;Compiler Verification;Peephole Optimization;Alive;Termination},
  doi={10.1145/2884781.2884809},
  ISSN={1558-1225},
  month={May},}

@INPROCEEDINGS{9954383,
  author={Krook, Robert and Hui, John and Svensson, Bo Joel and Edwards, Stephen A. and Claessen, Koen},
  booktitle={2022 20th ACM-IEEE International Conference on Formal Methods and Models for System Design (MEMOCODE)}, 
  title={Creating a Language for Writing Real-Time Applications for the Internet of Things}, 
  year={2022},
  volume={},
  number={},
  pages={1-20},
  abstract={We describe the development of a new programming language Scoria and its compiler. Scoria is a high-level reactive real-time language based on the sparse synchronous model (SSM), designed to produce time- and power-efficient low-level C code that can run on small IoT devices. While the compiler is not yet in a state where it is meaningful to measure power usage, we carefully profile the timing behaviour and identify bottlenecks that can improve performance. The language and compiler are implemented as an Embedded Domain-Specific Language (EDSL) on top of Haskell.},
  keywords={Codes;Program processors;Sensitivity;Computer bugs;Writing;Real-time systems;Internet of Things;Real-time;IoT;Compilers;Embedded Domain-Specific Languages},
  doi={10.1109/MEMOCODE57689.2022.9954383},
  ISSN={2832-6520},
  month={Oct},}

@INPROCEEDINGS{5381641,
  author={Hartmann, Tobias},
  booktitle={2009 Testing: Academic and Industrial Conference - Practice and Research Techniques}, 
  title={Model Based Testing of End-to-End Chains Using Domain Specific Languages}, 
  year={2009},
  volume={},
  number={},
  pages={82-91},
  abstract={In this paper, the author explains a new approach of model based end-to-end chain testing using scenarios with original and simulated equipment. The first goal is to automatically derive test data and test cases from the model, which is defined by a domain specific language. Several solvers can be attached to the conversion to quickly create a wide variety of stimuli for the system(s) under test. Furthermore, the system under test can be stimulated by either original equipment - which is connected to the test bench - or the test bench can simulate equipment and create inputs for the tested systems. Any mixture of simulated and original equipment is possible and can be changed on the fly. In the end, the results from the system under test are collected. These results can then be displayed back in the model. This method is currently used and improved in the project "E-Cab" in which the author is involved. Passengers travelling by plane are in the focus of this project. Complete services and service chains - from the booking at home up to leaving the destination airport - are created and used by many systems communicating with each other. The author expects advantages from testing these end-to-end chains with this approach.},
  keywords={Domain specific languages;System testing;Automatic testing;Logic testing;Constraint theory;Logistics;Operating systems;Airports;Complex networks;Robustness;model based;end-to-end chain;testing;automatic test case generation;automatic test data generation},
  doi={10.1109/TAICPART.2009.25},
  ISSN={},
  month={Sep.},}

@INPROCEEDINGS{10164710,
  author={Pollien, Baptiste and Garion, Christophe and Hattenberger, Gautier and Roux, Pierre and Thirioux, Xavier},
  booktitle={2023 IEEE/ACM 11th International Conference on Formal Methods in Software Engineering (FormaliSE)}, 
  title={A Verified UAV Flight Plan Generator}, 
  year={2023},
  volume={},
  number={},
  pages={130-140},
  abstract={FPL is a domain specific language used to specify complex drone missions for the Paparazzi open-source autopilot. FPL missions are compiled into C code that is directly embedded into the autopilot code. The FPL to C code generator, currently written in OCaml, is therefore a critical component when addressing the drone safety. This paper presents the formal verification of the FPL compilation process. First, we have developed in Coq a new three-pass code generator, targeting the Clight intermediate language from the CompCert suite. We have then formally defined an operational semantics for FPL. Finally, we have proved a bisimulation relation between FPL semantics and Clight semantics. In the course of the formalization and verification process, we have also unveiled several problems in the original Paparazzi code generator.},
  keywords={Codes;Semantics;Drones;Generators;Navigation;Autopilot;Syntactics;Code Generation;Compilation;Mechanized proof;Operational semantics},
  doi={10.1109/FormaliSE58978.2023.00021},
  ISSN={2575-5099},
  month={May},}

@INPROCEEDINGS{8729118,
  author={Tyugashev, A.},
  booktitle={2018 International Conference on Industrial Engineering, Applications and Manufacturing (ICIEAM)}, 
  title={On Use of Adaptive Schedules in Semantic Modeling of Real- Time Control Algorithms}, 
  year={2018},
  volume={},
  number={},
  pages={1-6},
  abstract={The paper presents a semantic model for time-driven real-time control algorithms based on adaptive schedule. In contrast to known models of semantics of programs and reactive systems based on the notion of state, use of adaptive schedules allows avoiding surplus detalization which leads to a `state explosion' problem. On the other hand, adaptive schedules correspond well to the nature of real-time control algorithms. The use of this model allows automated checking of the control algorithms' synchronization properties. The paper describes two case studies of application of a presented model for automation of design and verification of spacecraft's onboard flight control programs. The use of adaptive schedules semantics together with domain specific language, and automation toolset allow formal specification and verification of the important properties of realtime systems.},
  keywords={Semantics;Schedules;Adaptation models;Software;Real-time systems;Software algorithms;Task analysis;program semantics;control algorithm;real-time mode;flight control software;formal verification;control algorithm's logic;CASE toolset},
  doi={10.1109/ICIEAM.2018.8729118},
  ISSN={},
  month={May},}

@INPROCEEDINGS{5190273,
  author={Wang, Xinchun and Xu, Peijie},
  booktitle={2009 International Conference on Information Technology and Computer Science}, 
  title={Build an Auto Testing Framework Based on Selenium and FitNesse}, 
  year={2009},
  volume={2},
  number={},
  pages={436-439},
  abstract={Writing auto testing is a required engineering technique that can save time and money, and help businesses better respond to changes. But if we use testing framework improperly, more problems would possibly be caused. An auto testing framework based on Selenium and FitNesse is discussed in this article which can help with those problems. The framework use Selenium APIs to get page value, DbFit to init database, FitNesse to manage the test fixture, and a special DSL to write test fixture. It could greatly reduce the line numbers of testing code and the project developing period, lower the random error rate, facilitate writing fixture table, improve the coding productivity, and the quality of final product.},
  keywords={Software testing;Fixtures;Java;Automatic testing;Information security;Writing;Open source software;Programming;Information technology;Computer science;Auto Testing Framework;Selenium;FitNesse},
  doi={10.1109/ITCS.2009.228},
  ISSN={},
  month={July},}

@INPROCEEDINGS{5525697,
  author={Blondel, Enrico and Monney, Claude},
  booktitle={Intelec 2010}, 
  title={Efficient powering of communication and IT equipments using rotating UPS}, 
  year={2010},
  volume={},
  number={},
  pages={1-5},
  abstract={Today, the demand for uninterrupted power supply for Telecommunication and Internet services increases drastically. Following the same trend, cooling demand explodes. Supplying sufficient power with extremely high reliability becomes even more challenging. The era of telephone exchanges using 48V power supply and 8 hours of battery backup is past. Telecom Operators are currently migrating from POTS to all IP. In addition, broadband Internet access for everyone using DSL or fibre is already reality. More and more applications like TV on demand, streaming services, online gaming or entertainment are very power hungry. For such large systems, static UPS systems are no more efficient. A better alternative is offered by rotating UPS, also known as "No-break". In addition to requiring less space and no batteries, these systems have a better power efficiency too.},
  keywords={Uninterruptible power systems;Batteries;Alternators;Energy storage;Power quality;Power supplies;Cooling;Ventilation;Inductors;Acoustic testing},
  doi={10.1109/INTLEC.2010.5525697},
  ISSN={2158-5210},
  month={June},}

@INPROCEEDINGS{8821229,
  author={Purushothaman, Sarath Kumar and Kashyap, Nishant and HA, Divya and Agarwal, Sneha and Iqbal, Sandal and Behere, Vaishali},
  booktitle={2018 International Conference on Circuits and Systems in Digital Enterprise Technology (ICCSDET)}, 
  title={Unified Approach Towards Automation of any Desktop Web, Mobile Web, Android, iOS, REST and SOAP API Use Cases}, 
  year={2018},
  volume={},
  number={},
  pages={1-5},
  abstract={In today's world, testing cannot be efficient without automation. We need automation for avoiding repetitive work, making sure that the time from development to deployment is reduced with good quality. We just don't need automation tests, rather tests which are reliable, robust, easy to code, debug, scale and can run in parallel on distributed environment.The paper explores the challenges that can be faced while automating web, mobile web, android, ios, api tests via a common framework such as, a common way of interaction with web and mobile apps, automatically identifying connected mobile devices and run the parallel test instances, writing hybrid tests; a combination of web and api use cases, not tightly coupled with a specific application rather can be used for automating any web, mobile based application.This paper further discusses the approach to bring efficient, generic and re-usable solution for these challenges while ensuring that the users write reliable, robust and effective tests. The approach followed here spans across following areas:- Webdriver management and parallel run. DSL layer on top of the existing api libraries for easy to use interface. Reporting and logging mechanism. Utilities and integration with third parties (like Slack, Jira, Bitbucket, Jenkins, Device Farms, etc )The benefit of this framework is, it is application agnostic capability, which helps to use the same framework against any web applications (web, mobile) for test automation.},
  keywords={Testing;Libraries;Automation;Browsers;XML;Java;Simple object access protocol;api;rest;soap;webservice;automation;mobile;web;ios;android},
  doi={10.1109/ICCSDET.2018.8821229},
  ISSN={},
  month={Dec},}

@INPROCEEDINGS{5758967,
  author={Blondel, E. and Monney, C.},
  booktitle={4th International Telecommunication - Energy special conference}, 
  title={Efficient powering of communication and IT equipments using rotating UPS}, 
  year={2009},
  volume={},
  number={},
  pages={1-5},
  abstract={Today, the demand for uninterrupted power supply for Telecommunication and Internet services increases drastically. Following the same trend, cooling demand explodes. Supplying sufficient power with extremely high reliability becomes even more challenging. The era of telephone exchanges using 48V power supply and 8 hours of battery backup is past. Telecom Operators are currently migrating from POTS to all IP. In addition, broadband Internet access for everyone using DSL or fibre is already reality. More and more applications like TV on demand, streaming services, online gaming or entertainment are very power hungry. For such large systems, static UPS systems are no more efficient. A better alternative is offered by rotating UPS, also known as "No-break". In addition to requiring less space and no batteries, these systems have a better power efficiency too.},
  keywords={Uninterruptible power systems;Electromagnetic compatibility;Batteries;Generators;Inductors;Copper},
  doi={},
  ISSN={},
  month={May},}

@INPROCEEDINGS{6601570,
  author={Simko, Gabor and Lindecker, David and Levendovszky, Tihamer and Jackson, Ethan and Neema, Sandeep and Sztipanovits, Janos},
  booktitle={2013 20th IEEE International Conference and Workshops on Engineering of Computer Based Systems (ECBS)}, 
  title={A Framework for Unambiguous and Extensible Specification of DSMLs for Cyber-Physical Systems}, 
  year={2013},
  volume={},
  number={},
  pages={30-39},
  abstract={Increased emphasis on developing model-based design methods for Cyber-Physical Systems (CPS) brings new challenges to the specification of domain specific modeling languages (DSML) and the integration of heterogeneous CPS components.Since CPS are composed of tightly integrated physical and computational components, the modeled domains include both physical and computational systems.Formal specification of physical and computational languages as well as their integration remains an interesting challenge.In this paper we introduce a formal logic based framework for formal specification and simulation, that is supported by the fixed-point logic language FORMULA.As a representative case study, we define both the structural and behavioral semantics for a bond graph language, and demonstrate the reusability and extensibility provided by the approach by extending the language to support hybrid dynamics.},
  keywords={Semantics;Mathematical model;Solid modeling;Unified modeling language;Abstracts;Syntactics;Computational modeling;formal specification;domain-specific modeling languages;formal semantics;bond graph language},
  doi={10.1109/ECBS.2013.30},
  ISSN={},
  month={April},}

@INPROCEEDINGS{6737898,
  author={Parizad, A.},
  booktitle={2013 13th International Conference on Environment and Electrical Engineering (EEEIC)}, 
  title={Dynamic stability Analysis for damavand power plant considering PMS functions by DIGSILENT software}, 
  year={2013},
  volume={},
  number={},
  pages={145-155},
  abstract={For a number of years Power Management System (PMS) have been used to control islanded power systems. It can control system frequency and voltage as well as generated real and reactive power. The PMS would often have a load shedding facility to prevent the cascade failure of the generation system. The investigation is based on dynamics time domain simulations by DigSilent software and aims to define the main functions for the power management system (PMS) that will govern the operation of the power plant. In this paper, DigSilent Simulation Language (DSL) is used for simulation of Turbine-Governor and excitation system. Also DigSilent Program Language (DPL) is used to simulate action of PMS when it is required. Also, different dynamic contingencies related to Damavand power plant such as Trip of grid connection, Trip of One GT, Trip of load, Trip of Two GTs, Trip of Grid connection (According to governor logic, Automatic change mode acts, Solving frequency deviation by PMS) are considered. In each scenario, power plant stability is investigated and where required, PMS functions (such as load shedding and sending appropriate set points) are applied.},
  keywords={Power system stability;Turbines;Generators;Load modeling;Velocity control;Power system dynamics;power system stability;power management system (PMS);Digsilent Software;Turbine-Governor model},
  doi={10.1109/EEEIC-2.2013.6737898},
  ISSN={},
  month={Nov},}

@INPROCEEDINGS{7323104,
  author={Sanz, Concepción and Salas, Alejandro and de Miguel, Miguel and Alonso, Alejandro and de la Puente, Juan Antonio and Benac, Clara},
  booktitle={2015 3rd International Conference on Model-Driven Engineering and Software Development (MODELSWARD)}, 
  title={Automated model-based testing based on an agnostic-platform modeling language}, 
  year={2015},
  volume={},
  number={},
  pages={1-8},
  abstract={Currently multiple Domain Specific Languages (DSLs) are used for model-driven software development, in some specific domains. Software development methods, such as agile development, are test-centered, and their application in model-based frameworks requires model support for test development. We introduce a specific language to define generic test models, which can be automatically transformed into executable tests for particular testing platforms. The resulting test models represent the test plan for applications also built according to a model-based approach. The approach presented here includes some customisations for the application of the developed languages and transformation tools for some specific testing platforms. These languages and tools have been integrated with some specific DSL designed for software development.},
  keywords={Testing;Unified modeling language;Software;Architecture;Computer architecture;Complexity theory;Engines;Model-based Testing;Automated Testing;Agile Development},
  doi={},
  ISSN={},
  month={Feb},}

@ARTICLE{6834762,
  author={Xia, Wenfeng and Wen, Yonggang and Foh, Chuan Heng and Niyato, Dusit and Xie, Haiyong},
  journal={IEEE Communications Surveys & Tutorials}, 
  title={A Survey on Software-Defined Networking}, 
  year={2015},
  volume={17},
  number={1},
  pages={27-51},
  abstract={Emerging mega-trends (e.g., mobile, social, cloud, and big data) in information and communication technologies (ICT) are commanding new challenges to future Internet, for which ubiquitous accessibility, high bandwidth, and dynamic management are crucial. However, traditional approaches based on manual configuration of proprietary devices are cumbersome and error-prone, and they cannot fully utilize the capability of physical network infrastructure. Recently, software-defined networking (SDN) has been touted as one of the most promising solutions for future Internet. SDN is characterized by its two distinguished features, including decoupling the control plane from the data plane and providing programmability for network application development. As a result, SDN is positioned to provide more efficient configuration, better performance, and higher flexibility to accommodate innovative network designs. This paper surveys latest developments in this active research area of SDN. We first present a generally accepted definition for SDN with the aforementioned two characteristic features and potential benefits of SDN. We then dwell on its three-layer architecture, including an infrastructure layer, a control layer, and an application layer, and substantiate each layer with existing research efforts and its related research areas. We follow that with an overview of the de facto SDN implementation (i.e., OpenFlow). Finally, we conclude this survey paper with some suggested open research challenges.},
  keywords={Optical switches;Routing;Software;Computer architecture;Complexity theory;Software-defined networking;SDN;network virtualization;OpenFlow},
  doi={10.1109/COMST.2014.2330903},
  ISSN={1553-877X},
  month={Firstquarter},}

@ARTICLE{5388069,
  author={Walston, C. E. and Felix, C. P.},
  journal={IBM Systems Journal}, 
  title={A method of programming measurement and estimation}, 
  year={1977},
  volume={16},
  number={1},
  pages={54-73},
  abstract={The present approach to productivity estimation, although useful, is far from being optimized. Based on the results of the variable analysis described in this paper, and supplemented by the results of the continued investigation of additional variables related to productivity, an experimental regression model has been developed. Preliminary results indicate that the model reduces the scatter. Further work is being done to determine the potential of regression as an estimating tool, as well as to extend the analyses of the areas of computer usage, documentation volume, duration, and staffing.},
  keywords={},
  doi={10.1147/sj.161.0054},
  ISSN={0018-8670},
  month={},}

@ARTICLE{6805151,
  author={Jarraya, Yosr and Madi, Taous and Debbabi, Mourad},
  journal={IEEE Communications Surveys & Tutorials}, 
  title={A Survey and a Layered Taxonomy of Software-Defined Networking}, 
  year={2014},
  volume={16},
  number={4},
  pages={1955-1980},
  abstract={Software-defined networking (SDN) has recently gained unprecedented attention from industry and research communities, and it seems unlikely that this will be attenuated in the near future. The ideas brought by SDN, although often described as a “revolutionary paradigm shift” in networking, are not completely new since they have their foundations in programmable networks and control–data plane separation projects. SDN promises simplified network management by enabling network automation, fostering innovation through programmability, and decreasing CAPEX and OPEX by reducing costs and power consumption. In this paper, we aim at analyzing and categorizing a number of relevant research works toward realizing SDN promises. We first provide an overview on SDN roots and then describe the architecture underlying SDN and its main components. Thereafter, we present existing SDN-related taxonomies and propose a taxonomy that classifies the reviewed research works and brings relevant research directions into focus. We dedicate the second part of this paper to studying and comparing the current SDN-related research initiatives and describe the main issues that may arise due to the adoption of SDN. Furthermore, we review several domains where the use of SDN shows promising results. We also summarize some foreseeable future research challenges.},
  keywords={Control systems;Taxonomy;Software defined networking;Ports (Computers);Network operating systems;Software-defined networking;OpenFlow;programmable networks;controller;management;virtualization;flow},
  doi={10.1109/COMST.2014.2320094},
  ISSN={1553-877X},
  month={Fourthquarter},}

@ARTICLE{6179576,
  author={Katsigiannis, Yiannis A. and Georgilakis, Pavlos S. and Karapidakis, Emmanuel S.},
  journal={IEEE Transactions on Sustainable Energy}, 
  title={Hybrid Simulated Annealing–Tabu Search Method for Optimal Sizing of Autonomous Power Systems With Renewables}, 
  year={2012},
  volume={3},
  number={3},
  pages={330-338},
  abstract={Small autonomous power systems (SAPS) that include renewable energy sources are a promising option for isolated power generation at remote locations. The optimal sizing problem of SAPS is a challenging combinatorial optimization problem, and its solution may prove a very time-consuming process. This paper initially investigates the performance of two popular metaheuristic methods, namely, simulated annealing (SA) and tabu search (TS), for the solution of SAPS optimal sizing problem. Moreover, this paper proposes a hybrid SA-TS method that combines the advantages of each one of the above-mentioned metaheuristic methods. The proposed method has been successfully applied to design an SAPS in Chania region, Greece. In the study, the objective function is the minimization of SAPS cost of energy (€/kWh), and the design variables are: 1) wind turbines size, 2) photovoltaics size, 3) diesel generator size, 4) biodiesel generator size, 5) fuel cells size, 6) batteries size, 7) converter size, and 8) dispatch strategy. The performance of the proposed hybrid optimization methodology is studied for a large number of alternative scenarios via sensitivity analysis, and the conclusion is that the proposed hybrid SA-TS improves the obtained solutions, in terms of quality and convergence, compared to the solutions provided by individual SA or individual TS methods.},
  keywords={Generators;Batteries;Fuels;Power systems;Simulated annealing;Renewable energy resources;Hybrid power systems;optimal sizing;optimization methods;power generation dispatch;renewable energy sources;simulated annealing (SA);small autonomous power systems (SAPS);solar energy;tabu search (TS);wind energy},
  doi={10.1109/TSTE.2012.2184840},
  ISSN={1949-3037},
  month={July},}

@INPROCEEDINGS{1498464,
  author={Ott, J. and Kutscher, D.},
  booktitle={Proceedings IEEE 24th Annual Joint Conference of the IEEE Computer and Communications Societies.}, 
  title={A disconnection-tolerant transport for drive-thru Internet environments}, 
  year={2005},
  volume={3},
  number={},
  pages={1849-1862 vol. 3},
  abstract={Today's mobile, wireless, and ad-hoc communications often exhibit extreme characteristics challenging assumptions underlying the traditional way of end-to-end communication protocol design in the Internet. One specific scenario is Internet access from moving vehicles on the road as we are researching in the drive-thru Internet project. Using wireless LAN as a broadly available access technology leads to intermittent - largely unpredictable and usually short-lived - connectivity, yet providing high performance while available. To allow Internet applications to deal reasonably well with such intermittent connectivity patterns, we have introduced a supportive drive-thru architecture. A key component is a "session" protocol offering persistent end-to-end communications even in the presence of interruptions. In this paper, we present the design of the persistent connectivity management protocol (PCMP) and report on findings from our implementation.},
  keywords={Internet;Access protocols;Mobile communication;Transport protocols;Road vehicles;Wireless LAN;Wireless networks;Performance loss;Wireless application protocol;Vehicle driving},
  doi={10.1109/INFCOM.2005.1498464},
  ISSN={0743-166X},
  month={March},}

@ARTICLE{9189773,
  author={Anand, Pooja and Singh, Yashwant and Selwal, Arvind and Alazab, Mamoun and Tanwar, Sudeep and Kumar, Neeraj},
  journal={IEEE Access}, 
  title={IoT Vulnerability Assessment for Sustainable Computing: Threats, Current Solutions, and Open Challenges}, 
  year={2020},
  volume={8},
  number={},
  pages={168825-168853},
  abstract={Over the last few decades, sustainable computing has been widely used in areas like social computing, artificial intelligence-based agent systems, mobile computing, and Internet of Things (IoT). There are social, economic, and commercial impacts of IoT on human lives. However, IoT nodes are generally power-constrained with data transmission using an open channel, i.e., Internet which opens the gates for various types of attacks on them. In this context, several efforts are initiated to deal with the evolving security issues in IoT systems and make them self-sufficient to harvest energy for smooth functioning. Motivated by these facts, in this paper, we explore the evolving vulnerabilities in IoT devices. We provide a state-of-the-art survey that addresses multiple dimensions of the IoT realm. Moreover, we provide a general overview of IoT, Sustainable IoT, its architecture, and the Internet Engineering Task Force (IETF) protocol suite. Subsequently, we explore the open-source tools and datasets for the proliferation in research and growth of IoT. A detailed taxonomy of attacks associated with various vulnerabilities is also presented in the text. Then we have specifically focused on the IoT Vulnerability Assessment techniques followed by a case study on sustainability of Smart Agriculture. Finally, this paper outlines the emerging challenges related to IoT and its sustainability, and opening the doors for the beginners to start research in this promising area.},
  keywords={Security;Protocols;Internet of Things;Computer architecture;Temperature sensors;IoT;machine learning;sustainability;cyberattacks;vulnerabilities;security;privacy},
  doi={10.1109/ACCESS.2020.3022842},
  ISSN={2169-3536},
  month={},}

@ARTICLE{8456508,
  author={Yu, Yinbo and Li, Xing and Leng, Xue and Song, Libin and Bu, Kai and Chen, Yan and Yang, Jianfeng and Zhang, Liang and Cheng, Kang and Xiao, Xin},
  journal={IEEE Communications Surveys & Tutorials}, 
  title={Fault Management in Software-Defined Networking: A Survey}, 
  year={2019},
  volume={21},
  number={1},
  pages={349-392},
  abstract={Software-defined networking (SDN) has emerged as a new network paradigm that promises control/data plane separation and centralized network control. While these features simplify network management and enable innovative networking, they give rise to persistent concerns about reliability. The new paradigm suffers from the disadvantage that various network faults may consistently undermine the reliability of such a network, and such faults are often new and difficult to resolve with existing solutions. To ensure SDN reliability, fault management, which is concerned with detecting, localizing, correcting and preventing faults, has become a key component in SDN networks. Although many SDN fault management solutions have been proposed, we find that they often resolve SDN faults from an incomplete perspective which may result in side effects. More critically, as the SDN paradigm evolves, additional fault types are being exposed. Therefore, comprehensive reviews and constant improvements are required to remain on the leading edge of SDN fault management. In this paper, we present the first comprehensive and systematic survey of SDN faults and related management solutions identified through advancements in both the research community and industry. We apply a systematic classification of SDN faults, compare and analyze existing SDN fault management solutions in the literature, and conduct a gap analysis between solutions developed in an academic research context and practical deployments. The current challenges and emerging trends are also noted as potential future research directions. This paper aims to provide academic researchers and industrial engineers with a comprehensive survey with the hope of advancing SDN and inspiring new solutions.},
  keywords={Software;Hardware;Industries;Fault tolerance;Fault tolerant systems;Computer architecture;Software-defined networking (SDN);SDN reliability;SDN faults;fault classification;system monitoring;fault diagnosis;fault recovery and repair;fault tolerance},
  doi={10.1109/COMST.2018.2868922},
  ISSN={1553-877X},
  month={Firstquarter},}

@ARTICLE{4814954,
  author={Liggesmeyer, Peter and Trapp, Mario},
  journal={IEEE Software}, 
  title={Trends in Embedded Software Engineering}, 
  year={2009},
  volume={26},
  number={3},
  pages={19-25},
  abstract={Software's importance in the development of embedded systems has been growing rapidly over the last 20 years. Because of current embedded systems' complexity, they require sophisticated engineering methods for systematically developing high-quality software. Embedded software development differs from IT system development in several ways. For example, IT systems developers can use standard hardware and software platforms and don't face the resource requirements that embedded systems developers must take into account. To meet embedded software's extrafunctional requirements, embedded systems development is shifting from programming to model-driven development. Another important trend is the emphasis on the quality assurance of safety-related systems.},
  keywords={Embedded software;Object oriented modeling;Mathematical model;Embedded system;Hardware;IEC standards;Costs;Automotive engineering;Computer languages;Operating systems;embedded systems development;model-driven development;embedded software;quality assurance;safety-critical systems},
  doi={10.1109/MS.2009.80},
  ISSN={1937-4194},
  month={May},}

@ARTICLE{9310181,
  author={Rajabli, Nijat and Flammini, Francesco and Nardone, Roberto and Vittorini, Valeria},
  journal={IEEE Access}, 
  title={Software Verification and Validation of Safe Autonomous Cars: A Systematic Literature Review}, 
  year={2021},
  volume={9},
  number={},
  pages={4797-4819},
  abstract={Autonomous, or self-driving, cars are emerging as the solution to several problems primarily caused by humans on roads, such as accidents and traffic congestion. However, those benefits come with great challenges in the verification and validation (V&V) for safety assessment. In fact, due to the possibly unpredictable nature of Artificial Intelligence (AI), its use in autonomous cars creates concerns that need to be addressed using appropriate V&V processes that can address trustworthy AI and safe autonomy. In this study, the relevant research literature in recent years has been systematically reviewed and classified in order to investigate the state-of-the-art in the software V&V of autonomous cars. By appropriate criteria, a subset of primary studies has been selected for more in-depth analysis. The first part of the review addresses certification issues against reference standards, challenges in assessing machine learning, as well as general V&V methodologies. The second part investigates more specific approaches, including simulation environments and mutation testing, corner cases and adversarial examples, fault injection, software safety cages, techniques for cyber-physical systems, and formal methods. Relevant approaches and related tools have been discussed and compared in order to highlight open issues and opportunities.},
  keywords={Vehicles;Autonomous automobiles;Safety;Software;Accidents;Roads;Systematics;Advanced driver assistance systems;automotive engineering;autonomous vehicles;cyber-physical systems;formal verification;intelligent vehicles;machine learning;system testing;system validation;vehicle safety},
  doi={10.1109/ACCESS.2020.3048047},
  ISSN={2169-3536},
  month={},}

@ARTICLE{182763,
  author={},
  journal={IEEE Std 610}, 
  title={IEEE Standard Computer Dictionary: A Compilation of IEEE Standard Computer Glossaries}, 
  year={1991},
  volume={},
  number={},
  pages={1-217},
  abstract={Identifies terms currently in use in the computer field. Standard definitions for thoseterms are established. Compilation of IEEE Stds IEEE Std 1084, IEEE Std 610.2, IEEE Std 610.3, IEEE Std 610.4, IEEE Std 610.5 and IEEE Std 610.12},
  keywords={Terminology;terminology;computer;applications;glossary;definitions;dictionary;610},
  doi={10.1109/IEEESTD.1991.106963},
  ISSN={},
  month={Jan},}

@ARTICLE{898825,
  author={MacMillen, D. and Camposano, R. and Hill, D. and Williams, T.W.},
  journal={IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems}, 
  title={An industrial view of electronic design automation}, 
  year={2000},
  volume={19},
  number={12},
  pages={1428-1448},
  abstract={The automation of the design of electronic systems and circuits [electronic design automation (EDA)] has a history of strong innovation. The EDA business has profoundly influenced the integrated circuit (IC) business and vice-versa. This paper reviews the technologies, algorithms, and methodologies that have been used in EDA tools and the business impact of these technologies. In particular, we focus on four areas that have been key in defining the design methodologies over time: physical design, simulation/verification, synthesis, and test. We then look briefly into the future. Design will evolve toward more software programmability or some other kind of field configurability like field programmable gate arrays (FPGAs). We discuss the kinds of tool sets needed to support design in this environment.},
  keywords={Electronics industry;Electronic design automation and methodology;Integrated circuit technology;Field programmable gate arrays;Design automation;History;Technological innovation;Design methodology;Circuit simulation;Circuit synthesis},
  doi={10.1109/43.898825},
  ISSN={1937-4151},
  month={Dec},}

@INPROCEEDINGS{6263963,
  author={Maji, Amiya K. and Arshad, Fahad A. and Bagchi, Saurabh and Rellermeyer, Jan S.},
  booktitle={IEEE/IFIP International Conference on Dependable Systems and Networks (DSN 2012)}, 
  title={An empirical study of the robustness of Inter-component Communication in Android}, 
  year={2012},
  volume={},
  number={},
  pages={1-12},
  abstract={Over the last three years, Android has established itself as the largest-selling operating system for smartphones. It boasts of a Linux-based robust kernel, a modular framework with multiple components in each application, and a security-conscious design where each application is isolated in its own virtual machine. However, all of these desirable properties would be rendered ineffectual if an application were to deliver erroneous messages to targeted applications and thus cause the target to behave incorrectly. In this paper, we present an empirical evaluation of the robustness of Inter-component Communication (ICC) in Android through fuzz testing methodology, whereby, parameters of the inter-component communication are changed to various incorrect values. We show that not only exception handling is a rarity in Android applications, but also it is possible to crash the Android runtime from unprivileged user processes. Based on our observations, we highlight some of the critical design issues in Android ICC and suggest solutions to alleviate these problems.},
  keywords={Androids;Humanoid robots;Smart phones;Robustness;Testing;Runtime;Receivers;android;fuzz;security;smartphone;robustness;exception},
  doi={10.1109/DSN.2012.6263963},
  ISSN={2158-3927},
  month={June},}

@ARTICLE{7997723,
  author={Raibulet, Claudia and Arcelli Fontana, Francesca and Zanoni, Marco},
  journal={IEEE Access}, 
  title={Model-Driven Reverse Engineering Approaches: A Systematic Literature Review}, 
  year={2017},
  volume={5},
  number={},
  pages={14516-14542},
  abstract={This paper explores and describes the state of the art for what concerns the model-driven approaches proposed in the literature to support reverse engineering. We conducted a systematic literature review on this topic with the aim to answer three research questions. We focus on various solutions developed for model-driven reverse engineering, outlining in particular the models they use and the transformations applied to the models. We also consider the tools used for model definition, extraction, and transformation and the level of automation reached by the available tools. The model-driven reverse engineering approaches are also analyzed based on various features such as genericity, extensibility, automation of the reverse engineering process, and coverage of the full or partial source artifacts. We describe in detail and compare fifteen approaches applying model-driven reverse engineering. Based on this analysis, we identify and indicate some hints on choosing a model-driven reverse engineering approach from the available ones, and we outline open issues concerning the model-driven reverse engineering approaches.},
  keywords={Reverse engineering;Object oriented modeling;Tools;Systematics;Analytical models;Bibliographies;Search engines;Models;reverse engineering;model-driven reverse engineering;model transformation;legacy system},
  doi={10.1109/ACCESS.2017.2733518},
  ISSN={2169-3536},
  month={},}

@INPROCEEDINGS{1317496,
  author={Batory, D.},
  booktitle={Proceedings. 26th International Conference on Software Engineering}, 
  title={Feature-oriented programming and the AHEAD tool suite}, 
  year={2004},
  volume={},
  number={},
  pages={702-703},
  abstract={Feature oriented programming (FOP) is an emerging paradigm for application synthesis, analysis, and optimization. A target application is specified declaratively as a set of features, like many consumer products (e.g., personal computers, automobiles). FOP technology translates such declarative specifications into efficient programs.},
  keywords={Automatic programming;Algebra;Application software;Java;Software engineering;Design optimization;Query processing;Large-scale systems;Domain specific languages;Prototypes},
  doi={10.1109/ICSE.2004.1317496},
  ISSN={0270-5257},
  month={May},}

@INPROCEEDINGS{493448,
  author={Kieburtz, R.B. and McKinney, L. and Bell, J.M. and Hook, J. and Kotov, A. and Lewis, J. and Oliva, D.P. and Sheard, T. and Smith, I. and Walton, L.},
  booktitle={Proceedings of IEEE 18th International Conference on Software Engineering}, 
  title={A software engineering experiment in software component generation}, 
  year={1996},
  volume={},
  number={},
  pages={542-552},
  abstract={The paper presents results of a software engineering experiment in which a new technology for constructing program generators from domain-specific specification languages has been compared with a reuse technology that employs sets of reusable Ada program templates. Both technologies were applied to a common problem domain, constructing message translation and validation modules for military command, control, communications and information systems (C/sup 3/I). The experiment employed four subjects to conduct trials of use of the two technologies on a common set of test examples. The experiment was conducted with personnel supplied and supervised by an independent contractor. Test cases consisted of message specifications taken from Air Force C/sup 3/I systems. The main results are that greater productivity was achieved and fewer error were introduced when subjects used the program generator than when they used Ada templates to implement software modules from sets of specifications. The differences in the average performance of the subjects are statistically significant at confidence levels exceeding 99 percent.},
  keywords={Software engineering;Paper technology;Automatic programming;System testing;Specification languages;Military communication;Communication system control;Control systems;Information systems;Personnel},
  doi={10.1109/ICSE.1996.493448},
  ISSN={0270-5257},
  month={March},}

@ARTICLE{8016712,
  author={},
  journal={ISO/IEC/IEEE 24765:2017(E)}, 
  title={ISO/IEC/IEEE International Standard - Systems and software engineering--Vocabulary}, 
  year={2017},
  volume={},
  number={},
  pages={1-541},
  abstract={This document provides a common vocabulary applicable to all systems and software engineering work. It was prepared to collect and standardize terminology. This document is intended to serve as a useful reference for those in the information technology field, and to encourage the use of systems and software engineering standards prepared by ISO and liaison organizations IEEE Computer Society and Project Management Institute. This document includes references to the active source standards for definitions so that systems and software engineering concepts and requirements can be further explored.},
  keywords={IEEE Standards;ISO Standards;IEC Standards;Informatino technology;Software engineering;Systems engineering and theoryt;Terminology;computer;dictionary;information technology;software engineering;systems engineering;24765},
  doi={10.1109/IEEESTD.2017.8016712},
  ISSN={},
  month={Aug},}

@ARTICLE{8970242,
  author={Zhang, Jingjing and Yang, Lin and Cao, Weipeng and Wang, Qiang},
  journal={IEEE Access}, 
  title={Formal Analysis of 5G EAP-TLS Authentication Protocol Using Proverif}, 
  year={2020},
  volume={8},
  number={},
  pages={23674-23688},
  abstract={As a critical component of the security architecture of 5G network, the authentication protocol plays a role of the first safeguard in ensuring the communication security, such as the confidentiality of user data. EAP-TLS is one of such protocols being defined in the 5G standards to provide key services in the specific IoT circumstances. This protocol is currently under the process of standardization, and it is vital to guarantee that the standardized protocol is free from any design flaws, which may result in severe vulnerabilities and serious consequences when implemented in real systems. However, it is still unclear whether the proposed 5G EAP-TLS authentication protocol provides the claimed security guarantees. To fill this gap, we present in this work a comprehensive formal analysis of the security related properties of the 5G EAP-TLS authentication protocol based on the symbolic model checking approach. Specifically, we build the first formal model of the 5G EAP-TLS authentication protocol in the applied pi calculus, and perform an automated security analysis of the formal protocol model by using the ProVerif model checker. Our analysis results show that there are some subtle flaws in the current protocol design that may compromise the claimed security objectives. To this end, we also propose and verify a possible fix that is able to mitigate these flaws. To the best of our knowledge, this is the first thorough formal analysis of the 5G EAP-TLS authentication protocol.},
  keywords={Protocols;5G mobile communication;Authentication;Analytical models;Cryptography;Mathematical model;Authentication protocol;5G network;formal verification;model checking;applied pi calculus;ProVerif;EAP-TLS},
  doi={10.1109/ACCESS.2020.2969474},
  ISSN={2169-3536},
  month={},}

@INPROCEEDINGS{6227131,
  author={Thummalapenta, Suresh and Sinha, Saurabh and Singhania, Nimit and Chandra, Satish},
  booktitle={2012 34th International Conference on Software Engineering (ICSE)}, 
  title={Automating test automation}, 
  year={2012},
  volume={},
  number={},
  pages={881-891},
  abstract={Mention “test case”, and it conjures up the image of a script or a program that exercises a system under test. In industrial practice, however, test cases often start out as steps described in natural language. These are essentially directions a human tester needs to follow to interact with an application, exercising a given scenario. Since tests need to be executed repeatedly, such manual tests then have to go through test automation to create scripts or programs out of them. Test automation can be expensive in programmer time. We describe a technique to automate test automation. The input to our technique is a sequence of steps written in natural language, and the output is a sequence of procedure calls with accompanying parameters that can drive the application without human intervention. The technique is based on looking at the natural language test steps as consisting of segments that describe actions on targets, except that there can be ambiguity in identifying segments, in identifying the action in a segment, as well as in the specification of the target of the action. The technique resolves this ambiguity by backtracking, until it can synthesize a successful sequence of calls. We present an evaluation of our technique on professionally created manual test cases for two open-source web applications as well as a proprietary enterprise application. Our technique could automate over 82% of the steps contained in these test cases with no human intervention, indicating that the technique can reduce the cost of test automation quite effectively.},
  keywords={Manuals;Automation;Optimization;Humans;Natural languages;Programming profession},
  doi={10.1109/ICSE.2012.6227131},
  ISSN={1558-1225},
  month={June},}

@INPROCEEDINGS{879426,
  author={Yu-Wen Tung and Aldiwan, W.S.},
  booktitle={2000 IEEE Aerospace Conference. Proceedings (Cat. No.00TH8484)}, 
  title={Automating test case generation for the new generation mission software system}, 
  year={2000},
  volume={1},
  number={},
  pages={431-437 vol.1},
  abstract={The significant expansion of autonomous control and information processing capabilities in the coming generation of mission software systems results in a qualitatively larger space of behaviors that needs to be "covered" during testing, not only at the system level but also at subsystem and unit levels. A major challenge in this area is to automatically generate a relatively small set of test cases that, collectively, guarantees a selected degree of coverage of the behavior space. This paper describes an algorithm for a parametric test case generation tool that applies a combinatorial design approach to the selection of candidate test cases. Evaluation of this algorithm on test parameters from the Deep Space One mission reveals a valuable reduction in the number of test cases, when compared to an earlier home-brewed generator.},
  keywords={Automatic testing;Space missions;Automatic generation control;Process control;Control systems;Information processing;Software systems;Software testing;System testing;Algorithm design and analysis},
  doi={10.1109/AERO.2000.879426},
  ISSN={1095-323X},
  month={March},}

@ARTICLE{9427143,
  author={Zheng, Miaomiao and Zhang, Shanshan and Zhang, Yidan and Hu, Baozhong},
  journal={IEEE Access}, 
  title={Construct Food Safety Traceability System for People’s Health Under the Internet of Things and Big Data}, 
  year={2021},
  volume={9},
  number={},
  pages={70571-70583},
  abstract={In the context of epidemic prevention and control, food safety monitoring, data analysis and food safety traceability have become more important. At the same time, the most important reason for food safety issues is incomplete, opaque, and asymmetric information. The most fundamental way to solve these problems is to do a good job of traceability, and establish a reasonable and reliable food safety traceability system. The traceability system is currently an important means to ensure food quality and safety and solve the crisis of trust between consumers and the market. Research on food safety traceability systems based on big data, artificial intelligence and the Internet of Things provides ideas and methods to solve the problems of low credibility and difficult data storage in the application of traditional traceability systems. Therefore, this research takes rice as an example and proposes a food safety traceability system based on RFID two-dimensional code technology and big data storage technology in the Internet of Things. This article applies RFID technology to the entire system by analyzing the requirements of the system, designing the system database and database tables, encoding the two-dimensional code and generating the design for information entry. Using RFID radio frequency technology and the data storage function in big data to obtain information in the food production process. Finally, the whole process of food production information can be traced through the design of dynamic query platform and mobile terminal. In this research, the food safety traceability system based on big data and the Internet of Things guarantees the integrity, reliability and safety of traceability information from a technical level. This is an effective solution for enhancing the credibility of traceability information, ensuring the integrity of information, and optimizing the data storage structure.},
  keywords={Safety;Big Data;Internet of Things;Production;Radiofrequency identification;Python;Epidemics;Two-dimensional code technology;Internet of Things;big data;artificial intelligence;food safety traceability system},
  doi={10.1109/ACCESS.2021.3078536},
  ISSN={2169-3536},
  month={},}

@ARTICLE{9950507,
  author={De Saqui-Sannes, Pierre and Vingerhoeds, Rob A. and Garion, Christophe and Thirioux, Xavier},
  journal={IEEE Access}, 
  title={A Taxonomy of MBSE Approaches by Languages, Tools and Methods}, 
  year={2022},
  volume={10},
  number={},
  pages={120936-120950},
  abstract={Systems engineering has gained in maturity over the last decades and started a transition from document-centric approaches to Model-Based Systems Engineering (MBSE). Several papers have discussed the benefits and potential, but also the limitations, of using MBSE, based on literature surveys and analyze feedback from academia and industry. The current paper explores a complementary avenue and aims at giving students and industry practitioners a set of keys and decision criteria to select MBSE languages, tools and methods. Languages, tools and methods are categorised and selection criteria are proposed for a panorama of languages that goes beyond SysML and other techniques commonly associated with MBSE. In addition, research avenues for the future of MBSE are identified. The discussion relies on the authors’ experience in teaching and using system engineering and MBSE in both academia and industry, as well as on the experience shared within the framework of Concorde, a French project dedicated to drone systems design methodologies.},
  keywords={Unified modeling language;Modeling;Analytical models;Systems engineering and theory;Taxonomy;Object oriented modeling;MBSE;formal methods;method;modeling tools;safety-critical systems;SysML;systems engineering},
  doi={10.1109/ACCESS.2022.3222387},
  ISSN={2169-3536},
  month={},}

@ARTICLE{7820199,
  author={Fleck, Martin and Troya, Javier and Kessentini, Marouane and Wimmer, Manuel and Alkhazi, Bader},
  journal={IEEE Transactions on Software Engineering}, 
  title={Model Transformation Modularization as a Many-Objective Optimization Problem}, 
  year={2017},
  volume={43},
  number={11},
  pages={1009-1032},
  abstract={Model transformation programs are iteratively refined, restructured, and evolved due to many reasons such as fixing bugs and adapting existing transformation rules to new metamodels version. Thus, modular design is a desirable property for model transformations as it can significantly improve their evolution, comprehensibility, maintainability, reusability, and thus, their overall quality. Although language support for modularization of model transformations is emerging, model transformations are created as monolithic artifacts containing a huge number of rules. To the best of our knowledge, the problem of automatically modularizing model transformation programs was not addressed before in the current literature. These programs written in transformation languages, such as ATL, are implemented as one main module including a huge number of rules. To tackle this problem and improve the quality and maintainability of model transformation programs, we propose an automated search-based approach to modularize model transformations based on higher-order transformations. Their application and execution is guided by our search framework which combines an in-place transformation engine and a search-based algorithm framework. We demonstrate the feasibility of our approach by using ATL as concrete transformation language and NSGA-III as search algorithm to find a trade-off between different well-known conflicting design metrics for the fitness functions to evaluate the generated modularized solutions. To validate our approach, we apply it to a comprehensive dataset of model transformations. As the study shows, ATL transformations can be modularized automatically, efficiently, and effectively by our approach. We found that, on average, the majority of recommended modules, for all the ATL programs, by NSGA-III are considered correct with more than 84 percent of precision and 86 percent of recall when compared to manual solutions provided by active developers. The statistical analysis of our experiments over several runs shows that NSGA-III performed significantly better than multi-objective algorithms and random search. We were not able to compare with existing model transformations modularization approaches since our study is the first to address this problem. The software developers considered in our experiments confirm the relevance of the recommended modularization solutions for several maintenance activities based on different scenarios and interviews.},
  keywords={Unified modeling language;Object oriented modeling;Adaptation models;Measurement;Algorithm design and analysis;Software engineering;Computer bugs;Model transformation;modularization;ATL;NSGA-III;MDE;SBSE},
  doi={10.1109/TSE.2017.2654255},
  ISSN={1939-3520},
  month={Nov},}

@ARTICLE{7992926,
  author={Garcia, Boni and Gortazar, Francisco and Lopez-Fernandez, Luis and Gallego, Micael and Paris, Miguel},
  journal={IEEE Communications Standards Magazine}, 
  title={WebRTC Testing: Challenges and Practical Solutions}, 
  year={2017},
  volume={1},
  number={2},
  pages={36-42},
  abstract={WebRTC comprises a set of novel technologies and standards that provide Real-Time Communication on Web browsers. WebRTC makes simple the embedding of voice and video communications in all types of applications. However, releasing those applications to production is still very challenging due to the complexity of their testing. Validating a WebRTC service requires assessing many functional (e.g. signaling logic, media connectivity, etc.) and non-functional (e.g. quality of experience, interoperability, scalability, etc.) properties on large, complex, distributed and heterogeneous systems that spawn across client devices, networks and cloud infrastructures. In this article, we present a novel methodology and an associated tool for doing it at scale and in an automated way. Our strategy is based on a blackbox end-to-end approach through which we use an automated containerized cloud environment for instrumenting Web browser clients, which benchmark the SUT (system under test), and fake clients, that load it. Through these benchmarks, we obtain, in a reliable and statistically significant way, both network-dependent QoS (Quality of Service) metrics and media-dependent QoE (Quality of Experience) indicators. These are fed, at a second stage, to a number of testing assertions that validate the appropriateness of the functional and non-functional properties of the SUT under controlled and configurable load and fail conditions. To finish, we illustrate our experiences using such tool and methodology in the context of the Kurento open source software project and conclude that they are suitable for validating large and complex WebRTC systems at scale.},
  keywords={WebRTC;Browsers;Telecommunication traffic;Media;Real-time systems;Quality of service;Internet},
  doi={10.1109/MCOMSTD.2017.1700005},
  ISSN={2471-2833},
  month={},}

@INPROCEEDINGS{9304609,
  author={Xinxin, Zhang and Fei, Li and Xiangbin, Wu},
  booktitle={2020 IEEE Intelligent Vehicles Symposium (IV)}, 
  title={CSG: Critical Scenario Generation from Real Traffic Accidents}, 
  year={2020},
  volume={},
  number={},
  pages={1330-1336},
  abstract={Autonomous driving (AD) is getting closer to our life, but the severe traffic accidents of autonomous vehicle (AV) happened in the past several years warn us that the safety of AVs is still a big challenge for the AD industry. Before volume production, the automotive industry and regulators must ensure the AV can deal with dangerous scenarios. Although road test is the most common method to test the performance and safety of an AV, it has some manifest disadvantages, e.g., highly risky and unrepeatable, low efficiency and lack of useful critical scenarios. Critical-scenario-based simulation can effectively address these problems and become an important complement to road test. In this paper, we present a novel approach to extract critical scenarios from real traffic accident videos and re-generate them in a simulator. We also introduce our integrated toolkit for scenario extraction and scenario test. With the toolkit, we can build a critical scenario library quickly and use it as a benchmark for AV safety assessment, among other purposes. On top of this, we further introduce our safety assessment criteria and scoring method.},
  keywords={Roads;Accidents;Safety;Videos;Libraries;Three-dimensional displays;Automobiles},
  doi={10.1109/IV47402.2020.9304609},
  ISSN={2642-7214},
  month={Oct},}

@ARTICLE{991333,
  author={Hieatt, E. and Mee, R.},
  journal={IEEE Software}, 
  title={Going faster: testing the Web application}, 
  year={2002},
  volume={19},
  number={2},
  pages={60-65},
  abstract={This article documents the experiences of Evant's Extreme Programming team with testing XP. Testing is fundamental to XP but is a practice that often falls by the wayside in today's fast-paced Web application development culture. From the beginning, Evant adhered to each of XP's principles, and testing was no exception. This article explains how the team found that testing, positioned as the drive behind development, was critical to the success of building Evant's application at speed while maintaining high quality.},
  keywords={Application software;Software testing;Writing;Buildings;Software engineering;Internet;Software quality;Software maintenance;Software tools;Java},
  doi={10.1109/52.991333},
  ISSN={1937-4194},
  month={March},}

@INPROCEEDINGS{8257807,
  author={Perera, Pulasthi and Silva, Roshali and Perera, Indika},
  booktitle={2017 Seventeenth International Conference on Advances in ICT for Emerging Regions (ICTer)}, 
  title={Improve software quality through practicing DevOps}, 
  year={2017},
  volume={},
  number={},
  pages={1-6},
  abstract={DevOps is extended from certain agile practices with a mix of patterns intended to improve collaboration between development and operation teams. The main purpose of this paper is to conduct a study on how DevOps practice has impacted to software quality. The secondary objective is to find how to improve quality efficiently. A literature survey has carried out to explore about current DevOps practices in industry. According to the literature survey, the conceptual research model was developed and five hypotheses were derived. Research objectives were accomplished by testing hypotheses using Pearson correlation. A linear model is derived based on the linear regression analysis. An online questionnaire was used to collect quantitative data whereas interviews with experts on DevOps and Quality assurance have been used to identify how to improve the quality of software by practicing DevOps. Recommendations are given based on interview feedback, hypotheses testing with regression analysis. According to the quantitative study, researchers have identified that quality of the software gets improved when practice DevOps by following CAMS (Culture, Automation, Measurement, Sharing) framework. Automation is the most critical factor to improve the software quality. As per the results of multiple regression analysis, it has proved culture, automation, measurement and sharing are important factors to consider to improve quality of the software. In conclusion it can be recommended to use DevOps to achieve high quality software.},
  keywords={Companies;Automation;Software quality;Testing;Software measurement;DevOps;CAMS Framework;Quality;ISO 9126;Automation},
  doi={10.1109/ICTER.2017.8257807},
  ISSN={2472-7598},
  month={Sep.},}

@ARTICLE{8451922,
  author={SAYAGH, Mohammed and Kerzazi, Noureddine and Adams, Bram and Petrillo, Fabio},
  journal={IEEE Transactions on Software Engineering}, 
  title={Software Configuration Engineering in Practice Interviews, Survey, and Systematic Literature Review}, 
  year={2020},
  volume={46},
  number={6},
  pages={646-673},
  abstract={Modern software applications are adapted to different situations (e.g., memory limits, enabling/disabling features, database credentials) by changing the values of configuration options, without any source code modifications. According to several studies, this flexibility is expensive as configuration failures represent one of the most common types of software failures. They are also hard to debug and resolve as they require a lot of effort to detect which options are misconfigured among a large number of configuration options and values, while comprehension of the code also is hampered by sprinkling conditional checks of the values of configuration options. Although researchers have proposed various approaches to help debug or prevent configuration failures, especially from the end users’ perspective, this paper takes a step back to understand the process required by practitioners to engineer the run-time configuration options in their source code, the challenges they experience as well as best practices that they have or could adopt. By interviewing 14 software engineering experts, followed by a large survey on 229 Java software engineers, we identified 9 major activities related to configuration engineering, 22 challenges faced by developers, and 24 expert recommendations to improve software configuration quality. We complemented this study by a systematic literature review to enrich the experts’ recommendations, and to identify possible solutions discussed and evaluated by the research community for the developers’ problems and challenges. We find that developers face a variety of challenges for all nine configuration engineering activities, starting from the creation of options, which generally is not planned beforehand and increases the complexity of a software system, to the non-trivial comprehension and debugging of configurations, and ending with the risky maintenance of configuration options, since developers avoid touching and changing configuration options in a mature system. We also find that researchers thus far focus primarily on testing and debugging configuration failures, leaving a large range of opportunities for future work.},
  keywords={Software systems;Interviews;Systematics;Facebook;Bibliographies;Software algorithms;Empirical study;configuration;configuration engineering;Systematic literature review;interviews;survey},
  doi={10.1109/TSE.2018.2867847},
  ISSN={1939-3520},
  month={June},}

@ARTICLE{4346539,
  author={Chen, Yan and Bindel, David and Song, Han Hee and Katz, Randy H.},
  journal={IEEE/ACM Transactions on Networking}, 
  title={Algebra-Based Scalable Overlay Network Monitoring: Algorithms, Evaluation, and Applications}, 
  year={2007},
  volume={15},
  number={5},
  pages={1084-1097},
  abstract={Overlay network monitoring enables distributed Internet applications to detect and recover from path outages and periods of degraded performance within seconds. For an overlay network with end hosts, existing systems either require measurements, and thus lack scalability, or can only estimate the latency but not congestion or failures. Our earlier extended abstract [Y. Chen, D. Bindel, and R. H. Katz, ldquoTomography-based overlay network monitoring,rdquo Proceedings of the ACM SIGCOMM Internet Measurement Conference (IMC), 2003] briefly proposes an algebraic approach that selectively monitors linearly independent paths that can fully describe all the paths. The loss rates and latency of these paths can be used to estimate the loss rates and latency of all other paths. Our scheme only assumes knowledge of the underlying IP topology, with links dynamically varying between lossy and normal. In this paper, we improve, implement, and extensively evaluate such a monitoring system. We further make the following contributions: i) scalability analysis indicating that for reasonably large n (e.g., 100), the growth of k is bounded as O(n log n), ii) efficient adaptation algorithms for topology changes, such as the addition or removal of end hosts and routing changes, iii) measurement load balancing schemes, iv) topology measurement error handling, and v) design and implementation of an adaptive streaming media system as a representative application. Both simulation and Internet experiments demonstrate we obtain highly accurate path loss rate estimation while adapting to topology changes within seconds and handling topology errors.},
  keywords={Topology;Delay;Condition monitoring;IP networks;Scalability;Algorithm design and analysis;Degradation;Routing;Load management;Measurement errors;Dynamics;load balancing;network measurement and monitoring;numerical linear algebra;overlay;scalability},
  doi={10.1109/TNET.2007.896251},
  ISSN={1558-2566},
  month={Oct},}

@INPROCEEDINGS{6903575,
  author={Kane, Aaron and Fuhrman, Thomas and Koopman, Philip},
  booktitle={2014 44th Annual IEEE/IFIP International Conference on Dependable Systems and Networks}, 
  title={Monitor Based Oracles for Cyber-Physical System Testing: Practical Experience Report}, 
  year={2014},
  volume={},
  number={},
  pages={148-155},
  abstract={Testing Cyber-Physical Systems is becoming increasingly challenging as they incorporate advanced autonomy features. We investigate using an external runtime monitor as a partial test oracle to detect violations of critical system behavioral requirements on an automotive development platform. Despite limited source code access and using only existing network messages, we were able to monitor a hardware-in-the-loop vehicle simulator and analyze prototype vehicle log data to detect violations of high-level critical properties. Interface robustness testing was useful to further exercise the monitors. Beyond demonstrating feasibility, the experience emphasized a number of remaining research challenges, including: approximating system intent based on limited system state observability, how to best balance the simplicity and expressiveness of the specification language used to define monitored properties, how to warm up monitoring of system variable state after mode change discontinuities, and managing the differences between simulation and real vehicles when conducting such tests.},
  keywords={Monitoring;Vehicles;Testing;Runtime;Robustness;Safety;Prototypes;runtime monitoring;testing;cyber-physical systems},
  doi={10.1109/DSN.2014.28},
  ISSN={2158-3927},
  month={June},}

@INPROCEEDINGS{8667986,
  author={Brito, Gleison and Mombach, Thais and Valente, Marco Tulio},
  booktitle={2019 IEEE 26th International Conference on Software Analysis, Evolution and Reengineering (SANER)}, 
  title={Migrating to GraphQL: A Practical Assessment}, 
  year={2019},
  volume={},
  number={},
  pages={140-150},
  abstract={GraphQL is a novel query language proposed by Facebook to implement Web-based APIs. In this paper, we present a practical study on migrating API clients to this new technology. First, we conduct a grey literature review to gain an in-depth understanding on the benefits and key characteristics normally associated to GraphQL by practitioners. After that, we assess such benefits in practice, by migrating seven systems to use GraphQL, instead of standard REST-based APIs. As our key result, we show that GraphQL can reduce the size of the JSON documents returned by REST APIs in 94% (in number of fields) and in 99% (in number of bytes), both median results.},
  keywords={Servers;Bibliographies;Computer hacking;Databases;Database languages;Uniform resource locators;Blogs;GraphQL;REST;APIs;Migration Study},
  doi={10.1109/SANER.2019.8667986},
  ISSN={1534-5351},
  month={Feb},}

@ARTICLE{10064002,
  author={Zhou, Yuan and Sun, Yang and Tang, Yun and Chen, Yuqi and Sun, Jun and Poskitt, Christopher M. and Liu, Yang and Yang, Zijiang},
  journal={IEEE Transactions on Software Engineering}, 
  title={Specification-Based Autonomous Driving System Testing}, 
  year={2023},
  volume={49},
  number={6},
  pages={3391-3410},
  abstract={Autonomous vehicle (AV) systems must be comprehensively tested and evaluated before they can be deployed. High-fidelity simulators such as CARLA or LGSVL allow this to be done safely in very realistic and highly customizable environments. Existing testing approaches, however, fail to test simulated AVs systematically, as they focus on specific scenarios and oracles (e.g., lane following scenario with the “no collision” requirement) and lack any coverage criteria measures. In this paper, we propose $\mathtt {AVUnit}$AVUnit, a framework for systematically testing AV systems against customizable correctness specifications. Designed modularly to support different simulators, $\mathtt {AVUnit}$AVUnit consists of two new languages for specifying dynamic properties of scenes (e.g., changing pedestrian behaviour after waypoints) and fine-grained assertions about the AV's journey. $\mathtt {AVUnit}$AVUnit further supports multiple fuzzing algorithms that automatically search for test cases that violate these assertions, using robustness and coverage measures as fitness metrics. We evaluated the implementation of $\mathtt {AVUnit}$AVUnit for the LGSVL+Apollo simulation environment, finding 19 kinds of issues in Apollo, which indicate that the open-source Apollo does not perform well in complex intersections and lane-changing related scenarios.},
  keywords={Testing;Fuzzing;Planning;Roads;Vehicle dynamics;Sun;Sensors;Autonomous driving system;coverage criteria;fuzzing;specification languages;testing},
  doi={10.1109/TSE.2023.3254142},
  ISSN={1939-3520},
  month={June},}

@INPROCEEDINGS{1387329,
  author={Kundu, S. and Mak, T.M. and Galivanche, R.},
  booktitle={2004 International Conferce on Test}, 
  title={Trends in manufacturing test methods and their implications}, 
  year={2004},
  volume={},
  number={},
  pages={679-687},
  abstract={Driven by market applications in the areas of computing, networking, storage, optical, wireless, portable, and consumer electronics, semiconductor chips today are as diverse as ever. Confluence of multiple applications and rapid integration has also driven the heterogeneity of chips. Test methods have evolved with the products. However, the basic goals in testing remain the same: quality of product, recurring and non-recurring costs and time to market. In this paper we try to catalog some commonly used test methods, identify their associated DFT requirements and trends in terms of tester requirements. Given the diversity of semiconductors chips today such as various PLDs, volatile and non-volatile memories, analog, mixed signal, FPGA, ASIC, SOC, MEMs and processors, it is impossible for a paper of this nature to be fully comprehensive. So we limit our focus on processor, ASIC and SOCs.},
  keywords={Manufacturing;Testing;Optical computing;Application specific integrated circuits;Semiconductor device manufacture;Computer networks;Portable computers;Optical fiber networks;Consumer electronics;Costs},
  doi={10.1109/TEST.2004.1387329},
  ISSN={},
  month={Oct},}

@ARTICLE{9373305,
  author={Alnafessah, Ahmad and Gias, Alim Ul and Wang, Runan and Zhu, Lulai and Casale, Giuliano and Filieri, Antonio},
  journal={IEEE Access}, 
  title={Quality-Aware DevOps Research: Where Do We Stand?}, 
  year={2021},
  volume={9},
  number={},
  pages={44476-44489},
  abstract={DevOps is an emerging paradigm that reduces the barriers between developers and operations teams to offer continuous fast delivery and enable quick responses to changing requirements within the software life cycle. A significant volume of activity has been carried out in recent years with the aim of coupling DevOps stages with tools and methods to improve the quality of the produced software and the underpinning delivery methodology. While the research community has produced a sustained effort by conducting numerous studies and innovative development tools to support quality analyses within DevOps, there is still a limited cohesion between the research themes in this domain and a shortage of surveys that holistically examine quality engineering work within DevOps. In this paper, we address the gap by comprehensively surveying existing efforts in this area, categorizing them according to the stage of the DevOps lifecycle to which they primarily contribute. The survey holistically spans across all the DevOps stages, identify research efforts to improve architectural design, modeling and infrastructure-as-code, continuous-integration/continuous-delivery (CI/CD), testing and verification, and runtime management. Our analysis also outlines possible directions for future work in quality-aware DevOps, looking in particular at AI for DevOps and DevOps for AI software.},
  keywords={Software;Testing;Artificial intelligence;Computer architecture;Tools;Production;Software architecture;DevOps;CI/CD;infrastructure as code;testing;artificial intelligence;verification},
  doi={10.1109/ACCESS.2021.3064867},
  ISSN={2169-3536},
  month={},}

@INPROCEEDINGS{7140503,
  author={de Bayser, Maximilien and Azevedo, Leonardo G. and Cerqueira, Renato},
  booktitle={2015 IFIP/IEEE International Symposium on Integrated Network Management (IM)}, 
  title={ResearchOps: The case for DevOps in scientific applications}, 
  year={2015},
  volume={},
  number={},
  pages={1398-1404},
  abstract={DevOps (a portmanteau of “development” and “operations”) is a software development method that extends the agile philosophy to rapidly produce software products and services and to improve operations performance and quality assurance. It was born to accelerate the delivery of Web-based systems and quickly bring new value to users. Many Web-based systems evolve according to usage trends without a clear long-term goal. Before the widespread use of Web services, most software with a clear goal were delivered as packages that users installed on their own system. New versions were delivered with a much lower frequency, with periods in between versions ranging from months to years. Development cycles were divided into large design, coding and testing phases culminating in the release of a new stable version. In software development in the context of applied science, even when the goal is clear, the process to attain it is not. Hence, working releases that capture the current software state must be released frequently in order to reduce the risks for all stakeholders and to make it possible to assess the current state of a project and steer it in the right direction. This paper explores the usefulness of DevOps concepts to improve the development of software that supports scientific projects. We establish the similarities and differences between scientific projects and Web applications development, and discuss where the related methodologies need to be extended. Unique challenges are discussed herewith developed solutions, and still open questions. Lessons learned are highlighted as best practices to be followed in research projects. This discussion is rooted in our experience in real-life projects at the IBM Research Brazil Lab, which just as well apply to other research institutions.},
  keywords={Software;Testing;Prototypes;Conferences;Libraries;Servers;Production},
  doi={10.1109/INM.2015.7140503},
  ISSN={1573-0077},
  month={May},}

@ARTICLE{8985259,
  author={Aziz, Omer and Farooq, Muhammad Shoaib and Abid, Adnan and Saher, Rubab and Aslam, Naeem},
  journal={IEEE Access}, 
  title={Research Trends in Enterprise Service Bus (ESB) Applications: A Systematic Mapping Study}, 
  year={2020},
  volume={8},
  number={},
  pages={31180-31197},
  abstract={In recent years, enterprise service bus (ESB) has become a favorable adoption as a technology category in the IT industry as it provides secure and guaranteed delivery of services. The elasticity of Enterprise Service Bus (ESB) enables numerous applications to exchange information makes it a significant middleware layer responsible for transferring information in a Service-Oriented Architecture (SOA). ESB is presently the utmost promising tactic for the integration of business applications in distributed and diverse environments. It also offers essential infrastructure support for transforming messages or data, intelligent routing, and protocol transformation. The idea of ESBs emerged from the requirements to move out from traditional integration patterns, that becomes difficult to manage with the passage of time. Our study aim is to understand and provide ongoing research topics, challenges and future directions concerning ESB applications. A systematic mapping study (SMS) is therefore implemented to categorize the selected papers into the following classification: contribution type, ESB applications, research type and their approaches. We have extracted a total of twenty-two papers for this systematic study and they are classified according to defined criteria. The findings of this SMS are discussed and researchers were provided with suggestions on possible directions for future research.},
  keywords={Service-oriented architecture;Systematics;Market research;Standards;Routing;Enterprise service bus (ESB);applications;classification;service oriented architecture (SOA);systematic mapping study (SMS);criteria},
  doi={10.1109/ACCESS.2020.2972195},
  ISSN={2169-3536},
  month={},}

@INPROCEEDINGS{8049134,
  author={Crapo, Andrew and Moitra, Abha and McMillan, Craig and Russell, Daniel},
  booktitle={2017 IEEE 25th International Requirements Engineering Conference (RE)}, 
  title={Requirements Capture and Analysis in ASSERT(TM)}, 
  year={2017},
  volume={},
  number={},
  pages={283-291},
  abstract={Capturing high-level requirements in a human readable but formal representation suitable for analysis is an important goal for GE. To that end we have augmented an existing controlled-English modeling language with a new controlled-English requirements capture language to create the Requirements Capture frontend of the ASSERT(TM) tool suite. Requirements captured in ASSERT can be analyzed for a number of possible shortcomings, both individually and collectively. Once a set of requirements has reached a satisfactory level of completeness, consistency, etc., it can then be further used to generate test cases and test procedures. This paper will focus on the requirements capture and analysis functions of ASSERT and will illustrate its capabilities with a sample problem previously used as a challenge problem for requirements specification.},
  keywords={Mathematical model;OWL;Unified modeling language;Object oriented modeling;Tools;Ontologies;Set theory;Model;requirements capture;formal methods},
  doi={10.1109/RE.2017.54},
  ISSN={2332-6441},
  month={Sep.},}

@ARTICLE{1166666,
  author={Baines, R. and Pulley, D.},
  journal={IEEE Communications Magazine}, 
  title={A total cost approach to evaluating different reconfigurable architectures for baseband processing in wireless receivers}, 
  year={2003},
  volume={41},
  number={1},
  pages={105-113},
  abstract={There is growing interest in the use of flexible digital signal processors for wireless systems, driven by the demands of time to market, cost pressure, the requirement for flexibility to cope with evolving standards, and rapidly increasing processing needs. Much of the discussion of these techniques involves terms like "efficient" or "cost-effective" without necessarily quantifying the terms. This article considers the various architectures applicable to a wideband CDMA node-B base station (ASIC, FPGA, traditional DSP, and two varieties of flexible DSP) and builds a quantitative total cost approach to evaluating them, including benchmarked performance data.},
  keywords={Costs;Reconfigurable architectures;Baseband;Digital signal processing;Digital signal processors;Time to market;Wideband;Multiaccess communication;Base stations;Application specific integrated circuits},
  doi={10.1109/MCOM.2003.1166666},
  ISSN={1558-1896},
  month={Jan},}

@ARTICLE{4657364,
  author={Mattsson, Anders and Lundell, Björn and Lings, Brian and Fitzgerald, Brian},
  journal={IEEE Transactions on Software Engineering}, 
  title={Linking Model-Driven Development and Software Architecture: A Case Study}, 
  year={2009},
  volume={35},
  number={1},
  pages={83-93},
  abstract={A basic premise of model driven development (MDD) is to capture all important design information in a set of formal or semi-formal models which are then automatically kept consistent by tools. The concept however is still relatively immature and there is little by way of empirically validated guidelines. In this paper we report on the use of MDD on a significant real-world project over several years. Our research found the MDD approach to be deficient in terms of modelling architectural design rules. Furthermore, the current body of literature does not offer a satisfactory solution as to how architectural design rules should be modelled. As a result developers have to rely on time-consuming and error-prone manual practices to keep a system consistent with its architecture. To realise the full benefits of MDD it is important to find ways of formalizing architectural design rules which then allow automatic enforcement of the architecture on the system model. Without this, architectural enforcement will remain a bottleneck in large MDD projects.},
  keywords={Joining processes;Software architecture;Computer architecture;Guidelines;Context modeling;Computer industry;Computer errors;Programming;Keyword search;Portals;Software Architecture;Model-Driven Development;Case Study Research;Software Architecture;Model-Driven Development;Case Study Research},
  doi={10.1109/TSE.2008.87},
  ISSN={1939-3520},
  month={Jan},}

@INPROCEEDINGS{7546497,
  author={Argyros, George and Stais, Ioannis and Kiayias, Aggelos and Keromytis, Angelos D.},
  booktitle={2016 IEEE Symposium on Security and Privacy (SP)}, 
  title={Back in Black: Towards Formal, Black Box Analysis of Sanitizers and Filters}, 
  year={2016},
  volume={},
  number={},
  pages={91-109},
  abstract={We tackle the problem of analyzing filter and sanitizer programs remotely, i.e. given only the ability to query the targeted program and observe the output. We focus on two important and widely used program classes: regular expression (RE) filters and string sanitizers. We demonstrate that existing tools from machine learning that are available for analyzing RE filters, namely automata learning algorithms, require a very large number of queries in order to infer real life RE filters. Motivated by this, we develop the first algorithm that infers symbolic representations of automata in the standard membership/equivalence query model. We show that our algorithm provides an improvement of x15 times in the number of queries required to learn real life XSS and SQL filters of popular web application firewall systems such as mod-security and PHPIDS. % Active learning algorithms require the usage of an equivalence oracle, i.e. an oracle that tests the equivalence of a hypothesis with the target machine. We show that when the goal is to audit a target filter with respect to a set of attack strings from a context free grammar, i.e. find an attack or infer that none exists, we can use the attack grammar to implement the equivalence oracle with a single query to the filter. Our construction finds on average 90% of the target filter states when no attack exists and is very effective in finding attacks when they are present. For the case of string sanitizers, we show that existing algorithms for inferring sanitizers modelled as Mealy Machines are not only inefficient, but lack the expressive power to be able to infer real life sanitizers. We design two novel extensions to existing algorithms that allow one to infer sanitizers represented as single-valued transducers. Our algorithms are able to infer many common sanitizer functions such as HTML encoders and decoders. Furthermore, we design an algorithm to convert the inferred models into BEK programs, which allows for further applications such as cross checking different sanitizer implementations and cross compiling sanitizers into different languages supported by the BEK backend. We showcase the power of our techniques by utilizing our black-box inference algorithms to perform an equivalence checking between different HTML encoders including the encoders from Twitter, Facebook and Microsoft Outlook email, for which no implementation is publicly available.},
  keywords={Algorithm design and analysis;Machine learning algorithms;Learning automata;Transducers;Grammar;HTML;Inference algorithms;sanitizers;filters;automata;learning;web security},
  doi={10.1109/SP.2016.14},
  ISSN={2375-1207},
  month={May},}

@ARTICLE{4042539,
  author={Beckert, Bernhard and Hoare, Tony and Hahnle, Reiner and Smith, Douglas R. and Green, Cordell and Ranise, Silvio and Tinelli, Cesare and Ball, Thomas and Rajamani, Sriram K.},
  journal={IEEE Intelligent Systems}, 
  title={Intelligent Systems and Formal Methods in Software Engineering}, 
  year={2006},
  volume={21},
  number={6},
  pages={71-81},
  abstract={Over the last few years, technologies for the formal description, construction, analysis, and validation of software - based mostly on logics and formal reasoning - have matured. We can expect them to complement and partly replace traditional software engineering methods in the future. Formal methods in software engineering are an increasingly important application area for intelligent systems. The field has outgrown the area of academic case studies, and industry is showing serious interest. We convincingly argue that we've reached the point where we can solve the problem of how to formally verify industrial-scale software. We propose program verification as a computer science Grand Challenge. Deductive software verification is a core technology of formal methods. We describe recent dramatic changes in the way it's perceived and used. Another important base technique of formal methods, besides software verification, is synthesizing software that's correct by construction because it's formally derived from its specification. We discuss recent developments and trends in this area. Surprisingly efficient decision procedures for the satisfiability modulo theories problem have recently emerged. We explain these techniques and why they're important for all formal-methods tools. We look at formal methods from an industry perspective. We explain the success of Microsoft Research's SLAM project, which has developed a verification tool for device drivers},
  keywords={Intelligent systems;Software engineering;Costs;Application software;Programming profession;Computer science;Computer errors;Humans;Energy management;Financial management;formal methods;software engineering;program verification;deductive software verification;satisfiability modulo theories;software synthesis},
  doi={10.1109/MIS.2006.117},
  ISSN={1941-1294},
  month={Nov},}

@ARTICLE{9448036,
  author={Sardar, Muhammad Usama and Musaev, Saidgani and Fetzer, Christof},
  journal={IEEE Access}, 
  title={Demystifying Attestation in Intel Trust Domain Extensions via Formal Verification}, 
  year={2021},
  volume={9},
  number={},
  pages={83067-83079},
  abstract={In August 2020, Intel asked the research community for feedback on the newly offered architecture extensions, called Intel Trust Domain Extensions (TDX), which give more control to Trust Domains (TDs) over processor resources. One of the key features of these extensions is the remote attestation mechanism, which provides a unified report verification mechanism for TDX and its predecessor Software Guard Extensions (SGX). Based on our experience and intuition, we respond to the request for feedback by formally specifying the attestation mechanism in the TDX using ProVerif's specification language. Although the TDX technology seems very promising, the process of formal specification reveals a number of subtle discrepancies in Intel's specifications that could potentially lead to design and implementation flaws. After resolving these discrepancies, we also present fully automated proofs that our specification of TD attestation preserves the confidentiality of the secret and authentication of the report by considering the state-of-the-art Dolev-Yao adversary in the symbolic model using ProVerif. We have submitted the draft to Intel, and Intel is in the process of making the changes.},
  keywords={Security;Software;Tools;Virtual machine monitors;Computer bugs;Analytical models;Runtime;Formal verification;symbolic security analysis;ProVerif;trusted execution environment;trust domains;Intel TDX;remote attestation},
  doi={10.1109/ACCESS.2021.3087421},
  ISSN={2169-3536},
  month={},}

@INPROCEEDINGS{8836763,
  author={Sippl, Christoph and Bock, Florian and Lauer, Christoph and Heinz, Aaron and Neumayer, Thomas and German, Reinhard},
  booktitle={2019 IEEE International Systems Conference (SysCon)}, 
  title={Scenario-Based Systems Engineering: An Approach Towards Automated Driving Function Development}, 
  year={2019},
  volume={},
  number={},
  pages={1-8},
  abstract={In this contribution, we propose an approach for scenario-based systems engineering for automated driving functions. The growing complexity of future driving systems presents an enormous challenge for the development and testing processes, the validation and release procedure, as well as the organization of work. Accompanying, the end of development and the concerning effort is difficult to estimate. This often leads to postponements and rise of development costs. Furthermore, recent research activities show that established test methods such as statistical testing are not well suited for the validation/verification and the release process of automated driving functions. Thus, we propose a method for continuous usage of scenarios, embedded in the systems engineering process, to divide complex and intangible development goals in smaller solvable tasks. Furthermore, we suggest how scenarios can be used for validation and verification of the target system. In this approach, various artifacts are generated, which help to plan, organize, and trace the development and validation process. As a result, temporal expenditure and development costs get estimable. Moreover, the technical progress of the system-to-develop becomes apparent at an earlier point of time.},
  keywords={Tools;Testing;Complexity theory;Task analysis;Requirements engineering;Hazards;scenario;systems engineering;automated driving;development and validation process},
  doi={10.1109/SYSCON.2019.8836763},
  ISSN={2472-9647},
  month={April},}

@INPROCEEDINGS{9081794,
  author={Annighoefer, Bjoern and Halle, Martin and Schweiger, Andreas and Reich, Marina and Watkins, Christopher and VanderLeest, Steven H. and Harwarth, Stefan and Deiber, Patrick},
  booktitle={2019 IEEE/AIAA 38th Digital Avionics Systems Conference (DASC)}, 
  title={Challenges and Ways Forward for Avionics Platforms and their Development in 2019}, 
  year={2019},
  volume={},
  number={},
  pages={1-10},
  abstract={Today's air vehicles depend on digital technology. It accounts for more than 30% of their development costs. The number of functions, the lines of code, the degree of autonomy, and the number of vehicles rise. This is why there is a need for cutting-edge technology and development methods. There is a gap between academia's methods and industrial applications due to multi-disciplinary challenges. We summarize the state-of-the-art in avionics, namely avionics platforms, requirements engineering, model-based development, automated verification, emerging technologies, and emerging demands. Experts review the most demanding challenges, research gaps, and promising solutions. They provide recommendations for the enhancement of the cooperation between industry and academia and suggest necessary research topics. This article is an introduction for those who are new to avionics. It is an up-to-date summary, for insiders looking for most promising solutions to their current problems; and it is a guide for those advancing avionics research.},
  keywords={Adaptation models;Reviews;Computational modeling;Education;Transportation;Aerospace electronics;Requirements engineering;Security;Virtualization;Standards;avionics platforms;requirements engineering;model-based development;automated verification;multi-core},
  doi={10.1109/DASC43569.2019.9081794},
  ISSN={2155-7209},
  month={Sep.},}

@INPROCEEDINGS{8718229,
  author={King, Tariq M. and Arbon, Jason and Santiago, Dionny and Adamo, David and Chin, Wendy and Shanmugam, Ram},
  booktitle={2019 IEEE International Conference On Artificial Intelligence Testing (AITest)}, 
  title={AI for Testing Today and Tomorrow: Industry Perspectives}, 
  year={2019},
  volume={},
  number={},
  pages={81-88},
  abstract={With modern advances in artificial intelligence (AI) and machine learning and their applications to software testing, the intersection of AI and testing is receiving close attention. The 2018 Annual Western Conference on Software Testing Analysis and Review featured a two-session panel on AI for Software Testing (AIST). The panel brought together six industry experts with experience developing AIST products, services, and research prototypes. Questions sourced from the industrial testing community were used to provoke thought, stimulate conversation, and guide panel discussions. This paper provides a review of the industry panel, which includes discussions on the visions, ideas, thoughts, strategies, directions, and lessons learned developing systems that use AI to test software, applying methods to test AI systems, and designing self-testing systems. Both the testing community survey and the expert panel yielded insightful perspectives on AIST in practice.},
  keywords={Artificial intelligence;Software;Manuals;Software testing;Industries;Built-in self-test;Artificial Intelligence;Machine Learning;Software Testing;Industry;Panel;Survey},
  doi={10.1109/AITest.2019.000-3},
  ISSN={},
  month={April},}

@INPROCEEDINGS{5431981,
  author={Esquivel, Holly and Akella, Aditya and Mori, Tatsuya},
  booktitle={2010 Second International Conference on COMmunication Systems and NETworks (COMSNETS 2010)}, 
  title={On the effectiveness of IP reputation for spam filtering}, 
  year={2010},
  volume={},
  number={},
  pages={1-10},
  abstract={Modern SMTP servers apply a variety of mechanisms to stem the volume of spam delivered to users. These techniques can be broadly classified into two categories: pre-acceptance approaches, which apply prior to a message being accepted (e.g. IP reputation), and post-acceptance techniques which apply after a message has been accepted (e.g. content based signatures). We argue that the effectiveness of these measures varies based on the SMTP sender type. This paper focuses on the most light-weight pre-acceptance filtering mechanism-IP reputation. We first classify SMTP senders into three main categories: legitimate servers, end-hosts, and spam gangs, and empirically study the limits of effectiveness regarding IP reputation filtering for each category. Next, we develop new techniques that build custom IP reputation lists, which significantly improve the performance of existing IP reputation lists. In compiling these lists, we leverage a somewhat surprising fact that both legitimate domains and spam domains often use the DNS Sender Policy Framework (SPF) in an attempt to pass simple authentication checks. That is, good/bad IP addresses can be systematically compiled by collecting good/bad domains and looking up their SPF resource records. We also evaluate the effectiveness of these lists over time. Finally, we aim to understand the characteristics of the three categories of email senders in depth. Overall, we find that it is possible to construct IP reputation lists that can identify roughly 90% of all spam and legitimate mail, but some of the lists, i.e. the lists for spam gangs, must be updated on a constant basis to maintain this high level of accuracy.},
  keywords={Filtering;Testing;Optical filters;Authentication;Protocols;Optical character recognition software;Unsolicited electronic mail;Laboratories;Postal services;Optical recording},
  doi={10.1109/COMSNETS.2010.5431981},
  ISSN={2155-2509},
  month={Jan},}

@INPROCEEDINGS{6984094,
  author={Costa, Pedro and Paiva, Ana C.R. and Nabuco, Miguel},
  booktitle={2014 9th International Conference on the Quality of Information and Communications Technology}, 
  title={Pattern Based GUI Testing for Mobile Applications}, 
  year={2014},
  volume={},
  number={},
  pages={66-74},
  abstract={This paper presents a study aiming to assess the feasibility of using the Pattern Based GUI Testing approach, PBGT, to test mobile applications. PBGT is a new model based testing approach that aims to increase systematization, reusability and diminish the effort in modelling and testing. It is based on the concept of User Interface Test Patterns (UITP) that contain generic test strategies for testing common recurrent behaviour, the so-called UI Patterns, on GUIs through its possible different implementations after a configuration step. Although PBGT was developed having web applications in mind, it is possible to develop drivers for other platforms in order to test a wide set of applications. However, web and mobile applications are different and only the development of a new driver to execute test cases over mobile applications may not be enough. This paper describes a study aiming to identify the adaptations and updates the PBGT should undergo in order to test mobile applications.},
  keywords={Mobile communication;Testing;Graphical user interfaces;Connectors;Androids;Humanoid robots;Optical character recognition software},
  doi={10.1109/QUATIC.2014.16},
  ISSN={},
  month={Sep.},}

@ARTICLE{9118898,
  author={Mukhiya, Suresh Kumar and Wake, Jo Dugstad and Inal, Yavuz and Lamo, Yngve},
  journal={IEEE Access}, 
  title={Adaptive Systems for Internet-Delivered Psychological Treatments}, 
  year={2020},
  volume={8},
  number={},
  pages={112220-112236},
  abstract={Internet-Delivered Psychological Treatments (IDPT) are based on evidence-based psychological treatment models adjusted for interaction through the Internet. The use of Internet technologies has the potential to increase the availability of evidence-based mental health services for a far-reaching population with the use of fewer resources. Despite evidence that Internet Interventions can be effective means in mental health morbidities, most current IDPT systems are tunnel-based, inflexible, and non-interoperable. Hence it becomes essential to understand which elements of an Internet intervention contribute to effectiveness and treatment outcomes. By analogy, adaptation is a central aspect of successful face-to-face mental health therapy. Adaptability to patient needs can be regarded as an essential outcome factor in online systems for mental health interventions as well. While some aspects of rule-based and machine-learning-based adaptation have attracted attention in recent IDPT development, systematic reporting of core components, dimensions of adaptiveness, information architecture, and strategies for adaptation in the IDPT system are still lacking. To bridge this gap, we propose a model that shows how adaptive systems are represented in classical control theory and discuss how the model can be used to specify adaptive IDPT systems. Concerning the reference model, we outline the core components of adaptive IDPT systems, the main adaptive elements, dimensions of adaptiveness, information architecture applied to adaptive systems, and strategies used in the adaptation process. We also provide comprehensive guidelines on how to develop an adaptive IDPT system based on the Person-Based Approach.},
  keywords={Adaptive systems;Mental health;Internet;Medical treatment;Adaptation models;Information architecture;Adaptive systems;Internet delivered psychological treatments;person based approach;information architecture;personalized Internet interventions;tailored Internet interventions;ICBT},
  doi={10.1109/ACCESS.2020.3002793},
  ISSN={2169-3536},
  month={},}

@INPROCEEDINGS{7927983,
  author={Artho, Cyrille and Gros, Quentin and Rousset, Guillaume and Banzai, Kazuaki and Ma, Lei and Kitamura, Takashi and Hagiya, Masami and Tanabe, Yoshinori and Yamamoto, Mitsuharu},
  booktitle={2017 IEEE International Conference on Software Testing, Verification and Validation (ICST)}, 
  title={Model-Based API Testing of Apache ZooKeeper}, 
  year={2017},
  volume={},
  number={},
  pages={288-298},
  abstract={Apache ZooKeeper is a distributed data storage that is highly concurrent and asynchronous due to network communication, testing such a system is very challenging. Our solution using the tool "Modbat" generates test cases for concurrent client sessions, and processes results from synchronous and asynchronous callbacks. We use an embedded model checker to compute the test oracle for non-deterministic outcomes, the oracle model evolves dynamically with each new test step. Our work has detected multiple previously unknown defects in ZooKeeper. Finally, a thorough coverage evaluation of the core classes show how code and branch coverage strongly relate to feature coverage in the model, and hence modeling effort.},
  keywords={Servers;Testing;Electronic mail;Computational modeling;Computer science;Tools;Complexity theory;Model-based testing;concurrency;asynchronous systems;networked systems;test oracle;Apache ZooKeeper},
  doi={10.1109/ICST.2017.33},
  ISSN={},
  month={March},}

@INPROCEEDINGS{9476896,
  author={Mirabella, A. Giuliano and Martin-Lopez, Alberto and Segura, Sergio and Valencia-Cabrera, Luis and Ruiz-Cortés, Antonio},
  booktitle={2021 IEEE/ACM Third International Workshop on Deep Learning for Testing and Testing for Deep Learning (DeepTest)}, 
  title={Deep Learning-Based Prediction of Test Input Validity for RESTful APIs}, 
  year={2021},
  volume={},
  number={},
  pages={9-16},
  abstract={Automated test case generation for RESTful web APIs is a thriving research topic due to their key role in software integration. Most approaches in this domain follow a black-box approach, where test cases are randomly derived from the API specification. These techniques show promising results, but they neglect constraints among input parameters (so-called inter-parameter dependencies), as these cannot be formally described in current API specification languages. As a result, when testing real-world services, most random test cases tend to be invalid since they violate some of the inter-parameter dependencies of the service, making human intervention indispensable. In this paper, we propose a deep learning-based approach for automatically predicting the validity of an API request (i.e., test input) before calling the actual API. The model is trained with the API requests and responses collected during the generation and execution of previous test cases. Preliminary results with five real-world RESTful APIs and 16K automatically generated test cases show that test inputs validity can be predicted with an accuracy ranging from 86% to 100% in APIs like Yelp, GitHub, and YouTube. These are encouraging results that show the potential of artificial intelligence to improve current test case generation techniques.},
  keywords={Deep learning;Conferences;Neural networks;Restful API;Software;Distance measurement;Specification languages;RESTful web API;web services testing;artificial neural network},
  doi={10.1109/DeepTest52559.2021.00008},
  ISSN={},
  month={June},}

@INPROCEEDINGS{7515466,
  author={Hughes, John and Pierce, Benjamin C. and Arts, Thomas and Norell, Ulf},
  booktitle={2016 IEEE International Conference on Software Testing, Verification and Validation (ICST)}, 
  title={Mysteries of DropBox: Property-Based Testing of a Distributed Synchronization Service}, 
  year={2016},
  volume={},
  number={},
  pages={135-145},
  abstract={File synchronization services such as Dropbox are used by hundreds ofmillions of people to replicate vital data. Yet rigorous models of theirbehavior are lacking. We present the first formal -- and testable -- model ofthe core behavior of a modern file synchronizer, and we use it to discoversurprising behavior in two widely deployed synchronizers. Our model isbased on a technique for testing nondeterministic systems that avoidsrequiring that the system's internal choices be made visible to the testing framework.},
  keywords={Synchronization;Testing;Servers;Virtual machining;Google;Context modeling;Cloud computing;Property-based testing;QuickCheck;File synchronization;Dropbox},
  doi={10.1109/ICST.2016.37},
  ISSN={},
  month={April},}

@INPROCEEDINGS{6080786,
  author={Yazdanshenas, Amir Reza and Moonen, Leon},
  booktitle={2011 27th IEEE International Conference on Software Maintenance (ICSM)}, 
  title={Crossing the boundaries while analyzing heterogeneous component-based software systems}, 
  year={2011},
  volume={},
  number={},
  pages={193-202},
  abstract={One way to manage the complexity of software systems is to compose them from reusable components, instead of starting from scratch. Components may be implemented in different programming languages and are tied together using configuration files, or glue code, defining instantiation, initialization and interconnections. Although correctly engineering the composition and configuration of components is crucial for the overall behavior, there is surprisingly little support for incorporating this information in the static verification and validation of these systems. Analyzing the properties of programs within closed code boundaries has been studied for some decades and is well-established. This paper contributes a method to support analysis across the components of a component-based system. We build upon the Knowledge Discovery Metamodel to reverse engineer homogeneous models for systems composed of heterogeneous artifacts. Our method is implemented in a prototype tool that has been successfully used to track information flow across the components of a component-based system using program slicing.},
  keywords={Complexity theory;Software systems;Component architectures;Computer languages;Prototypes;Knowledge engineering},
  doi={10.1109/ICSM.2011.6080786},
  ISSN={1063-6773},
  month={Sep.},}

@ARTICLE{6152131,
  author={Schäfer, Max and Thies, Andreas and Steimann, Friedrich and Tip, Frank},
  journal={IEEE Transactions on Software Engineering}, 
  title={A Comprehensive Approach to Naming and Accessibility in Refactoring Java Programs}, 
  year={2012},
  volume={38},
  number={6},
  pages={1233-1257},
  abstract={Automated tool support for refactoring is now widely available for mainstream programming languages such as Java. However, current refactoring tools are still quite fragile in practice and often fail to preserve program behavior or compilability. This is mainly because analyzing and transforming source code requires consideration of many language features that complicate program analysis, in particular intricate name lookup and access control rules. This paper introduces J_L, a lookup-free, access control-free representation of Java programs. We present algorithms for translating Java programs into J_L and vice versa, thereby making it possible to formulate refactorings entirely at the level of J_L and to rely on the translations to take care of naming and accessibility issues. We demonstrate how complex refactorings become more robust and powerful when lifted to J_L. Our approach has been implemented using the JastAddJ compiler framework, and evaluated by systematically performing two commonly used refactorings on an extensive suite of real-world Java applications. The evaluation shows that our tool correctly handles many cases where current refactoring tools fail to handle the complex rules for name binding and accessibility in Java.},
  keywords={Java;Access control;Feature extraction;Reverse engineering;Object oriented programming;Shadow mapping;Program processors;Restructuring;reverse engineering;and reengineering;object-oriented languages;Java},
  doi={10.1109/TSE.2012.13},
  ISSN={1939-3520},
  month={Nov},}

@INPROCEEDINGS{794352,
  author={Smith, B. and Millar, W. and Dunphy, J. and Yu-Wen Tung and Nayak, P. and Gamble, E. and Clark, M.},
  booktitle={1999 IEEE Aerospace Conference. Proceedings (Cat. No.99TH8403)}, 
  title={Validation and verification of the remote agent for spacecraft autonomy}, 
  year={1999},
  volume={1},
  number={},
  pages={449-468 vol.1},
  abstract={The six-day Remote Agent Experiment (RAX) on the Deep Space 1 mission will be the first time that an artificially intelligent agent will control a NASA spacecraft. Successful completion of this experiment will open the way for AI-based autonomy technology on future missions. An important validation objective for RAX is implementation of a credible validation and verification strategy for RAX that also "scales up" to missions that make full use of spacecraft autonomy. Autonomous flight software presents novel and difficult testing challenges that traditional flight software (FSW) does not face. Since autonomous software must respond robustly in an immense number of situations, the all-paths testing approaches used for traditional FSW is not feasible. Instead, we advocate a combination of scenario-based testing and model-based validation. This paper describes the testing challenges faced by autonomous spacecraft commanding software, discusses the testing strategies and model-validation methods that we found effective for RAX, and argues that these methods will "scale up" to missions that make full use of spacecraft autonomy. Among the key challenges for validating autonomous systems such as the RAX are ensuring adequate coverage for scenario-based tests, developing methods for specifying the expected behavior, and developing automated tools for verifying the observed behavior against those specifications. Another challenge, also faced by traditional FSW, is the scarcity of high-fidelity test-beds. The test plan must be designed to take advantage of lower-fidelity test-beds without compromising test effectiveness.},
  keywords={Space vehicles;Space technology;Software testing;Automatic testing;Space missions;Propulsion;Laboratories;Intelligent agent;NASA;Robustness},
  doi={10.1109/AERO.1999.794352},
  ISSN={},
  month={March},}

@INPROCEEDINGS{9012666,
  author={Kundel, Ralf and Nobach, Leonhard and Blendin, Jeremias and Kolbe, Hans-Joerg and Schyguda, Georg and Gurevich, Vladimir and Koldehofe, Boris and Steinmetz, Ralf},
  booktitle={2019 15th International Conference on Network and Service Management (CNSM)}, 
  title={P4-BNG: Central Office Network Functions on Programmable Packet Pipelines}, 
  year={2019},
  volume={},
  number={},
  pages={1-9},
  abstract={Large-scale telecommunications providers have to continuously challenge and evolve their network infrastructure to efficiently serve growing markets demands. They must increase performance, lower time-to-market, provide new services, and lower the cost of the infrastructure and its operation. Network Functions Virtualization (NFV) on commodity hardware offers an attractive, low-cost platform to establish innovations much faster than with purpose-built hardware products. Unfortunately, implementing NFV on commodity processors does not match the performance requirements of the high-throughput data plane components in large carrier access networks. In this article, we propose a way to offer residential network access with programmable packet processing architectures. Based on the highly flexible P4 programming language, we present a design and open source implementation of a BNG data plane that meets the challenging demands of Broadband Network Gateways in carrier-grade environments. The proposed evaluation results show the desired performance characteristics and our proposed design together with upcoming P4 hardware can offer a giant leap towards highest performance NFV network access.},
  keywords={Hardware;Central office;Logic gates;Protocols;Pipelines;Network function virtualization;NFV;P4;Access Networks;Network Functions;Hardware Acceleration;Computer Networks},
  doi={10.23919/CNSM46954.2019.9012666},
  ISSN={2165-963X},
  month={Oct},}

@ARTICLE{9869831,
  author={Sarjan, Hamed and Ameli, Amir and Ghafouri, Mohsen},
  journal={IEEE Access}, 
  title={Cyber-Security of Industrial Internet of Things in Electric Power Systems}, 
  year={2022},
  volume={10},
  number={},
  pages={92390-92409},
  abstract={Electric Power Systems (EPSs) are among the most critical infrastructures of any society, since they significantly impact other infrastructures. Recently, there has been a trend toward implementing modern technologies, such as Industrial Internet of Things (IIoT), in EPSs to enhance their real-time monitoring, control, situational awareness, and intelligence. This movement, however, has exposed EPSs to various cyber intrusions that originate from the IIoT ecosystem. Statistics show that 38% of reported attacks have been against power and water infrastructure, and so far at least 91% of power utilities have experienced a cyber-attack. The cyber-security problem is even more severe for IIoT applications in EPSs due to the vulnerabilities and resource limitations of such applications. Thus, based on the above statistics, it is necessary to investigate the vulnerabilities of IIoT-based applications in EPSs, identify probable attacks and their consequences, and develop intrusion prevention and detection approaches to secure IIoT systems. On this basis, this paper first elaborates on the applications of IIoT-based systems in EPSs, and evaluates their security challenges. Afterwards, it comprehensively reviews various cyber-attacks against IIoT-assisted EPSs, with a particular focus on attack entry points and adversarial methods. Finally, efforts to prevent cyber-intrusions against IIoT systems in EPSs are explained, and different attack detection techniques are discussed.},
  keywords={Industrial Internet of Things;Security;Ecosystems;Reliability;Real-time systems;Protocols;Power generation;Computer crime;Computer security;Intrusion detection;Cyber-attacks;cyber-security;electric power systems;industrial internet of things;intrusion detection systems},
  doi={10.1109/ACCESS.2022.3202914},
  ISSN={2169-3536},
  month={},}

@INPROCEEDINGS{7332467,
  author={Kim, Jongwook and Batory, Don and Dig, Danny},
  booktitle={2015 IEEE International Conference on Software Maintenance and Evolution (ICSME)}, 
  title={Scripting parametric refactorings in Java to retrofit design patterns}, 
  year={2015},
  volume={},
  number={},
  pages={211-220},
  abstract={Retrofitting design patterns into a program by hand is tedious and error-prone. A programmer must distinguish refactorings that are provided by an Integrated Development Environment (IDE) from those that must be realized manually, determine a precise sequence of refactorings to apply, and perform this sequence repetitively to a laborious degree. We designed, implemented, and evaluated Reflective Refactoring (R2), a Java package to automate the creation of classical design patterns (Visitor, Abstract Factory, etc.), their inverses, and variants. We encoded 18 out of 23 Gang-of-Four design patterns as R2 scripts and explain why the remaining are inappropriate for refactoring engines. We evaluate the productivity and scalability of R2 with a case study of 6 real-world applications. In one case, R2 automatically created a Visitor with 276 visit methods by invoking 554 Eclipse refactorings in 10 minutes - an achievement that could not be done manually. R2 also sheds light on why refactoring correctness, expressiveness, and speed are critical issues for scripting in next-generation refactoring engines.},
  keywords={Manuals;Graphics;DVD},
  doi={10.1109/ICSM.2015.7332467},
  ISSN={},
  month={Sep.},}

@INPROCEEDINGS{8730198,
  author={Koo, Jinkyu and Saumya, Charitha and Kulkarni, Milind and Bagchi, Saurabh},
  booktitle={2019 12th IEEE Conference on Software Testing, Validation and Verification (ICST)}, 
  title={PySE: Automatic Worst-Case Test Generation by Reinforcement Learning}, 
  year={2019},
  volume={},
  number={},
  pages={136-147},
  abstract={Stress testing is an important task in software testing, which examines the behavior of a program under a heavy load. Symbolic execution is a useful tool to find out the worst-case input values for the stress testing. However, symbolic execution does not scale to a large program, since the number of paths to search grows exponentially with an input size. So far, such a scalability issue has been mostly managed by pruning out unpromising paths in the middle of searching based on heuristics, but this kind of work easily eliminates the true worst case as well, providing sub-optimal one only. Another way to achieve scalability is to learn a branching policy of worst-case complexity from small scale tests and apply it to a large scale. However, use cases of such a method are restricted to programs whose worst-case branching policy has a simple pattern. To address such limitations, we propose PySE that uses symbolic execution to collect the behaviors of a given branching policy, and updates the policy using a reinforcement learning approach through multiple executions. PySE's branching policy keeps evolving in a way that the length of an execution path increases in the long term, and ultimately reaches the worst-case complexity. PySE can also learn the worst-case branching policy of a complex or irregular pattern, using an artificial neural network in a fully automatic way. Experiment results demonstrate that PySE can effectively find a path of worst-case complexity for various Python benchmark programs and scales.},
  keywords={Complexity theory;Testing;Explosions;Stress;Python;History;Genetic algorithms;Machine learning;Q-learning;Symbolic execution;Worst-case complexity;Stress testing},
  doi={10.1109/ICST.2019.00023},
  ISSN={2159-4848},
  month={April},}

@INPROCEEDINGS{9794080,
  author={Noller, Yannic and Shariffdeen, Ridwan and Gao, Xiang and Roychoudhury, Abhik},
  booktitle={2022 IEEE/ACM 44th International Conference on Software Engineering (ICSE)}, 
  title={Trust Enhancement Issues in Program Repair}, 
  year={2022},
  volume={},
  number={},
  pages={2228-2240},
  abstract={Automated program repair is an emerging technology that seeks to automatically rectify bugs and vulnerabilities using learning, search, and semantic analysis. Trust in automatically generated patches is necessary for achieving greater adoption of program repair. Towards this goal, we survey more than 100 software practitioners to understand the artifacts and setups needed to enhance trust in automatically generated patches. Based on the feedback from the survey on developer preferences, we quantitatively evaluate existing test-suite based program repair tools. We find that they cannot produce high-quality patches within a top-10 ranking and an acceptable time period of 1 hour. The developer feedback from our qualitative study and the observations from our quantitative examination of existing repair tools point to actionable insights to drive program repair research. Specifically, we note that producing repairs within an acceptable time-bound is very much dependent on leveraging an abstract search space representation of a rich enough search space. Moreover, while additional developer inputs are valuable for generating or ranking patches, developers do not seem to be interested in a significant human-in-the-loop interaction.},
  keywords={Semantics;Computer bugs;Maintenance engineering;Software;Human in the loop;Software engineering;program repair},
  doi={10.1145/3510003.3510040},
  ISSN={1558-1225},
  month={May},}

@INPROCEEDINGS{8952200,
  author={Rahat, Tamjid Al and Feng, Yu and Tian, Yuan},
  booktitle={2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE)}, 
  title={OAUTHLINT: An Empirical Study on OAuth Bugs in Android Applications}, 
  year={2019},
  volume={},
  number={},
  pages={293-304},
  abstract={Mobile developers use OAuth APIs to implement Single-Sign-On services. However, the OAuth protocol was originally designed for the authorization for third-party websites not to authenticate users in third-party mobile apps. As a result, it is challenging for developers to correctly implement mobile OAuth securely. These vulnerabilities due to the misunderstanding of OAuth and inexperience of developers could lead to data leakage and account breach. In this paper, we perform an empirical study on the usage of OAuth APIs in Android applications and their security implications. In particular, we develop OAUTHLINT, that incorporates a query-driven static analysis to automatically check programs on the Google Play marketplace. OAUTHLINT takes as input an anti-protocol that encodes a vulnerable pattern extracted from the OAuth specifications and a program P. Our tool then generates a counter-example if the anti-protocol can match a trace of Ps possible executions. To evaluate the effectiveness of our approach, we perform a systematic study on 600+ popular apps which have more than 10 millions of downloads. The evaluation shows that 101 (32%) out of 316 applications that use OAuth APIs make at least one security mistake.},
  keywords={Protocols;Authorization;Mobile applications;Authentication;Google;Computer bugs;Security, OAuth, Android, Static Analysis, Bug Finding},
  doi={10.1109/ASE.2019.00036},
  ISSN={2643-1572},
  month={Nov},}

@INPROCEEDINGS{7019292,
  author={Prathibhan, C.Mano and Malini, A. and Venkatesh, N. and Sundarakantham, K.},
  booktitle={2014 IEEE International Conference on Advanced Communications, Control and Computing Technologies}, 
  title={An automated testing framework for testing Android mobile applications in the cloud}, 
  year={2014},
  volume={},
  number={},
  pages={1216-1219},
  abstract={The testing of mobile application faces many issues due to the complexity of testing these applications and the limited resources available in mobile devices. Testing in various mobile devices under varying conditions takes a lot of time when done manually. Also by using emulators it is not possible to generate the same real time network connections and real device characteristics. There is a need for a testing framework that allows automated testing of mobile applications in many mobile devices in limited time. In this paper we propose a mobile testing framework in the cloud environment that aims to provide automated testing of mobile applications in various mobile devices. This testing framework has an automated testing tool, the Mobile Application Testing (MAT) Tool integrated to it that performs functional, performance and compatibility testing of mobile applications.},
  keywords={Testing;Mobile communication;Performance evaluation;Androids;Humanoid robots;Mobile Testing;Automated Testing;Testing as a Service;Cloud Testing},
  doi={10.1109/ICACCCT.2014.7019292},
  ISSN={},
  month={May},}

@ARTICLE{9127413,
  author={Górski, Tomasz and Bednarski, Jakub},
  journal={IEEE Access}, 
  title={Applying Model-Driven Engineering to Distributed Ledger Deployment}, 
  year={2020},
  volume={8},
  number={},
  pages={118245-118261},
  abstract={Distributed Ledger Technology (DLT) enables data storage in a decentralized manner among collaborating parties. The software architecture of such solutions encompasses models placed in the relevant architectural views. A lot of research is devoted to smart contracts and consensus algorithms, which are realized by distributed applications and can be positioned within the Logical view. However, we see the need to provide modeling support for the Deployment view of distributed ledger solutions. Especially since the chosen DLT framework has a significant impact on implementation and deployment. Besides, consistency between models and configuration deployment scripts should be ensured. So, we have applied Model-Driven Engineering (MDE) that allows on the transformation of models into more detailed models, source code, or tests. We have proposed Unified Modeling Language (UML) stereotypes and tagged values for distributed ledger deployment modeling and placed them in the UML Profile for Distributed Ledger Deployment. We have also designed the UML2Deployment model-to-code transformation for the R3 Corda DLT framework. A UML Deployment model is the source whereas a Gradle Groovy deployment script is the target of the transformation. We have provided the complete solution by incorporating the transformation into the Visual Paradigm modeling tool. Furthermore, we have designed a dedicated plug-in to validate generated deployment scripts. In the paper, we have shown how to design transformation for generating deployment scripts for the R3 Corda DLT framework with the ability to switch to another one.},
  keywords={Unified modeling language;Distributed ledger;Object oriented modeling;Blockchain;Distributed databases;Model driven engineering;Consensus algorithm;Distributed ledger;model-driven engineering;architectural views model 1+5;deployment view;unified modeling language extensibility mechanisms},
  doi={10.1109/ACCESS.2020.3005519},
  ISSN={2169-3536},
  month={},}

@ARTICLE{7548916,
  author={Lübke, Daniel and van Lessen, Tammo},
  journal={IEEE Software}, 
  title={Modeling Test Cases in BPMN for Behavior-Driven Development}, 
  year={2016},
  volume={33},
  number={5},
  pages={15-21},
  abstract={Testing large-scale process integration solutions is complex and cumbersome. To tackle this problem, researchers employed behavior-driven development. They used the Business Process Model and Notation language to model domain-specific test cases. These test cases can be understood by both developers and business stakeholders and can be executed automatically.},
  keywords={Simple object access protocol;Business process management;Modeling;Testing;Software engineering;Behaviorial sciences;business processes;Business Process Model and Notation;BPMN;behavior-driven development;BDD;test-driven development;TDD;software testing;software development;software engineering},
  doi={10.1109/MS.2016.117},
  ISSN={1937-4194},
  month={Sep.},}

@INPROCEEDINGS{8539069,
  author={Mai, Phu X. and Pastore, Fabrizio and Goknil, Arda and Briand, Lionel C.},
  booktitle={2018 IEEE 29th International Symposium on Software Reliability Engineering (ISSRE)}, 
  title={A Natural Language Programming Approach for Requirements-Based Security Testing}, 
  year={2018},
  volume={},
  number={},
  pages={58-69},
  abstract={To facilitate communication among stakeholders, software security requirements are typically written in natural language and capture both positive requirements (i.e., what the system is supposed to do to ensure security) and negative requirements (i.e., undesirable behavior undermining security). In this paper, we tackle the problem of automatically generating executable security test cases from security requirements in natural language (NL). More precisely, since existing approaches for the generation of test cases from NL requirements verify only positive requirements, we focus on the problem of generating test cases from negative requirements. We propose, apply and assess Misuse Case Programming (MCP), an approach that automatically generates security test cases from misuse case specifications (i.e., use case specifications capturing the behavior of malicious users). MCP relies on natural language processing techniques to extract the concepts (e.g., inputs and activities) appearing in requirements specifications and generates executable test cases by matching the extracted concepts to the members of a provided test driver API. MCP has been evaluated in an industrial case study, which provides initial evidence of the feasibility and benefits of the approach.},
  keywords={Ontologies;Software reliability;Natural languages;Test pattern generators;Authorization;Computer languages;Password;System Security Testing;Natural Language Requirements;Natural Language Processing;Natural Language Programming},
  doi={10.1109/ISSRE.2018.00017},
  ISSN={2332-6549},
  month={Oct},}

@ARTICLE{9115830,
  author={Chen, Yanjiao and Ou, Runmin and Li, Zhiyang and Wu, Kaishun},
  journal={IEEE Transactions on Mobile Computing}, 
  title={WiFace: Facial Expression Recognition Using Wi-Fi Signals}, 
  year={2022},
  volume={21},
  number={1},
  pages={378-391},
  abstract={Facial expressions are an essential form of human nonverbal communication. Recognition of this nonverbal sign may enable developers to understand the feedbacks on smart device functionality and advertising. Existing approaches for facial expression recognition are mainly based on cameras or on-body sensors, which are either sensitive to lighting conditions or cumbersome for users to wear devices on their faces. In this paper, we propose a new facial expression recognition system based on Wi-Fi signals, named WiFace. Our fundamental intuition is that facial muscle movements in different expressions will induce distinctive waveform patterns in the time-series of channel state information (CSI) in Wi-Fi signals. We develop a series of algorithms to process the CSI signals and extract the most representative waveform patterns for facial expression classification. We build a fully-functional prototype of WiFace using commercial off-the-shelf devices, which can recognize six typical facial expressions. We conduct extensive experiments to evaluate the performance of WiFace, and the experimental results show that the average recognition accuracy is 94.80 percent.},
  keywords={Wireless fidelity;Face recognition;Feature extraction;Sensors;Lighting;Transmitting antennas;Smart devices;Facial expression recognition;Wi-Fi based sensing;channel state information},
  doi={10.1109/TMC.2020.3001989},
  ISSN={1558-0660},
  month={Jan},}

@ARTICLE{50774,
  author={Ehrlich, W.K. and Lee, S.K. and Molisani, R.H.},
  journal={IEEE Software}, 
  title={Applying reliability measurement: a case study}, 
  year={1990},
  volume={7},
  number={2},
  pages={56-64},
  abstract={The problem of knowing when to stop testing software is considered, focusing on the strategy of stopping when a reliability level or rate of failure occurrence acceptable to the customer is reached. The system's reliability is monitored throughout the system test, and the system is released to the field only when the measured reliability is at or above this objective. This approach was applied to test-failure data collected on Remote Measurement System-Digital 1, a large telecommunications testing system that had already gone through system test and been released to the field. The RMS-D1 failure data, which consisted of command-response errors versus commands executed, had been routinely collected by the system-test organization during testing. The testing phase analyzed, the load test, was an operational-profile-driven test in which a controlled load was imposed on the system reflective of the system's busy-hour usage pattern. It was found to be feasible to apply the reliability-measurement approach in real time, to systems actually undergoing system test, given a controlled load-test environment.<>},
  keywords={System testing;Control systems;Software testing;Reliability;Condition monitoring;Remote monitoring;Pattern analysis;System buses;Real time systems},
  doi={10.1109/52.50774},
  ISSN={1937-4194},
  month={March},}

@INPROCEEDINGS{8369576,
  author={Hains, Gaétan and Jakobsson, Arvid and Khmelevsky, Youry},
  booktitle={2018 Annual IEEE International Systems Conference (SysCon)}, 
  title={Towards formal methods and software engineering for deep learning: Security, safety and productivity for dl systems development}, 
  year={2018},
  volume={},
  number={},
  pages={1-5},
  abstract={Deep Learning (DL) techniques are now widespread and being integrated into many important systems. Their classification and recognition abilities ensure their relevance for multiple application domains far beyond pure signal processing. As a machine-learning technique that relies on training instead of explicit algorithm programming they offer a high degree of productivity. But recent research has shown that they can be vulnerable to attacks and the verification of their correctness is only just emerging as a scientific and engineering possibility. Moreover DL tools are not integrated into classical software engineering so software tools to specify, modify and verify them would make them even more mainstream as software-hardware systems. This paper surveys recent work and proposes research directions and methodologies for this purpose.},
  keywords={Artificial neural networks;Safety;Testing;Machine learning;Tools;Security;Training;deep-learning systems;neural networks;vulnerability of deep learning;security;verification;software engineering},
  doi={10.1109/SYSCON.2018.8369576},
  ISSN={2472-9647},
  month={April},}

@INPROCEEDINGS{8534549,
  author={Schneider, Michael and Hippchen, Benjamin and Abeck, Sebastian and Jacoby, Michael and Herzog, Reinhard},
  booktitle={2018 Global Internet of Things Summit (GIoTS)}, 
  title={Enabling IoT Platform Interoperability Using a Systematic Development Approach by Example}, 
  year={2018},
  volume={},
  number={},
  pages={1-6},
  abstract={Today, the IoT landscape consists of a large number of vertical IoT platforms that are rarely interconnected. To enable creation of applications across platforms and domain boundaries interoperability needs to be established between IoT platforms. In this paper we present how this task can be simplified by utilizing a systematic software development process based on behavior- and domain-driven development. This process is illustrated using an example that uses the open source IoT interoperability framework symbIoTe to connect two indoor navigation platforms. We show that developers can actually profit from this approach but existing IoT interoperability frameworks are still cumbersome to use.},
  keywords={Interoperability;Semantics;Internet of Things;Software;Syntactics;Standards;Registers;Internet of Things;IoT;interoperability;semantic interoperability;behavior-driven development;domain-driven design},
  doi={10.1109/GIOTS.2018.8534549},
  ISSN={},
  month={June},}

@ARTICLE{8352077,
  author={Rubinov, Konstantin and Baresi, Luciano},
  journal={Computer}, 
  title={What Are We Missing When Testing Our Android Apps?}, 
  year={2018},
  volume={51},
  number={4},
  pages={60-68},
  abstract={Android’s broad adoption drives the development of millions of new apps. Apps on this OS are not just trivial games; many of them handle sensitive information, exhibit complex structure, and require high reliability and trustworthiness. The authors discuss the problem of testing Android apps—the results achieved with current approaches, and what is still missing and requires fresh solutions.},
  keywords={Software testing;Androids;Humanoid robots;Software development;Graphical user interfaces;Analytical models;Runtime;Computer applications;Android;mobile;software testing;mobile computing;mobile applications;debugging},
  doi={10.1109/MC.2018.2141024},
  ISSN={1558-0814},
  month={April},}

@INPROCEEDINGS{878373,
  author={Polk, J.E. and Kakuda, R.Y. and Anderson, J.R. and Brophy, J.R. and Rawlin, V.K. and Sovey, J. and Hamley, J.},
  booktitle={2000 IEEE Aerospace Conference. Proceedings (Cat. No.00TH8484)}, 
  title={In-flight performance of the NSTAR ion propulsion system on the Deep Space One mission}, 
  year={2000},
  volume={4},
  number={},
  pages={123-148 vol.4},
  abstract={Deep Space 1 is the first interplanetary spacecraft to use an ion propulsion system for the primary delta-v maneuvers. The purpose of the mission is to validate a number of technologies, including ion propulsion and a high degree of spacecraft autonomy, on a flyby of an asteroid and two comets. The ion propulsion system has operated for a total of 3500 hours at engine power levels ranging from 0.48 to 1.94 kW and has completed the encounter with the asteroid 1992KD and the first set of deterministic burns required for a 2001 encounter with comet Wilson-Harrington. The system has worked extremely well after an initial grid short was cleared after launch. Operation during this primary mission phase has demonstrated all ion propulsion system and autonomous navigation functions. All propulsion system operating parameters are very close to the expected values with the exception of the thrust at higher power levels, which is about 2 percent lower than that calculated from the electrical parameters. This paper provides an overview of the system and presents the first flight validation data on an ion propulsion system in interplanetary space.},
  keywords={Propulsion;Space missions;Space technology;Space vehicles;Engines;NASA;Xenon;Instruments;Control systems;Feeds},
  doi={10.1109/AERO.2000.878373},
  ISSN={1095-323X},
  month={March},}

@INPROCEEDINGS{5070969,
  author={Jaaskelainen, Antti and Katara, Mika and Kervinen, Antti and Maunumaa, Mika and Paakkonen, Tuula and Takala, Tommi and Virtanen, Heikki},
  booktitle={2009 31st International Conference on Software Engineering - Companion Volume}, 
  title={Automatic GUI test generation for smartphone applications - an evaluation}, 
  year={2009},
  volume={},
  number={},
  pages={112-122},
  abstract={We present the results of an evaluation where we studied the effectiveness of automatic test generation for graphical user interface (GUI) testing of smartphone applications. To describe the context of our evaluation, the tools and the test model library we have developed for the evaluation are also presented. The library contains test models for basic S60 applications, such as camera, contacts, etc. The tools include an on-line test generator that produces sequences of so called keywords to be executed on the test targets. In our evaluation, we managed to find over 20 defects from applications that had been on the market for several months. We also describe the problems we faced during the evaluation.},
  keywords={Graphical user interfaces;Automatic testing;Software testing;System testing;Application software;Software systems;Libraries;Cameras;Optical character recognition software;Context modeling},
  doi={10.1109/ICSE-COMPANION.2009.5070969},
  ISSN={},
  month={May},}

@INPROCEEDINGS{8854494,
  author={Deutschmann, Jörg and Hielscher, Kai-Steffen and German, Reinhard},
  booktitle={2019 International Conference on Networked Systems (NetSys)}, 
  title={Satellite Internet Performance Measurements}, 
  year={2019},
  volume={},
  number={},
  pages={1-4},
  abstract={Satellite Internet with geostationary satellites is one way to provide Internet access all over the world. In Europe, there are three major satellite operators. In this paper, their performance is compared with respect to one-way delays, bulk data transfers, and website download times. When one-way delays are measured with UDP packets, the forward link generally shows lower delays than the return link. The bulk data transfers reveal how close actual data rates get to the advertised link rates. Regarding website download times, the web protocols HTTP/1.1, HTTP/2, and QUIC are considered. The results of our TCP measurements show that satellite Internet heavily relies on Performance Enhancement Proxies (PEPs). TCP connections over VPN tunnels cannot benefit from PEPs and therefore result in poor performance. QUIC can perform better than TCP tunneled in VPNs, but performs worse than TCP connections optimized by PEPs. One operator generally shows more stable performance regarding delays, data rates and page load times.},
  keywords={Satellites;Internet;Delays;Data transfer;Virtual private networks;Throughput;Servers;Satellite Internet;Performance Measurements;UDP;TCP;HTTP;QUIC},
  doi={10.1109/NetSys.2019.8854494},
  ISSN={},
  month={March},}

@INPROCEEDINGS{7338242,
  author={Kusel, Angelika and Etzlstorfer, Jürgen and Kapsammer, Elisabeth and Retschitzegger, Werner and Schwinger, Wieland and Schönböck, Johannes},
  booktitle={2015 ACM/IEEE 18th International Conference on Model Driven Engineering Languages and Systems (MODELS)}, 
  title={Consistent co-evolution of models and transformations}, 
  year={2015},
  volume={},
  number={},
  pages={116-125},
  abstract={Evolving metamodels are in the center of Model-Driven Engineering, necessitating the co-evolution of dependent artifacts like models and transformations. While model co-evolution has been extensively studied, transformation co-evolution has received less attention up to now. Current approaches for transformation co-evolution provide a fixed, restricted set of metamodel (MM) changes, only. Furthermore, composite changes are treated as monolithic units, which may lead to inconsistent co-evolution for overlapping atomic changes and prohibits extensibility. Finally, transformation co-evolution is considered in isolation, possibly inducing inconsistencies between model and transformation co-evolution. To overcome these limitations, we propose a complete set of atomic MM changes being able to describe arbitrary MM evolutions. Reusability and extensibility are supported by means of change composition, ensuring an intra-artifact consistent co-evolution. Furthermore, each change provides resolution actions for both, models and transformations, ensuring an inter-artifact consistent co-evolution. Based on our conceptual approach, a prototypical implementation is presented.},
  keywords={Biological system modeling;Semantics;Systematics;Feature extraction;Software;Syntactics;Companies},
  doi={10.1109/MODELS.2015.7338242},
  ISSN={},
  month={Sep.},}

@ARTICLE{9760699,
  author={Wang, Ziyuan and Bu, Dexin and Sun, Aiyue and Gou, Shanyi and Wang, Yong and Chen, Lin},
  journal={IEEE Transactions on Reliability}, 
  title={An Empirical Study on Bugs in Python Interpreters}, 
  year={2022},
  volume={71},
  number={2},
  pages={716-734},
  abstract={Python is an interpreted programming language that has been widely used in many fields. The successful execution of a Python program depends on both the correctness of Python program and the correctness of Python interpreter. As an infrastructure software, there are many bugs in the Python interpreter. Exploring the bugs in Python interpreters can help developers and maintainers of Python interpreters detect and fix bugs and help users of Python avoid risks. In this article, we conduct an empirical study on the bugs in two mainstream Python interpreters: CPython and PyPy. By analyzing 25 958 fixed bugs, 18 824 revisions, 2 116 test cases, and root causes of randomly sampled 510 bugs, we have summarized the following findings. 1)The distribution of bugs in the Python interpreter is so uneven that the vast majority of bugs are distributed in a few components and source files. 2)The scales of the testing programs that reveal bugs are small. 3)The fixing works seem to be not complicated since the number of modified source files and lines of code are limited; however, most bugs need a long time to be fixed; nearly 15% of the bugs need more than one year to fix. 4)The priorities of bugs are independent of their locations, but they significantly correlate with duration of bugs. 5)Semantic bugs are the most frequent root causes of bugs, and their proportion exceeds other types of root causes.  These results could indicate some potential problems during the detecting and fixing of Python interpreter’s bugs, and provide some assistance to developers and maintainers of Python interpreters, users of Python, as well as researchers in related fields.},
  keywords={Computer bugs;Python;Testing;Correlation;Sun;Software development management;Optimization;CPython;empirical study;PyPy;Python interpreter;software bug},
  doi={10.1109/TR.2022.3159812},
  ISSN={1558-1721},
  month={June},}

@INPROCEEDINGS{6693140,
  author={Gambi, Alessio and Hummer, Waldemar and Dustdar, Schahram},
  booktitle={2013 28th IEEE/ACM International Conference on Automated Software Engineering (ASE)}, 
  title={Automated testing of cloud-based elastic systems with AUToCLES}, 
  year={2013},
  volume={},
  number={},
  pages={714-717},
  abstract={Cloud-based elastic computing systems dynamically change their resources allocation to provide consistent quality of service and minimal usage of resources in the face of workload fluctuations. As elastic systems are increasingly adopted to implement business critical functions in a cost-efficient way, their reliability is becoming a key concern for developers. Without proper testing, cloud-based systems might fail to provide the required functionalities with the expected service level and costs. Using system testing techniques, developers can expose problems that escaped the previous quality assurance activities and have a last chance to fix bugs before releasing the system in production. System testing of cloud-based systems accounts for a series of complex and time demanding activities, from the deployment and configuration of the elastic system, to the execution of synthetic clients, and the collection and persistence of execution data. Furthermore, clouds enable parallel executions of the same elastic system that can reduce the overall test execution time. However, manually managing the concurrent testing of multiple system instances might quickly overwhelm developers' capabilities, and automatic support for test generation, system test execution, and management of execution data is needed. In this demo we showcase AUToCLES, our tool for automatic testing of cloud-based elastic systems. Given specifications of the test suite and the system under test, AUToCLES implements testing as a service (TaaS): It automatically instantiates the SUT, configures the testing scaffoldings, and automatically executes test suites. If required, AUToCLES can generate new test inputs. Designers can inspect executions both during and after the tests.},
  keywords={Elasticity;Monitoring;Standards;System testing;Cloud computing},
  doi={10.1109/ASE.2013.6693140},
  ISSN={},
  month={Nov},}

@INPROCEEDINGS{9014711,
  author={Medhat, Noha and Moussa, Sherin and Badr, Nagwa and Tolba, Mohamed F.},
  booktitle={2019 Ninth International Conference on Intelligent Computing and Information Systems (ICICIS)}, 
  title={Testing Techniques in IoT-based Systems}, 
  year={2019},
  volume={},
  number={},
  pages={394-401},
  abstract={Internet of Things (IoT) systems are fast evolving nowadays, in which huge amounts of data are produced rapidly from heterogeneous sources. The nature of IoT-based systems implies many challenges, in terms of operation, security, quality control and data management. Thus, testing such systems is a key element to their success. In this paper, we present a comprehensive study for the main testing techniques and tools that have been considered for the IoT-based systems. Detailed comparison and analytical criticism are conducted, identifying the different testing types that have been applied for the main application domains. The research gaps are addressed, which highlight the future directions that can be adopted.},
  keywords={Testing;Sensors;Encryption;Internet of Things;Cloud computing;Tools;Testing;Internet of things (IoT);IoT-based systems;Testing Framework;Testing Tools;IoT Applications},
  doi={10.1109/ICICIS46948.2019.9014711},
  ISSN={},
  month={Dec},}

@ARTICLE{9734792,
  author={Cofer, Darren and Amundson, Isaac and Babar, Junaid and Hardin, David and Slind, Konrad and Alexander, Perry and Hatcliff, John and Robby and Klein, Gerwin and Lewis, Corey and Mercer, Eric and Shackleton, John},
  journal={IEEE Security & Privacy}, 
  title={Cyberassured Systems Engineering at Scale}, 
  year={2022},
  volume={20},
  number={3},
  pages={52-64},
  abstract={Our team has developed a model-based systems engineering environment that integrates formal methods at all levels of system design. Our methodology and tools enable systems engineers to address cybersecurity concerns early in the development of complex high-assurance systems.},
  keywords={Computational modeling;Codes;Modeling;Contracts;Analytical models;Software;Computer architecture},
  doi={10.1109/MSEC.2022.3151733},
  ISSN={1558-4046},
  month={May},}

@INPROCEEDINGS{8539067,
  author={Camilli, Matteo and Bellettini, Carlo and Gargantini, Angelo and Scandurra, Patrizia},
  booktitle={2018 IEEE 29th International Symposium on Software Reliability Engineering (ISSRE)}, 
  title={Online Model-Based Testing under Uncertainty}, 
  year={2018},
  volume={},
  number={},
  pages={36-46},
  abstract={Modern software systems are required to operate in a highly uncertain and changing environment. They have to control the satisfaction of their requirements at run-time, and possibly adapt and cope with situations that have not been completely addressed at design-time. Software engineering methods and techniques are, more than ever, forced to deal with change and uncertainty (lack of knowledge) explicitly. For tackling the challenge posed by uncertainty in delivering more reliable systems, this paper proposes a novel online Model-based Testing technique that complements classic test case generation based on pseudo-random sampling strategies with an uncertainty-aware sampling strategy. To deal with system uncertainty during testing, the proposed strategy builds on an Inverse Uncertainty Quantification approach that is related to the discrepancy between the measured data at run-time (while the system executes) and a Markov Decision Process model describing the behavior of the system under test. To this purpose, a conformance game approach is adopted in which tests feed a Bayesian inference calibrator that continuously learns from test data to tune the system model and the system itself. A comparative evaluation between the proposed uncertainty-aware sampling policy and classical pseudo-random sampling policies is also presented using the Tele Assistance System running example, showing the differences in achieved accuracy and efficiency.},
  keywords={Uncertainty;Bayes methods;Probabilistic logic;Testing;Markov processes;Software reliability;Uncertainty quantification;Reliability under Uncertainty;Bayesian Calibration;Online Model-based Testing},
  doi={10.1109/ISSRE.2018.00015},
  ISSN={2332-6549},
  month={Oct},}

@INPROCEEDINGS{10184875,
  author={Liang, Jie and Chen, Yaoguang and Wu, Zhiyong and Fu, Jingzhou and Wang, Mingzhe and Jiang, Yu and Huang, Xiangdong and Chen, Ting and Wang, Jiashui and Li, Jiajia},
  booktitle={2023 IEEE 39th International Conference on Data Engineering (ICDE)}, 
  title={Sequence-Oriented DBMS Fuzzing}, 
  year={2023},
  volume={},
  number={},
  pages={668-681},
  abstract={The SQL specification consists of hundreds of statement types, which leads to difficulties in DBMS fuzzing: state-of-the-art works generally reuse the statements of predefined types; the limited types cannot cover the full input space and test the corresponding logic consequently. In this paper, we propose Lego, a fuzzer to generate SQL sequences with abundant types to improve DBMS fuzzing coverage. The key idea of sequence generation is type-affinity, which indicates the meaningful occurrence of SQL type pairs (e.g., INSERT and SELECT). During each fuzzing iteration, Lego first proactively explores SQL statements of different types and analyzes affinities with coverage feedback. Next, when a new affinity is discovered, Lego synthesizes new SQL sequences containing the types progressively.We evaluate Lego on PostgreSQL, MySQL, MariaDB, and Comdb2 against SQLancer, SQLsmith, and Squirrel. The sequence-oriented fuzzing helps Lego outperform other fuzzers on branch coverage by 44%–198%. More importantly, in the continuous fuzzing, Lego has discovered 102 new vulnerabilities confirmed by the corresponding vendors, including 6 bugs in PostgreSQL, 21 bugs in MySQL, 42 bugs in MariaDB, and 33 bugs in Comdb2. Among them, 22 CVEs have been assigned due to their severe security influences.},
  keywords={Computer bugs;Refining;Fuzzing;Data engineering;Security;DBMS fuzzing;SQL Type Sequence},
  doi={10.1109/ICDE55515.2023.00057},
  ISSN={2375-026X},
  month={April},}

@ARTICLE{8642838,
  author={Ruscheinski, Andreas and Warnke, Tom and Uhrmacher, Adelinde M.},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={Artifact-Based Workflows for Supporting Simulation Studies}, 
  year={2020},
  volume={32},
  number={6},
  pages={1064-1078},
  abstract={Valid models are central for credible simulation studies. If those models do not exist, they need to be developed. In fact, entire simulation studies are often aimed at developing valid models. Thereby, successive model refinement and execution of diverse simulation experiments are closely intertwined. Whereas software-based support for individual simulation experiments exists, the intricate interdependencies and the diversity of tasks that govern simulation studies have prevented a more comprehensive support. To achieve the required flexibility while adhering to the constraints that apply between individual tasks, we adopt a declarative, artifact-based workflow approach. Therefore, central products of these simulation studies are identified and specified as artifacts: the conceptual model (with a focus on formally defined requirements), the simulation model, and the experiment. Each artifact is characterized by stages the artifact moves through to reach certain milestones and which are guarded by conditions. Thereby, the relations and constraints between artifacts become explicit. This is instrumental to check and ensure the consistency between conceptual model and simulation model, to automatically execute simulation experiments to probe the specified requirements, and to develop plans to provide goal-directed guidance to the user. We demonstrate the approach by using it to repeat an existing simulation study.},
  keywords={Data models;Analytical models;Computational modeling;Mathematical model;Adaptation models;Biological system modeling;Context modeling;Modeling and simulation life cycle;simulation study;artifact-based workflows;planning;user support},
  doi={10.1109/TKDE.2019.2899840},
  ISSN={1558-2191},
  month={June},}

@INPROCEEDINGS{8748595,
  author={Lenka, Rakesh Kumar and Kumar, Srikant and Mamgain, Sunakshi},
  booktitle={2018 International Conference on Advances in Computing, Communication Control and Networking (ICACCCN)}, 
  title={Behavior Driven Development: Tools and Challenges}, 
  year={2018},
  volume={},
  number={},
  pages={1032-1037},
  abstract={Nowadays testing usually applies Test Driven Development (TDD) which is an approach to software development in which developers write tests first which initially fail and by adding more application codes tests pass. However, the latest development in this field is an extension to Test Driven Development (TDD) which usually referred as Behavior Driven Development (BDD). As being a modified version of TDD, both the technologies have various similarities. Nevertheless, the differences are also not unnoticeable. Where BDD is more about communication and collaboration TDD is more about coders and coding. This paper focuses on the advantages and glitches of TDD which led to the development of along with the method of working of BDD and several tools along with their features and a comparison of their functionalities.},
  keywords={Testing;Tools;Cloud computing;Writing;Software;Business;Java;Manual Testing;Test Driven Development (TDD);Behavior Driven Development (BDD);Collaboration},
  doi={10.1109/ICACCCN.2018.8748595},
  ISSN={},
  month={Oct},}

@INPROCEEDINGS{9787838,
  author={Paduraru, Ciprian and Paduraru, Miruna and Stefanescu, Alin},
  booktitle={2022 IEEE Conference on Software Testing, Verification and Validation (ICST)}, 
  title={RiverGame - a game testing tool using artificial intelligence}, 
  year={2022},
  volume={},
  number={},
  pages={422-432},
  abstract={As is the case with any very complex and interactive software, many video games are released with various minor or major issues that can potentially affect the user experience, cause security issues for players, or exploit the companies that deliver the products. To test their games, companies invest important resources in quality assurance personnel who usually perform the testing mostly manually. The main goal of our work is to automate various parts of the testing process that involve human users (testers) and thus to reduce costs and run more tests in less time. The secondary goal is to provide mechanisms to make test specification writing easier and more efficient. We focus on solving initial real-world problems that have emerged from several discussions with industry partners. In this paper, we present RiverGame, a tool that allows game developers to automatically test their products from different points of view: the rendered output, the sound played by the game, the animation and movement of the entities, the performance and various statistical analyses. We also address the problem of input priorities, scheduling, and directing the testing effort towards custom and dynamic directions. At the core of our methods, we use state-of-the-art artificial intelligence methods for analysis and a behavior-driven development (BDD) methodology for test specifications. Our technical solution is open-source, independent of game engine, platform, and programming language.},
  keywords={Software testing;Quality assurance;Statistical analysis;Games;Companies;Dynamic scheduling;User experience;game testing;automated testing;BDD;deep learning;reinforcement learning;computer vision},
  doi={10.1109/ICST53961.2022.00048},
  ISSN={2159-4848},
  month={April},}

@INPROCEEDINGS{7886987,
  author={Kim, Jongwook and Batory, Don and Dig, Danny and Azanza, Maider},
  booktitle={2016 IEEE/ACM 38th International Conference on Software Engineering (ICSE)}, 
  title={Improving Refactoring Speed by 10X}, 
  year={2016},
  volume={},
  number={},
  pages={1145-1156},
  abstract={Refactoring engines are standard tools in today's Integrated Development Environments (IDEs). They allow programmers to perform one refactoring at a time, but programmers need more. Most design patterns in the Gang-of-Four text can be written as a refactoring script - a programmatic sequence of refactorings. In this paper, we present R3, a new Java refactoring engine that supports refactoring scripts. It builds a main-memory, non-persistent database to encode Java entity declarations (e.g., packages, classes, methods), their containment relationships, and language features such as inheritance and modifiers. Unlike classical refactoring engines that modify Abstract Syntax Trees (ASTs), R3 refactorings modify only the database; refactored code is produced only when pretty-printing ASTs that reference database changes. R3 performs comparable precondition checks to those of the Eclipse Java Development Tools (JDT) but R3's codebase is about half the size of the JDT refactoring engine and runs an order of magnitude faster. Further, a user study shows that R3 improved the success rate of retrofitting design patterns by 25% up to 50%.},
  keywords={Java;Engines;Databases;Graphics;Computer bugs;Graphical user interfaces;Maintenance engineering},
  doi={10.1145/2884781.2884802},
  ISSN={1558-1225},
  month={May},}

@INPROCEEDINGS{9318176,
  author={Imran, Hamza Ali and Latif, Usama and Ikram, Ataul Aziz and Ehsan, Maryam and Ikram, Ahmed Jamal and Khan, Waleed Ahmad and Wazir, Saad},
  booktitle={2020 IEEE 23rd International Multitopic Conference (INMIC)}, 
  title={Multi-Cloud: A Comprehensive Review}, 
  year={2020},
  volume={},
  number={},
  pages={1-5},
  abstract={In the span of a decade, innovations in cloud computing have led to a new understanding of computing to be used as a utility. Majority of cloud service providers are making the service better and competitive for end-user. Aside from the number of services introduced by these providers, users are feeling uneasy and are unaware of consequences while switching from one service to another. Internal architecture of the cloud makes it difficult for end-users to understand. To overcome this issue a new concept of multi-cloud has been introduced. In multi-cloud technology, we can use multiple clouds from different vendors without platform complexity. Hence summarized, Multi-cloud is the usage of autonomous cloud platforms with one interface which may clue to different administrative and implementation domains. This paper reviews the literature of recently presented solutions and architectures for multi-cloud platforms.},
  keywords={Cloud computing;Ubiquitous computing;High performance computing;Software as a service;Systematic literature review;Cloud Computing;Multi-Cloud;Ubiquitous Computing;High Performance Computing;Anything as a Service},
  doi={10.1109/INMIC50486.2020.9318176},
  ISSN={2049-3630},
  month={Nov},}

@INPROCEEDINGS{10132233,
  author={Zimmermann, Daniel and Koziolek, Anne},
  booktitle={2023 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)}, 
  title={Automating GUI-based Software Testing with GPT-3}, 
  year={2023},
  volume={},
  number={},
  pages={62-65},
  abstract={This paper introduces a new method for GUI-based software testing that utilizes GPT-3, a state-of-the-art language model. The approach uses GPT-3’s transformer architecture to interpret natural language test cases and programmatically navigate through the application under test. To overcome the memory limitations of the transformer architecture, we propose incorporating the current state of all GUI elements into the input prompt at each time step. Additionally, we suggest using a test automation framework to interact with the GUI elements and provide GPT-3 with information about the application’s current state. To simplify the process of acquiring training data, we also present a tool for this purpose. The proposed approach has the potential to improve the efficiency of software testing by eliminating the need for manual input and allowing non-technical users to easily input test cases for both desktop and mobile applications.},
  keywords={Software testing;Automation;Navigation;Natural languages;Training data;Computer architecture;Manuals;UI Testing;Test Automation;Deep Learning;Language Models},
  doi={10.1109/ICSTW58534.2023.00022},
  ISSN={2159-4848},
  month={April},}

@INPROCEEDINGS{6135869,
  author={Ferguson, Donald F. and Hadar, Ethan},
  booktitle={2011 8th International Conference & Expo on Emerging Technologies for a Smarter World}, 
  title={Optimizing the IT business supply chain utilizing cloud computing}, 
  year={2011},
  volume={},
  number={},
  pages={1-6},
  abstract={Information technology applications and systems are essential to businesses and enterprises as they implement business components of the enterprise. In some cases, IT is the business, such as with financial services. Optimizing Return-on-Investment (ROI) in the IT area is essential to the business performance. Reducing cost is one component of ROI, however the predominant value is increasing revenue. IT is essential to the enterprise agilely to exploit new business opportunities. Cloud computing is emerging as a technology for optimizing IT costs and supporting agility. Enterprises are incrementally moving to cloud computing in an exploratory, ad hoc manner. Since, enterprises think in terms of IT Services that IT provides to the business, and an IT service is interconnecting hardware and software resources, the management aspects are conceptually similar to a manufacture or retail supply chain. As a result, exploiting cloud computing is a supply chain management problem for IT services using cloud computing. This paper describes the architecture requirements and implementation of a set of components for optimizing the IT supply chain using cloud computing.},
  keywords={Business;Optimization;Servers;Monitoring;Cloud computing;Hardware;Automation;enterprise IT;cloud archtiecture;supply chain management;composite IT system},
  doi={10.1109/CEWIT.2011.6135869},
  ISSN={},
  month={Nov},}

@INPROCEEDINGS{1383107,
  author={Lil, M. and Wei, Y. and Desovski, D. and Nejad, H. and Ghose, S. and Cukic, B. and Smidts, C.},
  booktitle={15th International Symposium on Software Reliability Engineering}, 
  title={Validation of a methodology for assessing software reliability}, 
  year={2004},
  volume={},
  number={},
  pages={66-76},
  abstract={Software-based digital systems are progressively replacing analog systems in safety-critical applications. However the ability to predict their reliability is not well understood and needs further study. A first step towards systematic resolution of this issue was presented in a recent software engineering measure study. In that study a set of software engineering measures were ranked with respect to their ability in predicting software reliability through an expert opinion elicitation process. This study also proposed a concept of reliability prediction system (RePS) to bridge the gap between software engineering measures and software reliability. The research presented in this paper validates the rankings obtained and the concept of RePS proposed in the previous study.},
  keywords={Software reliability;Software measurement;Software engineering;Reliability engineering;Phase measurement;Application software;Object oriented modeling;Usability;Software testing;Educational institutions},
  doi={10.1109/ISSRE.2004.47},
  ISSN={1071-9458},
  month={Nov},}

@INPROCEEDINGS{6823884,
  author={Weißleder, Stephan and Schlingloff, Holger},
  booktitle={2014 IEEE Seventh International Conference on Software Testing, Verification and Validation}, 
  title={An Evaluation of Model-Based Testing in Embedded Applications}, 
  year={2014},
  volume={},
  number={},
  pages={223-232},
  abstract={Testing is one of the most important quality assurance techniques for software. Automating the test design allows for automatically creating test suites from high-level system descriptions or test descriptions. Furthermore, it enables testers to automatically adapt the test suites to potentially recently changed descriptions. In model-based testing, models are used to automatically create test cases. Case studies report of an effort reduction in test design between 20 and 85 percent. In this paper, we report on a pilot project for introducing model-based testing in an industrial context. For such a pilot project, it is necessary to adapt the existing workflows and tool chains, to train the staff, and to adapt the assets of the company. The goal is to show the full applicability of the technique at the customer site. We present the evaluations, the lessons learned, and compare the efforts of model-based and manual test design for this example. This paper is not 'generally valid' in the sense that the results are reproducible for other projects and domains. Instead, our intention is to provide guidance for setting up similar evaluations.},
  keywords={Unified modeling language;Generators;Testing;Companies;Adaptation models;Context;Manuals;model-based testing;tool integration;industrial pilot project;experience report;case study},
  doi={10.1109/ICST.2014.35},
  ISSN={2159-4848},
  month={March},}

@INPROCEEDINGS{8802773,
  author={Semeráth, Oszkár and Babikian, Aren A. and Pilarski, Sebastian and Varró, Dániel},
  booktitle={2019 IEEE/ACM 41st International Conference on Software Engineering: Companion Proceedings (ICSE-Companion)}, 
  title={VIATRA Solver: A Framework for the Automated Generation of Consistent Domain-Specific Models}, 
  year={2019},
  volume={},
  number={},
  pages={43-46},
  abstract={Viatra Solver [1] is a novel open source software tool to automatically synthesize consistent and diverse domain-specific graph models to be used as a test suite for the systematic testing of CPS modelling tools. Taking a metamodel, and a set of well-formedness constraints of a domain as input, the solver derives a diverse set of consistent graph models where each graph is compliant with the metamodel, satisfies consistency constraints, and structurally different from each other. The tool is integrated into the Eclipse IDE or it is executable from the command line.},
  keywords={Biological system modeling;Tools;Object oriented modeling;Testing;Adaptation models;Metals;Generators;Tool testing;Logic solver;Graph generation;Test generation;Model based system engineering},
  doi={10.1109/ICSE-Companion.2019.00034},
  ISSN={2574-1934},
  month={May},}

@INPROCEEDINGS{7107423,
  author={Gmeiner, Johannes and Ramler, Rudolf and Haslinger, Julian},
  booktitle={2015 IEEE Eighth International Conference on Software Testing, Verification and Validation Workshops (ICSTW)}, 
  title={Automated testing in the continuous delivery pipeline: A case study of an online company}, 
  year={2015},
  volume={},
  number={},
  pages={1-6},
  abstract={Companies running an online business need to be able to frequently push new features and bug fixes from development into production. Successful high-performance online companies deliver code changes often several times per day. Their continuous delivery model supports the business needs of the online world. At the same time, however, such practices increase the risk of introducing quality issues and unwanted side effects. Rigorous test automation is therefore a key success factor for continuous delivery. In this paper we describe how automated testing is used in the continuous delivery pipeline of an Austrian online business company. The paper illustrates the complex technical and organizational challenges involved and summarizes the lessons from more than six years of practical experience in establishing and maintaining an effective continuous delivery pipeline.},
  keywords={Pipelines;Testing;Databases;Companies;Production;Software;automated testing;continuous integration;continuous delivery;continusous deployment;dev ops},
  doi={10.1109/ICSTW.2015.7107423},
  ISSN={},
  month={April},}

@INPROCEEDINGS{6958413,
  author={Kracht, Jeshua S. and Petrovic, Jacob Z. and Walcott-Justice, Kristen R.},
  booktitle={2014 14th International Conference on Quality Software}, 
  title={Empirically Evaluating the Quality of Automatically Generated and Manually Written Test Suites}, 
  year={2014},
  volume={},
  number={},
  pages={256-265},
  abstract={The creation, execution, and maintenance of tests are some of the most expensive tasks in software development. To help reduce the cost, automated test generation tools can be used to assist and guide developers in creating test cases. Yet, the tests that automated tools produce range from simple skeletons to fully executable test suites, hence their complexity and quality vary. This paper compares the complexity and quality of test suites created by sophisticated automated test generation tools to that of developer-written test suites. The empirical study in this paper examines ten real-world programs with existing test suites and applies two state-of-the-art automated test generation tools. The study measures the resulting test suite quality in terms of code coverage and fault-finding capability. On average, manual tests covered 31.5% of the branches while the automated tools covered 31.8% of the branches. In terms of mutation score, the tests generated by automated tools had an average mutation score of 39.8% compared to the average mutation score of 42.1% for manually written tests. Even though automatically created tests often contain more lines of source code than those written by developers, this paper's empirical results reveal that test generation tools can provide value by creating high quality test suites while reducing the cost and effort needed for testing.},
  keywords={Manuals;Complexity theory;Software;Testing;Writing;Standards;Java},
  doi={10.1109/QSIC.2014.33},
  ISSN={2332-662X},
  month={Oct},}

@INPROCEEDINGS{5771265,
  author={Madhavapeddy, Anil and Singh, Satnam},
  booktitle={2011 IEEE 19th Annual International Symposium on Field-Programmable Custom Computing Machines}, 
  title={Reconfigurable Data Processing for Clouds}, 
  year={2011},
  volume={},
  number={},
  pages={141-145},
  abstract={Reconfigurable computing in the cloud helps to solve many practical problems relating to scaling out data-centers where computation is limited by energy consumption or latency. However, for reconfigurable computing in the cloud to become practical several research challenges have to be addressed. This paper identifies some of the perquisites for reconfigurable computing systems in the cloud and picks out several scenarios made possible with immense cloud-based computing capability.},
  keywords={Field programmable gate arrays;Cloud computing;USA Councils;Hardware;Computational modeling;Programming;Operating systems;reconfigurable computing;cloud computing},
  doi={10.1109/FCCM.2011.35},
  ISSN={},
  month={May},}

@INPROCEEDINGS{9445321,
  author={Jayasuryapal, G and Pranay, P. Meher and Kaur, Harpreet and Swati},
  booktitle={2021 2nd International Conference on Intelligent Engineering and Management (ICIEM)}, 
  title={A Survey on Network Penetration Testing}, 
  year={2021},
  volume={},
  number={},
  pages={373-378},
  abstract={Penetration on network is an important security measurements every company want to take into the consideration. Day-to-Day life as it is seen that cybercrimes are increasing due to lack of security practice. Penetration testing is an outstanding approach where pen tester evaluates the security of network and numerous applications by simulating attacks from attacker’s view point .Additionally, penetration testing process follow certain rules and agreement that both the parties ( client and pen testing team). By this testing, the company’s weakness will be detected like open servers and open ports etc. so we can take a countermeasure by doing penetration testing on the company. This paper contains some of the important terms and steps to do a strong penetration testing on organizations. Hence, this paper covered all the mechanisms including information gathering to the post exploitation.},
  keywords={Productivity;Companies;Security;Servers;Computer crime;Robots;Penetration testing;Network Penetration Testing;Server Security;Vulnerability examination;External Enumeration},
  doi={10.1109/ICIEM51511.2021.9445321},
  ISSN={},
  month={April},}

@INPROCEEDINGS{8453176,
  author={Semeráth, Oszkár and Nagy, András Szabolcs and Varró, Dániel},
  booktitle={2018 IEEE/ACM 40th International Conference on Software Engineering (ICSE)}, 
  title={A Graph Solver for the Automated Generation of Consistent Domain-Specific Models}, 
  year={2018},
  volume={},
  number={},
  pages={969-980},
  abstract={Many testing and benchmarking scenarios in software and systems engineering depend on the systematic generation of graph models. For instance, tool qualification necessitated by safety standards would require a large set of consistent (well-formed or malformed) instance models specific to a domain. However, automatically generating consistent graph models which comply with a metamodel and satisfy all well-formedness constraints of industrial domains is a significant challenge. Existing solutions which map graph models into first-order logic specification to use back-end logic solvers (like Alloy or Z3) have severe scalability issues. In the paper, we propose a graph solver framework for the automated generation of consistent domain-specific instance models which operates directly over graphs by combining advanced techniques such as refinement of partial models, shape analysis, incremental graph query evaluation, and rule-based design space exploration to provide a more efficient guidance. Our initial performance evaluation carried out in four domains demonstrates that our approach is able to generate models which are 1-2 orders of magnitude larger (with 500 to 6000 objects!) compared to mapping-based approaches natively using Alloy.},
  keywords={Analytical models;Object oriented modeling;Tools;Biological system modeling;IP networks;Testing;Graph generation;Test generation;Domain Specific Modeling Languages;Logic Solver;Graph Solver},
  doi={10.1145/3180155.3180186},
  ISSN={1558-1225},
  month={May},}

@INPROCEEDINGS{9438135,
  author={Hobbs, Kerianne L. and Davis, Jennifer and Wagner, Lucas and Feron, Eric},
  booktitle={2021 IEEE Aerospace Conference (50100)}, 
  title={Formal Specification and Analysis of Spacecraft Collision Avoidance Run Time Assurance Requirements}, 
  year={2021},
  volume={},
  number={},
  pages={1-16},
  abstract={One of the greatest challenges preventing the use of advanced controllers in aerospace is developing methods to verify, validate, and certify them with high assurance. One emerging approach is to push the burden of assurance from offline verification of an autonomous controller at design time, to online verification of safe behavior through a monitor and high assurance backup controller at run time. Run time assurance goes a step beyond alerting systems by detecting imminent unsafe behavior and intervening with a trusted control response. In the spacecraft domain, autonomous operations could be approved if run time assurance systems can provide collision avoidance capabilities. While several approaches to run time assurance have been developed and successfully demonstrated, the design and verification of these systems is ad hoc and specific to the application. This paper describes the elicitation, formal specification, and analysis of general collision avoidance system requirements for a conceptual spacecraft conducting autonomous close-proximity operations based on a run time assurance construct. This includes the first formally specified and analyzed generalized run time assurance architecture for spacecraft that includes a fault monitor, interlock monitor, and human-machine interface. Mathematically precise requirements are elicited through the process of formal specification based on common design elements, spacecraft guidance constraints in the literature, and a structured hazard assessment. Finally, the requirements are analyzed using compositional reasoning and formal model checking verification techniques.},
  keywords={Space vehicles;Computer architecture;Hazards;Software;Trajectory;Formal specifications;Collision avoidance},
  doi={10.1109/AERO50100.2021.9438135},
  ISSN={1095-323X},
  month={March},}

@ARTICLE{9179780,
  author={Mathieson, John T. J. and Mazzuchi, Thomas and Sarkani, Shahram},
  journal={IEEE Systems Journal}, 
  title={The Systems Engineering DevOps Lemniscate and Model-Based System Operations}, 
  year={2021},
  volume={15},
  number={3},
  pages={3980-3991},
  abstract={Systems engineering is defined as a “full life cycle” discipline and provides methodologies and processes to support the design, development, verification, sustainment, and disposal of systems. While this cradle-to-grave concept is well documented throughout literature, there has been recent emphasis on evolving and digitally transforming systems engineering methodologies, practices, and tools for the latter phases of system life cycles. This article adapts principles from the software engineering domain DevOps concept (a collaborative merger of system development and operations) into a Systems Engineering DevOps (SEDevOps) life cycle model. This facilitates a merger of systems engineering processes, tools, and products into a surrogate operational environment in which the sustainment of a system is tied closely to the curation of a system model expanded to include the enabling system elements necessary for operations and sustainment (procedures, scripts, etc.). This progression of the systems engineering mindset, focused on digitally transforming and enhancing system operations and sustainment, improves agility in later life cycle phases. A framework for applying SEDevOps is introduced as a new systems modeling language profile. A use-case leveraging this “model-based system operations” framework, shows how merging support elements into a spacecraft system model improves adaptability during operations, exemplifying elements of a DevOps approach to cyber-physical system sustainment.},
  keywords={Modeling;Tools;Unified modeling language;Systems operation;Software engineering;Adaptation models;Agile;DevOps;digital transformation;life cycle;model-based systems engineering (MBSE);systems engineering},
  doi={10.1109/JSYST.2020.3015595},
  ISSN={1937-9234},
  month={Sep.},}

@INPROCEEDINGS{9680292,
  author={Paduraru, Ciprian and Paduraru, Miruna and Stefanescu, Alin},
  booktitle={2021 36th IEEE/ACM International Conference on Automated Software Engineering Workshops (ASEW)}, 
  title={Automated game testing using computer vision methods}, 
  year={2021},
  volume={},
  number={},
  pages={65-72},
  abstract={Video game development is a growing industry nowadays with high revenues. However, even if there are many resources invested in the software development process, many games still contain bugs or performance issues that affect the user experience. This paper presents ideas on how computer vision methods can be used to automate the process of game testing. The goal is to replace the parts of the testing process that require human users (testers) with machines as much as possible, in order to reduce costs and perform more tests in less time by scaling with hardware resources. The focus is on solving existing real-world problems that have emerged from several discussions with industry partners. We base our methods on previous work in this area using intelligent agents playing video games and deep learning methods that interpret feedback from their actions based on visual output. The paper proposes several methods and a set of open-source tools, independent of the operating system or deployment platform, to evaluate the efficiency of the presented methods.},
  keywords={Industries;Computer vision;Visualization;Operating systems;Games;User experience;Intelligent agents;AI agents;game testing;automated testing;deep learning;reinforcement learning;software architecture},
  doi={10.1109/ASEW52652.2021.00024},
  ISSN={2151-0830},
  month={Nov},}

@INPROCEEDINGS{9377950,
  author={Wright, Steven A.},
  booktitle={2020 IEEE International Conference on Big Data (Big Data)}, 
  title={AI in the Law: Towards Assessing Ethical Risks}, 
  year={2020},
  volume={},
  number={},
  pages={2160-2169},
  abstract={The exponential growth in data over the past decade has impacted the legal industry; both requiring automated solutions for the cost effective and efficient management of the volume and variety of big (legal) data; and, enabling artificial intelligence techniques based on machine learning for the analysis of that data. While many legal practitioners focus on specific services niches, the impact of AI in the law is much broader than individual niches. While AI systems and concerns for their ethical operation are not new, the scale of impact and adoption of AI systems in legal practice makes consideration of the ethics of these systems timely. While there has been recent progress in development of ethical guidelines for AI systems, much of this is targeted at the developers of these systems in general, or the actions of these AI systems as autonomous entities, rather than in the legal practice context. Much of the ethical guidance - whether for AI systems or legal professional is captured in high level principles within more narrowly defined domains, more specific guidance may be appropriate to identify and assess ethical risks. As adoption and operation of AI software in routine legal practice becomes more commonplace, more detailed guidance on assessing the scope and scale of ethical risks is needed.},
  keywords={Industries;Ethics;Law;Software;Artificial intelligence;Standards;Guidelines;Ethics;AI;Law;Assessment},
  doi={10.1109/BigData50022.2020.9377950},
  ISSN={},
  month={Dec},}

@INPROCEEDINGS{5615100,
  author={Torens, Christoph and Ebrecht, Lars},
  booktitle={2010 Fifth International Conference on Software Engineering Advances}, 
  title={RemoteTest: A Framework for Testing Distributed Systems}, 
  year={2010},
  volume={},
  number={},
  pages={441-446},
  abstract={This work deals with general difficulties and aims when testing complex distributed systems, especially when heterogeneous interfaces are used. As a solution RemoteTest is proposed, a framework for the test of distributed systems and their interfaces. This is done by integrating individual system components into a virtual environment that emulates the adjacent modules of the system. The interface details are thereby abstracted by the framework and there is no special interface knowledge necessary by the tester. In addition to the decoupling of components and interface abstraction, RemoteTest facilitates the testing of distributed systems with flexible mechanisms to write test scripts and an architecture that can be easily adapted to different systems.},
  keywords={Testing;Software;Computer architecture;Virtual environment;Hardware;Programming;Complexity theory;software test;distributed systems;test framework;test tools;test methods},
  doi={10.1109/ICSEA.2010.75},
  ISSN={},
  month={Aug},}

@INPROCEEDINGS{792630,
  author={Mattsson, M.},
  booktitle={Proceedings IEEE International Conference on Software Maintenance - 1999 (ICSM'99). 'Software Maintenance for Business Change' (Cat. No.99CB36360)}, 
  title={Effort distribution in a six year industrial application framework project}, 
  year={1999},
  volume={},
  number={},
  pages={326-333},
  abstract={It has for a long time been claimed that object oriented framework technology delivers reduced application development efforts but no quantitative evidence has been shown. We present quantitative data from a six year object oriented application framework project in the telecommunication domain. During six years four major versions of the application framework have been developed and over 30 installations at customer sites has been done. We present effort data both for the framework development and the installations as well as relative effort per phase for the framework versions. The effort data presented gives quantitative support for the claim that framework technology delivers reduced application development efforts. In fact, the effort data shows that the average application development effort is less than 2% of the framework development effort.},
  keywords={Mediation;Application software;Costs;System testing;Programming;Large-scale systems;Software engineering;Computer science;Technology planning;Path planning},
  doi={10.1109/ICSM.1999.792630},
  ISSN={1063-6773},
  month={Aug},}

@INPROCEEDINGS{6823865,
  author={Lackner, Hartmut and Thomas, Martin and Wartenberg, Florian and Weißleder, Stephan},
  booktitle={2014 IEEE Seventh International Conference on Software Testing, Verification and Validation}, 
  title={Model-Based Test Design of Product Lines: Raising Test Design to the Product Line Level}, 
  year={2014},
  volume={},
  number={},
  pages={51-60},
  abstract={System quality assurance techniques like testing are important for high-quality products and processes. The effort for applying them is usually high, but can be reduced using automation. Automated test design is possible by using models to specify test-relevant aspects and by generating tests on this basis. Testing multiple variants of a system like, e.g., a product line of a German car manufacturer, results in a significant, additional effort. In this paper, we deal with model-based testing of product lines. We combine feature models that are used to describe product lines and models that are used for automated model-based test design. Our main contribution is the definition of a test generation approach on the product line level, i.e., that does not depend on resolving single product variants. Furthermore, we compare our approach to other test generation approaches and evaluate it using our tool chain SPLTestbench for some product line examples.},
  keywords={Unified modeling language;Testing;Biological system modeling;Standards;Security;Quality assurance;Credit cards;Software Product Lines;Quality Assurance;Software Testing;Model-Based Testing;Software Reuse;Domain Level Testing},
  doi={10.1109/ICST.2014.16},
  ISSN={2159-4848},
  month={March},}

@INPROCEEDINGS{9159060,
  author={Mai, Phu X. and Pastore, Fabrizio and Goknil, Arda and Briand, Lionel},
  booktitle={2020 IEEE 13th International Conference on Software Testing, Validation and Verification (ICST)}, 
  title={Metamorphic Security Testing for Web Systems}, 
  year={2020},
  volume={},
  number={},
  pages={186-197},
  abstract={Security testing verifies that the data and the resources of software systems are protected from attackers. Unfortunately, it suffers from the oracle problem, which refers to the challenge, given an input for a system, of distinguishing correct from incorrect behavior. In many situations where potential vulnerabilities are tested, a test oracle may not exist, or it might be impractical due to the many inputs for which specific oracles have to be defined. In this paper, we propose a metamorphic testing approach that alleviates the oracle problem in security testing. It enables engineers to specify metamorphic relations (MRs) that capture security properties of the system. Such MRs are then used to automate testing and detect vulnerabilities. We provide a catalog of 22 system-agnostic MRs to automate security testing in Web systems. Our approach targets 39% of the OWASP security testing activities not automated by state-of-the-art techniques. It automatically detected 10 out of 12 vulnerabilities affecting two widely used systems, one commercial and the other open source (Jenkins).},
  keywords={Testing;Software;Uniform resource locators;Graphical user interfaces;Authorization;Transforms;Software Engineering;Software Security},
  doi={10.1109/ICST46399.2020.00028},
  ISSN={2159-4848},
  month={Oct},}

@INPROCEEDINGS{10343989,
  author={David, Istvan and Archambault, Pascal and Wolak, Quentin and Vu, Cong Vinh and Lalonde, Timothé and Riaz, Kashif and Syriani, Eugene and Sahraoui, Houari},
  booktitle={2023 ACM/IEEE 26th International Conference on Model Driven Engineering Languages and Systems (MODELS)}, 
  title={Digital Twins for Cyber-Biophysical Systems: Challenges and Lessons Learned}, 
  year={2023},
  volume={},
  number={},
  pages={1-12},
  abstract={Digital twinning is gaining popularity in domains outside of traditional engineered systems, including cyber-physical systems (CPS) with biological modalities, or cyber-biophysical systems (CBPS) in short. While digital twinning has well-established practices in CPS settings, it raises special challenges in the context of CBPS. In this paper, we identify such challenges and lessons learned through an industry case of a digital twin for CBPS in controlled environment agriculture.},
  keywords={Digital transformation;Data integrity;Companies;Biology;Agriculture;Digital twins;Safety;controlled environment agriculture;industry;model-driven;report;simulation},
  doi={10.1109/MODELS58315.2023.00014},
  ISSN={},
  month={Oct},}

@ARTICLE{7206603,
  author={Jamro, Marcin},
  journal={IEEE Transactions on Industrial Informatics}, 
  title={POU-Oriented Unit Testing of IEC 61131-3 Control Software}, 
  year={2015},
  volume={11},
  number={5},
  pages={1119-1129},
  abstract={Software testing is an important part of project development. Depending on system type and size, it is performed variously. Unit testing is one of the available approaches that is used to ensure that behavior of small software parts is consistent with requirements. It allows to improve software quality and decrease overall costs. Despite the fact that such an approach is commonly judged as a vital concept, it is not usual in control software. In this paper, the comprehensive approach to test the IEC 61131-3 software using unit tests is presented. It supports to create tests in two ways-either in textual and graphical IEC 61131-3 languages or in the CPTest+ dedicated test definition language. The latter is equipped with many advanced features, such as test fixtures and inclusions, parameterized and analog signal extensions, mock objects, as well as a few kinds of suites. The overall solution runs on the developer and testing station; hence, it does not have significant impact on performance of the control program and tests are more reliable and repeatable. To explain the concept, the simple running example is presented in this paper. The described solution has been introduced in the CPDev engineering environment for programming controllers.},
  keywords={Testing;Software;IEC Standards;Informatics;Automation;Control systems;control software;IEC 61131-3;testing;unit test;Control software;IEC 61131-3;testing;unit test},
  doi={10.1109/TII.2015.2469257},
  ISSN={1941-0050},
  month={Oct},}

@ARTICLE{9534688,
  author={Cherrueau, Ronan-Alexandre and Delavergne, Marie and van Kempen, Alexandre and Lebre, Adrien and Pertin, Dimitri and Balderrama, Javier Rojas and Simonet, Anthony and Simonin, Matthieu},
  journal={IEEE Transactions on Parallel and Distributed Systems}, 
  title={EnosLib: A Library for Experiment-Driven Research in Distributed Computing}, 
  year={2022},
  volume={33},
  number={6},
  pages={1464-1477},
  abstract={Despite the importance of experiment-driven research in the distributed computing community, there has been little progress in helping researchers conduct their experiments. In most cases, they have to achieve tedious and time-consuming development and instrumentation activities to deal with the specifics of testbeds and the system under study. In order to relieve researchers of the burden of those efforts, we have developed EnosLib: a Python library that takes into account best experimentation practices and leverages modern toolkits on automatic deployment and configuration systems. EnosLib helps researchers not only in the process of developing their experimental artifacts, but also in running them over different infrastructures. To demonstrate the relevance of our library, we discuss three experimental engines built on top of EnosLib, and used to conduct empirical studies on complex software stacks between 2016 and 2019 (database systems, communication buses and OpenStack). By introducing EnosLib, our goal is to gather academic and industrial actors of our community around a library that aggregates everyday experiment-driven research operations. A library that has been already adopted by open-source projects and members of the scientific community thanks to its ease of use and extension.},
  keywords={Libraries;Software;Task analysis;Tools;Protocols;Codes;Benchmark testing;Experiment-driven research;performance evaluation;distributed computing experimentation library},
  doi={10.1109/TPDS.2021.3111159},
  ISSN={1558-2183},
  month={June},}

@ARTICLE{1638205,
  author={},
  journal={IEEE Std 1100-2005 (Revision of IEEE Std 1100-1999)}, 
  title={IEEE Recommended Practice for Powering and Grounding Electronic Equipment (Color Book® Series)}, 
  year={2006},
  volume={},
  number={},
  pages={1-703},
  abstract={The IEEE Emerald Book(TM) presents a collection of consensus best practices for the powering and grounding of electronic equipment used in commercial and industrial applications. The main objective is to provide consensus recommended practices in an area where conflicting information and conflicting design philosophies have dominated. The recommended practices described are intended to enhance equipment performance while maintaining a safe installation. A description of the nature and origin of power disturbances is provided, followed by theory on the various parameters that impact power quality. Information on quantifying and resolving power and grounding related concerns using measurement and diagnostic instrumentation and standardized investigative procedures are included. Recommended power protection equipment and wiring and grounding system design practices are presented. Information on telecommunications system power protection as well as grounding, industrial system grounding, and noise control is included. Finally a selection of case studies are presented to support the recommended practices presented throughout the book.},
  keywords={IEEE Standards;Commercialization;Electronic equipment;Industry applications;Power conditioning;Power system quality;Monitoring;Grounding;commercial applications;electrical power;electronic equipment;grounding;industrial applications;power conditioning;power disturbance;power monitor;power quality;color book},
  doi={10.1109/IEEESTD.2006.216391},
  ISSN={},
  month={May},}

@INPROCEEDINGS{7813841,
  author={Kumar, Satendra and Rajkumar},
  booktitle={2016 International Conference on Computing, Communication and Automation (ICCCA)}, 
  title={Test case prioritization techniques for software product line: A survey}, 
  year={2016},
  volume={},
  number={},
  pages={884-889},
  abstract={Software product line (SPL) testing is a tougher work than testing of single systems. Still testing of each individual SPL product would be perfect but it is too costly in practice. In fact, when the number of features increases then the number of possible products also increases exponentially usually derived from a feature model. Number of features is leading to thousands of different products. Due to cost and time constraints, it is infeasible or large number of effort to run all the test cases in an existing test suite. To decrease the cost of testing, various techniques have been proposed. One of them is test case prioritization (TCP) techniques. Here we presented a survey for TCP techniques for software SPL.},
  keywords={Software;Software product lines;Testing;Frequency modulation;Fault detection;Automation;Libraries;Software product lines;Test Case Prioritization;Variability;Commonality;Feature Model},
  doi={10.1109/CCAA.2016.7813841},
  ISSN={},
  month={April},}

@INPROCEEDINGS{658216,
  author={Murphy, D.M. and Allen, D.M.},
  booktitle={IECEC-97 Proceedings of the Thirty-Second Intersociety Energy Conversion Engineering Conference (Cat. No.97CH6203)}, 
  title={SCARLET development, fabrication, and testing for the Deep Space 1 spacecraft}, 
  year={1997},
  volume={4},
  number={},
  pages={2237-2245 vol.4},
  abstract={An advanced version of "Solar Concentrator Arrays with Refractive Linear Element Technology" (SCARLET) is being assembled for use on the first NASA/JPL New Millennium spacecraft: Deep Space One (DS1). The array is scaled up from the first SCARLET array that was built for the METEOR satellite in 1995 and incorporates advanced technologies such as dual-junction solar cells and an improved structural design. Due to the failure of the Conestoga launch vehicle, this will be the first flight of a modular concentrator array. SCARLET will provide 2.6 kW to the DS1 spacecraft to be launched in July 1998 for a mission that includes fly-bys of the asteroid McAuliffe, Mars and the comet West-Kohoutek-Ikemura. This paper describes the SCARLET design, fabrication/assembly and testing program for the flight system.},
  keywords={Fabrication;Testing;Space technology;Space vehicles;Building integrated photovoltaics;Assembly;NASA;Satellites;Photovoltaic cells;Mars},
  doi={10.1109/IECEC.1997.658216},
  ISSN={},
  month={July},}

@INPROCEEDINGS{4138229,
  author={Sombrutzki, Robert and Zubow, Anatolij and Kurth, Mathias and Redlich, Jens-Peter},
  booktitle={2006 1st Workshop on Operator-Assisted (Wireless Mesh) Community Networks}, 
  title={Self-Organization in Community Mesh Networks The Berlin RoofNet}, 
  year={2006},
  volume={},
  number={},
  pages={1-11},
  abstract={A community network must be usable for inexperienced end users; thus self-organization is essential. On the one hand, we propose an approach for self-organization in ad-hoc wireless multi-hop mesh networks, where the client is fully freed from such mundane tasks as IP configuration, etc. On the other hand, the community mesh network itself is fully self-organized thus no operator or provider is required. We present the architecture of the Berlin RoofNet (BRN) and a distributed realization of services like DHCP, ARP and Internet gateway discovery and selection. In addition, results of a detailed simulation and experimental evaluation comparing our distributed hash table based approach to traditional methods are presented. We show that our approach is more reliable, efficient and responsive},
  keywords={Mesh networks;Computer architecture;Spread spectrum communication;IP networks;Cities and towns;Protocols;Web and internet services;Computer network reliability;Telecommunication network reliability;Wireless mesh networks;Community Networks;Self-Organization;Distributed Hash Table},
  doi={10.1109/WOACN.2006.337188},
  ISSN={},
  month={Sep.},}

@INPROCEEDINGS{843835,
  author={Bommireddy, A. and Khare, J. and Shaikh, S. and Su, S.-T.},
  booktitle={Proceedings 18th IEEE VLSI Test Symposium}, 
  title={Test and debug of networking SoCs-a case study}, 
  year={2000},
  volume={},
  number={},
  pages={121-126},
  abstract={This paper describes the test challenges faced and testability features implemented on Level One's networking System on Chip (SoC), IXE2000. The IXE2000 SoC is a 20+ million transistor Layer 2/3/4 Switch with 24 10/100 Mbps and 2 1000 Mbps Ethernet ports, and a predominantly IP-based design. The chip had constraints in terms of both design time and total system costs, which added an extra burden on test. The paper discusses how these constraints led to the current testability solutions and debug features on the chip.},
  keywords={Computer aided software engineering;Switches;Clocks;System testing;Ethernet networks;System-on-a-chip;Costs;Business communication;Design for manufacture;IP networks},
  doi={10.1109/VTEST.2000.843835},
  ISSN={1093-0167},
  month={April},}

@INPROCEEDINGS{9796424,
  author={Camilli, Matteo and Guerriero, Antonio and Janes, Andrea and Russo, Barbara and Russo, Stefano},
  booktitle={2022 IEEE/ACM International Conference on Automation of Software Test (AST)}, 
  title={Microservices Integrated Performance and Reliability Testing}, 
  year={2022},
  volume={},
  number={},
  pages={29-39},
  abstract={Continuous quality assurance for extra-functional properties of modern software systems is today a big challenge as their complexity is constantly increasing to satisfy market demands. This is the case of microservice systems. They provide high control on the scale of operation by means of fine-grained service decomposition, but this demands careful consideration of the relations between performance of individual microservices and service failures. In this work, we propose MlPaRT, a novel methodology, and platform to automatically test microservice operations for performance and reliability in combination. The proposed platform can be integrated into a DevOps cycle to support continuous testing and monitoring by the automatic (1) generation and execution of performance-reliability ex-vivo testing sessions, (2) collection of monitoring data, (3) computation of performance and reliability metrics, and (4) integrated visualization of the results. We apply our approach by operating the platform on an open source benchmark. Results show that our integrated approach can provide additional insights into the performance and reliability behaviour of microservices as well as their mutual relationships. CCS CONCEPTS • Software and its engineering →Software performance; Software reliability; Software verification and validation.},
  keywords={Measurement;Q-factor;Quality assurance;Costs;Microservice architectures;Data visualization;Benchmark testing;Microservices systems;reliability testing;performance testing},
  doi={10.1145/3524481.3527233},
  ISSN={},
  month={May},}

@INPROCEEDINGS{7778090,
  author={Belschner, Tim and Müller, Peter and Kraus, Florian and Reichel, Reinhard},
  booktitle={2016 IEEE/AIAA 35th Digital Avionics Systems Conference (DASC)}, 
  title={Automated generation of certification relevant documentation for a distributed avionics platform approach}, 
  year={2016},
  volume={},
  number={},
  pages={1-10},
  abstract={The development of distributed and fault tolerant avionics systems from the first drafts to the finalization of the certification process is complex, resource intensive (financial and man power) and involves high risks. Therefore, implementations like fly-by-wire systems are usually limited to the FAR25 (CS25) domain. Previous research at the Institute of Aircraft Systems at University of Stuttgart was focused on the development of an avionics platform and a tool suite for a cost-efficient development process for class 23 and class 25 aircraft. Now, the aim is to extend the platform idea towards certification. In doing so, the generation of the specification documentation and testing activities shall be automated while avoiding tool qualification where feasible. This paper presents an approach for the documentation generation and provides an outline to interface with the testing activities.},
  keywords={Aerospace electronics;Software;Hardware;Documentation;Actuators;Complexity theory;Computer architecture},
  doi={10.1109/DASC.2016.7778090},
  ISSN={2155-7209},
  month={Sep.},}

@INPROCEEDINGS{4730478,
  author={Wieczorek, Sebastian and Roth, Andreas and Stefanescu, Alin and Charfi, Anis},
  booktitle={2008 IEEE International Symposium on Service-Oriented System Engineering}, 
  title={Precise Steps for Choreography Modeling for SOA Validation and Verification}, 
  year={2008},
  volume={},
  number={},
  pages={148-153},
  abstract={Service-oriented architecture (SOA) enables organizations to transform their existing IT infrastructure into a more flexible business process platform. In this architecture, decoupled components that provide standard services can be composed to form individually configured and highly flexible applications. When building such applications it is important to have a formal specification of the interaction protocols between the composed services not only because such a specification provides an accurate and unambiguous description of the interactions and their ordering but also to enable automated verification and validation. In this paper, we present a case study from the SAP context showing the interactions between two SAP service components and use that case study to derive a set of modeling requirements. This motivates a discussion about applicable techniques for service choreography modeling and whether existing choreography languages cover the identified needs.},
  keywords={Semiconductor optical amplifiers;Service oriented architecture;Enterprise resource planning;Marketing and sales;Software systems;Systems engineering and theory;Buildings;Formal specifications;Protocols;Context-aware services;SOA;Choreography;Service;Modeling},
  doi={10.1109/SOSE.2008.43},
  ISSN={},
  month={Dec},}

@ARTICLE{9933038,
  author={Zamprogno, Lucas and Hall, Braxton and Holmes, Reid and Atlee, Joanne M.},
  journal={IEEE Transactions on Software Engineering}, 
  title={Dynamic Human-in-the-Loop Assertion Generation}, 
  year={2023},
  volume={49},
  number={4},
  pages={2337-2351},
  abstract={Test cases use assertions to check program behaviour. While these assertions may not be complex, they are themselves code that must be written correctly in order to determine whether a test case should pass or fail. We claim that most test assertions are relatively repetitive and straight-forward, making their construction well suited to automation and that this automation can reduce developer effort while improving assertion quality. Examining 33,873 assertions from 105 projects revealed that developer-written assertions fall into twelve high-level categories, confirming that the vast majority ($>$>90%) of test assertions are fairly simple in practice. We created AutoAssert, a human-in-the-loop tool to fit naturally into a developer's test-writing workflow by automatically generating assertions for JavaScript and TypeScript test cases. A developer invokes AutoAssert by identifying the variable they want validated; AutoAssert uses dynamic analysis to generate assertions relevant for this variable and its runtime values, injecting the assertions into the test case for the developer to accept, modify, delete. Comparing AutoAssert's assertions to those written by developers, we found that the assertions generated by AutoAssert are the same kind of assertion as was written by developers 84% of the time in a sample of over 1,000 assertions. Additionally we validated the utility of AutoAssert-generated assertions with 17 developers who found the majority of generated assertions to be useful and expressed considerable interest in using such a tool for their own projects.},
  keywords={Complexity theory;Codes;Semantics;Human in the loop;Testing;Runtime;Libraries},
  doi={10.1109/TSE.2022.3217544},
  ISSN={1939-3520},
  month={April},}

@INPROCEEDINGS{9402034,
  author={Mayr-Dorn, Christoph and Vierhauser, Michael and Bichler, Stefan and Keplinger, Felix and Cleland-Huang, Jane and Egyed, Alexander and Mehofer, Thomas},
  booktitle={2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE)}, 
  title={Supporting Quality Assurance with Automated Process-Centric Quality Constraints Checking}, 
  year={2021},
  volume={},
  number={},
  pages={1298-1310},
  abstract={Regulations, standards, and guidelines for safety-critical systems stipulate stringent traceability but do not prescribe the corresponding, detailed software engineering process. Given the industrial practice of using only semi-formal notations to describe engineering processes, processes are rarely "executable" and developers have to spend significant manual effort in ensuring that they follow the steps mandated by quality assurance. The size and complexity of systems and regulations makes manual, timely feedback from Quality Assurance (QA) engineers infeasible. In this paper we propose a novel framework for tracking processes in the background, automatically checking QA constraints depending on process progress, and informing the developer of unfulfilled QA constraints. We evaluate our approach by applying it to two different case studies; one open source community system and a safety-critical system in the air-traffic control domain. Results from the analysis show that trace links are often corrected or completed after the fact and thus timely and automated constraint checking support has significant potential on reducing rework.},
  keywords={Quality assurance;Manuals;Regulation;Complexity theory;Standards;Software engineering;Guidelines;software engineering process;traceability;developer support},
  doi={10.1109/ICSE43902.2021.00118},
  ISSN={1558-1225},
  month={May},}

@INPROCEEDINGS{8904799,
  author={Meyers, Bart and Gadeyne, Klaas and Oakes, Bentley and Bernaerts, Matthias and Vangheluwe, Hans and Denil, Joachim},
  booktitle={2019 ACM/IEEE 22nd International Conference on Model Driven Engineering Languages and Systems Companion (MODELS-C)}, 
  title={A Model-Driven Engineering Framework to Support the Functional Safety Process}, 
  year={2019},
  volume={},
  number={},
  pages={619-623},
  abstract={The design of safety-related systems traditionally has long and costly development cycles due to the highly manual safety engineering process, which is guided by industry standards. In this paper, we present a modelling framework that supports DevOps principles of continuous testing and fast development iterations for the design of safety-critical systems. We show how modelling can help introducing DevOps in the context of functional safety analysis, and we also report how DevOps was used during the development of the framework.},
  keywords={devops;safety critical;verification;automotive;iso26262},
  doi={10.1109/MODELS-C.2019.00094},
  ISSN={},
  month={Sep.},}

@ARTICLE{5389507,
  author={Bose, P. and Surya, S.},
  journal={IBM Journal of Research and Development}, 
  title={Architectural timing verification of CMOS RISC processors}, 
  year={1995},
  volume={39},
  number={1.2},
  pages={113-129},
  abstract={We consider the problem of verification and testing of architectural timing models ("timers") coded to predict cycles-per-instruction (CPI) performance of advanced CMOS superscalar (RISC) processors. Such timers are used for pre-hardware performance analysis and prediction. As such, these software models play a vital role in processor performance tuning as well as application-based competitive analysis, years before actual product availability. One of the key problems facing a designer, modeler, or application analyst who uses such a tool is to understand how accurate the model is, in terms of the actual design. In contrast to functional simulators, there is no direct way of testing timers in the classical sense, since the “correct” execution time (in cycles) of a program on the machine model under test is not directly known or computable from equations, truth tables, or other formal specifications. Ultimate validation (or invalidation) of such models can be achieved under actual hardware availability, by direct comparisons against measured performance. However, deferring validation solely to that stage would do little to achieve the overall purpose of accurate pre-hardware analysis, tuning, and projection. We describe a multilevel validation method which has been used successfully to transform evolving timers into highly accurate pre-hardware models. In this paper, we focus primarily on the following aspects of the methodology: a) establishment of cause-effect relationships in terms of model defects and the associated fault signatures; b) derivation of application-based test loop kernels to verify steady-state (periodic) behavior of pipeline flow, against analytically predicted signatures; and c) derivation of synthetic test cases to verify the “core” parameters characterizing the pipeline-level machine organization as implemented in the timer model. The basic tenets of the theory and its application are described in the context of an example processor, comparable in complexity to an advanced member of the PowerPC™ 6XX processor family.},
  keywords={},
  doi={10.1147/rd.391.0113},
  ISSN={0018-8646},
  month={Jan},}

@INPROCEEDINGS{10301222,
  author={Tang, Shuncheng and Zhang, Zhenya and Zhou, Jixiang and Zhou, Yuan and Li, Yan-Fu and Xue, Yinxing},
  booktitle={2023 IEEE 34th International Symposium on Software Reliability Engineering (ISSRE)}, 
  title={EvoScenario: Integrating Road Structures into Critical Scenario Generation for Autonomous Driving System Testing}, 
  year={2023},
  volume={},
  number={},
  pages={309-320},
  abstract={Autonomous Driving Systems (ADS) are safety-critical and require comprehensive testing before their deployment on public roads. Most existing testing approaches consist in generating scenarios that vary the behaviors of dynamic objects, while leaving a predefined road environment unchanged. Consequently, these approaches overlook the influence of different road structures on ADS safety, e.g., collisions can happen more frequently than usual on a merging road, because of the specific road structure. In this paper, we propose EvoScenario, a novel approach that integrates road structures into the generation of critical scenarios for exposing safety risks of ADS. Specifically, EvoScenario models a driving road as a sequence of road segments characterized in different aspects, such as their shapes and widths. Then, a test case is defined by concatenating the sequence of road segments and the sequence of dynamic object maneuvers. Inspired by EvoSuite that generates sequential method calls for Java unit testing, EvoScenario leverages the sequential models of test cases and constructs a multi-objective optimization framework to search for critical scenarios. We implement and demonstrate EvoScenario on an ADS provided by our industrial partner. Evaluation results show that EvoScenario can identify 6 types of safety violations, and outperform existing baseline testing approaches.},
  keywords={System testing;Java;Shape;Roads;Merging;Safety;Software reliability;autonomous driving systems;search-based testing;safety violations},
  doi={10.1109/ISSRE59848.2023.00054},
  ISSN={2332-6549},
  month={Oct},}

@INPROCEEDINGS{7066751,
  author={Malini, A. and Venkatesh, N. and Sundarakantham, K. and Mercyshalinie, S.},
  booktitle={International Conference on Computing and Communication Technologies}, 
  title={Mobile application testing on smart devices using MTAAS framework in cloud}, 
  year={2014},
  volume={},
  number={},
  pages={1-5},
  abstract={Testing of mobile applications which run on smart devices is more complex due to diversity of mobile devices and computational resources. Mobile testing using emulators which doesn't include real network traffic and testing mobile applications in more than one portable device in single system was also not possible in normal testing. In order to overcome the draw backs of normal testing, in this paper we deployed a Mobile Testing as a Service (MTAAS) framework in cloud environment. By using MTAAS framework many mobile applications can be tested in different portable devices and different mobile platforms. Testing of mobile applications using MTAAS provides most realistic results since it includes real network speed. Finally, we conducted an experiment in MTAAS framework and testing results shows that MTAAS can effectively reduce the complexity of mobile testing on different smart devices.},
  keywords={Testing;Mobile communication;Mobile computing;Performance evaluation;Smart phones;Load modeling;Cloud Testing;Mobile Application Testing;Testing as a service},
  doi={10.1109/ICCCT2.2014.7066751},
  ISSN={},
  month={Dec},}

@INPROCEEDINGS{7964337,
  author={Ringert, Jan Oliver and Rumpe, Bernhard and Schulze, Christoph and Wortmann, Andreas},
  booktitle={2017 IEEE/ACM 39th International Conference on Software Engineering: Software Engineering Education and Training Track (ICSE-SEET)}, 
  title={Teaching agile model-driven engineering for cyber-physical systems}, 
  year={2017},
  volume={},
  number={},
  pages={127-136},
  abstract={Agile development methods, model-driven engineering, and cyber-physical systems are important topics in software engineering education. It is not obvious how to teach their combination while respecting individual challenges posed to students and educators. We have devised a software project class for teaching the agile MDE for CPS. The project class was held in three different semesters. In this paper, we report on the setup of our exploratory study and its goals for teaching. We base our evaluation and insights on interviews and questionnaires. Our results show the feasibility of combination of agile MDE for CPS but also the challenges this combination poses to students and educators.},
  keywords={Education;Unified modeling language;Educational robots;Software engineering;Model driven engineering;teaching;model-driven engineering;cyber-physical systems;case study},
  doi={10.1109/ICSE-SEET.2017.16},
  ISSN={},
  month={May},}

@INPROCEEDINGS{4031222,
  author={Almeida, Joao Paulo and van Eck, Pascal and Iacob, Maria-eugenia},
  booktitle={2006 10th IEEE International Enterprise Distributed Object Computing Conference (EDOC'06)}, 
  title={Requirements Traceability and Transformation Conformance in Model-Driven Development}, 
  year={2006},
  volume={},
  number={},
  pages={355-366},
  abstract={The variety of design artefacts (models) produced in a model-driven design process results in an intricate relationship between requirements and the various models. This paper proposes a methodological framework that simplifies management of this relationship. This framework is a basis for tracing requirements, assessing the quality of model transformation specifications, metamodels, models and realizations. We propose a notion of conformance between application models which reduces the effort needed for assessment activities. We discuss how this notion of conformance can be integrated with model transformations},
  keywords={Process design;Application software;Testing;Design engineering;Telematics;Information technology;Buildings;Distributed computing;requirements traceability;assessment;con-formance;model transformation;model-driven design},
  doi={10.1109/EDOC.2006.45},
  ISSN={1541-7719},
  month={Oct},}

@INPROCEEDINGS{6759193,
  author={Clark, Tony and Kulkarni, Vinay and Barn, Balbir and France, Robert and Frank, Ulrich and Turk, Dan},
  booktitle={2014 47th Hawaii International Conference on System Sciences}, 
  title={Towards the Model Driven Organization}, 
  year={2014},
  volume={},
  number={},
  pages={4817-4826},
  abstract={Modern organizations are faced with the need to rapidly respond to frequent changes arising from external business pressures. The effect of such continuous evolution eventually leads to organizational misalignment, that is, situations in which sub-optimal configurations of underlying systems significantly reduce an organization's ability to meet its strategic goals. Ensuring alignment of an organization's systems and its goals has been a concern of researchers and practitioners in the enterprise architecture (EA) domain. Unfortunately, current approaches do not adequately address alignment problems that modern organizations face. In this paper we propose that alignment concerns can be better addressed by making models the primary entities that stakeholders within and outside of an organization use to interact with the organization. We call an organization that maintains and uses an integrated set of models to manage alignment concerns a Model Driven Organization (MDO). In this paper we characterize the alignment problem, discuss the shortcomings of current alignment management approaches and present our MDO vision.},
  keywords={Organizations;Analytical models;Context modeling;Adaptation models;Electronic mail;Context;Enterprise Architecture;Enterprise Modelling;Simulation},
  doi={10.1109/HICSS.2014.591},
  ISSN={1530-1605},
  month={Jan},}

@INPROCEEDINGS{9448913,
  author={Marksteiner, Stefan and Marko, Nadja and Smulders, Andre and Karagiannis, Stelios and Stahl, Florian and Hamazaryan, Hayk and Schlick, Rupert and Kraxberger, Stefan and Vasenev, Alexandr},
  booktitle={2021 IEEE 93rd Vehicular Technology Conference (VTC2021-Spring)}, 
  title={A Process to Facilitate Automated Automotive Cybersecurity Testing}, 
  year={2021},
  volume={},
  number={},
  pages={1-7},
  abstract={Modern vehicles become increasingly digitalized with advanced information technology-based solutions like advanced driving assistance systems and vehicle-to-x communications. These systems are complex and interconnected. Rising complexity and increasing outside exposure has created a steadily rising demand for more cyber-secure systems. Thus, also standardization bodies and regulators issued standards and regulations to prescribe more secure development processes. This security, however, also has to be validated and verified. In order to keep pace with the need for more thorough, quicker and comparable testing, today's generally manual testing processes have to be structured and optimized. Based on existing and emerging standards for cybersecurity engineering, this paper therefore outlines a structured testing process for verifying and validating automotive cybersecurity, for which there is no standardized method so far. Despite presenting a commonly structured framework, the process is flexible in order to allow implementers to utilize their own, accustomed toolsets.},
  keywords={Regulators;Conferences;Manuals;Regulation;Complexity theory;Computer security;Standards;Security;Cybersecurity;Testing;Automotive;Validation;Verification;Process},
  doi={10.1109/VTC2021-Spring51267.2021.9448913},
  ISSN={2577-2465},
  month={April},}

@INPROCEEDINGS{6949287,
  author={Angmo, Rigzin and Sharma, Monika},
  booktitle={2014 5th International Conference - Confluence The Next Generation Information Technology Summit (Confluence)}, 
  title={Performance evaluation of web based automation testing tools}, 
  year={2014},
  volume={},
  number={},
  pages={731-735},
  abstract={In today's 21st century era countless software applications are written as a web based application which runs in a web browsers. With new technologies and commercialization of I.T. sector, the web based system has undergoes frequent and rapid changes. Today Softwares are coded as a web based application, which help to access data from any part of the globe. Even the economic relevance of web based enhances the control and quality of software. The quality assurance of any system depends on its test. But to do manually testing in most of the cases is time consuming, expensive and hectic. For the better business purpose and to save time and money automation testing is required. There are variety of tools are available in the market for this. One of the best known tool is selenium suite which is a combination of different automation testing tool. In this paper we will discuss about the selenium suite. It provides testers with different framework for different test cases. The main objective of this paper is to find the best tool in selenium suite and then compare it with some other tool for same task. For this purpose, performance evaluation is done on the basis of some criteria.},
  keywords={Automation;Performance evaluation;Browsers;Software;Software testing;Information technology;Web applications;Selenium;Performance;Watir-webdriver;Test case;Automation testing},
  doi={10.1109/CONFLUENCE.2014.6949287},
  ISSN={},
  month={Sep.},}

@INPROCEEDINGS{1559629,
  author={Noll, J. and Steel, R.},
  booktitle={2005 IEEE Aerospace Conference}, 
  title={EKLOPS: An Adaptive Approach to a Mission Planning System}, 
  year={2005},
  volume={},
  number={},
  pages={1-9},
  abstract={EKLOPS is the Enhanced Kernel Library for Operational Planning and Scheduling. This paper will discuss the area of Mission Planning, and present EKLOPS as a generic mission planning solution proposed by the Mission Planning Group of Anite Systems GmbH3. EKLOPS has evolved from Mission planning systems that were developed under contracts of the European Space Agency. It implements an Adaptive Object Model architecture to integrate the common elements of mission planning systems. The model of a specific satellite mission is expressed as metadata, which configure the MPS. Rules implement functions of the planning process for which a number of specific roles can be identified. The paper will present a language that has so far been utilized to express constraint-checking rules. The experience made with EKLOPS is shown using the examples of the ENVISAT and Mars Express missions. The generic nature of EKLOPS facilitates an extension of its usage outside the field of spacecraft operations planning.—},
  keywords={Satellites;Space missions;Process planning;Programmable control;Testing;Hardware;Adaptive systems;Steel;Kernel;Libraries},
  doi={10.1109/AERO.2005.1559629},
  ISSN={1095-323X},
  month={March},}

@INPROCEEDINGS{9527970,
  author={Gylling, Andreas and Ekstedt, Mathias and Afzal, Zeeshan and Eliasson, Per},
  booktitle={2021 IEEE International Conference on Cyber Security and Resilience (CSR)}, 
  title={Mapping Cyber Threat Intelligence to Probabilistic Attack Graphs}, 
  year={2021},
  volume={},
  number={},
  pages={304-311},
  abstract={As cyber threats continue to grow and expertise resources are limited, organisations need to find ways to evaluate their resilience efficiently and take proactive measures against an attack from a specific adversary before it occurs. Threat modelling is an excellent method of assessing the resilience of ICT systems, forming Attack (Defense) Graphs (ADGs) that illustrate an adversary’s attack vectors. Cyber Threat Intelligence (CTI) is information that helps understand the current cyber threats, but has little integration with ADGs. This paper contributes with an approach that resolves this problem by using CTI feeds of known threat actors to enrich ADGs under multiple reuse. This enables security analysts to take proactive measures and strengthen their ICT systems against current methods used by any threat actor that is believed to pose a threat to them.},
  keywords={Current measurement;Conferences;Probabilistic logic;Security;Feeds;Computer crime;Resilience},
  doi={10.1109/CSR51186.2021.9527970},
  ISSN={},
  month={July},}

@INPROCEEDINGS{9274755,
  author={Kannengiesser, Udo and Krenn, Florian and Stary, Christian},
  booktitle={2020 IEEE Conference on Industrial Cyberphysical Systems (ICPS)}, 
  title={A Behaviour-Driven Development Approach for Cyber-Physical Production Systems}, 
  year={2020},
  volume={1},
  number={},
  pages={179-184},
  abstract={This paper proposes a method for iterative engineering of cyber-physical production systems (CPPS) that allows early testing of virtual prototypes and early involvement of domain experts. It is based on behaviour-driven development (BDD) from agile software engineering, which is adapted to address a set of issues relevant for CPPS engineering including the use of standardised CPPS models, integration testing, test environments, and brownfield development. The paper describes these adaptations and synthesises them into a procedural model of BDD for CPPS. Finally, a prototypical test system for CPPS is presented that partially implements the approach.},
  keywords={Testing;Modeling;Unified modeling language;Software;Production systems;Task analysis;Iterative methods;CPPS;Behaviour-Driven Development;Testing;Iterative Development},
  doi={10.1109/ICPS48405.2020.9274755},
  ISSN={},
  month={June},}

@INPROCEEDINGS{8116418,
  author={Jimenez, Ivo and Arpaci-Dusseau, Andrea and Arpaci-Dusseau, Remzi and Lofstead, Jay and Maltzahn, Carlos and Mohror, Kathryn and Ricci, Robert},
  booktitle={2017 IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS)}, 
  title={PopperCI: Automated reproducibility validation}, 
  year={2017},
  volume={},
  number={},
  pages={450-455},
  abstract={This paper introduces PopperCI, a continous integration (CI) service hosted at UC Santa Cruz that allows researchers to automate the end-to-end execution and validation of experiments. PopperCI assumes that experiments follow Popper, a convention for implementing experiments and writing articles following a DevOps approach that has been proposed recently. PopperCI runs experiments on public, private or government-fundend cloud infrastructures in a fully automated way. We describe how PopperCI executes experiments and present a use case that illustrates the usefulness of the service.},
  keywords={Tools;Runtime;Measurement;Conferences;Manuals;Writing},
  doi={10.1109/INFCOMW.2017.8116418},
  ISSN={},
  month={May},}

@ARTICLE{1021118,
  author={Chou, E. and Sheu, B.},
  journal={IEEE Circuits and Devices Magazine}, 
  title={Nanometer mixed-signal system-on-a-chip design}, 
  year={2002},
  volume={18},
  number={4},
  pages={7-17},
  abstract={A mixed-signal system-on-a-chip (SoC) design methodology and the supporting CAD tools are presented. A known tools set is identified for illustration purposes and some alternative tools can equally accomplish the task.},
  keywords={System-on-a-chip;Integrated circuit modeling;Design methodology;Integrated circuit testing;Circuit testing;Design automation;System testing;Research and development;Design engineering;Prototypes},
  doi={10.1109/MCD.2002.1021118},
  ISSN={1558-1888},
  month={July},}

@INPROCEEDINGS{6912254,
  author={Pruski, Piotr and Lohar, Sugandha and Aquanette, Rundale and Ott, Greg and Amornborvornwong, Sorawit and Rasin, Alexander and Cleland-Huang, Jane},
  booktitle={2014 IEEE 22nd International Requirements Engineering Conference (RE)}, 
  title={TiQi: Towards natural language trace queries}, 
  year={2014},
  volume={},
  number={},
  pages={123-132},
  abstract={One of the surprising observations of traceability in practice is the under-utilization of existing trace links. Organizations often create links in order to meet compliance requirements, but then fail to capitalize on the potential benefits of those links to provide support for activities such as impact analysis, test regression selection, and coverage analysis. One of the major adoption barriers is caused by the lack of accessibility to the underlying trace data and the lack of skills many project stakeholders have for formulating complex trace queries. To address these challenges we introduce TiQi, a natural language approach, which allows users to write or speak trace queries in their own words. TiQi includes a vocabulary and associated grammar learned from analyzing NL queries collected from trace practitioners. It is evaluated against trace queries gathered from trace practitioners for two different project environments.},
  keywords={Unified modeling language;Databases;Vocabulary;Natural languages;Software;Speech;Hazards;Traceability;Queries;Speech Recognition;Natural Language Processing},
  doi={10.1109/RE.2014.6912254},
  ISSN={2332-6441},
  month={Aug},}

@INPROCEEDINGS{8843284,
  author={Schulz, Henning and Angerstein, Tobias and Okanović, Dušan and van Hoorn, André},
  booktitle={2019 IEEE 27th International Symposium on Modeling, Analysis, and Simulation of Computer and Telecommunication Systems (MASCOTS)}, 
  title={Microservice-Tailored Generation of Session-Based Workload Models for Representative Load Testing}, 
  year={2019},
  volume={},
  number={},
  pages={323-335},
  abstract={Load tests are commonly used to assess the performance of an application system. A representative load test uses workload characteristics according to the user behavior in production. Session-based systems have special workload characteristics as the system is used as sequences of inter-related requests. Approaches exist to automatically extract session-based workload models from production request logs. However, they focus on system-level testing, which is in stark contrast with modern development practices, where one development team is in charge of developing, testing, and deploying a single microservice. Hence, representative session-based workload models for testing single microservices and their integration are desirable. To deal with these issues, we propose a concept for tailoring a representative load test workload to target only certain services, instead of targeting the whole system. Our goal is to transform the workload for one or more specified service(s) from the system-level workload collected in production. Using this approach, only a subset of the application's microservices is deployed for a load test, specifically the targeted services and the services they depend on. We propose two algorithms. The log-based algorithm deals with extracting the workload for a specific service from collected production traces. The model-based algorithm performs the workload tailoring on the level of the workload model. In an experiment series with a representative microservice application, we compare both algorithms with system-level and request-based workoad models. The results show that when load testing a set of services, the tailored workload models outperform untailored workload models in terms of test duration and the capacity of the test infrastructure, and outperform request-based workload models in terms of representativeness.},
  keywords={Load modeling;Testing;Unified modeling language;Production;Markov processes;Data mining;Transforms;load testing;microservices;workload model generation},
  doi={10.1109/MASCOTS.2019.00043},
  ISSN={2375-0227},
  month={Oct},}

@INPROCEEDINGS{6934560,
  author={Agnelli, S. and Feltz, P. and Griffiths, P-F. and Roth, D.},
  booktitle={2014 7th Advanced Satellite Multimedia Systems Conference and the 13th Signal Processing for Space Communications Workshop (ASMS/SPSC)}, 
  title={Satellite's role in the penetration of broadband connectivity within the European Union}, 
  year={2014},
  volume={},
  number={},
  pages={306-311},
  abstract={The European Commission (EC) has recently acknowledged that broadband coverage for all - the 2013 target of the Digital Agenda for Europe (DAE) - has been achieved thanks to satellite broadband services, such as Tooway™, the consumer-grade Internet access at 20 Mbps via the Eutelsat KA-SAT satellite. However, broadband take-up in the European Union (EU) is still far from being satisfactory, notably in rural and remote areas where satellite solutions are ideally suited.},
  keywords={Broadband communication;Satellites;Europe;Internet;Investment;Multimedia systems;Signal processing;Broadband Coverage;Broadband Take-Up;Digital Divide;Digital Agenda for Europe;European Union;KA-SAT;SABER Project;Voucher Schemes},
  doi={10.1109/ASMS-SPSC.2014.6934560},
  ISSN={2326-5949},
  month={Sep.},}

@INPROCEEDINGS{7733581,
  author={Awad, Ramez and Heppner, Georg and Roennau, Arne and Bordignon, Mirko},
  booktitle={2016 IEEE 21st International Conference on Emerging Technologies and Factory Automation (ETFA)}, 
  title={ROS engineering workbench based on semantically enriched app models for improved reusability}, 
  year={2016},
  volume={},
  number={},
  pages={1-9},
  abstract={In this work, the ReApp Engineering Workbench and its underlying semantically enriched app models are presented. The usage of a model, which describes the apps functionality, interfaces and other attributes, allows the utilization of engineering tools for code generation and automated testing. Further, it ensures the compatibility of the generated interfaces, which in turn enhances the reusability of the developed apps in larger applications.},
  keywords={Biological system modeling;Unified modeling language;Hardware;Model driven engineering;Robot kinematics},
  doi={10.1109/ETFA.2016.7733581},
  ISSN={},
  month={Sep.},}

@INPROCEEDINGS{577990,
  author={Kransner, S.M. and Bernard, D.E.},
  booktitle={1997 IEEE Aerospace Conference}, 
  title={Integrating autonomy technologies into an embedded spacecraft system-flight software system engineering for new millennium}, 
  year={1997},
  volume={2},
  number={},
  pages={409-420 vol.2},
  abstract={Deep Space 1 (DS1) is the first deep-space mission of NASA's New Millennium technology validation program. The DS1 flight software will validate five autonomy technologies: 1) Planner/Scheduler, which receives ground or on-board requests for spacecraft activities and schedules them to resolve any resource conflicts or timing constraints; 2) Smart Executive, which expands planned activities into lower-level commands, deduces required hardware configurations or other actions, and provides detection and avoidance of constraint violations; 3) Mode Identification and Reconfiguration engine, which incorporates models of hardware and software behavior, detects discrepancies due to hardware or software failures, and requests recovery actions via the Smart Executive. 4) Autonomous Navigation, which determines the spacecraft trajectory from images of asteroids against the celestial sphere, and autonomously adjusts the trajectory to reach the target asteroid or comet. 5) Beacon Monitoring, which uses radio carrier modification and telemetry summarization to simplify ground monitoring of spacecraft health. Integration of these technologies into the spacecraft flight software architecture has presented a number of system engineering challenges, Some of these technologies were developed in a research-oriented, non-real-time, artificial intelligence organizational culture while spacecraft software is typically developed in a strong real-time, algorithmically-oriented culture. The Navigation technology has been developed in a ground-based environment. Integration of these different cultures and mutual education of the software team has been achieved. An early rapid prototype of an existing spacecraft design proved very valuable in educating the team members and in working out the development process.},
  keywords={Space technology;Space vehicles;Hardware;Scheduling;Trajectory;Space missions;Image resolution;Timing;Engines;Radio navigation},
  doi={10.1109/AERO.1997.577990},
  ISSN={},
  month={Feb},}

@ARTICLE{8839463,
  author={Hofer, Florian and Russo, Barbara},
  journal={IEEE Transactions on Industrial Informatics}, 
  title={IEC 61131-3 Software Testing: A Portable Solution for Native Applications}, 
  year={2020},
  volume={16},
  number={6},
  pages={3942-3951},
  abstract={Programmable logic controllers (PLCs) are the most used digital systems in manufacturing industry, but there is little support for test automation of such systems. As net result, testing is mostly done manually or not at all despite the recommendations of the IEC 61131-3 Standard. Attempts to provide an automated testing framework for PLCs have been recently performed with first successful results. The most advanced and promising framework proposes an approach close to object orientation that relies on nonnative language and platform. In this article, we propose a testing library called Advanced Program Organization Unit Testing Framework written in native language, built according to the unit testing paradigm, and supporting automated testing for simple and complex scenarios of IEC 61131-3-compliant PLCs. In this article, we present such library, discuss its performance and advantages, and illustrate its application to a real case study.},
  keywords={Testing;IEC Standards;Libraries;Tools;Software;Hardware;Automation;CoDeSys;control software;IEC 61131-3;testing;unit test},
  doi={10.1109/TII.2019.2941584},
  ISSN={1941-0050},
  month={June},}

@INPROCEEDINGS{6511809,
  author={Wanderley, Fernando and da Silveria, Denis Silva},
  booktitle={2012 Eighth International Conference on the Quality of Information and Communications Technology}, 
  title={A Framework to Diminish the Gap between the Business Specialist and the Software Designer}, 
  year={2012},
  volume={},
  number={},
  pages={199-204},
  abstract={Requirements Engineering establishes the process for defining requirements as one in which elicitation, modeling and analysis are tasks which must be carried out. This process should involve different stakeholders and their different viewpoints. Among these stakeholders, there is the software designer, responsible for creating models based on the information gathered by business specialists. However, this communication channel may create some "noise" that leads to information being lost. This loss produces a semantic gap between what is desired and what will be developed. The semantic gap is characterized by inconsistencies in the requirements represented by scenarios -- user stories in a behavior-driven context -- and by the conceptual model. This paper presents an interactive approach to the agile requirements modeling, thus fostering greater consistency between the artifacts of the scenarios and the conceptual model. This consistency is ensured by using a mind model specification which will serve as a basis for transforming the definitions of the scenario and generating a conceptual model represented by a UML class diagram. The mind model represents the main role of this approach, and functions as a bond that represents the business entities, thus enabling the requirements to be more consistent with the reality of the business.},
  keywords={Agile Modeling Requirements;Behaviour Driven Development;UML;Mind Map Modeling;Domain Model},
  doi={10.1109/QUATIC.2012.9},
  ISSN={},
  month={Sep.},}

@INPROCEEDINGS{7962325,
  author={Liechti, Olivier and Pasquier, Jacques and Reis, Rodney},
  booktitle={2017 IEEE/ACM 12th International Workshop on Automation of Software Testing (AST)}, 
  title={Supporting Agile Teams with a Test Analytics Platform: A Case Study}, 
  year={2017},
  volume={},
  number={},
  pages={9-15},
  abstract={Continuous improvement, feedback mechanisms and automated testing are cornerstones of agile methods. We introduce the concept of test analytics, which brings these three practices together. We illustrate the concept with an industrial case study and describe the experiments run by a team who had set a goal for itself to get better at testing. Beyond technical aspects, we explain how these experiments have changed the mindset and the behaviour of the team members. We then present an open source test analytics platform, later developed to share the positive learnings with the community. We describe the platform features and architecture and explain how it can be easily put to use. Before the conclusions, we explain how test analytics fits in the broader context of software analytics and present our ideas for future work.},
  keywords={Testing;Software;Companies;Context;Collaboration;agile development;automated testing;gamification;feedback channels},
  doi={10.1109/AST.2017.3},
  ISSN={},
  month={May},}

@INPROCEEDINGS{9488772,
  author={Shukla, Apoorv and Hudemann, Kevin and Vági, Zsolt and Hügerich, Lily and Smaragdakis, Georgios and Hecker, Artur and Schmid, Stefan and Feldmann, Anja},
  booktitle={IEEE INFOCOM 2021 - IEEE Conference on Computer Communications}, 
  title={Fix with P6: Verifying Programmable Switches at Runtime}, 
  year={2021},
  volume={},
  number={},
  pages={1-10},
  abstract={We design, develop, and evaluate P6, an automated approach to (a) detect, (b) localize, and (c) patch software bugs in P4 programs. Bugs are reported via a violation of pre-specified expected behavior that is captured by P6. P6 is based on machine learning-guided fuzzing that tests P4 switch non-intrusively, i.e., without modifying the P4 program for detecting runtime bugs. This enables an automated and real-time localization and patching of bugs. We used a P6 prototype to detect and patch existing bugs in various publicly available P4 application programs deployed on two different switch platforms: behavioral model (bmv2) and Tofino. Our evaluation shows that P6 significantly outperforms bug detection baselines while generating fewer packets and patches bugs in large P4 programs such as switch.p4 without triggering any regressions.},
  keywords={Location awareness;Runtime;Automation;Conferences;Computer bugs;Prototypes;Switches},
  doi={10.1109/INFOCOM42981.2021.9488772},
  ISSN={2641-9874},
  month={May},}

@INPROCEEDINGS{10646658,
  author={Ammann, Max and Hirschi, Lucca and Kremer, Steve},
  booktitle={2024 IEEE Symposium on Security and Privacy (SP)}, 
  title={DY Fuzzing: Formal Dolev-Yao Models Meet Cryptographic Protocol Fuzz Testing}, 
  year={2024},
  volume={},
  number={},
  pages={1481-1499},
  abstract={Critical and widely used cryptographic protocols have repeatedly been found to contain flaws in their design and their implementation. A prominent class of such vulnerabilities is logical attacks, e.g. attacks that exploit flawed protocol logic. Automated formal verification methods, based on the Dolev-Yao (DY) attacker, formally define and excel at finding such flaws, but operate only on abstract specification models. Fully automated verification of existing protocol implementations is today still out of reach. This leaves open whether such implementations are secure. Unfortunately, this blind spot hides numerous attacks, such as recent logical attacks on widely used TLS implementations introduced by implementation bugs.We answer by proposing a novel and effective technique that we call DY model-guided fuzzing, which precludes logical attacks against protocol implementations. The main idea is to consider as possible test cases the set of abstract DY executions of the DY attacker, and use a novel mutation-based fuzzer to explore this set. The DY fuzzer concretizes each abstract execution to test it on the program under test. This approach enables reasoning at a more structural and security-related level of messages represented as formal terms (e.g. decrypt a message and re-encrypt it with a different key) as opposed to random bit-level modifications that are much less likely to produce relevant logical adversarial behaviors. We implement a full-fledged and modular DY protocol fuzzer. We demonstrate its effectiveness by fuzzing three popular TLS implementations, resulting in the discovery of four novel vulnerabilities.},
  keywords={Privacy;Fuzzing;Cognition;Security;Logic;Cryptographic protocols;Formal verification;Formal methods and verification;Program and binary analysis;Protocol security;Systems security;Fuzzing;Test},
  doi={10.1109/SP54263.2024.00096},
  ISSN={2375-1207},
  month={May},}

@ARTICLE{10013942,
  author={Trubiani, Catia and Pinciroli, Riccardo and Biaggi, Andrea and Fontana, Francesca Arcelli},
  journal={IEEE Transactions on Software Engineering}, 
  title={Automated Detection of Software Performance Antipatterns in Java-Based Applications}, 
  year={2023},
  volume={49},
  number={4},
  pages={2873-2891},
  abstract={The detection of performance issues in Java-based applications is not trivial since many factors concur to poor performance, and software engineers are not sufficiently supported for this task. The goal of this manuscript is the automated detection of performance problems in running systems to guarantee that no quality-based hinders prevent their successful usage. Starting from software performance antipatterns, i.e., bad practices (e.g., extensive interaction between software methods) expressing both the problem and the solution with the purpose of identifying shortcomings and promptly fixing them, we develop a framework that automatically detects seven software antipatterns capturing a variety of performance issues in Java-based applications. Our approach is applied to real-world case studies from different domains, and it captures four real-life performance issues of Hadoop and Cassandra that were not predicted by state-of-the-art approaches. As empirical evidence, we calculate the accuracy of the proposed detection rules, we show that code commits inducing and fixing real-life performance issues present interesting variations in the number of detected antipattern instances, and solving one of the detected antipatterns improves the system performance up to 50%.},
  keywords={Codes;Software performance;Software;Java;Runtime;Monitoring;System performance;Dynamic analysis;Java-based applications;software performance antipatterns},
  doi={10.1109/TSE.2023.3234321},
  ISSN={1939-3520},
  month={April},}

@INPROCEEDINGS{5954390,
  author={Efkemann, Christof and Peleska, Jan},
  booktitle={2011 IEEE Fourth International Conference on Software Testing, Verification and Validation Workshops}, 
  title={Model-Based Testing for the Second Generation of Integrated Modular Avionics}, 
  year={2011},
  volume={},
  number={},
  pages={55-62},
  abstract={In this paper the authors present the current research and development activities regarding automated testing of Integrated Modular Avionics controllers in the European research project SCARLETT. The authors describe the goals of the SCARLETT project and explain its background of Integrated Modular Avionics. Furthermore, they explain different levels of testing of components required for certification. A domain-specific modelling language designed for the IMA platform is presented. This language is used to create models from which tests of different levels can be generated automatically. The authors expect significant improvements in terms of effort to create and maintain test procedures compared to conventional test creation.},
  keywords={Aerospace electronics;Testing;Generators;Aircraft;Europe;Random access memory;Concrete;IMA;SCARLETT;TTCN-3;avionics;domain-specific modelling;model-based testing},
  doi={10.1109/ICSTW.2011.72},
  ISSN={},
  month={March},}

@INPROCEEDINGS{5254276,
  author={Mei, Lijun and Chan, W. K. and Tse, T. H. and Kuo, Fei-Ching},
  booktitle={2009 33rd Annual IEEE International Computer Software and Applications Conference}, 
  title={An Empirical Study of the Use of Frankl-Weyuker Data Flow Testing Criteria to Test BPEL Web Services}, 
  year={2009},
  volume={1},
  number={},
  pages={81-88},
  abstract={Programs using service-oriented architecture (SOA) often feature ultra-late binding among components. These components have well-defined interfaces and are known as Web services. Messages between every pair of Web services dually conform to the output interface of a sender and the input interface of a receiver. Unit testing of Web services should not only test the logic of Web services, but also assure the correctness of the Web services during input, manipulation, and output of messages. There is, however, little software testing research in this area. In this paper, we study the unit testing problem to assure components written in orchestration languages, WS-BPEL in particular. We report an empirical study of the effectiveness of the Frankl-Weyuker data flow testing criteria (particularly the all-uses criterion) on WS-BPEL subject programs. Our study shows that conventional data flow testing criteria can be much less effective in revealing faults in interface artifacts (WSDL documents) and message manipulations (XPath queries) than revealing faults in BPEL artifacts.},
  keywords={Web services;Logic testing;XML;Software testing;Application software;Service oriented architecture;Councils;Information retrieval;Computer applications;Data flow computing;WS-BPEL;XPath;data flow testing},
  doi={10.1109/COMPSAC.2009.21},
  ISSN={0730-3157},
  month={July},}

@INPROCEEDINGS{6120072,
  author={Jiao Yu and Wilamowski, Bogdan M.},
  booktitle={IECON 2011 - 37th Annual Conference of the IEEE Industrial Electronics Society}, 
  title={Recent advances in in-vehicle embedded systems}, 
  year={2011},
  volume={},
  number={},
  pages={4623-4625},
  abstract={The number of computer based functions embedded in vehicles has increased significantly in the past two decades. An in-vehicle embedded electronic architecture is a complex distributed system; the development of which is a cooperative work involving different manufacturers and suppliers. There are several key demands in the development process, such as safety requirements, real-time assessment, schedulability, composability, etc. Intensive research is being conducted to address these issues. This paper reviews recent technology advances in relevant aspects and covers a range of topics highlighted above.},
  keywords={Real time systems;Embedded systems;Automotive engineering;Field programmable gate arrays;Multicore processing;In-vehicle embedded electronic architecture;FPGA;real-time assessment;composability},
  doi={10.1109/IECON.2011.6120072},
  ISSN={1553-572X},
  month={Nov},}

@INPROCEEDINGS{6550482,
  author={Esnaashari, Shadi and Welch, Ian and Komisarczuk, Peter},
  booktitle={2013 27th International Conference on Advanced Information Networking and Applications Workshops}, 
  title={Determining Home Users' Vulnerability to Universal Plug and Play (UPnP) Attacks}, 
  year={2013},
  volume={},
  number={},
  pages={725-729},
  abstract={Universal Plug and Play (UPnP) technology is used worldwide since it has simplified the installation and management of the devices. As a result, many devices are now equipped with UPnP capabilities. Unfortunately using UPnP in home routers puts routers at risk of abuse. For example, it is easier for hackers to discover the devices and use device vulnerabilities in order to make malicious attacks to cause financial or reputation damage to the users. In this paper, we have analyzed the UPnP protocol and its different vulnerabilities. Furthermore, we have emphasized how common the problem is with the home users' devices. Hence, we suggest a tool to achieve transparency in the health of the Internet by detecting UPnP enabled devices which are likely to be attacked on home networks. The tool will look for UPnP based attacks when people's routers have been compromised. The tool is easy to install and use for novice home users and maintains their privacy too. This project aims not only to implement a tool for a user to determine whether his/her system is vulnerable to a particular attack, but also to measure the prevalence of vulnerabilities at national or global level. Thus a larger framework is required to collect and manage the results from individual users.},
  keywords={Servers;Protocols;Ports (Computers);Logic gates;Plugs;IP networks;Internet;Security;Network Measurement;UPnP},
  doi={10.1109/WAINA.2013.225},
  ISSN={},
  month={March},}

@INPROCEEDINGS{7516835,
  author={Tomlein, Matú and Grønbæk, Kaj},
  booktitle={2016 13th Working IEEE/IFIP Conference on Software Architecture (WICSA)}, 
  title={Semantic Model of Variability and Capabilities of IoT Applications for Embedded Software Ecosystems}, 
  year={2016},
  volume={},
  number={},
  pages={247-252},
  abstract={Applications in embedded open software ecosystems for Internet of Things devices open new challenges regarding how their variability and capabilities should be modeled. In collaboration with an industrial partner, we have recognized that such applications have complex constraints on the context. We have also identified a need to model their deployment topology and functionality in order to enable their seamless integration into the platform. In this paper, we draw from research in related fields and present a model of IoT applications. It is built using semantic annotations and uses semantic reasoning to resolve context requirements. We present the implications on the architecture of the ecosystem and the concepts defined in the model. Finally, we discuss the evaluation of the model and its benefits and liabilities. Although the approach results in more complex descriptions of applications, we conclude that it is suitable for modeling applications in IoT software ecosystems since it is more adaptable and expressive than the alternatives.},
  keywords={Ecosystems;Context;Computational modeling;Context modeling;Semantics;Cognition;Unified modeling language;ecosystem;iot;semantic;model;architecture},
  doi={10.1109/WICSA.2016.17},
  ISSN={},
  month={April},}

@INPROCEEDINGS{217601,
  author={Lyu, M.R. and Chen, J.-H. and Avizienis, A.},
  booktitle={[1992] Proceedings. The Sixteenth Annual International Computer Software and Applications Conference}, 
  title={Software diversity metrics and measurements}, 
  year={1992},
  volume={},
  number={},
  pages={69-78},
  abstract={The authors define and formalize the concept of software diversity which characterizes N-Version software (NVS) from four different points of view that are designated as structural diversity, fault diversity, tough-spot diversity, and failure diversity. The goals are to find a way to quantify software diversity and to investigate the measurements which can be applied during the life cycle of NVS to gain confidence that operation will be dependable when NVS is actually used. The versions from a six-language N-Version programming project for fault-tolerant flight control software were used in the software diversity measurement.<>},
  keywords={Software measurement;Fault tolerant systems;Aerospace control;Fault tolerance;Gain measurement;Functional programming;Error correction;Delay;Software algorithms;Quality control},
  doi={10.1109/CMPSAC.1992.217601},
  ISSN={},
  month={Sep.},}

@INPROCEEDINGS{8590169,
  author={García, Boni and Lonetti, Francesca and Gallego, Micael and Miranda, Breno and Jiménez, Eduardo and De Angelis, Guglielmo and Santos, Carlos and Marchetti, Eda},
  booktitle={2018 11th International Conference on the Quality of Information and Communications Technology (QUATIC)}, 
  title={A Proposal to Orchestrate Test Cases}, 
  year={2018},
  volume={},
  number={},
  pages={38-46},
  abstract={This paper presents the concept of test orchestration, understood as a novel way to select, order, and execute in parallel a group of tests. Our view of test orchestration can be seen as a process in which different test cases are organized, assembled and executed following a topology that determines how their executions coordinate. We distinguish two types of orchestrations techniques: i) verdict-driven, which organizes tests using their outcome (i.e., passed or failed) to drive the workflow; and ii) data-driven, in which test data (input) and test outcomes (output) are handled within the graph. Both approaches are being implemented in the project ElasTest, an open source platform aimed to simplify the end-to-end test process of large software systems.},
  keywords={Testing;Topology;Pipelines;Tools;Signal to noise ratio;Software systems;Engines;Software testing;test composition;test parallelization},
  doi={10.1109/QUATIC.2018.00016},
  ISSN={},
  month={Sep.},}

@ARTICLE{7492282,
  author={Datta, Subhajit and Sarkar, Santonu and Sajeev, A. S. M.},
  journal={IEEE Transactions on Big Data}, 
  title={How Long Will This Live? Discovering the Lifespans of Software Engineering Ideas}, 
  year={2016},
  volume={2},
  number={2},
  pages={124-137},
  abstract={We all want to be associated with long lasting ideas; as originators, or at least, expositors. For a tyro researcher or a seasoned veteran, knowing how long an idea will remain interesting in the community is critical in choosing and pursuing research threads. In the physical sciences, the notion of half-life is often evoked to quantify decaying intensity. In this paper, we study a corpus of 19,000+ papers written by 21,000+ authors across 16 software engineering publication venues from 1975 to 2010, to empirically determine the half-life of software engineering research topics. In the absence of any consistent and well-accepted methodology for associating research topics to a publication, we have used natural language processing techniques to semi-automatically identify and associate a set of topics with a paper. We adapted measures of half-life already existing in the bibliometric context for our study, and also defined a new measure based on publication and citation counts. We find evidence that some of the identified research topics show a mean half-life of close to 15 years, and there are topics with sustaining interest in the community. We report the methodology of our study in this paper, as well as the implications and utility of our results.},
  keywords={Software engineering;Big data;Measurement;Collaboration;Context;Special issues and sections;Software;Big data;software engineering;research;half-life},
  doi={10.1109/TBDATA.2016.2580541},
  ISSN={2332-7790},
  month={June},}

@INPROCEEDINGS{7313460,
  author={Berger, Christian and Block, Delf and Hons, Christian and Kühnel, Stefan and Leschke, André and Plotnikov, Dimitri and Rumpe, Bernhard},
  booktitle={2015 IEEE 18th International Conference on Intelligent Transportation Systems}, 
  title={Large-Scale Evaluation of an Active Safety Algorithm with EuroNCAP and US NCAP Scenarios in a Virtual Test Environment -- An Industrial Case Study}, 
  year={2015},
  volume={},
  number={},
  pages={2280-2286},
  abstract={Context: Recently, test protocols from organizations like European New Car Assessment Programme (EuroNCAP) were extended to also cover active safety systems. Objective: The official EuroNCAP test protocol for Autonomous Emergency Braking (AEB)/Forward Collision Warning (FCW) systems explicitly defines to what extent a Vehicle-Under-Test (VUT) is allowed to vary in its lateral position. In addition, the United States New Car Assessment Programme (US NCAP) test protocol has broader tolerance ranges. The goal for automotive OEMs is to understand the impact of such allowed variations on a the overall vehicle's performance. Method: A simulation-based approach is outlined that allows systematic, large-scale analysis of such influences to effectively plan time-consuming and resource-intense real-world vehicle tests. Our models allow a profound analysis of an AEB algorithm by modeling and conducting more than 3,000 simulation runs with EuroNCAP's dynamic CCRm and CCRb scenarios including those with adopted USNCAP parameters. Results: Our structured analysis of such test procedures involving dynamic actors is the first of its kind in a relevant industrial setting. Several anomalies were unveiled under US NCAP conditions to support real-world test runs. Hence, we could show that the proposed method supports all possible scenarios in AEB consumer tests and scales as we had to timely process approx. 7.7GB of simulation data. Conclusion: To achieve the expected performance and to study a system's behavior in potential misuse cases from a functional point of view, large scale, model-based simulations complement traditional testing on proving ground.},
  keywords={Vehicles;Safety;Data models;Protocols;Vehicle dynamics;Trajectory;Systematics},
  doi={10.1109/ITSC.2015.368},
  ISSN={2153-0017},
  month={Sep.},}

@INPROCEEDINGS{4148978,
  author={Liu, Jing and Dehlinger, Josh and Sun, Hongyu and Lutz, Robyn},
  booktitle={14th Annual IEEE International Conference and Workshops on the Engineering of Computer-Based Systems (ECBS'07)}, 
  title={State-Based Modeling to Support the Evolution and Maintenance of Safety-Critical Software Product Lines}, 
  year={2007},
  volume={},
  number={},
  pages={596-608},
  abstract={Changes to safety-critical product lines can jeopardize the safety properties that they must ensure. Thus, evolving software product lines must consider the impact that changes to requirements may have on the existing systems and their safety. The contribution of this work is a systematic, tool-supported technique to support safe evolution of product-line requirements using a model-based approach. We show how the potential feature interactions that need to be modeled are scoped and identified with the aid of product-line software fault tree analysis. Further, we show how reuse of the state-based models is effectively exploited in the evolution phase of product-line engineering. To illustrate this approach, we apply our technique to the evolution of a safety-critical cardiac pacemaker product line},
  keywords={Software maintenance;Software safety;Product safety;Fault trees;Pacemakers;Software tools;Systems engineering and theory;Costs;Reliability engineering;Computer science},
  doi={10.1109/ECBS.2007.66},
  ISSN={},
  month={March},}

@INPROCEEDINGS{7107547,
  author={Biňas, M.},
  booktitle={2014 IEEE 12th IEEE International Conference on Emerging eLearning Technologies and Applications (ICETA)}, 
  title={Identifying web services for automatic assessments of programming assignments}, 
  year={2014},
  volume={},
  number={},
  pages={45-50},
  abstract={The main objective of this article is to verify the assumption, if the web services can be used in the process of automatic assessment of programming assignments. It tries to identify general services for such processes and presents the experimental part by creating platform based on set of web services.},
  keywords={Web services;Programming;Testing;Electronic mail;Plagiarism;Uniform resource locators;Electronic learning},
  doi={10.1109/ICETA.2014.7107547},
  ISSN={},
  month={Dec},}

@ARTICLE{5567088,
  author={Afek, Yehuda and Drepper, Ulrich and Felber, Pascal and Fetzer, Christof and Gramoli, Vincent and Hohmuth, Michael and Rivière, Etienne and Stenström, Per and Unsal, Osman and Moreira, Walther Maldonado and Harmanci, Derin and Marlier, Patrick and Diestelhorst, Stephan and Pohlack, Martin and Cristal, Adrian and Hur, Ibrahim and Dragojevic, Aleksandar and Guerraoui, Rachid and Kapalka, Michal and Tomić, Sasa and Korland, Guy and Shavit, Nir and Nowack, Martin and Riegel, Torvald},
  journal={IEEE Micro}, 
  title={The Velox Transactional Memory Stack}, 
  year={2010},
  volume={30},
  number={5},
  pages={76-87},
  abstract={The adoption of multi- and many-core architectures for mainstream computing undoubtedly brings profound changes in the way software is developed. In particular, the use of fine grained locking as the multi-core programmer's coordination methodology is considered by more and more experts as a dead-end. The transactional memory (TM) programming paradigm is a strong contender to become the approach of choice for replacing locks and implementing atomic operations in concurrent programming. Combining sequences of concurrent operations into atomic transactions allows a great reduction in the complexity of both programming and verification, by making parts of the code appear to execute sequentially without the need to program using fine-grained locking. Transactions remove from the programmer the burden of figuring out the interaction among concurrent operations that happen to conflict when accessing the same locations in memory. The EU-funded FP7 VELOX project designs, implements and evaluates an integrated TM stack, spanning from programming language to the hardware support, and including runtime and libraries, compilers, and application environments. This paper presents an overview of the VELOX TM stack and its associated challenges and contributions.},
  keywords={Libraries;Runtime;Hardware;Java;Programming;Program processors;concurrent programming;software transactional memory;hardware transactional memory;compilers;language extensions},
  doi={10.1109/MM.2010.80},
  ISSN={1937-4143},
  month={Sep.},}

@INPROCEEDINGS{9101221,
  author={Jasser, Stefanie},
  booktitle={2020 IEEE International Conference on Software Architecture (ICSA)}, 
  title={Enforcing Architectural Security Decisions}, 
  year={2020},
  volume={},
  number={},
  pages={35-45},
  abstract={Software architects should specify security measures for a software system on an architectural level. However, the implementation often diverges from this intended architecture including its security measures. This may lead to severe vulnerabilities that have a wide impact on the system and are hard to fix afterwards. In this paper, we propose an approach for checking the implementation’s conformance with the defined security measures using architectural security rules: We extend a controlled natural language approach to formalize these rules and use dynamic analysis techniques to extract information on the actual system behavior for the conformance check. We evaluate our approach by an industrial case study to show the applicability and flexibility of our conformance checking approach.},
  keywords={Computer architecture;Authorization;Software measurement;Software architecture;Software systems;Software-Architecture;Security-Architecture;Architecture-Rules;Security-Rules;Security-by-Design;Architecture-Enforcement;Architecture-Conformance;CNL;Ontology},
  doi={10.1109/ICSA47634.2020.00012},
  ISSN={},
  month={March},}

@ARTICLE{10179899,
  author={Gjorgjevikj, Ana and Mishev, Kostadin and Antovski, Ljupcho and Trajanov, Dimitar},
  journal={IEEE Access}, 
  title={Requirements Engineering in Machine Learning Projects}, 
  year={2023},
  volume={11},
  number={},
  pages={72186-72208},
  abstract={Over the last decade, machine learning methods have revolutionized a large number of domains and provided solutions to many problems that people could hardly solve in the past. The availability of large amounts of data, powerful processing architectures, and easy-to-use software frameworks have made machine learning a popular, readily available, and affordable option in many different domains and contexts. However, the development and maintenance of production-level machine learning systems have proven to be quite challenging, as these activities require an engineering approach and solid best practices. Software engineering offers a mature development process and best practices for conventional software systems, but some of them are not directly applicable to the new programming paradigm imposed by machine learning. The same applies to the requirements engineering best practices. Therefore, this article provides an overview of the requirements engineering challenges in the development of machine learning systems that have been reported in the research literature, along with their proposed solutions. Furthermore, it presents our approach to overcoming those challenges in the form of a case study. Through this mixed-method study, the article tries to identify the necessary adjustments to (1) the best practices for conventional requirements engineering and (2) the conventional understanding of certain types of requirements to better fit the specifics of machine learning. Moreover, the article tries to emphasize the relevance of properly conducted requirements engineering activities in addressing the complexity of machine learning systems, as well as to motivate further discussion on the requirements engineering best practices in developing such systems.},
  keywords={Data models;Machine learning;Artificial intelligence;Requirements engineering;Software engineering;Best practices;Requirements engineering;Machine learning;requirements engineering;software engineering;software requirements},
  doi={10.1109/ACCESS.2023.3294840},
  ISSN={2169-3536},
  month={},}

@INPROCEEDINGS{8247727,
  author={Pfrang, Steffen and Meier, David and Kautz, Valentin},
  booktitle={2017 22nd IEEE International Conference on Emerging Technologies and Factory Automation (ETFA)}, 
  title={Towards a modular security testing framework for industrial automation and control systems: ISuTest}, 
  year={2017},
  volume={},
  number={},
  pages={1-5},
  abstract={Industrial automation and control systems (IACS) play a key role in modern production facilities. On the one hand, they provide real-time functionality to the connected field devices. On the other hand, they get more and more connected to local networks and the internet in order to facilitate use cases promoted by “Industry 4.0”. This makes IACS susceptible to cyber-attacks which exploit vulnerabilities, for example in order to interrupt the automation process. Security testing targets at discovering those vulnerabilities before they are exploited. In order to enable IACS manufacturers and integrators to perform security testing for their devices, we present ISuTest, a modular security testing framework for IACS. ISuTest is designed to be extendable regarding all kinds of automation protocols, different connection paths as well as evaluating arbitrary outputs of the tested devices. This paper describes the fundamental ideas behind ISuTest, its design and a basic evaluation in which the ISuTest framework was able to discover a vulnerability in a programmable logic controller (PLC). The paper concludes with a broad overview of the planned future work.},
  keywords={Testing;Security;Automation;Protocols;Control systems;Software;Hardware},
  doi={10.1109/ETFA.2017.8247727},
  ISSN={1946-0759},
  month={Sep.},}

@INPROCEEDINGS{9081629,
  author={Darwesh, Darbaz Nawzad and Annighöfer, Björn and Reichel, Reinhard},
  booktitle={2019 IEEE/AIAA 38th Digital Avionics Systems Conference (DASC)}, 
  title={Semi-automated deployment of a High-lift system on IMA using the Selective Middleware}, 
  year={2019},
  volume={},
  number={},
  pages={1-10},
  abstract={This paper presents a case study concerning the implementation of a High-Lift System on an Integrated Modular Avionics (IMA) platform using a system management middleware and a knowledge-based automatic instantiation process. Safety-critical avionics systems usually consist several system management functions besides the specific control functions, which contribute to high development costs. System management functions such as redundancy, signal path, and fault management could be generally constructed on generic logics. The Flexible Avionics Platform approach developed at the Institute of Aircraft Systems at the University of Stuttgart describes these logics in generic software bricks to instantiate the system management. A model and knowledge-based tool suite assists to derive the system management instantiation from an abstract high-level system architecture model. Ongoing research has introduced the Flexible Avionics Platform into the IMA environment as a selectively useable middleware called “Selective Middleware” in a partition space according to the ARINC653P4 interface specification. Consequently, a development process for the Selective Middleware has to be defined and validated by the development of an appropriate system to show the applicability in an industrial development process. Hence, this paper presents a development process for a selective Middleware, which focuses on system development aspects and gives an outlook on relevant qualification procedures. Aspects of DO-297 and ARP4754A are considered to derive the development process. Subsequently, a High-Lift System function is established on grounds of the Selective Middleware approach. This High-Lift System function is divided into two development processes of (1) the generation of the system management and IMA configuration files and (2) the development of the specific control functions.},
  keywords={IMA;DO-297;model-based;knowledge-based;process automation;system design;system management;safety-critical system;distributed system;High-Lift System},
  doi={10.1109/DASC43569.2019.9081629},
  ISSN={2155-7209},
  month={Sep.},}

@INPROCEEDINGS{738516,
  author={Bennett, K.H.},
  booktitle={Proceedings. International Conference on Software Maintenance (Cat. No. 98CB36272)}, 
  title={Do program transformations help reverse engineering?}, 
  year={1998},
  volume={},
  number={},
  pages={247-254},
  abstract={Program transformations have been advocated as a method for accomplishing reverse engineering. The hypothesis is that the original source code can be progressively transformed into alternative forms, but with the same semantics. At the end of the process, an equivalent program is acquired, but one which is much easier to understand and more maintainable. We have been undertaking an extensive programme of research over twelve years into the design and development of transformations for the support of software maintenance. The paper very briefly explains the theory, practice and tool support for transformational systems, but does not present new theoretical results. The main results are on an analysis of the strengths and weaknesses of the approach, based on experience with case studies and industrial applications. The evaluation framework used (called DERE) is that presented in Bennett and Munro (1998). It is hoped that the results will be of benefit to industry, who might be considering using the technology; and to other researchers, interested in addressing the open problems. The overall conclusion is that transformations can help in the bottom-up analysis and manipulation of source code at approximately the 3GL level, and have proved successful in code migration, but need to be complemented by other top-down techniques to be useful at higher levels of abstraction or in more ambitious re-engineering projects.},
  keywords={Reverse engineering;Libraries;Computer science;Electronic mail;Electrical capacitance tomography;Computer languages;Read only memory},
  doi={10.1109/ICSM.1998.738516},
  ISSN={1063-6773},
  month={Nov},}

@INPROCEEDINGS{7923802,
  author={Ramos De Oliveira, Ricardo and Martins, Rafael Messias and Da Silva Simao, Adenilso},
  booktitle={2017 IEEE International Conference on Cloud Engineering (IC2E)}, 
  title={Impact of the Vendor Lock-in Problem on Testing as a Service (TaaS)}, 
  year={2017},
  volume={},
  number={},
  pages={190-196},
  abstract={Testing as a Service (TaaS) is a new business and service model that provides efficient and effective software quality assurance and enables the use of a cloud for the meeting of quality standards, requirements and consumer's needs. However, problems that limit the effective use of TaaS involve lack of standardization in writing, execution, configuration and management of tests and lack of portability and interoperability among TaaS platforms - the so-called lock-in problem. The lock-in problem is a serious threat to software testing in the cloud and may become critical when a provider decides to suddenly increase prices, or shows serious technical availability problems. This paper proposes a novel approach for solving the lock-in problem in TaaS with the use of design patterns. The aim to assist software engineers and quality control managers in building testing solutions that are both portable and interoperable and promote a more widespread adoption of the TaaS model in cloud computing.},
  keywords={Cloud computing;Testing;Computational modeling;Interoperability;Browsers;Context;Cloud Computing;Testing as a Service (TaaS);Design Patterns;Vendor Lock-in;Testing Service},
  doi={10.1109/IC2E.2017.30},
  ISSN={},
  month={April},}

@INPROCEEDINGS{4539573,
  author={Hargassner, Walter and Hofer, Thomas and Klammer, Claus and Pichler, Josef and Reisinger, Gernot},
  booktitle={2008 1st International Conference on Software Testing, Verification, and Validation}, 
  title={A Script-Based Testbed for Mobile Software Frameworks}, 
  year={2008},
  volume={},
  number={},
  pages={448-457},
  abstract={Software testing is essential and takes a large part of resources during software development. This motivates automating software testing as far as possible. Frameworks for automating unit testing are approved and applied for a plethora of programming languages to write tests for small units in the same programming language. Both constraints, unit size and programming language, inhibit automation of software testing in domain of mobile software frameworks. This circumstance has motivated the development of a new testbed for a framework in the domain of mobile systems. In this paper, we describe requirements and challenges in testing mobile software frameworks in general and present a novel testbed for the APOXI framework that addresses these requirements. The main ideas behind this testbed are the usage of a scripting language to specify test cases and to incorporate domain-specific aspects on the language level. The testbed facilitates component and system testing but can be used for unit testing as well.},
  keywords={Software testing;System testing;Protocols;Application software;Computer languages;Control systems;Automatic testing;Hardware;Mobile handsets;Electronic equipment testing;Software testing},
  doi={10.1109/ICST.2008.51},
  ISSN={2159-4848},
  month={April},}

@INPROCEEDINGS{8668006,
  author={Włodarski, Leszek and Pereira, Boris and Povazan, Ivan and Fabry, Johan and Zaytsev, Vadim},
  booktitle={2019 IEEE 26th International Conference on Software Analysis, Evolution and Reengineering (SANER)}, 
  title={Qualify First! A Large Scale Modernisation Report}, 
  year={2019},
  volume={},
  number={},
  pages={569-573},
  abstract={Typically in modernisation projects any concerns for code quality are silenced until the end of the migration, to simplify an already complex process. Yet, we claim from experience that prioritising quality above many other issues has many benefits. In this experience report, we discuss a modernisation project of mBank, a big Polish bank, where bad smell detection and elimination, automated testing and refactoring played a crucial rule, provided pay-offs early in the project, increased buy-in, and ensured maintainability of the end result.},
  keywords={C# languages;Databases;Testing;Production;Software;Product development;Aging},
  doi={10.1109/SANER.2019.8668006},
  ISSN={1534-5351},
  month={Feb},}

@INPROCEEDINGS{9236829,
  author={Li, Hongwei and Wang, Junsheng and Dai, Hua and Lv, Bang},
  booktitle={2020 IEEE 3rd International Conference on Information Systems and Computer Aided Education (ICISCAE)}, 
  title={Research on Microservice Application Testing System}, 
  year={2020},
  volume={},
  number={},
  pages={363-368},
  abstract={In response to the development plan of national informatization construction, The State Grid Corporation of China attaches great importance to the development of information technology and pioneers the successful application of a new information technology generation in the power grid business. The development of the core components of “State Grid Cloud” and site deployment are also the company's focus. Microservices, as a key technology for deployment in the cloud, because of its independent development, independent deployment, independent release, decentralized management, and support for high concurrency and feature to support the rich technology stack, have been applied to cloud service construction by The State Grid. However, in the process of wide application of microservice applications, the entire microservice system hasn't been perfected, and the research of the corresponding test scheme is still in its infancy. This paper compares the difference between microservice and traditional service, and puts forward a set of microservice application testing system based on the current microservice development and software testing technology. This testing system divides the microservice test into six stages, unifies the test tools of each stage and explains the test methods. In addition, it also explains the automation strategy of the microservice test. It is of great significance to perfect the microservice testing system of The State Grid Corporation.},
  keywords={Software testing;Automation;Tools;Software;Information technology;Research and development;Testing;Microservices;Cloud Platform;Microservice Testing System;Automatic Deployment},
  doi={10.1109/ICISCAE51034.2020.9236829},
  ISSN={},
  month={Sep.},}

@ARTICLE{9631239,
  author={Zhu, Penghua and Li, Ying and Li, Tongyu and Yang, Wei and Xu, Yihan},
  journal={IEEE Access}, 
  title={GUI Widget Detection and Intent Generation via Image Understanding}, 
  year={2021},
  volume={9},
  number={},
  pages={160697-160707},
  abstract={Aerospace control software is the most important part of aerospace software. Since its potential defects endanger life and safety, there are strict requirements on product quality. Therefore, efficient and reliable software testing is essential. The traditional testing method has been challenging to meet its development requirements, and software automation testing has gradually become the main tool for testing aerospace control software. For the automation testing of aerospace control software, the core problem is to locate the GUI widgets on the software screenshots and identify their intent, which directly affects the accuracy of the test. Because of this, we use the widget recognition technology based on image matching and use the image understanding and analysis technology to extract the widget image in the screenshots. After obtaining the widget image, we use a convolutional neural network to extract image features and use the encoder module to encode the extracted information features as a tensor. The decoder module generates a word sequence conditional on tensor and previous output based on the encoded information. We also conduct an empirical study to evaluate the accuracy of widget recognition and intention generation. For widget recognition, our average IoU reached 0.81. For widget intent generation, our model BLEU-1 is 0.567, BLEU-2 is 0.356, BLEU-3 is 0.261, BLEU-4 is 0.131. The results show that our method is very effective.},
  keywords={Software;Testing;Aerospace control;Graphical user interfaces;Feature extraction;Image edge detection;Convolutional neural networks;GUI widget detection;GUI widget intent generation;aerospace control software},
  doi={10.1109/ACCESS.2021.3131753},
  ISSN={2169-3536},
  month={},}

@INPROCEEDINGS{8115704,
  author={Pietsch, Christopher and Ohrndorf, Manuel and Kelter, Udo and Kehrer, Timo},
  booktitle={2017 32nd IEEE/ACM International Conference on Automated Software Engineering (ASE)}, 
  title={Incrementally slicing editable submodels}, 
  year={2017},
  volume={},
  number={},
  pages={913-918},
  abstract={Model slicers are tools which provide two services: (a) finding parts of interest in a model and (b) displaying these parts somehow or extract these parts as a new, autonomous model, which is referred to as slice or sub-model. This paper focuses on the creation of editable slices, which can be processed by model editors, analysis tools, model management tools etc. Slices are useful if, e.g., only a part of a large model shall be analyzed, compared or processed by time-consuming algorithms, or if sub-models shall be modified independently. We present a new generic incremental slicer which can slice models of arbitrary type and which creates slices which are consistent in the sense that they are editable by standard editors. It is built on top of a model differencing framework and does not require additional configuration data beyond those available in the differencing framework. The slicer can incrementally extend or reduce an existing slice if model elements shall be added or removed, even if the slice has been edited meanwhile. We demonstrate the usefulness of our slicer in several scenarios using a large UML model. A screencast of the demonstrated scenarios is provided at http://pi.informatik.uni-siegen.de/projects/SiLift/ase2017.},
  keywords={Unified modeling language;Adaptation models;Tools;Servers;Load modeling;Computational modeling;Data models},
  doi={10.1109/ASE.2017.8115704},
  ISSN={},
  month={Oct},}

@INPROCEEDINGS{1203468,
  author={De, P. and Neogi, A. and Chiueh, T.-C.},
  booktitle={23rd International Conference on Distributed Computing Systems, 2003. Proceedings.}, 
  title={VirtualWire: a fault injection and analysis tool for network protocols}, 
  year={2003},
  volume={},
  number={},
  pages={214-221},
  abstract={The prevailing practice for testing protocol implementations is direct code instrumentation to trigger specific states in the code. This leaves very little scope for reuse of the test cases. In this paper, we present the design, implementation, and evaluation of VirtualWire, a network fault injection and analysis system designed to facilitate the process of testing network protocol implementations. VirtualWire injects user-specified network faults and matches network events against anticipated responses based on high-level specifications written in a declarative scripting language. With VirtualWire, testing requires no code instrumentation and fault specifications can be reused across versions of a protocol implementation. We illustrate the effectiveness of VirtualWire with examples drawn from testing Linux's TCP implementation and a real-time Ethernet protocol called Rether. In each case, 10 to 20 lines of script is sufficient to specify the test scenario. VirtualWire is completely transparent to the protocols under test, and additional overhead in protocol processing latency it introduces is below 10% of the normal.},
  keywords={Instruments;Access protocols;Ethernet networks;Kernel;System testing;Filters;Laboratories;Computer science;Delay;Formal verification},
  doi={10.1109/ICDCS.2003.1203468},
  ISSN={1063-6927},
  month={May},}

@INPROCEEDINGS{4700654,
  author={Khoche, Ajay and Burlison, Phil and Rowe, John and Plowman, Glenn},
  booktitle={2008 IEEE International Test Conference}, 
  title={A Tutorial on STDF Fail Datalog Standard}, 
  year={2008},
  volume={},
  number={},
  pages={1-10},
  abstract={Advances in technology are making it imperative to collect detailed structural IC fail data during manufacturing test to improve yield. However, there is currently no standard format for communicating and storing such structural fail data efficiently. This leads to ad-hoc tool-specific solutions, which do not offer interoperability required in a typical multi-tool, multi-vendor customer environment. These ad-hoc solutions result in unnecessary investment in development of point-to-point interface solutions that are ultimately still not integrated with the traditional data collection for a unified yield analysis data format. Expanding an established datalogging standard to accommodate the new requirements solves these issues. Standard Test Data Format (STDF) is the predominant format used today for traditional failure datalogging storage, but in its current form falls far short in handling the new high-volume structural failures for yield learning. A group of more than 20 companies from ATE, EDA, Semiconductor and Yield Management companies has been working on a new enhanced STDF standard that addresses the new requirements. This paper provides the overview of the new enhanced standard.},
  keywords={Tutorial;Electronic design automation and methodology;Production;Manufacturing;Silicon;Integrated circuit testing;Communication standards;Investments;Data analysis;Test pattern generators},
  doi={10.1109/TEST.2008.4700654},
  ISSN={2378-2250},
  month={Oct},}

@INPROCEEDINGS{8945647,
  author={Bhagya, Thilini and Dietrich, Jens and Guesgen, Hans},
  booktitle={2019 26th Asia-Pacific Software Engineering Conference (APSEC)}, 
  title={Generating Mock Skeletons for Lightweight Web-Service Testing}, 
  year={2019},
  volume={},
  number={},
  pages={181-188},
  abstract={Modern application development allows applications to be composed using lightweight HTTP services. Testing such an application requires the availability of services that the application makes requests to. However, access to dependent services during testing may be restrained. Simulating the behaviour of such services is, therefore, useful to address their absence and move on application testing. This paper examines the appropriateness of Symbolic Machine Learning algorithms to automatically synthesise HTTP services' mock skeletons from network traffic recordings. These skeletons can then be customised to create mocks that can generate service responses suitable for testing. The mock skeletons have human-readable logic for key aspects of service responses, such as headers and status codes, and are highly accurate.},
  keywords={Testing;Artificial intelligence;Decision trees;Semantics;Web services;Prediction algorithms;Skeleton;HTTP, Web services, REST, service oriented computing, mocking, service virtualisation, application testing, symbolic machine learning},
  doi={10.1109/APSEC48747.2019.00033},
  ISSN={2640-0715},
  month={Dec},}

@INPROCEEDINGS{10187499,
  author={Biffl, Stefan and Hoffmann, David and Kiesling, Elmar and Meixner, Kristof and Lüder, Arndt and Winkler, Dietmar},
  booktitle={2023 IEEE 25th Conference on Business Informatics (CBI)}, 
  title={Validating Production Test Scenarios with Cyber-Physical System Design Models}, 
  year={2023},
  volume={},
  number={},
  pages={1-10},
  abstract={Information systems are key facilitators for production planning, configuration, and monitoring in increasingly flexible Cyber-Physical Production Systems (CPPSs), such as automated welding lines for joining car parts. The guideline VDI 2206 provides a V-model to organize the engineering disciplines. It emphasizes the importance of validation in CPPS engineering but provides limited guidance on the up-front specification and validation of test scenarios. This paper introduces the Production Test Scenario Validation (PTSV) approach to validate to what extent a multi-view CPPS design model and associated data sources from production engineering and operation represent the concepts used in test scenarios. The PTSV (i) elicits concepts in pre- and post-conditions of production test scenarios, (ii) maps these concepts to a multi-view CPPS design model, such as a production asset network, and (iii) identifies mismatches between production test scenarios and CPPS design models, such as missing model elements or data sources required for validation. We evaluated the effectiveness and efficiency of the approach in a feasibility study on a real-world welding line for joining car parts. The results indicate that the PTSV is feasible and effective for improving the capability to measure and analyze outcomes of test scenarios in CPPS engineering according to the VDI 2206.},
  keywords={Industries;Production systems;Welding;Soft sensors;Production planning;Production;Data models;Industry 4.0;cyber-physical production system;VDI 2206;behavior-driven development;knowledge graph},
  doi={10.1109/CBI58679.2023.10187499},
  ISSN={2378-1971},
  month={June},}

@INPROCEEDINGS{7588753,
  author={Chen, Lei and James, Phillip and Kirkwood, David and Nguyen, Hoang Nga and Nicholson, Gemma L and Roggenbach, Markus},
  booktitle={2016 IEEE International Conference on Intelligent Rail Transportation (ICIRT)}, 
  title={Towards integrated simulation and formal verification of rail yard designs - an experience report based on the UK East Coast Main Line}, 
  year={2016},
  volume={},
  number={},
  pages={347-355},
  abstract={The development of railway systems is often supported by a range of tools, each addressing individual, but overlapping concerns such as, e.g., performance or safety analysis. However, it is a challenge for users to organise work-flows; results are often in different, non-aligning data formats; furthermore, tools work on different levels of abstraction from macro to microscopic. Thus, tool integration would be beneficial, and also allow for more playful, experimental prototyping and design. This paper reports on lessons learned from the integration of BRaVE - the Birmingham Railway Virtual Environment - and OnTrack from Swansea University. BRaVE is an easy-to-use railway simulation software for development, modelling and flow analysis. OnTrack allows for the automatic verification of scheme plans against a number of safety properties via different formal methods. We present an approach that bridges the gap that occurs from varying details in data sources through automated transformations. This integration provides a first step towards a seamless environment for prototyping, concept development, and safety analysis under ”one roof”. We demonstrate the usefulness of our approach by giving integrated simulation and verification results for the UK East Coast Main Line. This work is part of the wider RSSB's Future Traffic Regulation Optimisation research programme.},
  keywords={Rail transportation;Solid modeling;Analytical models;Data models;Computational modeling;Vehicles},
  doi={10.1109/ICIRT.2016.7588753},
  ISSN={},
  month={Aug},}

@ARTICLE{10480701,
  author={Siddiqui, Asif and Rimal, Bhaskar P. and Reisslein, Martin and Wang, Yong},
  journal={IEEE Communications Surveys & Tutorials}, 
  title={Survey on Unified Threat Management (UTM) Systems for Home Networks}, 
  year={2024},
  volume={26},
  number={4},
  pages={2459-2509},
  abstract={Home networks increasingly support important networked applications with limited professional network administration support, while sophisticated attacks pose enormous security risks for networked applications. A Unified Threat Management (UTM) system strives to comprehensively protect a network by providing firewall, intrusion detection and prevention, as well as antibot protection in an integrated, easy-to-configure manner. Previous surveys have extensively covered the individual components of a UTM system, i.e., there is extensive literature on firewall surveys, intrusion detection and prevention surveys, and antibot protection surveys. Importantly, the previous surveys covered these protection services separately, without considering their integration (however, this integration is critical for comprehensive home network protection). In contrast, the present survey covers for the first time home network UTM systems, i.e., the integrated network security services provided by a UTM system for a home network. This UTM survey is organized according to the UTM components, i.e., we comprehensively survey the firewall methods, the intrusion detection and prevention methods, as well as the antibot protection methods that are suitable for a UTM system for a home network. Throughout, we view these methods from the perspective of integration into a UTM system with limited computational resources and limited network administration support. Our survey includes the protection capabilities, as well as the design and deployment aspects and software/hardware limitations of available off-the-shelf and open-source UTM systems. We find that effective integrated home network protection where the UTM system components synergistically support each other while operating with limited computational resources and network administration support still requires extensive future research and development.},
  keywords={Home automation;Surveys;Firewalls (computing);Security;Internet of Things;Inspection;Payloads;Authentication;firewall;home network;Internet of Things (IoT);intrusion detection;privacy;proxy;security;Unified Threat Management (UTM);vulnerabilities},
  doi={10.1109/COMST.2024.3382470},
  ISSN={1553-877X},
  month={Fourthquarter},}

@INPROCEEDINGS{9652642,
  author={Cluzel, Guillaume and Georgiou, Kyriakos and Moy, Yannick and Zeller, Clément},
  booktitle={2021 IEEE Secure Development Conference (SecDev)}, 
  title={Layered Formal Verification of a TCP Stack}, 
  year={2021},
  volume={},
  number={},
  pages={86-93},
  abstract={The Transmission Control Protocol (TCP) at the heart of TCP/IP protocol stacks is a critical part of our current digital infrastructure. In this article, we show how an existing professional-grade open source embedded TCP/IP library can benefit from a formally verified TCP reimplementation. Our approach is to apply formal verification to the TCP layer only, relying on validated models of the lower layers on which it depends.},
  keywords={Heart;Protocols;Conferences;TCPIP;Libraries;Formal verification;Network protocols;deductive verification;symbolic execution},
  doi={10.1109/SecDev51306.2021.00028},
  ISSN={},
  month={Oct},}

@INPROCEEDINGS{4293588,
  author={Fletcher, Matt and Bereza, William and Karlesky, Mike and Williams, Greg},
  booktitle={Agile 2007 (AGILE 2007)}, 
  title={Evolving into Embedded Develop}, 
  year={2007},
  volume={},
  number={},
  pages={150-155},
  abstract={In late 2005 we had the opportunity to start our first embedded development project. We apply agile practices to a variety of domains from web development to desktop applications to factory floor test equipment. The challenge for this new work was not learning the environment and technology. Our challenge was applying the practices of the agile world to the small and complex world of embedded systems. The hurdles were numerous: we battled the archaic state of many embedded tool sets, the lack of integration with tools like Rake that provide easy automation, and poor support for object oriented design. We've overcome each of these difficulties. This report is about our yearlong experience in introducing our development practices to embedded development.},
  keywords={Velocity control;Microprogramming;Vehicle safety;Automatic testing;Embedded system;Vehicle driving;Automation;System testing;Sampling methods;Production facilities},
  doi={10.1109/AGILE.2007.25},
  ISSN={},
  month={Aug},}

@INPROCEEDINGS{7073242,
  author={Biliri, Evmorfia and Petychakis, Michael and Alvertis, Iosif and Lampathaki, Fenareti and Koussouris, Sotirios and Askounis, Dimitrios},
  booktitle={2014 IEEE/ACS 11th International Conference on Computer Systems and Applications (AICCSA)}, 
  title={Infusing social data analytics into Future Internet applications for manufacturing}, 
  year={2014},
  volume={},
  number={},
  pages={515-522},
  abstract={Today, a new age of engagement and collaboration has emerged with the proliferation of usergenerated content in social networks and generally the Web 2.0, rendering it particularly difficult for enterprises to monitor and act upon all content following conventional data mining methodologies. In this paper, we present our approach for a Future Internet enabler (FITMAN Anlzer) that provides automated, social data analytics and aims at assisting enterprises in becoming more tuned to their customer needs and gaining insights into current and future trends to early embed them into product design. The FITMAN Anlzer implementation is domainindependent and allows any manufacturer to effectively train it based on his needs and create personalized reports to timely capture the right information. Our methodology includes trend analytics, polarity detection through machine learning, data querying through flexible reports and finally informative charts to visualize the results in order to help companies in their decision making procedures.},
  keywords={Media;Sentiment analysis;Market research;Facebook;Twitter;Data mining;Context;social media monitoring;trend analysis;opinion mining;natural language processing;sentiment analysis;social data analytics},
  doi={10.1109/AICCSA.2014.7073242},
  ISSN={2161-5330},
  month={Nov},}

@INPROCEEDINGS{10172628,
  author={Valle, Pablo and Arrieta, Aitor and Arratibel, Maite},
  booktitle={2023 IEEE/ACM 45th International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP)}, 
  title={Automated Misconfiguration Repair of Configurable Cyber-Physical Systems with Search: an Industrial Case Study on Elevator Dispatching Algorithms}, 
  year={2023},
  volume={},
  number={},
  pages={396-408},
  abstract={Real-world Cyber-Physical Systems (CPSs) are usually configurable. Through parameters, it is possible to configure, select or unselect different system functionalities. While this provides high flexibility, it also becomes a source for failures due to misconfigurations. The large number of parameters these systems have and the long test execution time in this context due to the use of simulation-based testing make the manual repair process a cumbersome activity. Subsequently, in this context, automated repairing methods are paramount. In this paper, we propose an approach to automatically repair CPSs’ misconfigurations. Our approach is evaluated with an industrial CPS case study from the elevation domain. Experiments with a real building and data obtained from operation suggests that our approach outperforms a baseline algorithm as well as the state of the practice (i.e., manual repair carried out by domain experts).},
  keywords={Software algorithms;Buildings;Manuals;Maintenance engineering;Cyber-physical systems;Elevators;Dispatching;Cyber-Physical Systems;Repair;Debugging;Configurable Systems},
  doi={10.1109/ICSE-SEIP58684.2023.00042},
  ISSN={2832-7659},
  month={May},}

@INPROCEEDINGS{6821183,
  author={Meyer, Stefan and Healy, Philip and Lynn, Theo and Morrison, John},
  booktitle={2013 15th International Symposium on Symbolic and Numeric Algorithms for Scientific Computing}, 
  title={Quality Assurance for Open Source Software Configuration Management}, 
  year={2013},
  volume={},
  number={},
  pages={454-461},
  abstract={Commonly used open source configuration management systems, such as Puppet, Chef and CFEngine, allow for system configurations to be expressed as scripts. A number of quality issues that may arise when executing these scripts are identified. An automated quality assurance service is proposed that identifies the presence of these issues by automatically executing scripts across a range of environments. Test results are automatically published to a format capable of being consumed by script catalogues and social coding sites. This would serve as an independent signal of script trustworthiness and quality to script consumers and would allow developers to be made quickly aware of quality issues. As a result, potential consumers of scripts can be assured that a script is likely to work when applied to their particular environment. Script developers can be notified of compatibility issues and take steps to address them.},
  keywords={Servers;Operating systems;Communities;Linux;Quality assurance;Testing;Automated configuration;configuration management;continuous integration;automated deployment;service orchestration;assurance},
  doi={10.1109/SYNASC.2013.66},
  ISSN={},
  month={Sep.},}

@INPROCEEDINGS{8887378,
  author={Sachidananda, Vinay and Bhairav, Suhas and Ghosh, Nirnay and Elovici, Yuval},
  booktitle={2019 18th IEEE International Conference On Trust, Security And Privacy In Computing And Communications/13th IEEE International Conference On Big Data Science And Engineering (TrustCom/BigDataSE)}, 
  title={PIT: A Probe Into Internet of Things by Comprehensive Security Analysis}, 
  year={2019},
  volume={},
  number={},
  pages={522-529},
  abstract={One of the major issues which are hindering widespread and seamless adoption of Internet of Thing (IoT) is security. The IoT devices are vulnerable and susceptible to attacks which became evident from a series of recent large-scale distributed denial-of-service (DDoS) attacks, leading to substantial business and financial losses. Furthermore, in order to find vulnerabilities in IoT, there is a lack of comprehensive security analysis framework. In this paper, we present a modular, adaptable and tunable framework, called PIT, to probe IoT systems at different layers of design and implementation. PIT consists of several security analysis engines, viz., penetration testing, fuzzing, static analysis, and dynamic analysis and an exploitation engine to discover multiple IoT vulnerabilities, respectively. We also develop a novel grey-box fuzzer, called Applica, as a part of the fuzzing engine to overcome the limitations of the present day fuzzers. The proposed framework has been evaluated on a real-world IoT testbed comprising of the state-of-the-art devices. We discovered several network and system-level vulnerabilities such as Buffer Overflow, Denial-of-Service, SQL Injection, etc., and successfully exploited them to demonstrate the presence of security loopholes in the IoT devices.},
  keywords={Security;Engines;Fuzzing;Generators;Static analysis;Protocols;Internet of Things;Internet of Things;Security and Privacy;Security Analysis;Vulnerability;Framework;Fuzzing},
  doi={10.1109/TrustCom/BigDataSE.2019.00076},
  ISSN={2324-9013},
  month={Aug},}

@INPROCEEDINGS{687918,
  author={Aljabri, A.S. and Bernard, D.E. and Dvorak, D.L. and Man, G.K. and Pell, B. and Starbird, T.W.},
  booktitle={1998 IEEE Aerospace Conference Proceedings (Cat. No.98TH8339)}, 
  title={Infusion of autonomy technology into space missions: DS1 lessons learned}, 
  year={1998},
  volume={2},
  number={},
  pages={315-329 vol.2},
  abstract={The impact of infusing breakthrough autonomy technology into a flight project was a big surprise. Valuable technical and cultural lessons, many of general applicability when introducing system-level autonomy, have been learned by infusing the Remote Agent (RA) into NASA's Deep Space 1 (DS1) spacecraft. The RA's architecture embodies system-level autonomy in three major components: planning and scheduling, execution, and fault diagnosis and reconfiguration. Lessons learned include: the architecture was confirmed; active participation by nonautonomy personnel in the development is essential; communication of new concepts is essential, difficult, and hampered by differences in terminology; giving a spacecraft system-level autonomy changes organizational roles in operating the spacecraft after launch, and hence changes roles during development; software models supporting functions traditionally handled on the ground must be developed early enough to get on-board; shortfalls in planned features must be technically and developmentally accomodatable, in particular not to threaten the launch schedule; traditional commanding must be supported; testing must be emphasized. These lessons and others, on incremental system releases and use of autocode generation, are based on 16 months of spiral development from start of project through the project's decision to reduce the role of the RA from full-time control of the spacecraft to a separable experiment.},
  keywords={Space technology;Space missions;Space vehicles;Scheduling;Cultural differences;Fault diagnosis;Computer architecture;Personnel;Terminology;Communication system software},
  doi={10.1109/AERO.1998.687918},
  ISSN={1095-323X},
  month={March},}

@INPROCEEDINGS{6114166,
  author={Ioannides, Charalambos and Barrett, Geoff and Eder, Kerstin},
  booktitle={2011 IEEE International High Level Design Validation and Test Workshop}, 
  title={Introducing XCS to Coverage Directed test Generation}, 
  year={2011},
  volume={},
  number={},
  pages={57-64},
  abstract={Coverage Directed test Generation (CDG) is rife with challenges and problems, despite the relative successes of machine learning methodologies over the years in automating it. This paper introduces the use of the eXtended Classifier System (XCS) in simulation-based digital design verification. It argues for the use of this novel genetics-based machine learning technique to perform effective CDG by learning the full mapping between coverage results and test generator directives. Using the resulting production rules, efficient test suites can be constructed, and inference on the validity of the verification environment can be made. There is great potential in using XCS for design verification and this paper forms an initial attempt to highlight the associated advantages. The technique requires no domain knowledge to setup and satisfies important CDG requirements. Once matured, it is expected to be utilized seamlessly in any industrial level simulation-based verification process.},
  keywords={Generators;Fires;Pipelines;Genetic algorithms;Machine learning;Measurement;Learning systems;Electronic Design Automation and Methodology;Digital Simulation;Learning Systems;Learning Classifier Systems;XCS},
  doi={10.1109/HLDVT.2011.6114166},
  ISSN={1552-6674},
  month={Nov},}

@ARTICLE{9509755,
  author={Cheng, Lin and Pan, Peitian and Zhao, Zhongyuan and Ranjan, Krithik and Weber, Jack and Veluri, Bandhav and Ehsani, Seyed Borna and Ruttenberg, Max and Jung, Dai Cheol and Ivanov, Preslav and Richmond, Dustin and Taylor, Michael B. and Zhang, Zhiru and Batten, Christopher},
  journal={IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems}, 
  title={A Tensor Processing Framework for CPU-Manycore Heterogeneous Systems}, 
  year={2022},
  volume={41},
  number={6},
  pages={1620-1635},
  abstract={Future CPU-manycore heterogeneous systems can provide high peak throughput by integrating thousands of simple, independent, energy-efficient cores in a single die. However, there are two key challenges to translating this high peak throughput into improved end-to-end workload performance: 1) manycore co-processors rely on simple hardware putting significant demands on the software programmer and 2) manycore co-processors use in-order cores that struggle to tolerate long memory latencies. To address the manycore programmability challenge, this article presents a dense and sparse tensor processing framework based on PyTorch that enables domain experts to easily accelerate off-the-shelf workloads on CPU-manycore heterogeneous systems. To address the manycore memory latency challenge, we use our extended PyTorch framework to explore the potential for decoupled access/execute (DAE) software and hardware mechanisms. More specifically, we propose two software-only techniques, naïve-software DAE and systolic-software DAE, along with a lightweight hardware access accelerator to further improve area-normalized throughput. We evaluate our techniques using a combination of PyTorch operator microbenchmarking and real-world PyTorch workloads running on a detailed register-transfer-level model of a 128-core manycore architecture. Our evaluation on three real-world dense and sparse tensor workloads suggests these workloads can achieve approximately 2– $6\times $  performance improvement when scaled to a future 2000-core CPU-manycore heterogeneous system compared to an 18-core out-of-order CPU baseline, while potentially achieving higher area-normalized throughput and improved energy efficiency compared to general-purpose graphics processing units.},
  keywords={Computer architecture;Software;Hardware;Throughput;Multicore processing;Tensors;Central Processing Unit;Accelerator architectures;open source software;parallel programming;software libraries},
  doi={10.1109/TCAD.2021.3103825},
  ISSN={1937-4151},
  month={June},}

@INPROCEEDINGS{7880429,
  author={Jamous, Naoum and Bosse, Sascha and Görling, Carsten and Hintsch, Johannes and Khan, Ateeq and Kramer, Frederik and Müller, Hendrik and Turowski, Klaus},
  booktitle={2016 4th International Conference on Enterprise Systems (ES)}, 
  title={Towards an IT Service Lifecycle Management (ITSLM) Concept}, 
  year={2016},
  volume={},
  number={},
  pages={29-38},
  abstract={Information Technology (IT) usage in enterprises has evolved over the last years. This led to today's complex, heterogeneous, and dynamic IT system landscapes that support business processes in enterprises. To manage these landscapes, the IT Service Management (ITSM) concept is gaining more importance in today's business and research. Studies demonstrate that introducing ITSM standards lead to positive effects, such as improved customer-orientation as well as efficiency and transparency of IT support, which justify the costs of implementation. However, companies still face difficulties in deciding which processes to be implement (first), and to which extent. Questions like: "How can the currently applied ITSM be adapted or extended when new business-related or technological challenges appear?" arise. Goods producing companies started early relying on Product Lifecycle Management (PLM). PLM delivers a solid means to define, discuss, analyze, and better standardize value creation processes. With PLM in mind, we propose a concept to adopt and further develop it towards IT Service Lifecycle Management (ITSLM) suitable for the IT services provider environment. After introducing ITSLM, analyzing its processes, and its correlation to PLM, we design ITSLM as a model-driven process support. The selection of appropriate models with different complexity can be used to implement and adapt standard supporting tasks with minimum effort. Two use cases are detailed: fault-tolerance design optimization as well as automation of IT service provisioning. In these areas, suitable model complexity levels, computer-aided task support as well as the knowledge transfer among these models are discussed.},
  keywords={Business;Biological system modeling;Computational modeling;Adaptation models;Standards;Complexity theory;Information technology;IT Service Management (ITSM);Moddeling;Information Technology Infrastructure Library (ITIL);A Model-Driven IT Service Engineering},
  doi={10.1109/ES.2016.10},
  ISSN={},
  month={Nov},}

@ARTICLE{9195875,
  author={Jiang, Shunning and Ou, Yanghui and Pan, Peitian and Cheng, Kaishuo and Zhang, Yixiao and Batten, Christopher},
  journal={IEEE Design & Test}, 
  title={PyH2: Using PyMTL3 to Create Productive and Open-Source Hardware Testing Methodologies}, 
  year={2021},
  volume={38},
  number={2},
  pages={53-61},
  abstract={Editor's note: This article proposes a new model testing and verification methodology, PyH2, using property-based random testing in Python. PyH2 leverages the whole Python ecosystem to build test benches and models. - Sherief Reda, Brown University - Leon Stock, IBM - Pierre-Emmanuel Gaillardon, University of Utah},
  keywords={Design automation;Hardware;Open source software;Object oriented modeling;Python;Computer bugs;Cathode ray tubes},
  doi={10.1109/MDAT.2020.3024144},
  ISSN={2168-2364},
  month={April},}

@INPROCEEDINGS{10793128,
  author={Herten, Andreas and Achilles, Sebastian and Alvarez, Damian and Badwaik, Jayesh and Behle, Eric and Bode, Mathis and Breuer, Thomas and Caviedes-Voullième, Daniel and Cherti, Mehdi and Dabah, Adel and Sayed, Salem El and Frings, Wolfgang and Gonzalez-Nicolas, Ana and Gregory, Eric B. and Mood, Kaveh Haghighi and Hater, Thorsten and Jitsev, Jenia and John, Chelsea Maria and Meinke, Jan H. and Meyer, Catrin I. and Mezentsev, Pavel and Mirus, Jan-Oliver and Nassyr, Stepan and Penke, Carolin and Römmer, Manoel and Sinha, Ujjwal and Vieth, Benedikt von St. and Stein, Olaf and Suarez, Estela and Willsch, Dennis and Zhukov, Ilya},
  booktitle={SC24: International Conference for High Performance Computing, Networking, Storage and Analysis}, 
  title={Application-Driven Exascale: The JUPITER Benchmark Suite}, 
  year={2024},
  volume={},
  number={},
  pages={1-45},
  abstract={Benchmarks are essential in the design of modern HPC installations, as they define key aspects of system components. Beyond synthetic workloads, it is crucial to include real applications that represent user requirements into benchmark suites, to guarantee high usability and widespread adoption of a new system. Given the significant investments in leadership-class supercomputers of the exascale era, this is even more important and necessitates alignment with a vision of Open Science and reproducibility. In this work, we present the JUPITER Benchmark Suite, which incorporates 16 applications from various domains. It was designed for and used in the procurement of JUPITER, the first European exascale supercomputer. We identify requirements and challenges and outline the project and software infrastructure setup. We provide descriptions and scalability studies of selected applications and a set of key takeaways. The JUPITER Benchmark Suite is released as open source software with this work at github.com/FZJ-JSC/jubench},
  keywords={Procurement;Jupiter;Scalability;High performance computing;Benchmark testing;Supercomputers;Reproducibility of results;Usability;Open source software;Investment;Benchmark;Procurement;Exascale;System Design;System Architecture;GPU;Accelerator},
  doi={10.1109/SC41406.2024.00038},
  ISSN={},
  month={Nov},}

@ARTICLE{4012600,
  author={Ayers, Danny},
  journal={IEEE Internet Computing}, 
  title={The Shortest Path to the Future Web}, 
  year={2006},
  volume={10},
  number={6},
  pages={76-79},
  abstract={This column's title could suggest that there is only one best path forward for the Web. The path begins with document metadata and travels through the world of microformats and embedded data. A waypoint is a semantic Web that leverages these approaches, along with those offered by an environment more capable of managing first-class data directly. This is only one path, however, and it probably isn't the shortest. The Internet is a rich environment with billions of active agents. Natural selection, mutation, and genetic breeding of sorts all happen to software systems, together with a significantly higher proportion of "intelligent design" than found in the real world. The net effect is that many different evolutionary paths are being explored simultaneously, and several could lead to a better Web},
  keywords={Access protocols;HTML;Computer networks;Humans;Semantic Web;Network servers;Web server;Pediatrics;Internet;Electronic mail;Semantic Web;Web 2.0;Web programming},
  doi={10.1109/MIC.2006.137},
  ISSN={1941-0131},
  month={Nov},}

@INPROCEEDINGS{10275637,
  author={Göttel, Christian and Kabir-Querrec, Maëlle and Kozhaya, David and Sivanthi, Thanikesavan and Vuković, Ognjen},
  booktitle={2023 IEEE 28th International Conference on Emerging Technologies and Factory Automation (ETFA)}, 
  title={Qualitative Analysis for Validating IEC 62443-4-2 Requirements in DevSecOps}, 
  year={2023},
  volume={},
  number={},
  pages={1-8},
  abstract={Validation of conformance to cybersecurity standards for industrial automation and control systems is an expensive and time consuming process which can delay the time to market. It is therefore crucial to introduce conformance validation stages into the continuous integration/continuous delivery pipeline of products. However, designing such conformance validation in an automated fashion is a highly non-trivial task that requires expert knowledge and depends upon available security tools, ease of integration into the DevOps pipeline, as well as support for IT and OT interfaces and protocols.This paper addresses the aforementioned problem focusing on the automated validation of ISA/IEC 62443-4-2 standard component requirements. We present an extensive qualitative analysis of the standard requirements and the current tooling landscape to perform validation. Our analysis demonstrates the coverage established by the currently available tools and sheds light on current gaps to achieve full automation and coverage. Furthermore, we showcase for every component requirement where in the CI/CD pipeline stage it is recommended to test it and the tools to do so.},
  keywords={Pipelines;Time to market;Process control;Focusing;Control systems;Delays;Task analysis;cybersecurity;IEC 62443;DevOps;security testing;CI/CD;continuous integration},
  doi={10.1109/ETFA54631.2023.10275637},
  ISSN={1946-0759},
  month={Sep.},}

@INPROCEEDINGS{9843754,
  author={Bocchino, Robert L and Levison, Jeffrey W. and Starch, Michael D.},
  booktitle={2022 IEEE Aerospace Conference (AERO)}, 
  title={FPP: A Modeling Language for F Prime}, 
  year={2022},
  volume={},
  number={},
  pages={1-15},
  abstract={We present F Prime Prime (FPP), a new open-source modeling language for F Prime. F Prime is an open-source flight software framework developed at JPL and deployed, among other places, on the Mars helicopter Ingenuity. FPP provides a convenient way to model the architectural elements of an F Prime application, e.g., components, ports, and their connections. It has a succinct and readable syntax, a well-defined semantics, and robust error checking and reporting. The FPP tool suite, written in Scala, analyzes FPP models, reports errors, and translates correct FPP models to a combination of XML and C++. Existing F Prime tools translate the XML to a partial implementation in C++, to be completed by the developers. The model elements have clean interfaces and are highly reusable. An accompanying visualization tool constructs diagrams of components and connections that FSW developers can use to understand and communicate their designs, for example at reviews. We discuss the design and implementation of FPP and the integration of FPP into F Prime. We also discuss our experience using FPP to construct F Prime models. Finally, we discuss our plans for future work, including improved code generation, improved visualization, and more advanced analysis capabilities.},
  keywords={Ports (computers);Visualization;Analytical models;Mars;Semantics;XML;C++ languages},
  doi={10.1109/AERO53065.2022.9843754},
  ISSN={1095-323X},
  month={March},}

@INPROCEEDINGS{7133545,
  author={Cheng, Jing and Zhu, Yian and Zhang, Tao and Zhu, Chuanxi and Zhou, Wenqiang},
  booktitle={2015 IEEE Symposium on Service-Oriented System Engineering}, 
  title={Mobile Compatibility Testing Using Multi-objective Genetic Algorithm}, 
  year={2015},
  volume={},
  number={},
  pages={302-307},
  abstract={Mobile compatibility testing has been identified as one urgent and challenging issue. Mobile apps are expected to work on thousand kinds of mobile devices with diverse device features and mobile platforms. So mobile compatibility testing is complex and costly, it is impossible to test mobile apps on all mobile devices and in all environments with limited test resources. Then the question is how to select test devices in cost-effective mobile app compatibility testing. This paper proposes a novel test device selection approach using multi-objective genetic algorithm. Using the proposed approach, the minimum number of mobile devices is selected, and the multiple test coverage requirements are met simultaneously. Furthermore, the case study results have successfully demonstrated that the proposed approach is effective for mobile compatibility testing.},
  keywords={Mobile communication;Testing;Mobile handsets;Biological cells;Genetic algorithms;Sociology;Statistics;software testing;mobile testing;compatibility testing;clustering algorithm;test coverage},
  doi={10.1109/SOSE.2015.36},
  ISSN={},
  month={March},}

@INPROCEEDINGS{1383123,
  author={Alyokhin, V. and Elbel, B. and Rothfelder, M. and Pretschner, A.},
  booktitle={15th International Symposium on Software Reliability Engineering}, 
  title={Coverage metrics for Continuous Function Charts}, 
  year={2004},
  volume={},
  number={},
  pages={257-268},
  abstract={Continuous Function Charts are a diagrammatical language for the specification of mixed discrete-continuous embedded systems, similar to the languages of Matlab/Simulink, and often used in the domain of transportation systems. Both control and data flows are explicitly specified when atomic units of computation are composed. The obvious way to assess the quality of integration test suites is to compute known coverage metrics for the generated code. This production code does not exhibit those structures that would make it amenable to "relevant" coverage measurements. We define a translation scheme that results in structures relevant for such measurements, apply coverage criteria for both control and dataflows at the level of composition of atomic computational units, and argue for their usefulness on the grounds of detected errors.},
  keywords={System testing;Atomic measurements;Data flow computing;Timing;Production;Mathematical model;Power system modeling;Automatic control;Rail transportation;Certification;Integration testing;model-based testing;mixed continuous-discrete and real-time systems;MC/DC;data flow testing;block diagrams},
  doi={10.1109/ISSRE.2004.15},
  ISSN={1071-9458},
  month={Nov},}

@INPROCEEDINGS{4626838,
  author={Lutz, Robyn},
  booktitle={2008 12th International Software Product Line Conference}, 
  title={Enabling Verifiable Conformance for Product Lines}, 
  year={2008},
  volume={},
  number={},
  pages={35-44},
  abstract={NASA is, with the rest of industry, turning to product-line engineering to reduce costs and improve quality by effectively managing reuse. Experience in industry has shown that it is the verifiable conformance of each system to the product-line specifications that makes or breaks the product-line practice. Verification that the software for each project satisfies its intended product-line constraints is thus essential. This paper reports early results from aneffort to assemble from previous, industrial experience a set of enablers of verifiable conformance for use in the application engineering of NASA product lines. Lessons learned may be useful for developers of safety-critical, long-lived, or highly autonomous productlines, as well as for companies that integrate product line subsystems developed by multiple contractors.},
  keywords={NASA;Software;Computer architecture;Organizations;Industries;Evolution (biology);Safety;software product line;application engineering;verifying conformance;experience},
  doi={10.1109/SPLC.2008.12},
  ISSN={},
  month={Sep.},}

@INPROCEEDINGS{7338271,
  author={Ali, Shaukat and Yue, Tao},
  booktitle={2015 ACM/IEEE 18th International Conference on Model Driven Engineering Languages and Systems (MODELS)}, 
  title={Formalizing the ISO/IEC/IEEE 29119 Software Testing Standard}, 
  year={2015},
  volume={},
  number={},
  pages={396-405},
  abstract={Model-based testing (MBT) provides a systematic and automated way to facilitate rigorous testing of software systems. MBT has been an intense area of research and a large number of MBT techniques have been developed in the literature and in the practice. However, all of the techniques have been developed using their own concepts and terminology of MBT, which are very often different than other techniques and at times have conflicting semantics. Moreover, while working on MBT projects with our industrial partners in the last several years, we were unable to find a unified way of defining MBT techniques based on standard terminology. To precisely define MBT concepts with the aim of providing common understanding of MBT terminology across techniques, we formalize a small subset of the recently released ISO/IEC/IEEE 29119 Software Testing Standard as a conceptual model (UML class diagrams) together with OCL constraints. The conceptual model captures all the necessary concepts based on the standard terminology that are mandatory or optional in the context of MBT techniques and can be used to define new MBT tools and techniques. To validate the conceptual model, we instantiated its concepts for various MBT techniques previously developed in the context of our industrial partners. Such instantiation automatically enforces the specified OCL constraints. This type of validation provided us feedback to further refine the conceptual model. Finally, we also provide our experiences and lessons learnt for such formalization and validation.},
  keywords={Unified modeling language;Concrete;Testing;Terminology;Data models;ISO Standards;Model-Based Testing;ISO/IEC/IEEE 29119;UML;Test Case Generation;Modeling Methodology},
  doi={10.1109/MODELS.2015.7338271},
  ISSN={},
  month={Sep.},}

@ARTICLE{8472900,
  author={Duan, Pengfei and Zhou, Ying and Gong, Xufang and Li, Bixin},
  journal={IEEE Access}, 
  title={A Systematic Mapping Study on the Verification of Cyber-Physical Systems}, 
  year={2018},
  volume={6},
  number={},
  pages={59043-59064},
  abstract={Cyber-physical system (CPS) is a kind of complex real-time hybrid system which involves deep interactions between computation processors, communication network, and physical environments are deemed as the key enablers of next generation computer applications. However, how to verify CPS effectively is always a great challenge. Based on current scientific works about CPS verification, this paper aims at identifying the gap of current studies and suggesting promising areas for the future works. For this purpose, we conduct a systematic mapping study over the topic on verification of cyber-physical system. We carry out a widely search of publications from 2006 to 2018 in 11 electronic databases. After the step of study selection, 80 papers are selected as primary studies for answering proposed research questions, focused questions, and statistical questions. According to these questions and their answers, this paper not only presents a quantitative and comprehensive analysis of verification challenges, abstraction methods, verification techniques, assistance tools, and verification scenarios that represent each step of verification works, but also summarizes CPS systematic natures, main routine of verification and future research directions. We believe that this survey can identify gaps in current research works and reveal new insights for the future works.},
  keywords={Systematics;Cyber-physical systems;Tools;Databases;Guidelines;Sociology;Statistics;Systematic mapping study;verification of cyber-physical system;verification challenges;abstraction methods;verification techniques;assistance tools;verification scenarios},
  doi={10.1109/ACCESS.2018.2872015},
  ISSN={2169-3536},
  month={},}

@INPROCEEDINGS{7281537,
  author={Ladiges, Jan and Fay, Alexander and Haubeck, Christopher and Lamersdorf, Winfried and Lity, Sascha and Schaefer, Ina},
  booktitle={2015 IEEE 24th International Symposium on Industrial Electronics (ISIE)}, 
  title={Supporting commissioning of production plants by model-based testing and model learning}, 
  year={2015},
  volume={},
  number={},
  pages={606-611},
  abstract={During the commissioning phase of production systems the identification and correction of malfunctions is a tedious task mainly done manually by commissioning engineers. This task is of high importance because missed malfunctions may result in hazardous behavior during operation phase. At this point, regardless of the engineers expertise a systematic support can drastically decrease the risk of missed malfunctions. A promising systematic approach is to use engineering artifacts of the system design phase as an information source to identify unexpected behavior regarding the specification. This paper proposes such a systematic approach based on model-based testing resulting in automatic test case generation and execution which allows to support engineers with learned models representing the expected transient system behavior. Subsequently, the obtained models are used for detection of unexpected behavior during commissioning. The unexpected behavior is presented to a commissioning engineer who decides if the behavior (1) is correct and will be added to the models or (2) represents an identified system malfunction. The approach is evaluated on a demonstration plant.},
  keywords={Software;Testing;Sensors;Actuators;Production;Hardware;Monitoring},
  doi={10.1109/ISIE.2015.7281537},
  ISSN={2163-5145},
  month={June},}

@ARTICLE{4267608,
  author={Bassett, Paul G.},
  journal={IEEE Software}, 
  title={The Case for Frame-Based Software Engineering}, 
  year={2007},
  volume={24},
  number={4},
  pages={90-99},
  abstract={Frame technology adapts generic components into custom information structures. Its facility for maximizing reuse and minimizing redundancy has demonstrated dramatic improvements across software's life cycle.},
  keywords={Software engineering;Programming profession;Software maintenance;Redundancy;Software systems;Software quality;Computer industry;Humans;Statistics;Organizing;reuse models;automatic programming;evolutionary programming;software engineering process},
  doi={10.1109/MS.2007.119},
  ISSN={1937-4194},
  month={July},}

@INPROCEEDINGS{9951197,
  author={Riesebos, Leon and Brown, Kenneth R.},
  booktitle={2022 IEEE International Conference on Quantum Computing and Engineering (QCE)}, 
  title={Functional simulation of real-time quantum control software}, 
  year={2022},
  volume={},
  number={},
  pages={535-544},
  abstract={Modern quantum computers rely heavily on real-time control systems for operation. Software for these systems is becoming increasingly more complex due to the demand for more features and more real-time devices to control. Unfortunately, testing real-time control software is often a complex process, and existing simulation software is not usable or practical for software testing. For this purpose, we implemented an interactive simulator that simulates signals at the application programming interface level. We show that our simulation infrastructure simulates kernels 6.9 times faster on average compared to execution on hardware, while the position of the timeline cursor is simulated with an average accuracy of 97.9% when choosing the appropriate configuration.},
  keywords={Software testing;Quantum computing;Computational modeling;Process control;Real-time systems;Software;Device drivers;real-time control software;signal simulation;software testing;quantum computing},
  doi={10.1109/QCE53715.2022.00076},
  ISSN={},
  month={Sep.},}

@INPROCEEDINGS{8109427,
  author={Nobach, Leonhard and Blendin, Jeremias and Kolbe, Hans-Jörg and Schyguda, Georg and Hausheer, David},
  booktitle={2017 IEEE 42nd Conference on Local Computer Networks (LCN)}, 
  title={Bare-Metal Switches and Their Customization and Usability in a Carrier-Grade Environment}, 
  year={2017},
  volume={},
  number={},
  pages={649-657},
  abstract={The current ecosystem of network elements, such as switches and appliances, is largely dominated by devices supplied and sold with a bundled operating system, and software dedicated to manage the device's forwarding hardware, however, these platforms are not open-source and cannot be arbitrarily customized, and there is no cost transparency or flexibility in choosing software different to the bundled components.,,,,In this paper, we explore the capabilities of bare-metal switches, which are equipped with commodity switching hardware components, but shipped without an operating system. We evaluate the feasibility of these commonly lower-cost devices to meet the requirements of a customized, carrier-grade network function. Therefore, we have implemented a prototype on generic hardware, re-using as much open-source software as possible. Our Broadband Remote Access Server (BRAS) prototype can lower the cost compared to proprietary network appliances, and, known to have a hardware backplane capacity of 720 Gbps, the merchant-silicon / ASIC approach can highly outperform the state of the art of current x86-based virtualized network functions, while implementing the most important BRAS features.},
  keywords={Hardware;Software;Servers;Linux;Ports (Computers);Switches;Bare-Metal Switching;Dataplanes;Network Functions;Middleboxes;Sofware-Defined Networking;Cost-Efficiency},
  doi={10.1109/LCN.2017.104},
  ISSN={0742-1303},
  month={Oct},}

@ARTICLE{10017338,
  author={Kenjić, Dušan and Živkov, Dušan and Antić, Marija},
  journal={IEEE Transactions on Intelligent Vehicles}, 
  title={Automated Data Transfer From ADAS to Android-Based IVI Domain Over SOME/IP}, 
  year={2023},
  volume={8},
  number={4},
  pages={3166-3177},
  abstract={Automotive systems are complex and comprise of numerous components, which are developed simultaneously by different companies, rely on different technologies, and have different requirements in terms of safety and user experience. This leads to the multiple implementations of the similar functionalities within different domains of the vehicle, and the redundancy of hardware resources. Having separate domains communicating with each other would certainly lower the cost of the vehicle hardware and facilitate software development. Nevertheless, establishing inter-domain connectivity is not a trivial solution, and requires determining the suitable communication protocol, as well as the consideration of the use-case variety, specialized domain knowledge of automotive developers, platforms and software standards heterogeneity, safety and security issues, etc. This article presents a verified automated solution for data exchange between Advanced Driver Assistance System (ADAS) and In-Vehicle Infotainment (IVI) domains over SOME/IP. The backbone of the generated solution uses the most common interface definition languages for specific domains - ARXML, FIDL and AIDL, and enables the translation between those languages, as well as the generation of the required cross-domain services and clients, while supporting various architectural aspects and different software platforms.},
  keywords={Protocols;Operating systems;Automotive engineering;Hardware;Safety;Intelligent vehicles;Middleware;SOME/IP;ADAS;IVI;inter-domain communication;model-driven development;SOA in automotive},
  doi={10.1109/TIV.2023.3236581},
  ISSN={2379-8904},
  month={April},}

@INPROCEEDINGS{7357239,
  author={Barnett, Scott and Avazpour, Iman and Vasa, Rajesh and Grundy, John},
  booktitle={2015 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC)}, 
  title={A multi-view framework for generating mobile apps}, 
  year={2015},
  volume={},
  number={},
  pages={305-306},
  abstract={This paper demonstrates a multi-view framework for Rapid APPlication Tool (RAPPT). RAPPT enables rapid development of mobile applications. It employs a multilevel approach to mobile application development: a Domain Specific Visual Language to define the high level structure of mobile apps, a Domain Specific Textual Language to define behavioural concepts, and concrete source code for fine grained improvements.},
  keywords={Navigation},
  doi={10.1109/VLHCC.2015.7357239},
  ISSN={},
  month={Oct},}

@ARTICLE{8747425,
  author={Lin, Ying-Dar and Lai, Yu-Kuen and Tsou, Yung-Liang and Lai, Yuan-Cheng and Liou, En-Cheng and Chiang, Yita},
  journal={IEEE Systems Journal}, 
  title={Generic Validation Criteria and Methodologies for SDN Applications}, 
  year={2019},
  volume={13},
  number={4},
  pages={3909-3920},
  abstract={Programmable control plane in software-defined networking (SDN), plays an essential role in the SDN architecture. The network function provided by the specialized hardware in a legacy network can be created in the form of software-based “SDN application” running on the controllers to manipulate entire network configurations. Therefore, the risk of having software bugs and errors in the SDN applications may threaten the normal operations of SDN networks. This paper presents systematic validation criteria and test cases based on the proposed novel methodologies for SDN application testing. The test framework can perform testbed build-up, generate desired packet sequences, and analyze results automatically. According to the results of a generic test suite, several issues are unveiled in the application under test (AUT). Some AUTs, which need to check all the incoming packets from OpenFlow switches, fail to meet the test criteria of burst packet-in and flow self-recycling. For most of the applications based on the Ryu controller, the evaluation results reveal that some are unable to recycle flow entries after they are unloaded. It is recommended that all flows populated by SDN applications must have timeout value specified to prevent unnecessary entries kept in the flow table.},
  keywords={Control systems;Benchmark testing;Computer bugs;Software;Stability criteria;OpenFlow;performance evaluation;software defined network (SDN);system performance;testing;validation},
  doi={10.1109/JSYST.2019.2921599},
  ISSN={1937-9234},
  month={Dec},}

@INPROCEEDINGS{9302829,
  author={Pranata, Alif Akbar and Barais, Olivier and Bourcier, Johann and Noirie, Ludovic},
  booktitle={2020 IEEE/ACM 13th International Conference on Utility and Cloud Computing (UCC)}, 
  title={Misconfiguration Discovery with Principal Component Analysis for Cloud-Native Services}, 
  year={2020},
  volume={},
  number={},
  pages={269-278},
  abstract={Cloud applications and services have significantly increased the importance of system and service configuration activities. These activities include updating (i) these services, (ii) their dependencies on third parties, (iii) their configurations, (iv) the configuration of the execution environment, (v) network configurations. The high frequency of updates results in significant configuration complexity that can lead to failures or performance drops. To mitigate these risks, service providers extensively rely on testing techniques, such as metamorphic testing, to detect these failures before moving to production. However, the development and maintenance of these tests are costly, especially the oracle, which must determine whether a system's performance remains within acceptable boundaries. This paper explores the use of a learning method called Principal Component Analysis (PCA) to learn about acceptable performance metrics on cloudnative services and identify a metamorphic relationship between the nominal service behavior and the value of these metrics. We investigate the following research question: Is it possible to combine the metamorphic testing technique with learning methods on service monitoring data to detect error-prone reconfigurations before moving to production? We remove the developers' burden to define a specific oracle in detecting these configuration issues. For validation, we applied this proposal on a distributed media streaming application whose authentication was managed by an external identity and access management services. This application illustrates both the heterogeneity of the technologies used to build this type of service and its large configuration space. Our proposal demonstrated the ability to identify error-prone reconfigurations using PCA.},
  keywords={Principal component analysis;Testing;Measurement;Streaming media;Servers;Complexity theory;Cloud computing;Reconfigurations;Metamorphic testing;Principal component analysis;Cloud-native services},
  doi={10.1109/UCC48980.2020.00045},
  ISSN={},
  month={Dec},}

@INPROCEEDINGS{5614873,
  author={Abbors, Fredrik and Truşcan, Dragoş},
  booktitle={2010 Second International Conference on Advances in System Testing and Validation Lifecycle}, 
  title={Approaching Performance Testing from a Model-Based Testing Perspective}, 
  year={2010},
  volume={},
  number={},
  pages={125-128},
  abstract={The paper introduces the concept of model-based performance testing, which we plan to pursue in our research. The underlying idea is to describe various performance aspects as well as functional aspects of a software system using modeling languages like UML, and from the resulting models to automatically design tests that can be used for performance testing. In our research, we also plan to focus on how the modeling and traceability of performance requirements can be achieved across the testing process.},
  keywords={Unified modeling language;Testing;Adaptation model;Analytical models;Software systems;Load modeling;Model-Based Testing;Model Validation;Requirements Traceability},
  doi={10.1109/VALID.2010.22},
  ISSN={},
  month={Aug},}

@INPROCEEDINGS{8901892,
  author={Emmerich, Paul and Ellmann, Simon and Bonk, Fabian and Egger, Alex and Sánchez-Torija, Esaú García and Günzel, Thomas and di Luzio, Sebastian and Obada, Alexandru and Stadlmeier, Maximilian and Voit, Sebastian and Carle, Georg},
  booktitle={2019 ACM/IEEE Symposium on Architectures for Networking and Communications Systems (ANCS)}, 
  title={The Case for Writing Network Drivers in High-Level Programming Languages}, 
  year={2019},
  volume={},
  number={},
  pages={1-13},
  abstract={Drivers are written in C or restricted subsets of C++ on all production-grade server, desktop, and mobile operating systems. They account for 66 % of the code in Linux, but 39 out of 40 security bugs related to memory safety found in Linux in 2017 are located in drivers. These bugs could have been prevented by using high-level languages for drivers. We present user space drivers for the Intel ixgbe 10 Gbit/s network cards implemented in Rust, Go, C#, Java, OCaml, Haskell, Swift, JavaScript, and Python written from scratch in idiomatic style for the respective languages. We quantify costs and benefits of using these languages: High-level languages are safer (fewer bugs, more safety checks), but run-time safety checks reduce throughput and garbage collection leads to latency spikes. Out-of-order CPUs mitigate the cost of safety checks: Our Rust driver executes 63 % more instructions per packet but is only 4 % slower than a reference C implementation. Go's garbage collector keeps latencies below 100 μs even under heavy load. Other languages fare worse, but their unique properties make for an interesting case study. All implementations are available as free and open source at https://githud.com/ixy-languages/ixy-languages.},
  keywords={Rust;Go;C#;Java;OCaml;Haskell;Swift;JavaScript;Python},
  doi={10.1109/ANCS.2019.8901892},
  ISSN={},
  month={Sep.},}

@INPROCEEDINGS{7372061,
  author={Wölfl, Andreas and Siegmund, Norbert and Apel, Sven and Kosch, Harald and Krautlager, Johann and Weber-Urbina, Guillermo},
  booktitle={2015 30th IEEE/ACM International Conference on Automated Software Engineering (ASE)}, 
  title={Generating Qualifiable Avionics Software: An Experience Report (E)}, 
  year={2015},
  volume={},
  number={},
  pages={726-736},
  abstract={We report on our experience with enhancing the data-management component in the avionics software of the NH90 helicopter at Airbus Helicopters. We describe challenges regarding the evolution of avionics software by means of real-world evolution scenarios that arise in industrial practice. A key role plays a legally-binding certification process, called qualification, which is responsible for most of the development effort and cost. To reduce effort and cost, we propose a novel generative approach to develop qualifiable avionics software by combining model-based and product-line technology. Using this approach, we have already generated code that is running on the NH90 helicopter and that is in the process of replacing the current system code. Based on an interview with two professional developers at Airbus and an analysis of the software repository of the NH90, we systematically compare our approach with established development approaches in the avionics domain, in terms of implementation and qualification effort.},
  keywords={Aerospace electronics;Helicopters;System software;Interviews;Hardware;Encoding},
  doi={10.1109/ASE.2015.35},
  ISSN={},
  month={Nov},}

@INPROCEEDINGS{5280052,
  author={Krapfenbauer, Harald and Ertl, Dominik and Zoitl, Alois and Kupzog, Friederich},
  booktitle={2009 Fourth International Multi-Conference on Computing in the Global Information Technology}, 
  title={Improving Component Testing of Industrial Automation Software}, 
  year={2009},
  volume={},
  number={},
  pages={259-262},
  abstract={Industrial automation systems are tested nowadays mainly via system tests at a very late stage of development. These tests are conducted manually, are time-consuming and cost-intensive. Earlier testing of automation software, e.g., component testing, is therefore desired in order to reduce the effort for system testing by detecting errors sooner. In this paper we present an improved concept for a test environment that enables developers of industrial control electronics to test the functionality of IEC 61499 software components. Components can be tested on any hardware with an IEC 61499 runtime environment, even on the target hardware. There is no need to change the automation software for testing. We propose using dynamically typed languages to implement tests because such languages have inherent properties that are useful for this task. We provide example code of a typical test case.},
  keywords={Automatic testing;Software testing;Computer industry;Automation;System testing;Electronic equipment testing;IEC standards;Hardware;Industrial control;Industrial electronics;Industrial Automation Software;IEC 61499;Component Testing;Dynamically Typed Languages},
  doi={10.1109/ICCGI.2009.46},
  ISSN={},
  month={Aug},}

@ARTICLE{10143175,
  author={Sommer, Florian and Kriesten, Reiner and Kargl, Frank},
  journal={IEEE Access}, 
  title={Survey of Model-Based Security Testing Approaches in the Automotive Domain}, 
  year={2023},
  volume={11},
  number={},
  pages={55474-55514},
  abstract={Modern connected or autonomous vehicles (AVs) are highly complex cyber-physical systems. As a result of the high number of different technologies and connectivity features involved, testing these systems to identify security vulnerabilities is a big challenge. Security testing techniques, such as penetration testing, are often manual methods that are applied comparatively late in the vehicle development process. Thus, vulnerabilities are only detected late or after development, leading to higher costs and more patching effort. To reduce the amount of testing resources in general and enable early and automated testing, model-based testing methods have been established in several domains, such as information technology and the automotive domain. The transfer of model-based testing approaches to automotive security testing could help to detect vulnerabilities earlier than other, manual methods by automatically generating, executing, or simulating security tests. In this study, we review the literature on model-based test approaches in the automotive domain. First, we consider security-independent approaches to obtain an overview of applied models, formalisms, test selection criteria, and test generation techniques. In addition, we investigate, whether and how model-based approaches are applied for automotive security testing. Overall, we identified 63 publications related to model-based testing and 29 publications with regard to model-based security testing. The aim of this study is to provide an overview and direct comparison between these approaches. In this manner, the state of model-based security testing in the automotive domain, current challenges, and potential research areas are determined.},
  keywords={Testing;Unified modeling language;Security;Automotive engineering;Modeling;Mathematical models;Surveys;Automotive security;model-based testing;model-based security testing},
  doi={10.1109/ACCESS.2023.3282176},
  ISSN={2169-3536},
  month={},}

@ARTICLE{10083100,
  author={Joannou, Alexandre and Rugg, Peter and Woodruff, Jonathan and Fuchs, Franz A. and van der Maas, Marno and Naylor, Matthew and Roe, Michael and Watson, Robert N. M. and Neumann, Peter G. and Moore, Simon W.},
  journal={IEEE Design & Test}, 
  title={Randomized Testing of RISC-V CPUs Using Direct Instruction Injection}, 
  year={2024},
  volume={41},
  number={1},
  pages={40-49},
  abstract={This article presents a randomized testing framework for RISC-V implementations by using a technique called direct instruction injection for test injection. —Fei Su, Intel, USA},
  keywords={Testing;Reduced instruction set computing;Random processes},
  doi={10.1109/MDAT.2023.3262741},
  ISSN={2168-2364},
  month={Feb},}

@INPROCEEDINGS{232773,
  author={Simeu, E. and Puissochet, A. and Rainard, J.L. and Tagant, A.M. and Poize, M.},
  booktitle={Digest of Papers. 1992 IEEE VLSI Test Symposium}, 
  title={A new tool for random testability evaluation using simulation and formal proof}, 
  year={1992},
  volume={},
  number={},
  pages={321-326},
  abstract={A set of tools is described, allowing one to compute random testability measurement for combinational circuits, based on a black box worst case hypothesis. These tools provide enough information to allow circuit modification, in order to meet a prescribed testability value. The efficiency of these tools is due to the use of a statistical method combined with formal proof mechanisms. The random testability of the complete ISCAS benchmark of combinational circuits is computed. For the least testable circuits, a few modifications, guided by the testability measurements, are shown to be sufficient to make them randomly testable.<>},
  keywords={Circuit testing;Circuit faults;Computational modeling;Circuit simulation;Switching circuits;Combinational circuits;Benchmark testing;Built-in self-test;Telecommunication computing;Semiconductor device modeling},
  doi={10.1109/VTEST.1992.232773},
  ISSN={},
  month={April},}

@INPROCEEDINGS{8760965,
  author={Cavalcante, Maria Gerliane and Sales, José Iranildo},
  booktitle={2019 14th Iberian Conference on Information Systems and Technologies (CISTI)}, 
  title={The Behavior Driven Development Applied to the Software Quality Test:}, 
  year={2019},
  volume={},
  number={},
  pages={1-4},
  abstract={The mechanisms of private investment in Brazilian sport are one of the main sources of resources that athletes hold to promote participation in high-performance competitions. In order to improve this flow, softwares are designed to shorten the path between donors and recipients. These tools, such as “Meu Campeãu” used in this paper, require efficient analysis of the quality of the product offered. Concepts known as agile methodologies have brought a new style to the analysis of the software developed, and one of them has gained considerable prominence, known such as BDD (Behavior Driven Development), so this paper aims to analyze the implementation of Behavior Driven Development in the software quality verification process “Meu Campeãu”.},
  keywords={Testing;Business;Sports;Software quality;Tools;Games;Behavior Driven Development;Software Quality Test;Sports Financing},
  doi={10.23919/CISTI.2019.8760965},
  ISSN={2166-0727},
  month={June},}

@INPROCEEDINGS{7363910,
  author={Richardet, Renaud and Chappelier, Jean-Cédric and Tripathy, Shreejoy and Hill, Sean},
  booktitle={2015 IEEE International Conference on Big Data (Big Data)}, 
  title={Agile text mining with Sherlok}, 
  year={2015},
  volume={},
  number={},
  pages={1479-1484},
  abstract={The successful development of an intelligent text mining application requires the collaboration of two main stakeholders: subject matter experts and text miners. In this paper, we describe a new methodology, agile text mining to improve that collaboration. Agile text mining is characterized by short development cycles, frequent tasks redefinition and continuous performance monitoring through integration tests. We introduce Sherlok, a system supporting the development of agile text mining applications and present an application to extract mention of neurons from a very large corpus of scientific articles. The resulting code and models are publicly available.},
  keywords={Text mining;Pipelines;Ontologies;Engines;Proteins;Collaboration;natural language processing;text mining;big data;UIMA;agile data science},
  doi={10.1109/BigData.2015.7363910},
  ISSN={},
  month={Oct},}

@INPROCEEDINGS{6405436,
  author={Lauret, Jimmy and Waeselynck, Helene and Fabre, Jean-Charles},
  booktitle={2012 IEEE 23rd International Symposium on Software Reliability Engineering Workshops}, 
  title={Detection of Interferences in Aspect-Oriented Programs Using Executable Assertions}, 
  year={2012},
  volume={},
  number={},
  pages={165-170},
  abstract={Aspect-oriented programming (AOP) is a technique that promotes separation of concerns. Unfortunately, it still suffers from well-known composition issues, in particular from undesirable interferences when multiple concerns are applied at the same join point. In this paper we propose an approach to detect interferences side effect using executable assertions. The assertions are inserted in the aspect chain to detect various types of interferences. The implementation is based on the AIRIA resolver construct, recently introduced to better control conflicting aspects in AspectJ. Resolvers add observation points that were lacking in AspectJ. We propose to take advantage of this to implement automated detection of interferences at execution time. We study the feasibility of this approach and demonstrate it on artificial examples.},
  keywords={Interference;Monitoring;Instruments;Weaving;Encryption;Data structures;Programming;Aspect interference;executable assertions;verification},
  doi={10.1109/ISSREW.2012.34},
  ISSN={},
  month={Nov},}

@INPROCEEDINGS{8502650,
  author={Winkler, Dietmar and Meixner, Kristof and Biffl, Stefan},
  booktitle={2018 IEEE 23rd International Conference on Emerging Technologies and Factory Automation (ETFA)}, 
  title={Towards Flexible and Automated Testing in Production Systems Engineering Projects}, 
  year={2018},
  volume={1},
  number={},
  pages={169-176},
  abstract={Automated and systematic testing of automation systems (AS) and production systems (PS) require an integrated testing tool chain for test case development, execution and reporting. In practice, the test automation tool chain cannot be fully automated because of missing links between different tools used in the test automation process. Closing these gaps typically require (high) human effort. Furthermore, domain and software testing expertise is often bundled by one (expensive) engineer who is responsible for the application domain (reflected in use cases and test cases) and software tests (software test code). This paper presents a flexible Testing Automation Framework (TAF) that enables the configuration of test processes involving different tools and various layers for test automation and enables separated roles for the application domain and software tests. We build on best-practice test automation from Software Engineering and design a test automation process for the automation systems domain. We demonstrate the feasibility with a use case, derived from production systems automation, with selected tools covering all test automation layers. First results showed the feasibility of the framework in the evaluation use case making test processes more flexible and automated. Although the successful implementation of the TAF can support the efficient configuration and execution of test processes, there is additional effort for preparing the flexible and automated tool chain.},
  keywords={Automation;Tools;Testing;Software;Production systems;Unified modeling language;Software engineering;software and system testing;test automation framework;automation systems;production systems;test configuration;feasibility study},
  doi={10.1109/ETFA.2018.8502650},
  ISSN={1946-0759},
  month={Sep.},}

@INPROCEEDINGS{7102608,
  author={Jensen, Simon Holm and Thummalapenta, Suresh and Sinha, Saurabh and Chandra, Satish},
  booktitle={2015 IEEE 8th International Conference on Software Testing, Verification and Validation (ICST)}, 
  title={Test Generation from Business Rules}, 
  year={2015},
  volume={},
  number={},
  pages={1-10},
  abstract={Enterprise applications are difficult to test because their intended functionality is either not described precisely enough or described in cumbersome business rules. It takes a lot of effort on the part of a test architect to understand all the business rules and design tests that "cover" them, i.e., exercise all their constituent scenarios. Part of the problem is that it takes a complicated set up sequence to drive an application to a state in which a business rule can even fire. In this paper, we present a business rule modeling language that can be used to capture functional specification of an enterprise system. The language makes it possible to build tool support for rule authoring, so that obvious deficiencies in rules can be detected mechanically. Most importantly, we show how to mechanically generate test sequences--i.e., test steps and test data--needed to exercise these business rules. To this end, we translate the rules into logical formulae and use constraint solving to generate test sequences. One of our contributions is to overcome scalability issues in this process, and we do this by using a novel algorithm for organizing search through the space of candidate sequences to discover covering sequences. Our results on three case studies show the promise of our approach.},
  keywords={Business;Databases;Testing;Syntactics;Algorithm design and analysis;Systematics;Context},
  doi={10.1109/ICST.2015.7102608},
  ISSN={2159-4848},
  month={April},}

@ARTICLE{9606196,
  author={Sankaran, Ganesh C. and Sivalingam, Krishna M. and Gondaliya, Harsh},
  journal={IEEE Internet of Things Journal}, 
  title={P4 and NetFPGA-Based Secure In-Network Computing Architecture for AI-Enabled Industrial Internet of Things}, 
  year={2023},
  volume={10},
  number={4},
  pages={2979-2994},
  abstract={This article proposes a secure in-network computing system based on a simple reduced instruction set architecture, which can be used for processing artificial intelligence and machine learning models in network devices, in an AI-based Industrial Internet of Things (IoT) system. The architecture exploits the capabilities of upcoming generations of packet processing pipelines in programmable network switches. This instruction set enables processing of data at multiple terabits-per-second, which is beyond the processing power of current servers. Instructions for regular expressions, basic arithmetic, and logical operations are defined as a proof of concept. A packet containing both instruction and data blocks is presented as an input to the pipeline by bundling both the function and its arguments into the packet. The primary challenge in opening up network switches for executing a user-defined code is security. In this context, this article presents a secure execution model (SEM), which provides additional levels of security by deliberately disallowing memory allocation and modifications to persistent state of the network switch. Furthermore, real-life use cases are presented in this article to demonstrate the utility of the proposed instruction set architecture, as also applicable to IoT data processing. This instruction set is implemented in the programming protocol-independent packet processors language, verified on a mininet-based software switch and demonstrated on Xilinx NetFPGA SUME boards. The performance results show line rate packet processing with zero packet loss, at 10 Gb/s, and average packet latency of 3.66  ${\mu }\text{s}$ .},
  keywords={Computer architecture;Security;Computational modeling;Hardware;Codes;Active networking;Pipelines;Artificial intelligence and machine learning (AI/ML) techniques;Industrial Internet of Things (IoT) systems;industry 40;in-network aggregation;in-network computing (INC) system;instruction set architecture;IoT data processing;NetFPGA SUME implementation;programming protocol-independent packet processors (P4) language},
  doi={10.1109/JIOT.2021.3125862},
  ISSN={2327-4662},
  month={Feb},}

@INPROCEEDINGS{7589821,
  author={Tang, C.M. and Keung, Jacky and Yu, Y.T. and Chan, W.K.},
  booktitle={2016 IEEE International Conference on Software Quality, Reliability and Security (QRS)}, 
  title={DFL: Dual-Service Fault Localization}, 
  year={2016},
  volume={},
  number={},
  pages={412-422},
  abstract={In engineering a service, software developers often construct and deploy a newer (forthcoming) version of the service to replace the current version. A forthcoming version is often placed online for users to consume and report feedback. In the case of observed failures, the forthcoming version should be debugged and further evolved. In this paper, we propose the model of dual-service fault localization (DFL) to aid this evolution process. Many prior research studies on spectrum-based fault localization (SBFL) consider each version separately. The DFL model correlates the dynamic execution spectra of the current and the forthcoming versions of the same service placed for live test of the forthcoming version, and dynamically generates an adaptive fault localization formula to estimate the code regions in the forthcoming service responsible for the observed failures. We report an experiment in which we initialized the DFL model into six instances, each using an ensemble technique dynamically composed from 11 existing SBFL formulas, and applied the model to four benchmarks. The results show that DFL is feasible and multiple instances are statistically more effective than, if not as effective as, the best of these individual SBFL formulas on each benchmark.},
  keywords={Software;Debugging;Computer bugs;Adaptation models;Benchmark testing;Production;Companies;debugging;spectrum-based fault localization;ensemble techniques;dual-service fault localization},
  doi={10.1109/QRS.2016.53},
  ISSN={},
  month={Aug},}

@INPROCEEDINGS{8712045,
  author={Joaquin, Montero and Alexander, Atzberger and Matthias, Bleckmann and Jens, Holtmannspötter and Kristin, Paetzold},
  booktitle={2019 IEEE 10th International Conference on Mechanical and Intelligent Manufacturing Technologies (ICMIMT)}, 
  title={Enhancing the Additive Manufacturing process for spare parts by applying Agile Hardware Development principles}, 
  year={2019},
  volume={},
  number={},
  pages={109-116},
  abstract={The availability of spare parts can turn into an issue, when those components are out of stock and cannot be reproduced easily. In this article, the German Federal Armed Forces serves as a prime example, since the equipment used is several decades old and due to that extended lifespan, its availability can turn into a significant problem, especially when the original documentation is unavailable. This article further develops the design process of manufacturing spare parts by means of additive manufacturing (AM), using a component of the military industry as a case study. The design of spare parts by means of AM is a complex task, which designers must face and in order to encounter the issues arising, it is possible to use principles and practices of agile hardware development to clarify and promote the robustness of that process. Using the concept of iterations and increments, the traceability and the quality of the spare parts produced is improved as well as the readiness of the overall process.},
  keywords={Three-dimensional printing;Uncertainty;Hardware;Metals;Three-dimensional displays;Additive Manufacturing;Design Processes;Design for Additive Manufacturing;Agile Hardware Development;Selective Laser Melting;Metal AM},
  doi={10.1109/ICMIMT.2019.8712045},
  ISSN={},
  month={Feb},}

@INPROCEEDINGS{9502466,
  author={Wolschke, Christian and Sangchoolie, Behrooz and Simon, Jacob and Marksteiner, Stefan and Braun, Tobias and Hamazaryan, Hayk},
  booktitle={2021 51st Annual IEEE/IFIP International Conference on Dependable Systems and Networks Workshops (DSN-W)}, 
  title={SaSeVAL: A Safety/Security-Aware Approach for Validation of Safety-Critical Systems}, 
  year={2021},
  volume={},
  number={},
  pages={27-34},
  abstract={Increasing communication and self-driving capabilities for road vehicles lead to threats which could potentially be exploited by attackers. Especially attacks leading to safety violations have to be identified to address them by appropriate measures. The impact of an attack depends on the threat exploited, potential countermeasures and the traffic situation. In order to identify such attacks and to use them for testing, we propose the systematic approach SaSeVAL for deriving attacks of autonomous vehicles.SaSeVAL is based on threats identification and safety-security analysis. The impact of automotive use cases to attacks is considered. The threat identification considers the attack interface of vehicles and classifies threat scenarios according to threat types, which are then mapped to attack types. The safety-security analysis identifies the necessary requirements which have to be tested based on the architecture of the system under test. It determines which safety impact a security violation may have, and in which traffic situations the highest impact is expected. Finally, the results of threat identification and safety-security analysis are used to describe attacks.The goal of SaSeVAL is to achieve safety validation of the vehicle w.r.t. security concerns. It traces safety goals to threats and to attacks explicitly. Hence, the coverage of safety concerns by security testing is assured. Two use cases of vehicle communication and autonomous driving are investigated to prove the applicability of the approach.},
  keywords={Space vehicles;Privacy;Systematics;Road vehicles;Bandwidth;Safety;Security;safety;security testing;attack description;threats;threat library;risk assessment},
  doi={10.1109/DSN-W52860.2021.00016},
  ISSN={2325-6664},
  month={June},}

@INPROCEEDINGS{1687602,
  author={Caruso, F. and Milham, D. and Orobec, S.},
  booktitle={2006 IEEE/IFIP Network Operations and Management Symposium NOMS 2006}, 
  title={Emerging industry standard for managing next generation transport networks: TMF MTOSI}, 
  year={2006},
  volume={},
  number={},
  pages={1-15},
  abstract={There are enormous business benefits to being able to separate the business logic from the massive technical complexity at the network level. One of the greatest challenges to being able to achieve this abstraction at the OS level has been the requirement to communicate and manage many different sets of vendor technologies. The main inhibitor to overcoming this challenge has been the lack of standards supporting both the interface from an OS to an EMS and also that between OSs. Building upon the successful Multi Technology Network Management (MTNM) CORBA/IDL interface, MTOSI has extended MTNM work to support XML/Web Service interactions between various types of Operations Systems. MTOSI bridged the standard gap by defining a methodology and a framework to map the domain specific business activities into well defined TMF NGOSS contracts according to the Service Oriented Architecture principles. This session introduces the key aspects of MTOSI and presents a real use case of MTOSI in BT.},
  keywords={Next generation networking;SOA;MDA;Web Services;TMF NGOSS;MTOSI;MTNM},
  doi={10.1109/NOMS.2006.1687602},
  ISSN={2374-9709},
  month={April},}

@INPROCEEDINGS{6614319,
  author={Saadatmand, Mehrdad and Sjödin, Mikael},
  booktitle={2013 10th International Conference on Information Technology: New Generations}, 
  title={On Combining Model-Based Analysis and Testing}, 
  year={2013},
  volume={},
  number={},
  pages={260-266},
  abstract={Testing a computer system is a challenging task, both due to the large number of possible test cases and the limited resources allocated for testing activities. This means that only a subset of all possible test cases can be chosen to test a system, and therefore the decision on the selection of test cases becomes important. The result of static analysis of a system can be used to help with this decision, in the context of model-based development of systems, this means that the analysis performed on a system model can be used to prioritize and guide the testing efforts. Furthermore, since models allow expression of non-functional requirements (such as performance, timing and security), model-guided testing can be used to direct testing towards specific parts of the system which have large impact on such requirements. In this paper, we focus on modeling and trade-off analysis of non-functional requirements and how static analysis helps to identify problematic parts of a system and thus guide the selection of test cases to target such parts.},
  keywords={Unified modeling language;Testing;Analytical models;Software;Security;Timing;Batteries;Model-based development;static analysis;model-based testing;non-functional requirements;test-case prioritization},
  doi={10.1109/ITNG.2013.42},
  ISSN={},
  month={April},}

@INPROCEEDINGS{10549613,
  author={Goldstein, Harrison and Cutler, Joseph W. and Dickstein, Daniel and Pierce, Benjamin C. and Head, Andrew},
  booktitle={2024 IEEE/ACM 46th International Conference on Software Engineering (ICSE)}, 
  title={Property-Based Testing in Practice}, 
  year={2024},
  volume={},
  number={},
  pages={2307-2319},
  abstract={Property-based testing (PBT) is a testing methodology where users write executable formal specifications of software components and an automated harness checks these specifications against many automatically generated inputs. From its roots in the QuickCheck library in Haskell, PBT has made significant inroads in mainstream languages and industrial practice at companies such as Amazon, Volvo, and Stripe. As PBT extends its reach, it is important to understand how developers are using it in practice, where they see its strengths and weaknesses, and what innovations are needed to make it more effective. We address these questions using data from 30 in-depth interviews with experienced users of PBT at Jane Street, a financial technology company making heavy and sophisticated use of PBT. These interviews provide empirical evidence that PBT's main strengths lie in testing complex code and in increasing confidence beyond what is available through conventional testing methodologies, and, moreover, that most uses fall into a relatively small number of high- leverage idioms. Its main weaknesses, on the other hand, lie in the relative complexity of writing properties and random data generators and in the difficulty of evaluating their effectiveness. From these observations, we identify a number of potentially high-impact areas for future exploration, including performance improvements, differential testing, additional high-leverage testing scenarios, better techniques for generating random input data, test-case reduction, and methods for evaluating the effectiveness of tests.},
  keywords={Technological innovation;Companies;Writing;Software;Libraries;Generators;Formal specifications;property-based testing;random testing;human-centered research},
  doi={10.1145/3597503.3639581},
  ISSN={1558-1225},
  month={April},}

@ARTICLE{9896170,
  author={Kotti, Zoe and Gousios, Georgios and Spinellis, Diomidis},
  journal={IEEE Transactions on Software Engineering}, 
  title={Impact of Software Engineering Research in Practice: A Patent and Author Survey Analysis}, 
  year={2023},
  volume={49},
  number={4},
  pages={2020-2038},
  abstract={Existing work on the practical impact of software engineering (SE) research examines industrial relevance rather than adoption of study results, hence the question of how results have been practically applied remains open. To answer this and investigate the outcomes of impactful research, we performed a quantitative and qualitative analysis of 4 354 SE patents citing 1 690 SE papers published in four leading SE venues between 1975–2017. Moreover, we conducted a survey on 475 authors of 593 top-cited and awarded publications, achieving 26% response rate. Overall, researchers have equipped practitioners with various tools, processes, and methods, and improved many existing products. SE practice values knowledge-seeking research and is impacted by diverse cross-disciplinary SE areas. Practitioner-oriented publication venues appear more impactful than researcher-oriented ones, while industry-related tracks in conferences could enhance their impact. Some research works did not reach a wide footprint due to limited funding resources or unfavorable cost-benefit trade-off of the proposed solutions. The need for higher SE research funding could be corroborated through a dedicated empirical study. In general, the assessment of impact is subject to its definition. Therefore, academia and industry could jointly agree on a formal description to set a common ground for subsequent research on the topic.},
  keywords={Software;Patents;Industries;Companies;Software engineering;Interviews;Collaboration;Software engineering;practical impact;empirical study;survey;patent citations},
  doi={10.1109/TSE.2022.3208210},
  ISSN={1939-3520},
  month={April},}

@INPROCEEDINGS{7966874,
  author={Souza, Rodrigo and Oliveira, Allan},
  booktitle={2017 IEEE/ACM 39th International Conference on Software Engineering: New Ideas and Emerging Technologies Results Track (ICSE-NIER)}, 
  title={GuideAutomator: Continuous Delivery of End User Documentation}, 
  year={2017},
  volume={},
  number={},
  pages={31-34},
  abstract={User guides, also known as user manuals, are a type of documentation aimed at helping a user operate a specific system. For software systems, user guides usually include screenshots that show users how to interact with the user interface. Because creating such screenshots is a slow, manual process, keeping the user guide up-to-date with changes in the user interface is challenging. We propose an approach in which the documentation writer interleaves the user guide text with source code that automates screen capturing. As a result, screenshots always reflect the latest software version, which makes the approach suitable for a project that uses continuous delivery. The approach was implemented as a prototype, called GuideAutomator.},
  keywords={Documentation;Tools;Software;Browsers;Cascading style sheets;Programming;software documentation;automated documentation generator;literate programming;continuous delivery},
  doi={10.1109/ICSE-NIER.2017.10},
  ISSN={},
  month={May},}

@INPROCEEDINGS{1321646,
  author={Hau Lam},
  booktitle={IEEE/CPMT/SEMI 29th International Electronics Manufacturing Technology Symposium (IEEE Cat. No.04CH37585)}, 
  title={New design-to-test software strategies accelerate time-to-market}, 
  year={2004},
  volume={},
  number={},
  pages={140-143},
  abstract={Today's growing device complexity and new manufacturing requirements have presented significant challenges for manufacturers looking to speed time-to-market. One such challenge is the need to contain test costs, of which a major component is the time and resources required for test program development. Some test development tools that exist today can translate a device's functional events and scan patterns into test programs for targeted ATE. Identification and specification of critical timing parameters that require conversion into cycle-based ATE formats have become an increasing cost factor, which can also significantly impact test accuracy. Traditionally, timing specifications from microprocessor and IP cores, multiple bus types, and other device components can be established via published timing specifications and by a manageable, iterative process between design and test engineering. Likewise, automatic test pattern generation tools for structural test can address simple timing, and are capable of generating cycle-based timing. Today's complex SoC may consist of over 60 IP cores made more complicated by increased challenges from high-speed serial bus technology and multiple-time domain designs. Further complicating test program development is the need for compatibility with multiple ATE platforms to accommodate global manufacturing strategies. Next generation design-to-test software tools have to address these factors to help reduce the ever growing cost-of-test. Tools must support standard industry test languages such as standard test interface language (STIL), support both functional events and scan patterns, and validate outputs to ensure first-pass success of test programs pre- and post silicon, across multiple ATE platforms.},
  keywords={Software design;Acceleration;Time to market;Testing;Timing;Manufacturing;Costs;Automatic test pattern generation;Microprocessors;Engineering management},
  doi={10.1109/IEMT.2004.1321646},
  ISSN={1089-8190},
  month={July},}

@INPROCEEDINGS{114002,
  author={Bonet, L. and Ganger, J. and Girardeau, J. and Greaves, C. and Pendleton, M. and Yatim, D.},
  booktitle={Proceedings. International Test Conference 1990}, 
  title={Test features of the MC145472 ISDN U-transceivers}, 
  year={1990},
  volume={},
  number={},
  pages={68-79},
  abstract={The design of a single-chip implementation of a 2B1Q ISDN (integrated services digital network) U transceiver that meets the ANSI T1.601 standards has been completed. The MC145472 was designed with testability in mind and to be consistent with Motorola's design-for-manufacturability goals. The authors describe in detail the design-for-testability techniques specifically intended for the IC manufacturer production test and other ad hoc test/diagnostic structures for the customer to use in evaluating system performance. A global test strategy for testing the ISDN U transceiver is presented. The test features have been used extensively not only for testing the device in the production environment but also for conducting evaluations and design verification experiments during the chip debugging phase. The test features described are well integrated with the architecture of the chip, thus minimizing incremental cost.<>},
  keywords={ISDN;Transceivers;Integrated circuit testing;System testing;ANSI standards;Manufacturing;Production systems;System performance;Debugging;Costs},
  doi={10.1109/TEST.1990.114002},
  ISSN={},
  month={Sep.},}

@ARTICLE{5389336,
  author={Wile, B. and Mullen, M. P. and Hanson, C. and Bair, D. G. and Lasko, K. M. and Duffy, P. J. and Kaminski, E. J. and Gilbert, T. E. and Licker, S. M. and Sheldon, R. G. and Wollyung, W. D. and Lewis, W. J. and Adkins, R. J.},
  journal={IBM Journal of Research and Development}, 
  title={Functional verification of the CMOS S/390 Parallel Enterprise Server G4 system}, 
  year={1997},
  volume={41},
  number={4.5},
  pages={549-566},
  abstract={Verification of the S/390® Parallel Enterprise Server G4 processor and level 2 cache (L2) chips was performed using a different approach than previously. This paper describes the methods employed by our functional verification team to demonstrate that its logical system complied with the S/390 architecture while staying within the changing cost structure and time-to-market constraints. Verification proceeded at four basic levels defined by the breadth of logic being tested. The lowest level, designer macro verification, contained a single designer's hardware description language (in VHDL). Unit-level verification consisted of a logical portion of function that generally contained four or five designers' logic. The third level of verification was the chip level, in which the processor or L2 chips were individually tested. Finally, system-level verification was performed on symmetric multiprocessor (SMP) configurations that included bus-switching network (BSN) chips and I/O connection chips, designated as memory bus adaptors (MBAs), along with multiple copies of the processor and L2 chips.},
  keywords={},
  doi={10.1147/rd.414.0549},
  ISSN={0018-8646},
  month={July},}

@INPROCEEDINGS{9589614,
  author={Käyrä, Matti and Hämäläinen, Timo D.},
  booktitle={IECON 2021 – 47th Annual Conference of the IEEE Industrial Electronics Society}, 
  title={A Survey on System-on-a-Chip Design Using Chisel HW Construction Language}, 
  year={2021},
  volume={},
  number={},
  pages={1-6},
  abstract={This paper presents a survey of functional programming languages in System-on-a-Chip (SoC) design. The motivation is improving the design productivity by better source code expressiveness, increased abstraction level in design entry, or improved automation. The survey focuses on Chisel that is one of the most potential High Level Language (HLL) based design frameworks. We include 26 papers that report implementations ranging from IP blocks to complete chips. The result is that functional programming languages are viable for SoC design and can also be deployed in production use. However, Chisel does not increase the abstraction level in a similar way as High Level Synthesis (HLS), since it is used to create circuit generators instead of direct descriptions. Additional benefit is that Chisel offloads user effort from control and connectivity structures, and makes reusability and configurability improved over traditional Hardware Description Language (HDL) designs.},
  keywords={Productivity;Industrial electronics;Codes;Tools;Generators;System-on-chip;Functional programming;Survey;RTL Design;HDL;Chisel;FPGA;ASIC},
  doi={10.1109/IECON48115.2021.9589614},
  ISSN={2577-1647},
  month={Oct},}

@INPROCEEDINGS{8491148,
  author={Jorge, Dalton N. and Machado, Patrícia D. L. and Alves, Everton L. G. and Andrade, Wilkerson L.},
  booktitle={2018 IEEE 26th International Requirements Engineering Conference (RE)}, 
  title={Integrating Requirements Specification and Model-Based Testing in Agile Development}, 
  year={2018},
  volume={},
  number={},
  pages={336-346},
  abstract={In agile development, Requirements Engineering (RE) and testing have to cope with a number of challenges such as continuous requirement changes and the need for minimal and manageable documentation. In this sense, extensive research has been conducted to automatically generate test cases from (structured) natural language documents using Model-Based Testing (MBT). However, the imposed structure may impair agile practices or test case generation. In this paper, inspired by cooperation with industry partners, we propose CLARET, a notation that allows the creation of use case specifications using natural language to be used as central artifacts for both RE and MBT practices. A tool set supports CLARET specification by checking syntax of use cases structure as well as providing visualization of flows for use case revisions. We also present exploratory studies on the use of CLARET to create RE documents as well as on their use as part of a system testing process based on MBT. Results show that, with CLARET, we can document use cases in a cost-effective way. Moreover, a survey with professional developers shows that CLARET use cases are easy to read and write. Furthermore, CLARET has been successfully applied during specification, development and testing of industrial applications.},
  keywords={Testing;Industries;Requirements engineering;Tools;Natural languages;Manuals;Syntactics;Agile Development;Requirements Engineering;Model-Based Testing;Use case specification},
  doi={10.1109/RE.2018.00041},
  ISSN={2332-6441},
  month={Aug},}

@INPROCEEDINGS{8816743,
  author={Yang, Aidan Z.H. and Alencar da Costa, Daniel and Zou, Ying},
  booktitle={2019 IEEE/ACM 16th International Conference on Mining Software Repositories (MSR)}, 
  title={Predicting Co-Changes between Functionality Specifications and Source Code in Behavior Driven Development}, 
  year={2019},
  volume={},
  number={},
  pages={534-544},
  abstract={Behavior Driven Development (BDD) is an agile approach that uses. feature files to describe the functionalities of a software system using natural language constructs (English-like phrases). Because of the English-like structure of. feature files, BDD specifications become an evolving documentation that helps all (even non-technical) stakeholders to understand and contribute to a software project. After specifying a. feature files, developers can use a BDD tool (e.g., Cucumber) to automatically generate test cases and implement the code of the specified functionality. However, maintaining traceability between. feature files and source code requires human efforts. Therefore,. feature files can be out-of-date, reducing the advantages of using BDD. Furthermore, existing research do not attempt to improve the traceability between. feature files and source code files. In this paper, we study the co-changes between. feature files and source code files to improve the traceability between. feature files and source code files. Due to the English-like syntax of. feature files, we use natural language processing to identify co-changes, with an accuracy of 79%. We study the characteristics of BDD co-changes and build random forest models to predict when a. feature files should be modified before committing a code change. The random forest model obtains an AUC of 0.77. The model can assist developers in identifying when a. feature files should be modified in code commits. Once the traceability is up-to-date, BDD developers can write test code more efficiently and keep the software documentation up-to-date.},
  keywords={Software;Java;Stakeholders;Documentation;Testing;Registers;Tools;Behavior Driven Development;Traceability;Co-Changes;Empirical Software Engineering},
  doi={10.1109/MSR.2019.00080},
  ISSN={2574-3864},
  month={May},}

@INPROCEEDINGS{5985928,
  author={Siegl, Sebastian and Hielscher, Kai-Steffen and German, Reinhard and Berger, Christian},
  booktitle={2011 12th Latin American Test Workshop (LATW)}, 
  title={Automated testing of embedded automotive systems from requirement specification models}, 
  year={2011},
  volume={},
  number={},
  pages={1-6},
  abstract={Embedded software for modern automotive and avionic systems is increasingly complex. In early design phases, even when there is still uncertainty about the feasibility of the requirements, valuable information can be gained from models that describe the expected usage and the desired system reaction. The generation of test cases from these models indicates the feasibility of the intended solution and helps to identify scenarios for which the realization is hardly feasible or the intended system behavior is not properly defined. In this paper we present the formalization of requirements by models to simulate the expected field usage of a system. These so called usage models can be enriched by information about the desired system reaction. Thus, they are the basis for all subsequent testing activities: First, they can be used to verify the first implementation models and design decisions w.r.t. the fulfillment of requirements and second, test cases can be derived in a random or statistic manner. The generation can be controlled with operational profiles that describe different classes of field usage. We have applied our approach at a large German car manufacturer in the early development phase of active safety functionalities. Test cases were generated from the usage models to assess the implementation models in MATLAB/Simulink. The parametrization of the systems could be optimized and a faulty transition in the implementation models was revealed. These design and implementation faults had not been discovered with the established test method.},
  keywords={Safety;Testing;Timing;MATLAB;Belts;Clocks},
  doi={10.1109/LATW.2011.5985928},
  ISSN={2373-0862},
  month={March},}

@ARTICLE{9852775,
  author={Grandmaison, Arnaud de and Heydemann, Karine and Meunier, Quentin L.},
  journal={IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems}, 
  title={ARMISTICE: Microarchitectural Leakage Modeling for Masked Software Formal Verification}, 
  year={2022},
  volume={41},
  number={11},
  pages={3733-3744},
  abstract={Side-channel attacks are powerful attacks for retrieving secret data by exploiting physical measurements, such as power consumption or electromagnetic emissions. Masking is a popular countermeasure as it can be proven secure against an attacker model. In practice, software-masked implementations suffer from a security reduction due to a mismatch between the considered leakage sources in the security proof and the real ones, which depend on the microarchitecture. We propose ARMISTICE, a framework for formally verifying the absence of leakage in first-order masked implementations taking into account modeled microarchitectural sources of leakage. As a proof of concept, we present the modeling of an Arm Cortex-M3 core from its RTL description and leakage test vectors, as well as the modeling of the memory of an STM32F1 board, exclusively using leakage test vectors. We show that, with these models, ARMISTICE pinpoints vulnerable instructions in real-world masked implementations and helps the design of masked software implementations which are practically secure.},
  keywords={Computational modeling;Software;Registers;Codes;Security;Software algorithms;Integrated circuit modeling;Masking;microarchitectural leakage;side-channel attacks (SCA);verification},
  doi={10.1109/TCAD.2022.3197507},
  ISSN={1937-4151},
  month={Nov},}

@INPROCEEDINGS{1617602,
  author={Mingjing Chen and Haggag, H. and Orailoglu, A.},
  booktitle={24th IEEE VLSI Test Symposium}, 
  title={Decision tree based mismatch diagnosis in analog circuits}, 
  year={2006},
  volume={},
  number={},
  pages={6 pp.-285},
  abstract={Mismatch is a critical consideration in analog circuit design. Knowledge of mismatch locations and an understanding of their impact on circuit performance are crucial for design optimization and process improvement. We present a circuit level mismatch diagnosis methodology in this paper. The functional parameters with abnormal values are measured as manifestations of mismatch, from which reverse tracing is employed to determine the mismatch source. The methodology is implemented on a representative benchmark and its efficiency confirmed by simulation results.},
  keywords={Decision trees;Analog circuits;Design optimization;Circuit simulation;Fabrication;Degradation;Design automation;Predictive models;Circuit testing;Circuit optimization},
  doi={10.1109/VTS.2006.26},
  ISSN={2375-1053},
  month={April},}

@INPROCEEDINGS{7128891,
  author={Wanderley, Fernando and Silva, Antonio and Araújo, João},
  booktitle={2015 IEEE 9th International Conference on Research Challenges in Information Science (RCIS)}, 
  title={Evaluation of BehaviorMap: A user-centered behavior language}, 
  year={2015},
  volume={},
  number={},
  pages={309-320},
  abstract={In the software development process, one of the recurring problems is to ensure that the expectations of stakeholders are being met. These expectations must match the system's behavior and be present in the requirements specifications and models. The Requirements Engineering discipline studies how to capture, specify, validate and manage requirements. However, recent empirical studies show that stakeholders do not usually understand traditional requirements models. This paper focuses on the cognitive evaluation of a user-centered language called BehaviorMap that aims to specify behavioral user scenarios in a cognitive way, based on mind map modelling. This paper describes an experimental evaluation to verify the understandability of the BehaviorMap scenarios compared to the textual ones. The experiment gathered data from 15 individuals (naïve-users), with different backgrounds, that had to analyze 8 scenarios, being 4 graphical and 4 textual. To assess the participants' cognitive effort, it was used questionnaires. Also, the time effort to perform the tasks was measured. This experiment showed promising results for the BehaviorMap scenarios.},
  keywords={Data structures;Boolean functions;Visualization;Software;Atmospheric measurements;Particle measurements;Agile Requirements;Mind Map Modelling;Behavior-Driven Design;User-Centred Requirements;Cognitive Effort},
  doi={10.1109/RCIS.2015.7128891},
  ISSN={2151-1357},
  month={May},}

@ARTICLE{9622288,
  author={Leroy, Dorian and Sallou, June and Bourcier, Johann and Combemale, Benoit},
  journal={Computer}, 
  title={When Scientific Software Meets Software Engineering}, 
  year={2021},
  volume={54},
  number={12},
  pages={60-71},
  abstract={We investigate the different levels of abstraction, linked to the diverse artifacts of the scientific software development process, that a software language can propose and the validation and verification facilities associated with the corresponding level of abstraction the language can provide to the user.},
  keywords={Software;Software engineering},
  doi={10.1109/MC.2021.3102299},
  ISSN={1558-0814},
  month={Dec},}

@INPROCEEDINGS{5655567,
  author={Wieczorek, Sebastian and Stefanescu, Alin and Roth, Andreas},
  booktitle={2010 Seventh International Conference on the Quality of Information and Communications Technology}, 
  title={Model-Driven Service Integration Testing - A Case Study}, 
  year={2010},
  volume={},
  number={},
  pages={292-297},
  abstract={This paper presents a case study for the modeling and model-based testing (MBT) of enterprise service choreographies. Our proposed MBT approach uses proprietary models called Message Choreography Models (MCM) as test models. The case study illustrates how MCM-based service integration testing allows to formalize design decisions and enables full integration into an existing industrial test infrastructure by using the concepts of domain specific languages and model transformations. Further, the MBT tools integrated into the testing framework have been compared based on one concrete use case.},
  keywords={Testing;Business;Service oriented architecture;Generators;Unified modeling language;Data models;Context;Model-based Testing;Enterprise Systems;Service-oriented Architecture;Case Study;Service Choreographies},
  doi={10.1109/QUATIC.2010.49},
  ISSN={},
  month={Sep.},}

@INPROCEEDINGS{6037562,
  author={Agaram, Mukundan K. and Liu, Chang},
  booktitle={2011 IEEE 15th International Enterprise Distributed Object Computing Conference}, 
  title={An Engine-Independent Framework for Business Rules Development}, 
  year={2011},
  volume={},
  number={},
  pages={75-84},
  abstract={There is a compelling need for highly customized Domain Specific Languages and Business Vocabulary in certain industries such as insurance, mortgage, and finance to enable Knowledge Workers to articulate and to automate complex rules pertinent to their areas of function within their companies. Rule Engine vendors attempt to provide a solution to the problem by selling an integrated Rules Engine and Business Rules Management System. Usually, the BRMS's provided by vendors need to be customized and integrated into the overall Enterprise Architecture. This results in the Enterprise Architecture to be tightly coupled with the vendor's rule offering. Moreover, it poses a significant risk to the Enterprise as vendor solutions change between releases. The Enterprise Architecture needs a way to insulate itself from such impacts. This paper describes a framework that delivers the core BRMS functions of authoring and representation in a vendor neutral fashion. In addition, the paper sheds light on specific areas of the framework that can be standardized.},
  keywords={Vocabulary;Business;Engines;Production;Dentistry;Computer architecture;Syntactics;Business Rules;Business Rules Languages;Business Rule Components;Business Vocabulary;Domain Specific Languages},
  doi={10.1109/EDOC.2011.20},
  ISSN={1541-7719},
  month={Aug},}

@ARTICLE{7389278,
  author={Bellucci, Andrea and Romano, Marco and Aedo, Ignacio and Díaz, Paloma},
  journal={IEEE Pervasive Computing}, 
  title={Software Support for Multitouch Interaction: The End-User Programming Perspective}, 
  year={2016},
  volume={15},
  number={1},
  pages={78-86},
  abstract={The hardware development of the past years favored the widespread diffusion of multitouch devices (such as smartphones, tablets, and interactive tabletops) to such an extent that a wide variety of users are now exploiting them to perform different activities on a daily basis. In the heterogeneous and manifold context of modern computation, it is impossible to predict, at design time, all the possible configurations of such technologies, and especially the way users will be willing to interact with them. Therefore, empowering end users with tools for developing multitouch interaction is a promising step toward the materialization of ubiquitous computing. The aim of this survey is to frame the state of the art of existing multitouch software development tools from an end-user programming (EUP) perspective.},
  keywords={Software tools;Programming profession;Graphics;Complexity theory;Sensors;coding tools and techniques;ubiquitous computing;end-user programming;pervasive computing;software engineering;graphics;mobile},
  doi={10.1109/MPRV.2016.3},
  ISSN={1558-2590},
  month={Jan},}

@INPROCEEDINGS{8900962,
  author={Kritzinger, Lisa Maria and Krismayer, Thomas and Rabiser, Rick and Grünbacher, Paul},
  booktitle={2019 Working Conference on Software Visualization (VISSOFT)}, 
  title={A User Study on the Usefulness of Visualization Support for Requirements Monitoring}, 
  year={2019},
  volume={},
  number={},
  pages={56-66},
  abstract={Many requirements monitoring approaches have been proposed that check key properties of systems and their interactions at runtime. Some of these approaches also visualize monitoring results and provide details on requirements violations to end users. However, only few studies exist about the usefulness of requirements monitoring tools for practitioners, particularly regarding visualization. In this paper, we present a user study we have conducted with both industrial practitioners and researchers to assess the usefulness of visualization capabilities we have been developing for an event-based requirements monitoring tool. These capabilities allow users to monitor the status of the involved systems, to view trends and statistics, and to inspect the events and data that led to specific violations when diagnosing their root cause. We first performed a walkthrough of the tool using the cognitive dimensions of notations framework from the field of human-computer interaction. We then conducted a user study involving five software engineers of a large company from the automation software domain and four researchers. Using the tool's visualization capabilities all subjects succeeded in monitoring a real-world automation system and in diagnosing violations. Subjects regarded the visualization capabilities as essential for understanding the behavior of a complex system. Based on the study results we derive implications, opportunities, and risks of using visualization in requirements monitoring tools.},
  keywords={Monitoring;Tools;Visualization;Data visualization;Usability;Market research;requirements monitoring, visualization, systems of systems, usability study, user study},
  doi={10.1109/VISSOFT.2019.00015},
  ISSN={},
  month={Sep.},}

@INPROCEEDINGS{10556348,
  author={Weiss, Gereon and Zeller, Marc and Schoenhaar, Hannes and Fraunhofer, Christian Drabek and Kreutz, Andreas},
  booktitle={2024 IEEE/ACM 3rd International Conference on AI Engineering – Software Engineering for AI (CAIN)}, 
  title={Approach for Argumenting Safety on Basis of an Operational Design Domain}, 
  year={2024},
  volume={},
  number={},
  pages={184-193},
  abstract={The Operational Design Domain (ODD) is a representative model of the real world in which an Automated Driving System (ADS) is intended to operate. The definition of the ODD is a crucial part of the development process for such an artificial intelligence (AI)-enabled system. This is due to the fact that the ODD is the basis for several critical development activities, like defining system-level requirements, test & verification, and building a well-founded safety case for an AI-based ADS. Since an inadequately defined ODD poses a major safety concern for the entire development, an ODD must be defined completely and consistently during the development process. In this work, we present an approach for the ODD definition and maintenance during the development of safety-critical AI-based ADS functionalities and provide evidences to argue the sufficient completeness and consistency. We demonstrate the feasibility of our approach by an industrial use case of a fully automated system in the railway domain.},
  keywords={ISO Standards;Buildings;Standardization;Rail transportation;Safety;Maintenance;Artificial intelligence;Operational Design Domain;Autonomous System;Safety Assurance;AI;Machine Learning;Automated Train Operation},
  doi={},
  ISSN={},
  month={April},}

@INPROCEEDINGS{7374941,
  author={Suthar, Prakash and Stolic, Milan},
  booktitle={2015 IEEE Asia Pacific Conference on Wireless and Mobile (APWiMob)}, 
  title={Carrier grade Telco-Cloud}, 
  year={2015},
  volume={},
  number={},
  pages={101-107},
  abstract={The Telco service providers business is undergoing a fundamental shift, and operators are transforming their network to meet new business challenges. Biggest focus area for Telco Cloud is rapid time to market (TTM) for new services and reduction in total cost of ownership (TCO). Telco service providers are facing challenges from web and content providers because of agility and convergence of voice and data. Telco Cloud is transformation of traditional wireline, wireless, voice, text, data, and web etc. services to common compute cloud infrastructures. Cloud infrastructures can be set-up on-premise, off-premise or hybrid based upon service level agreements (SLA), security and maturity of services. Key component of Telco Cloud is IP Multimedia System (IMS) which provides convergence of voice, data, video, multimedia messaging etc. Designing and developing Telco Cloud, which meets criteria of “carrier grade”, is very important to gain confidence and comfort level of different stakeholders. This paper discusses design and deployment criteria for building high quality Telco Cloud.},
  keywords={Cloud computing;Business;Logic gates;Mobile communication;Hardware;Wireless communication},
  doi={10.1109/APWiMob.2015.7374941},
  ISSN={},
  month={Aug},}

@ARTICLE{9324982,
  author={Laibinis, Linas and Iliasov, Alexei and Romanovsky, Alexander},
  journal={IEEE Transactions on Reliability}, 
  title={Mutation Testing for Rule-Based Verification of Railway Signaling Data}, 
  year={2021},
  volume={70},
  number={2},
  pages={676-691},
  abstract={Industry applications of formal verification to signaling control tables require formulation of a large number of mathematical conjectures expressing verification rules. It is paramount to establish the validity and completeness of these conjectures. This article discusses a mutation-based validation technique that guides domain experts in the construction of such verification rules. Furthermore, we use genetic programming to quickly generate millions of well-formed data mutations of control tables and to synthesize mutation programs. The technique is illustrated by a synthetic running example and a discussion of our experience in using it in the industrial setting.},
  keywords={Rail transportation;Safety;Layout;Testing;Semantics;Junctions;Topology;Formal verification;mutation testing;railway;signaling;verification conditions},
  doi={10.1109/TR.2020.3047462},
  ISSN={1558-1721},
  month={June},}

@ARTICLE{5233611,
  author={Duenas, Juan C. and Ruiz, José L. and Cuadrado, Félix and Garcia, Boni and Parada G., Hugo A.},
  journal={IEEE Internet Computing}, 
  title={System Virtualization Tools for Software Development}, 
  year={2009},
  volume={13},
  number={5},
  pages={52-59},
  abstract={The configuration complexity of preproduction sites coupled with access-control mechanisms often impede the software development life cycle. Virtualization is a cost-effective way to remove such barriers and provide a test environment similar to the production site, reducing the burden in IT administrators. An Eclipse-based virtualization tool framework can offer developers a personal runtime environment for launching and testing their applications. The authors have followed a model-driven architecture (MDA) approach that integrates best-of-breed virtualization technologies, such as Xen and VDE.},
  keywords={Programming;Testing;Impedance;Production;Application virtualization;Runtime environment;Application software;virtualization;software development;distributed systems;Eclipse;model-driven architecture;MDA},
  doi={10.1109/MIC.2009.115},
  ISSN={1941-0131},
  month={Sep.},}

@INPROCEEDINGS{6405446,
  author={Carrozza, Gabriella and Faella, Mauro and Fucci, Francesco and Pietrantuono, Roberto and Russo, Stefano},
  booktitle={2012 IEEE 23rd International Symposium on Software Reliability Engineering Workshops}, 
  title={Integrating MDT in an Industrial Process in the Air Traffic Control Domain}, 
  year={2012},
  volume={},
  number={},
  pages={225-230},
  abstract={Air Traffic Control (ATC) systems are typical software-intensive mission-critical systems with stringent dependability requirements. The major providers of ATC systems are system integrators that address such requirements at the cost of a very expensive testing effort. They envisage Model Driven Testing (MDT) as a promising approach to reduce this effort while achieving better product quality. Within the context of a public-private partnership for software innovation in the ATC domain, we address the problem of integrating MDT into a software development process based on Model Driven Architecture. Specifically, we propose a solution to the integration of MDT into a V-model, focusing on a parallel MDA-MDT flow in a real industrial software process.},
  keywords={Unified modeling language;Software;Testing;Computer architecture;Adaptation models;Atmospheric modeling;Europe;MDA;MDT;Testing automation},
  doi={10.1109/ISSREW.2012.87},
  ISSN={},
  month={Nov},}

@INPROCEEDINGS{6676515,
  author={Nascimento, Amanda S. and Rubira, Cecilia M.F. and Castor, Fernando},
  booktitle={2013 IEEE 7th International Conference on Self-Adaptive and Self-Organizing Systems}, 
  title={Using CVL to Support Self-Adaptation of Fault-Tolerant Service Compositions}, 
  year={2013},
  volume={},
  number={},
  pages={261-262},
  abstract={We present a dynamic software product line to support fault-tolerant service compositions. Architectural variability is specified and resolved by Common Variability Language (CVL). CVL is a generic variability modeling language that enables the transformation of a product line model into a configured, new product model. At runtime, whenever it is necessary to determine a fault tolerance technique more adapted to the context (i.e. a new product) the correspondent product model is dynamically generated by executing CVL model-to-model transformation. Based on the comparison of the reflection model with the target product model, the adaptation process is fully automated.},
  keywords={Adaptation models;Fault tolerance;Fault tolerant systems;Software;Unified modeling language;Runtime;Quality of service;Fault-tolerant Systems;Self-Adaptation;CVL},
  doi={10.1109/SASO.2013.34},
  ISSN={1949-3681},
  month={Sep.},}

@ARTICLE{715185,
  author={},
  journal={IEEE Spectrum}, 
  title={What's ahead for design on the web}, 
  year={1998},
  volume={35},
  number={9},
  pages={53-63},
  abstract={Panel discussion: Experts: the Web offers better design collaboration and a path toward greater tool interoperability.},
  keywords={Internet;Computer aided engineering;Design engineering;Design automation;Computer science;Circuits;Process design;Web sites;Electrical engineering;Workstations},
  doi={10.1109/MSPEC.1998.715185},
  ISSN={1939-9340},
  month={Sep.},}

@INPROCEEDINGS{889571,
  author={Federici, D. and Bisgambiglia, P. and Santucci, J.-F.},
  booktitle={Proceedings IEEE International High-Level Design Validation and Test Workshop (Cat. No.PR00786)}, 
  title={High level fault simulation: experiments and results on ITC'99 benchmarks}, 
  year={2000},
  volume={},
  number={},
  pages={118-123},
  abstract={In this paper we present our approach for performing Behavioral Fault Simulation (BFS). This approach involves three main steps (i) the definition of an internal modeling of behavioral descriptions, and the determination of a fault model; (ii) the definition of a fault simulation technique; (iii) the implementation of this technique. Finally, this paper deals with experiments conducted on ITC'99 benchmarks in order to validate a VHDL behavioral fault simulator (BFS). The effectiveness of the BFS software is clearly demonstrated through the obtained results.},
  keywords={Circuit faults;Circuit simulation;Circuit testing;Benchmark testing;Very large scale integration;Test pattern generators;Electrical fault detection;Fault detection;Data structures;Software tools},
  doi={10.1109/HLDVT.2000.889571},
  ISSN={},
  month={Nov},}

@INPROCEEDINGS{6113487,
  author={Ren, Chunmei and Jiang, Daihong},
  booktitle={2011 International Conference of Information Technology, Computer Engineering and Management Sciences}, 
  title={A New Ontology of Resource Specification for Wireless Sensor Networks}, 
  year={2011},
  volume={2},
  number={},
  pages={138-140},
  abstract={The practical usage of ontology for resource specification is for effective resource control in wireless sensor networks. In order to support geographically and logically distinct resources to be co-scheduled and co-allocated when test beds are federated, we build a new ontology. It provides a simple schema that can be used to present a clear overview of network and the relation between sensing elements. Our ontology has been actively integrated with the common-used control framework. The ontology also provides service abstraction between service providers and users, which defines generalized resource request for users to describe the desired resources on different test beds. It has been shown to be useful for describing heterogeneous networked sensing substrate and be feasible for allocating resources across various test beds.},
  keywords={Ontologies;Sensors;Substrates;XML;Wireless sensor networks;Semantics;Wireless communication;Wireless Senor Networks;Ontology;Virtulization},
  doi={10.1109/ICM.2011.282},
  ISSN={},
  month={Sep.},}

@ARTICLE{1075437,
  author={Maxham, K. and Dugan, J. and McDonald, M. and Hogge, C.},
  journal={Journal of Lightwave Technology}, 
  title={1.13-Gbit/lightwave transmission system}, 
  year={1987},
  volume={5},
  number={10},
  pages={1510-1517},
  abstract={A new high capacity lightwave transmission system has been developed using GaAs semicustom logic arrays and a DFB single-mode laser, and is presently in production. The architecture of this product is designed for in-service upgrade of a 565-Mbit/s product. This paper reviews the technical characteristics and design considerations of the Rockwell LTS-21130 lightwave transmission system.},
  keywords={Optical transmitters;Protection;Gallium arsenide;Multiplexing;Circuits;Condition monitoring;Logic arrays;Production systems;High speed optical techniques;Optical receivers},
  doi={10.1109/JLT.1987.1075437},
  ISSN={1558-2213},
  month={October},}

@ARTICLE{10516612,
  author={Sokolowski, Daniel and Spielmann, David and Salvaneschi, Guido},
  journal={IEEE Transactions on Software Engineering}, 
  title={Automated Infrastructure as Code Program Testing}, 
  year={2024},
  volume={50},
  number={6},
  pages={1585-1599},
  abstract={Infrastructure as Code (IaC) enables efficient deployment and operation, which are crucial to releasing software quickly. As setups can be complex, developers implement IaC programs in general-purpose programming languages like TypeScript and Python, using PL-IaC solutions like Pulumi and AWS CDK. The reliability of such IaC programs is even more relevant than in traditional software because a bug in IaC impacts the whole system. Yet, even though testing is a standard development practice, it is rarely used for IaC programs. For instance, in August 2022, less than 1 % of the public Pulumi IaC programs on GitHub implemented tests. Available IaC program testing techniques severely limit the development velocity or require much development effort. To solve these issues, we propose Automated Configuration Testing (ACT), a methodology to test IaC programs in many configurations quickly and with low effort. ACT automatically mocks all resource definitions in the IaC program and uses generator and oracle plugins for test generation and validation. We implement ACT in ProTI, a testing tool for Pulumi TypeScript with a type-based generator and oracle, and support for application specifications. Our evaluation with 6 081 programs from GitHub and artificial benchmarks shows that ProTI can directly be applied to existing IaC programs, quickly finds bugs where current techniques are infeasible, and enables reusing existing generators and oracles thanks to its pluggable architecture.},
  keywords={Testing;Generators;Software;Cloud computing;Engines;Codes;Libraries;Property-based testing;fuzzing;infrastructure as code;DevOps},
  doi={10.1109/TSE.2024.3393070},
  ISSN={1939-3520},
  month={June},}

@INPROCEEDINGS{7528962,
  author={Thummala, Sunitha and Offutt, Jeff},
  booktitle={2016 IEEE Ninth International Conference on Software Testing, Verification and Validation Workshops (ICSTW)}, 
  title={Using Petri Nets to Test Concurrent Behavior of Web Applications}, 
  year={2016},
  volume={},
  number={},
  pages={189-198},
  abstract={Web applications are used by companies across the world to deploy their products and services. Because of the technologies used to build web applications, they are by nature concurrent, for example, multiple users can have problems when accessing the same limited resources. The combination of the stateless nature of web applications and concurrent behavior creates unique challenges. Models have traditionally been used to test specific aspects of systems. However, existing web application models do not effectively address the combination of concurrent behavior and stateless protocol. This research project is using a novel Petri net-based model for web applications. This paper defines a novel way to design model-based coverage criteria tests that address concurrent behavior involving HTTP browser-based sessions. A tool that extracts the Petri net model of a web application has been developed and used to study ten web applications totaling 17,535 lines of code. The tool was used to extract the model and generate tests, revealing 36 naturally occurring software faults that had not been found during previous testing.},
  keywords={Petri nets;Software;Browsers;Servers;Testing;Concurrent computing;Protocols;web applications;model-based testing;Petri nets;coverage criterion;multiple session},
  doi={10.1109/ICSTW.2016.15},
  ISSN={},
  month={April},}

@INPROCEEDINGS{10062395,
  author={Reichelt, David Georg and Kühne, Stefan and Hasselbring, Wilhelm},
  booktitle={2022 IEEE 22nd International Conference on Software Quality, Reliability and Security (QRS)}, 
  title={Automated Identification of Performance Changes at Code Level}, 
  year={2022},
  volume={},
  number={},
  pages={916-925},
  abstract={To develop software with optimal performance, even small performance changes need to be identified. Identifying performance changes is challenging since the performance of software is influenced by non-deterministic factors. Therefore, not every performance change is measurable with reasonable effort. In this work, we discuss which performance changes are measurable at code level with reasonable measurement effort and how to identify them. We present (1) an analysis of the boundaries of measuring performance changes, (2) an approach for determining a configuration for reproducible performance change identification, and (3) an evaluation comparing of how well our approach is able to identify performance changes in the application server Jetty compared with the usage of Jetty’s own performance regression benchmarks.Thereby, we find (1) that small performance differences are only measurable by fine-grained measurement workloads, (2) that performance changes caused by the change of one operation can be identified using a unit-test-sized workload definition and a suitable configuration, and (3) that using our approach identifies small performance regressions more efficiently than using Jetty’s performance regression benchmarks.},
  keywords={Codes;Software quality;Software performance;Benchmark testing;Reliability engineering;Software reliability;Software measurement;software performance engineering;performance measurement;benchmarking},
  doi={10.1109/QRS57517.2022.00096},
  ISSN={2693-9177},
  month={Dec},}

@INPROCEEDINGS{7102628,
  author={Rodrigues, Elder and Bernardino, Maicon and Costa, Leandro and Zorzo, Avelino and Oliveira, Flavio},
  booktitle={2015 IEEE 8th International Conference on Software Testing, Verification and Validation (ICST)}, 
  title={PLeTsPerf - A Model-Based Performance Testing Tool}, 
  year={2015},
  volume={},
  number={},
  pages={1-8},
  abstract={Performance testing is a highly specialized task, since it requires that a performance engineer knows the application to be tested, its usage profile, and the infrastructure where it will execute. Moreover, it requires that testing teams expend a considerable effort and time on its automation. In this paper, we present the PLeTsPerf, a model-based performance testing tool to support the automatic generation of scenarios and scripts from application models. PLetsPerf is a mature tool, developed in collaboration with an IT company, which has been used in several works, experimental studies and pilot studies. We present an example of use to demonstrate the process of generating test scripts and scenarios from UML models to test a Web application. We also present the lessons learned and discuss our conclusions about the use of the tool.},
  keywords={Unified modeling language;Testing;Load modeling;Software;Companies;Generators;Visualization},
  doi={10.1109/ICST.2015.7102628},
  ISSN={2159-4848},
  month={April},}

@ARTICLE{5389542,
  author={Brodnax, T. B. and Billings, R. V. and Glenn, S. C. and Patel, P. T.},
  journal={IBM Journal of Research and Development}, 
  title={Implementation of the PowerPC 601 microprocessor}, 
  year={1994},
  volume={38},
  number={5},
  pages={621-632},
  abstract={To produce a marketable PowerPC™ microprocessor on a short development schedule, the logic had to be designed in a manner flexible enough to allow quick modifications without sacrificing high performance and density when customized cells were required. This was accomplished for the PowerPC 601™ microprocessor (601) with a high-level design-language description, which was synthesized for a gate-level implementation and simulated for functional verification. In a similar way, the physical design strategy for the 601 struck an attractive balance between a highly automated, flexible floorplan and the additional density that had to be available for limited, well-conceived manual placements. Finally, a rigorous test strategy was implemented, which has proved very useful in analyzing the processor and in assembling 601-based systems. Careful adherence to this methodology led to a successful first-pass physical implementation, leaving the second iteration for additional customer requests.},
  keywords={},
  doi={10.1147/rd.385.0621},
  ISSN={0018-8646},
  month={Sep.},}

@INPROCEEDINGS{5979373,
  author={Wang, Shuanqi and Wu, Yumei and Lu, Minyan and Li, Haifeng},
  booktitle={The Proceedings of 2011 9th International Conference on Reliability, Maintainability and Safety}, 
  title={Software reliability modeling based on test coverage}, 
  year={2011},
  volume={},
  number={},
  pages={665-671},
  abstract={In order to incorporate the effect of test coverage, two novel software reliability growth models (SRGMs) are proposed in this paper using failure data and test coverage simultaneously. One is continuous using testing time, and the other is discrete with respect to the number of executed test cases instead of testing time. Since one of the most important factors of the coverage-based SRGMs is the test coverage function (TCF), we first discuss a discrete TCF based on Beta function. Then we develop mean value functions (MVF) of the two models integrating test coverage and imperfect debugging. Finally the proposed TCF and MVFs are evaluated and validated on actual software reliability data collected from real software development projects. The results demonstrate clearly that both the proposed TCF and SRGMs provide better estimation and fitting for the data sets under comparisons.},
  keywords={Software reliability;Testing;Software;Mathematical model;Fault detection;Equations;Software reliability growth model;test coverage function;beta function;non-homogeneous poisson process;mean value function},
  doi={10.1109/ICRMS.2011.5979373},
  ISSN={},
  month={June},}

@INPROCEEDINGS{5679044,
  author={Gallant, Scott and Gaughan, Chris},
  booktitle={Proceedings of the 2010 Winter Simulation Conference}, 
  title={Systems engineering for distributed live, virtual, and constructive (LVC) simulation}, 
  year={2010},
  volume={},
  number={},
  pages={1501-1511},
  abstract={Designing a distributed simulation environment across multiple domains that typically have disparate middleware transport protocols, data exchange formats and applications increases the difficulty of capturing and linking system design decisions to the resultant implementation. Systems engineering efforts for distributed simulation environments are typically based on the middleware transport used, the applications available and the constraints placed on the technical team including network, computer and personnel limitations. To facilitate community re-use, systems engineering should focus on integrated operational function decomposition. This links data elements produced within the simulation to the functional capabilities required by the user. The system design should be captured at a functional level and subsequently linked to the technical design. Doing this within a data-driven systems engineering infrastructure allows generative programming techniques to assist accurate, flexible and rapid architecture development. This paper describes the MATREX program systems engineering process, infrastructure and path forward.},
  keywords={Computer architecture;System analysis and design;Data models;Testing;Middleware},
  doi={10.1109/WSC.2010.5679044},
  ISSN={1558-4305},
  month={Dec},}

@INPROCEEDINGS{5476763,
  author={Mathieu, Bertrand and Paris, Pierre and Guelvouit, Gaëtan Le and Rouibia, Soufiane},
  booktitle={2010 Fifth International Conference on Internet and Web Applications and Services}, 
  title={A Secure and Legal Network-Aware P2P VoD System}, 
  year={2010},
  volume={},
  number={},
  pages={194-199},
  abstract={File sharing applications using Peer-to-Peer (P2P) networks such as Bittorrent or eDonkey rapidly attracted a lot of people and proved the efficiency and interest of this P2P technology. Distribution of video and of live contents also experienced the P2P mechanisms with success. PPLive, UUSee and others have many of customers, hundreds of channels and thousands of concurrent users. However, major content providers are reluctant to use this technology because no solution to ensure the distribution of only legal contents is provided. In the same way, network operators do not really push towards P2P content distribution because bad organization of the overlay can lead to overload the network and consume a lot of networks resources. In this paper, a secure and legal network-aware P2P video system is introduced, which aims at overcoming those two drawbacks. The design of the system and the evaluation of a prototype showed good results and let us be optimistic about a possible deployment of P2P systems for video delivery, having the support of content providers as well as network operators.},
  keywords={Law;Legal factors;Peer to peer computing;Video sharing;Prototypes;Watermarking;Computer architecture;IP networks;Web and internet services;Design optimization;P2P Video Streaming;network-awareness;secure distribution;legal contents;watermarking},
  doi={10.1109/ICIW.2010.35},
  ISSN={},
  month={May},}

@INPROCEEDINGS{10132236,
  author={Basciani, Francesco and Cortellessa, Vittorio and DiMartino, Sergio and Di Nucci, Dario and Di Pompeo, Daniele and Gravino, Carmine and Lucio Starace, Luigi Libero},
  booktitle={2023 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)}, 
  title={ADAS Verification in Co-Simulation: Towards a Meta-Model for Defining Test Scenarios}, 
  year={2023},
  volume={},
  number={},
  pages={28-35},
  abstract={Advanced Driver Assistance Systems (ADAS) are becoming mandatory for novel vehicles in many nations, as they are widely recognized as a key strategy to improve road safety. Due to their safety-critical nature, ADAS must guarantee the highest safety standards. Nevertheless, the verification and validation of these systems, which are often based on Artificial Intelligence techniques, is a pain point for the automotive industry, as field evaluations are not economically and temporally viable. Furthermore, the widely used Model-in-the-Loop (MiL) validation paradigm struggles when applied to novel ADAS due to the complexity of the scenarios to simulate. A strategy recently proposed in the literature to face this issue is co-simulation, namely the cooperation of a MiL framework with one or more tools, virtually simulating the environment around the vehicle. Although existing, these solutions are still highly tailored, requiring significant manual work to define testing scenarios, also due to the lack of a solid reference framework.This paper presents a preliminary model-based framework to support the design of co-simulation test scenarios for ADAS, featuring model-based testing assertions through first-order logic formulas. The proposed framework includes a visual editor which empowers domain experts to easily design test scenarios that can be automatically executed using the state-of-the-art virtual environment simulator BeamNG. The solution presented in this paper is our first step towards defining a more comprehensive framework for testing ADAS in co-simulation, providing an environment where testers are not burdened with the time-consuming and low-level task of manually defining each aspect of the virtual testbed.},
  keywords={Software testing;Solid modeling;Visualization;Pain;Virtual environments;Manuals;Solids;Road safety;Safety;Standards;ADAS Testing;Co-simulation;Model-based Testing;Visual Editor},
  doi={10.1109/ICSTW58534.2023.00018},
  ISSN={2159-4848},
  month={April},}

@INPROCEEDINGS{4670300,
  author={Guelfi, Nicolas and Ries, Benoit},
  booktitle={Testing: Academic & Industrial Conference - Practice and Research Techniques (taic part 2008)}, 
  title={Selection, Evaluation and Generation of Test Cases in an Industrial Setting: A Process and a Tool}, 
  year={2008},
  volume={},
  number={},
  pages={47-51},
  abstract={The test phase in safety-critical systems industry is a crucial phase of the development process. Some companies of these industries have their own test methods which do not reuse the notions available in the theory of software testing or model driven engineering. This paper reports on an experience in a testing process improvement made inside a safety-critical systems company in order to improve the quality of the test phase improvement. We present the initial situation, the objectives, the proposed process and the tools that are used to support it. In particular, we show that the most efficient improvements were achieved concerning the test process definition and in allowing a tailored and precise delimitation of the systempsilas elements to be tested.},
  keywords={System testing;Software testing;Computer industry;Embedded software;Performance evaluation;Laboratories;Software systems;Model driven engineering;Software performance;Software engineering;test selection;model-driven testing;industrial;tool-support;process},
  doi={10.1109/TAIC-PART.2008.12},
  ISSN={},
  month={Aug},}

@INPROCEEDINGS{8591171,
  author={Chen, Yingxin and Dai, Wenbin and Zhang, Zhijie and Pang, Cheng and Vyatkin, Valeriy},
  booktitle={IECON 2018 - 44th Annual Conference of the IEEE Industrial Electronics Society}, 
  title={A Case Study on Knowledge Driven Code Generation for Software-Defined Industrial Cyber-Physical Systems}, 
  year={2018},
  volume={},
  number={},
  pages={4687-4692},
  abstract={Industrial Cyber-Physical Systems (iCPS) enables coordination between various subsystems and devices based on real-time feedback data from sensors. iCPS must react rapidly to new requirements and adjust itself to fulfill new functionalities in no time. On the software side, control programs of iCPS need to be reconfigured dynamically. An efficient way for massive reconfiguration is automatic code generation. In this paper, a knowledge-driven code generation method is experimented for software-defined iCPS. Based on sensor values, actuators are controlled by the reasoning process with support of ontological knowledge base. The results demonstrate that iCPS could be driven by rules completely without programming control software.},
  keywords={IEEE Senior Members;Indexes;Cyber-physical systems;Requirements engineering;Ontologies;Cognition;Industrial Cyber-Physical Systems;Code Generation;Software-Defined Systems;Requirement Engineering;Ontology Reasoning;SWRL;SQWRL},
  doi={10.1109/IECON.2018.8591171},
  ISSN={2577-1647},
  month={Oct},}

@INPROCEEDINGS{6389024,
  author={Gananchchelvi, Parameshwaran and Jiao Yu and Pukish, Michael S.},
  booktitle={IECON 2012 - 38th Annual Conference on IEEE Industrial Electronics Society}, 
  title={Current trends in in-vehicle electrical engineering applications}, 
  year={2012},
  volume={},
  number={},
  pages={6268-6273},
  abstract={Novel electrical engineering applications play a major role in in-vehicle technology. Today's automobile industry is transforming from mechanically driven functions to electronic and software driven functions. We can see the impact of electrical engineering technology in many parts of vehicle and in all of its operations such as control, power conversion, navigation, communication, entertainment, safety and security. In addition, today's automobile industry is showing a renewed interest in electric transportation. This paper presents a review on the recent developments in the in-vehicle electrical engineering applications classified into two sections, the embedded systems and the power and energy system, both of which can strongly influence the automobile industry in the future.},
  keywords={Field programmable gate arrays;Supercapacitors;Lead;Batteries;Digital signal processing;Frequency conversion;Frequency estimation;in-vehicle electronics;embedded systems;ECU;FPGA;EV;ultracapacitors},
  doi={10.1109/IECON.2012.6389024},
  ISSN={1553-572X},
  month={Oct},}

@INPROCEEDINGS{7781923,
  author={Klein, Eduard and Gschwend, Adrian and Neuroni, Alessia C.},
  booktitle={2016 Conference for E-Democracy and Open Government (CeDEM)}, 
  title={Towards a Linked Data Publishing Methodology}, 
  year={2016},
  volume={},
  number={},
  pages={188-196},
  abstract={Linked open government data (LOGD) can be a catalyst in the development of value-added services and products. The vision of many Linked Open Data (LOD) projects is to make publishing and reuse of linked data as easy as possible for the end user thanks to a thriving marketplace with data publishers, developers, and consumers along the value chain. In the large scale LOD project "Fusepool P3", tourism-related applications and software components were developed that support data owners and open data enthusiasts in transforming legacy data to linked data. Based on experiences from this project, we present reflections and discuss pitfalls in drawing a linked data publishing methodology. An integrated view on all phases of the publishing process has not been described so far, for the technical phases linked data life-cycles have been identified only. The methodology developed enables stakeholders to transfer the lessons learned to other use cases and application contexts. This allows for better estimation of efforts and skills for future LOD projects.},
  keywords={Publishing;Stakeholders;Context;Government;Data models;Software;Portals;linked open data;data publishing;linked data life-cycle;publishing methodology;linked data platform},
  doi={10.1109/CeDEM.2016.12},
  ISSN={},
  month={May},}

@INPROCEEDINGS{6030046,
  author={Hutchesson, Stuart and McDermid, John},
  booktitle={2011 15th International Software Product Line Conference}, 
  title={Towards Cost-Effective High-Assurance Software Product Lines: The Need for Property-Preserving Transformations}, 
  year={2011},
  volume={},
  number={},
  pages={55-64},
  abstract={Generative programming and model transformation techniques are becoming widely used for the development of software components for product lines. The ability to develop components with identified common and variable parts, and rapidly instantiate product-specific versions is key to many software product line approaches. However if this approach is to be truly cost effective for high assurance applications, the instantiation process must be property-preserving, any verification evidence acquired on the product-line component must be demonstrably applicable to the instantiated component. In this paper we outline an approach that uses static analysis techniques and the SPARK language that can potentially demonstrate the correctness of model transformations.},
  keywords={Software;Unified modeling language;Sparks;Programming;Ignition;Contracts;UML;SPARK;M2M;Safety Critical;High Integrity;Software Product Lines;Verification;Static Analysis;DO-178B/ED-12B},
  doi={10.1109/SPLC.2011.32},
  ISSN={},
  month={Aug},}

@INPROCEEDINGS{6602477,
  author={Morrison, Patrick and Holmgreen, Casper and Massey, Aaron and Williams, Laurie},
  booktitle={2013 5th International Workshop on Software Engineering in Health Care (SEHC)}, 
  title={Proposing regulatory-driven automated test suites for electronic health record systems}, 
  year={2013},
  volume={},
  number={},
  pages={46-49},
  abstract={In regulated domains such as finance and health care, failure to comply with regulation can lead to financial, civil and criminal penalties. While systems vary from organization to organization, regulations apply across organizations. We propose the use of Behavior-Driven-Development (BDD) scenarios as the basis of an automated compliance test suite for standards such as regulation and interoperability. Such test suites could become a shared asset for use by all systems subject to these regulations and standards. Each system, then, need only create their own system-specific test driver code to automate their compliance checks. The goal of this research is to enable organizations to compare their systems to regulation in a repeatable and traceable way through the use of BDD. To evaluate our proposal, we developed an abbreviated HIPAA test suite and applied it to three open-source electronic health record systems. The scenarios covered all security behavior defined by the selected regulation. The system-specific test driver code covered all security behavior defined in the scenarios, and identified where the tested system lacked such behavior.},
  keywords={Data structures;Boolean functions;NIST;Certification;Behavior-Driven-Development Healthcare IT;Regulatory Compliance;Security;Software Engineering;Software Testing},
  doi={10.1109/SEHC.2013.6602477},
  ISSN={},
  month={May},}

@INPROCEEDINGS{7811570,
  author={Hoyos, Luis Cuellar and Rothenberg, Christian Esteve},
  booktitle={2016 8th IEEE Latin-American Conference on Communications (LATINCOM)}, 
  title={NOn: Network function virtualization ontology towards semantic service implementation}, 
  year={2016},
  volume={},
  number={},
  pages={1-6},
  abstract={A hazard of ongoing Network Function Virtualization (NFV) realizations is the lack of a common understanding in support of development, deployment and operation tasks related to Virtual Function Networks (VNFs), NFV components and interfaces. In the current state of affairs, NFV stakeholders commonly create their own terminology to define and describe NFV components, following going the specifications led by European Telecommunications Standard Institute but also adopting telecommunication- and software-centric definitions. As a consequence, portability and interoperability goals of NFV get compromised since NFV technology providers have hard times in understanding and using definitions and descriptions across different domains. Furthermore, VNF data models of operational systems and deployment configuration software need to be re-defined, re-coded, and re-compiled to make them work over different NFV platforms. In this work, we present the design and implementation of our proposed NFV Ontology (NOn) enabling Semantic nFV Services (SnS) to reduce manual intervention during the integration process of heterogeneous NFV domains and effectively overcome the costly re-work hazards of current NFV implementation approaches. We present the proof of concept implementation of a Generic Client leveraging SnS/NOn to create and consume dynamic workflows in an open source testbed based on OpenStack and OpenBaton.},
  keywords={Semantics;Ontologies;Interoperability;Software;Network function virtualization;Manuals;Engines;Network Function Virtualization;NFV;Semantic Services;Ontology},
  doi={10.1109/LATINCOM.2016.7811570},
  ISSN={},
  month={Nov},}

@INPROCEEDINGS{7905631,
  author={Fourati, Farah and Bhiri, Mohamed Tahar and Robbana, Riadh},
  booktitle={2016 5th International Conference on Multimedia Computing and Systems (ICMCS)}, 
  title={Verification and validation of PDDL descriptions using Event-B formal method}, 
  year={2016},
  volume={},
  number={},
  pages={770-776},
  abstract={The automatic planning community of Artificial Intelligence AI have developed a de facto standard language for PDDL, producing formal modeling of Planning problems. Equally it have conceived and produced tools called planners to automatically generate plans for PDDL descriptions. But the verification and validation of PDDL descriptions is little treated topic. In this paper, we shall treat this issue through the Event-B formal method. We illustrate the contribution of the static analysis tools associated with Event-B (provers, model checker, animator, and simulator) for verification and validation of PDDL descriptions.},
  keywords={Planning;Context;Boats;Context modeling;Poles and towers;Syntactics;Electronic mail;Artificial Intelligence;Planning and Scheduling;PDDL;Verification and Validation;Event-B;Transformation;Static and Dynamic analysis},
  doi={10.1109/ICMCS.2016.7905631},
  ISSN={2472-7652},
  month={Sep.},}

@INPROCEEDINGS{9042019,
  author={Ta, Tuan and Zhang, Xianwei and Gutierrez, Anthony and Beckmann, Bradford M.},
  booktitle={2019 IEEE International Symposium on Workload Characterization (IISWC)}, 
  title={Autonomous Data-Race-Free GPU Testing}, 
  year={2019},
  volume={},
  number={},
  pages={81-92},
  abstract={As the deep learning and high-performance computing markets continue to grow, hardware designers are increasingly optimizing future GPUs to run compute (a.k.a. GPGPU) workloads. A key area of optimization for these compute-oriented designs, which was not emphasized when GPUs exclusively executed graphics workloads, is inter-thread data sharing and synchronization. GPU cache coherence protocols now support these operations and are governed by a specified memory consistency model. In general, current GPU models are based on sequential consistency for data-race-free (SC for DRF), which mandates data written to memory must be globally visible only after certain synchronization points. GPU coherence protocols based on such relaxed memory models are particularly difficult to design and test due to the large number of memory accesses that may be reordered. This leaves GPU hardware designers struggling to validate the correctness of GPU cache coherence optimizations. To address this issue, this paper introduces a novel, completely autonomous random testing methodology for complex GPU cache coherence protocols. Our framework continuously generates sequences of memory requests with minimal user intervention using a mix of load, store, and atomic operations. The tester dynamically and autonomously checks each response against an expected global view of memory and immediately detects any inconsistencies in a target coherence protocol, providing designers detailed feedback on the issue. We then demonstrate the methodology on the popular cycle-level gem5 simulator by replacing its GPU core model with our unique testing framework. The results show that the GPU tester can cover 94% and 100% of all reachable state transitions in L1 and L2 caches respectively of a representative GPU coherence protocol. This coverage is 6.25% and 25% higher than the one achieved by a wide selection of 26 applications. In addition, the tester runs more than 50 times faster than those applications, which enables efficient and fast protocol debugging.},
  keywords={Cache coherence;central processing unit (CPU);graphics processing unit (GPU);memory consistency;simulation;testing},
  doi={10.1109/IISWC47752.2019.9042019},
  ISSN={},
  month={Nov},}

@INPROCEEDINGS{8109258,
  author={Duffau, Clément and Grabiec, Bartosz and Blay-Fornarino, Mireille},
  booktitle={2017 IEEE International Symposium on Software Reliability Engineering Workshops (ISSREW)}, 
  title={Towards Embedded System Agile Development Challenging Verification, Validation and Accreditation: Application in a Healthcare Company}, 
  year={2017},
  volume={},
  number={},
  pages={82-85},
  abstract={When Agile development meets critical embedded systems, verification, validation and accreditation activities are impacted. Challenges such as tests increase or accreditation documents production have to be managed in terms of time and resources. In this paper, we highlight these challenges and present a continuous integration ecosystem that aims to tackle these issues. We report on how this approach has been applied in a research and development healthcare company named AXONIC.},
  keywords={Accreditation;Testing;Embedded systems;Hardware;Companies;Ecosystems;agile development;embedded systems;justification;VV&A;continuous integration},
  doi={10.1109/ISSREW.2017.8},
  ISSN={},
  month={Oct},}

@INPROCEEDINGS{4610718,
  author={Nicolae, Maximilian and Dobrescu, Radu and Dobrescu, Matei and Popescu, Dan},
  booktitle={2008 6th International Symposium on Communication Systems, Networks and Digital Signal Processing}, 
  title={Embedded node around a DSP core for mobile sensor networks over 802.11 infrastructure}, 
  year={2008},
  volume={},
  number={},
  pages={643-646},
  abstract={Digital signal processors (DSPs) are very efficient devices to implement algorithms for signal processing and analyzing. Endowing sensorial nodes from a sensor network with such processing cores it could lead to high performance because of the possibility of parallel and distribute processing and thus reducing the quantity of information spread into the network (network load). If such a node uses for communication a mature infrastructure like 802.11 standard, will be obtained a solution for implementing mobile sensor networks with high performance end cost efficient. In the same way of obtaining high performances with low costs we suggest that on the information chain from the physical quantity to the numerical result, the acquisition part to be done by an audio codec. In this way, the entire network may look like a Voice over IP (VoIP) mobile network, yet the information exchanged will not be voice but measures and commands with quality of service (QoS) inherited from VoIP.},
  keywords={Digital signal processing;Computer architecture;Wireless sensor networks;Codecs;Mobile communication;Mobile computing;Hardware},
  doi={10.1109/CSNDSP.2008.4610718},
  ISSN={},
  month={July},}

@INPROCEEDINGS{6297159,
  author={Nagowah, Leckraj and Doorgah, Kishan},
  booktitle={2012 International Conference on Computer & Information Science (ICCIS)}, 
  title={Improving test data management in record and playback testing tools}, 
  year={2012},
  volume={2},
  number={},
  pages={931-937},
  abstract={It is almost impossible to prevent requirement change in the web development life cycle. Selenium despite being a widely used open source automated tool for testing web application, has its limitation when it concerns test data management. Frequent changes in requirement result in changes in the user interface which in turn requires additional effort to re-record the test script. Eventually keeping track of test data used for each test script becomes very problematic for the tester. In this paper, we analyse existing tools and provide a design of an automated testing tool, Kishanium that also manages the set of test data. A prototype was created during experimentation phase to prove the concept of the underlying ideas of the proposed tool. The prototype has been implemented based on the core technologies of DomDocument, XPath and Curl. The testing carried out proves that Kishanium is a useful automated tool that can be used on its own or in conjunction with Selenium. With a very systematic approach it automatically searches input and button objects, allows testers to add new test data, edit existing test data and delete previous test data in order to respond to frequent requirement changes. The power of Kishanium is that it is able to re-use existing test data even if there are a number of changes in the user interface. It also automatically runs the tests with the appropriate set of test data using its Poster Component. Moreover the Kishanium automated tool provides additional features such as Data generator, Spylink and Snapshot.},
  keywords={Presses;Fires;Manuals;Libraries;record and playback problem;automated testing;test data management},
  doi={10.1109/ICCISci.2012.6297159},
  ISSN={},
  month={June},}

@ARTICLE{7140715,
  author={},
  journal={IEEE Std 2030.2-2015}, 
  title={IEEE Guide for the Interoperability of Energy Storage Systems Integrated with the Electric Power Infrastructure}, 
  year={2015},
  volume={},
  number={},
  pages={1-138},
  abstract={This guide applies the smart grid interoperability reference model (SGIRM) process (IEEE Std 2030-2011) to energy storage by highlighting the information relevant to energy=storage system (ESS) interoperability with the energy power system (EPS). The process can be applied to ESS applications located on customer premises, at the distribution level, and on the transmission level (i.e., bulk storage). This guide provides useful industry-derived definitions for ESS characteristics, applications, and terminology that, in turn, simplify the task of defining system information and communications technology (ICT) requirements. As a result. these requirements can be communicated more clearly and consistently in project specifications. This guide also presents a methodology that can be used for most common ESS projects to describe the power system, communications, and information technology (IT) perspectives based on the IEEE 2030 definitions. From this framework, a seemingly complex system can be more clearly understood by all project stakeholders. Emerging cybersecurity requirements can also be incorporated into the framework as appropriate. Additionally, this guide provides the templates that can be used to develop requirements for an ESS project and goes through several real-world ESS project examples step by step.},
  keywords={IEEE Standards;Energy storage;Batteries;Smart grids;Electric power systems;Power system reliability;battery;communications technology;electric power system;energy storage system;IEEE 2030.2(TM);information technology;interoperability;power system;Smart Grid},
  doi={10.1109/IEEESTD.2015.7140715},
  ISSN={},
  month={June},}

@INPROCEEDINGS{6209957,
  author={Kanstrén, Teemu and Puolitaival, Olli-Pekka and Rytky, Veli-Matti and Saarela, Asmo and Keränen, Janne S.},
  booktitle={2012 IEEE International Conference on Industrial Technology}, 
  title={Experiences in setting up domain-specific model-based testing}, 
  year={2012},
  volume={},
  number={},
  pages={319-324},
  abstract={Model-based testing is a technique for generating test cases based on a model of the system under test. Typically the model is expressed in a specific notation of the test tool, using a generic notation intended to describe any system under test. In this paper we present experiences in using a domain-specific modeling layer on top of the specific model-based testing tools. This allows for easier change of the used testing tool, while providing a more familiar modeling notation in terms of the domain concepts familiar to the user. Our experiences show how this can significantly help in adopting the model-based testing approach and provide improved test results.},
  keywords={Generators;Encoding;Testing},
  doi={10.1109/ICIT.2012.6209957},
  ISSN={},
  month={March},}

@INPROCEEDINGS{10740213,
  author={Abi-Karam, Stefan and Sarkar, Rishov and Seigler, Allison and Lowe, Sean and Wei, Zhigang and Chen, Hanqiu and Rao, Nanditha and John, Lizy and Arora, Aman and Hao, Cong},
  booktitle={2024 ACM/IEEE 6th Symposium on Machine Learning for CAD (MLCAD)}, 
  title={HLSFactory: A Framework Empowering High-Level Synthesis Datasets for Machine Learning and Beyond}, 
  year={2024},
  volume={},
  number={},
  pages={1-9},
  abstract={Machine learning (ML) techniques have been applied to high-level synthesis (HLS) flows for quality-of-result (QoR) prediction and design space exploration (DSE). Nevertheless, the scarcity of accessible high-quality HLS datasets and the complexity of building such datasets present great challenges to FPGA and ML researchers. Existing datasets either cover only a subset of previously published benchmarks, provide no way to enumerate optimization design spaces, are limited to a specific vendor, or have no reproducible and extensible software for dataset construction. Many works also lack user-friendly ways to add more designs to existing datasets, limiting wider adoption and sustainability of such datasets. In response to these challenges, we introduce HLSFactory, a comprehensive framework designed to facilitate the curation and generation of high-quality HLS design datasets. HLSFactory has three main stages: 1) a design space expansion stage to elaborate single HLS designs into large design spaces using various optimization directives across multiple vendor tools, 2) a design synthesis stage to execute HLS and FPGA tool flows concurrently across designs, and 3) a data aggregation stage for extracting standardized data into packaged datasets for ML usage. This tripartite architecture not only ensures broad coverage of data points via design space expansion but also supports interoperability with tools from multiple vendors. Users can contribute to each stage easily by submitting their own HLS designs or synthesis results via provided user APIs. The framework is also flexible, allowing extensions at every step via user APIs with custom frontends, synthesis tools, and scripts. To demonstrate the framework functionality, we include an initial set of built-in base designs from PolyBench, MachSuite, Rosetta, CHStone, Kastner et al.’s Parallel Programming for FPGAs, and curated kernels from existing open-source HLS designs. We report the statistical analyses and design space visualizations to demonstrate the completed end-to-end compilation flow, and to highlight the effectiveness of our design space expansion beyond the initial base dataset, which greatly contributes to dataset diversity and coverage. In addition to its evident application in ML, we showcase the versatility and multi-functionality of our framework through seven case studies: I) Building an ML model for post-implementation QoR prediction; II) Using design space sampling in stage 1 to expand the design space covered from a small base set of HLS designs; III) Demonstrating the speedup from the fine-grained design parallelism backend; IV) Extending HLSFactory to target Intel’s HLS flow across all stages; V) Adding and running new auxiliary designs using HLSFactory; VI) Integration of previously published HLS data in stage 3; VII) Using HLSFactory to perform HLS tool version regression benchmarking. Code available at https://github.com/sharc-lab/HLSFactory.},
  keywords={Solid modeling;Statistical analysis;Buildings;Machine learning;Benchmark testing;Software;Space exploration;Sustainable development;Field programmable gate arrays;Optimization},
  doi={10.1109/MLCAD62225.2024.10740213},
  ISSN={},
  month={Sep.},}

@INPROCEEDINGS{9557519,
  author={Hielscher, Leon and Bloeck, Alexander and Viehl, Alexander and Reiter, Sebastian and Staiger, Marc and Bringmann, Oliver},
  booktitle={2021 IEEE 19th International Conference on Industrial Informatics (INDIN)}, 
  title={Platform Generation for Edge AI Devices with Custom Hardware Accelerators}, 
  year={2021},
  volume={},
  number={},
  pages={1-8},
  abstract={In recent years artificial neural networks (NNs) have been at the center of research on data processing. However, their high computational demand often prohibits deployment on resource-constrained Industrial IoT Systems. Custom hardware accelerators can enable real-time NN processing on small-scale edge devices but are generally hard to develop and integrate. In this paper we present a hardware generation approach to rapidly create, test, and deploy entire SoC platforms with application-specific NN hardware accelerators. The feasibility of the approach is demonstrated by the generation of a condition monitoring system for high-speed valves.},
  keywords={Condition monitoring;Conferences;Artificial neural networks;Valves;Data processing;Real-time systems;Informatics;Hardware Acceleration;Hardware Generation;Neural Networks;Edge Devices},
  doi={10.1109/INDIN45523.2021.9557519},
  ISSN={},
  month={July},}

@ARTICLE{10017156,
  author={Shukla, Apoorv and Hudemann, Kevin and Vági, Zsolt and Hügerich, Lily and Smaragdakis, Georgios and Hecker, Artur and Schmid, Stefan and Feldmann, Anja},
  journal={IEEE/ACM Transactions on Networking}, 
  title={Runtime Verification for Programmable Switches}, 
  year={2023},
  volume={31},
  number={4},
  pages={1822-1837},
  abstract={We introduce a runtime verification framework for programmable switches that complements static analysis. To evaluate our approach, we design and develop P6, a runtime verification system that automatically detects, localizes, and patches software bugs in P4 programs. Bugs are reported via a violation of pre-specified expected behavior that is captured by P6. P6 is based on machine learning-guided fuzzing that tests P4 switch non-intrusively, i.e., without modifying the P4 program for detecting runtime bugs. This enables an automated and real-time localization and patching of bugs. We used a P6 prototype to detect and patch existing bugs in various publicly available P4 application programs deployed on two different switch platforms, namely, behavioral model (bmv2) and Tofino. Our evaluation shows that P6 significantly outperforms bug detection baselines while generating fewer packets and patches bugs in large P4 programs, e.g., switch.p4 without triggering any regressions.},
  keywords={Computer bugs;Behavioral sciences;Runtime;Software;Fuzzing;Pipelines;Static analysis;Programmable networks;P4;verification},
  doi={10.1109/TNET.2023.3234931},
  ISSN={1558-2566},
  month={Aug},}

@ARTICLE{9796038,
  author={Lei, Zhanyao and Chen, Yixiong and Yang, Yang and Xia, Mingyuan and Qi, Zhengwei},
  journal={IEEE Transactions on Software Engineering}, 
  title={Bootstrapping Automated Testing for RESTful Web Services}, 
  year={2023},
  volume={49},
  number={4},
  pages={1561-1579},
  abstract={Modern RESTful services expose RESTful APIs to integrate with diversified applications. Most RESTful API parameters are weakly typed, which greatly increases the possible input value space. Weakly-typed parameters pose difficulties for automated testing tools to generate effective test cases to reveal web service defects related to parameter validation. We call this phenomenon the type collapse problem. To remedy this problem, we introduce FET (Format-encoded Type) techniques, including the FET, the FET lattice, and the FET inference to model fine-grained information for API parameters. Inferred FET can enhance parameter validation, such as generating a parameter validator for a certain RESTful server. Enhanced by FET techniques, automated testing tools can generate targeted test cases. We demonstrate Leif, a trace-driven fuzzing tool, as a proof-of-concept implementation of FET techniques. Experiment results on 27 commercial services show that FET inference precisely captures documented parameter definitions, which helps Leif discover 11 new bugs and reduce $72\% - 86\%$72%-86% fuzzing time compared to state-of-the-art fuzzers. Leveraged by the inter-parameter dependency inference, Leif saves $15\%$15% fuzzing time.},
  keywords={Field effect transistors;Lattices;Testing;Fuzzing;Codes;Web services;Restful API;Fuzz testing;RESTful web service;type inference},
  doi={10.1109/TSE.2022.3182663},
  ISSN={1939-3520},
  month={April},}

@ARTICLE{6046056,
  author={},
  journal={IEEE Std 1100-2005 (Revision of IEEE Std 1100-1999) - Redline}, 
  title={IEEE Recommended Practice for Powering and Grounding Electronic Equipment(Color Book® Series) - Redline}, 
  year={2006},
  volume={},
  number={},
  pages={1-703},
  abstract={The IEEE Emerald Book™ presents a collection of consensus best practices for the powering and grounding of electronic equipment used in commercial and industrial applications. The main objective is to provide consensus recommended practices in an area where conflicting information and conflicting design philosophies have dominated. The recommended practices described are intended to enhance equipment performance while maintaining a safe installation. A description of the nature and origin of power disturbances is provided, followed by theory on the various parameters that impact power quality. Information on quantifying and resolving power and grounding related concerns using measurement and diagnostic instrumentation and standardized investigative procedures are included. Recommended power protection equipment and wiring and grounding system design practices are presented. Information on telecommunications system power protection as well as grounding, industrial system grounding, and noise control is included. Finally a selection of case studies are presented to support the recommended practices presented throughout the book.},
  keywords={IEEE standards;Grounding;Industry applications;Industrial electronics;Power distribution;Power electronics;Power system reliability;commercial applications;electrical power;electronic equipment;grounding;industrial applications;power conditioning;power disturbance;power monitor;power quality;color book},
  doi={},
  ISSN={},
  month={May},}

@ARTICLE{9548078,
  author={Berquand, Audrey and Darm, Paul and Riccardi, Annalisa},
  journal={IEEE Access}, 
  title={SpaceTransformers: Language Modeling for Space Systems}, 
  year={2021},
  volume={9},
  number={},
  pages={133111-133122},
  abstract={The transformers architecture and transfer learning have radically modified the Natural Language Processing (NLP) landscape, enabling new applications in fields where open source labelled datasets are scarce. Space systems engineering is a field with limited access to large labelled corpora and a need for enhanced knowledge reuse of accumulated design data. Transformers models such as the Bidirectional Encoder Representations from Transformers (BERT) and the Robustly Optimised BERT Pretraining Approach (RoBERTa) are however trained on general corpora. To answer the need for domain-specific contextualised word embedding in the space field, we propose SpaceTransformers, a novel family of three models, SpaceBERT, SpaceRoBERTa and SpaceSciBERT, respectively further pre-trained from BERT, RoBERTa and SciBERT on our domain-specific corpus. We collect and label a new dataset of space systems concepts based on space standards. We fine-tune and compare our domain-specific models to their general counterparts on a domain-specific Concept Recognition (CR) task. Our study rightly demonstrates that the models further pre-trained on a space corpus outperform their respective baseline models in the Concept Recognition task, with SpaceRoBERTa achieving significant higher ranking overall.},
  keywords={Modeling;Task analysis;Bit error rate;Training;Data models;Transfer learning;Transformers;Language model;transformers;space systems;concept recognition;requirements},
  doi={10.1109/ACCESS.2021.3115659},
  ISSN={2169-3536},
  month={},}

@INPROCEEDINGS{1251041,
  author={Sinha, A. and Smidts, C.S. and Moran, A.},
  booktitle={14th International Symposium on Software Reliability Engineering, 2003. ISSRE 2003.}, 
  title={Enhanced testing of domain specific applications by automatic extraction of axioms from functional specifications}, 
  year={2003},
  volume={},
  number={},
  pages={181-190},
  abstract={Adequate testing is necessary and important to ensure reliability of software. Most test models are specification-based and fail to capture implicit domain specific properties. This paper presents a technique, which uses a HaskellDB specification of the software to extract domain specific properties and embed them into the test generation model. HaskellDB is an embedded domain specific functional and strongly typed language for database related applications. Specifying using HaskellDB ensures that a set of axioms based on type safeness of the database queries hold for the specification. The implementation of the application should also satisfy these properties and should be tested accordingly. We therefore propose a technique that extracts the axioms automatically from the HaskellDB specification and embeds additional test paths in the test model leading to an enriched test suite. We present an example application of the technique and compare the results against a manual testing technique.},
  keywords={Automatic testing;Databases;Software testing;Domain specific languages;Reliability engineering;Application software;Automation;System testing;Automata;Embedded software},
  doi={10.1109/ISSRE.2003.1251041},
  ISSN={1071-9458},
  month={Nov},}

@INPROCEEDINGS{9286236,
  author={Fredin, Zach and Zemanek, Jiri and Blackburn, Camron and Strand, Erik and Abdel-Rahman, Amira and Rowles, Premila and Gershenfeld, Neil},
  booktitle={2020 IEEE High Performance Extreme Computing Conference (HPEC)}, 
  title={Discrete Integrated Circuit Electronics (DICE)}, 
  year={2020},
  volume={},
  number={},
  pages={1-8},
  abstract={We introduce DICE (Discrete Integrated Circuit Electronics). Rather than separately develop chips, packages, boards, blades, and systems, DICE spans these scales in a direct-write process with the three-dimensional assembly of computational building blocks. We present DICE parts, discuss their assembly, programming, and design workflow, illustrate applications in machine learning and high performance computing, and project performance.},
  keywords={Performance evaluation;Thermodynamics;Three-dimensional displays;High performance computing;Memory management;Programming;Packaging;Automated assembly;additive manufacturing;chiplets;system integration;machine learning;high-performance computing},
  doi={10.1109/HPEC43674.2020.9286236},
  ISSN={2643-1971},
  month={Sep.},}

@INPROCEEDINGS{10188638,
  author={Chandler, Jared and Wick, Adam},
  booktitle={2023 IEEE Security and Privacy Workshops (SPW)}, 
  title={Research Report: Synthesizing Intrusion Detection System Test Data from Open-Source Attack Signatures}, 
  year={2023},
  volume={},
  number={},
  pages={198-208},
  abstract={Intrusion Detection Systems (IDS) act as a first line of defense for network infrastructure by identifying malicious traffic and reporting it to administrators. Signature-based IDS identify this traffic by attempting to parse packets according to user-supplied rules based on well-known examples of bad traffic. However, test data can be difficult to come by (due to its sensitive nature) which makes evaluating new rules difficult. In this work we discuss the limitations of an existing SMT-based synthesis approach to automatically generating malicious network traffic. We then present a survey of how IDS rules are written in practice using an open-source corpus of over 30,000 rules and discuss a road-map towards extending the existing approach with the goal of generating security test data characterizing a broad range of threats, as well as ancillary uses assisting users in writing IDS rules and identifying IDS implementation bugs. Finally, we share early results from an evaluation of one such extension which successfully generated IDS test data for over 90% of the rules evaluated.},
  keywords={Surveys;Data privacy;Conferences;Computer bugs;Intrusion detection;Telecommunication traffic;Writing;network security;intrusion detection system;synthesis},
  doi={10.1109/SPW59333.2023.00023},
  ISSN={2770-8411},
  month={May},}

@INPROCEEDINGS{10339770,
  author={Popchev, Ivan and Radeva, Irina and Doukovska, Lyubka and Dimitrova, Miroslava},
  booktitle={2023 International Conference on Big Data, Knowledge and Control Systems Engineering (BdKCSE)}, 
  title={A Web Application for Data Exchange Blockchain Platform}, 
  year={2023},
  volume={},
  number={},
  pages={1-7},
  abstract={The rapid advancement of blockchain technology and services based on blockchain applications inevitably faces the challenge of big data. To support the information and communication of smart crop production, the solution can be found in the implementation of blockchain technologies to provide services related to the acquisition, storage, processing and analysis of big data. The purpose of this paper is to present a web application for smart crop production data exchange platform integrated with Antelope blockchain and Interplanetary File System. The web application is designed using open-source solutions, enabling users to authenticate via a blockchain user account and corresponding private key, or via the standard blockchain wallet Anchor. The exchange of files and data, as well as the transfer of tokens between users, are facilitated through smart contracts. These contracts enable the tracking of chronological statements of completed transactions and provide an activity log overview of all file uploads, downloads, and edits. Future development phases include tests to evaluate the performance, speed, scalability and network security of the platform. The ultimate goal is for the web application to become a decentralized application (dApp).},
  keywords={Knowledge engineering;Scalability;Smart contracts;Crops;Production;Big Data;Network security;web application;blockchain;IPFS;Antelope.io;smart crop production;big data;platform},
  doi={10.1109/BdKCSE59280.2023.10339770},
  ISSN={},
  month={Nov},}

@ARTICLE{8862808,
  author={Siddiqui, Sidra and Khan, Tamim Ahmed},
  journal={IEEE Access}, 
  title={Test Patterns for Cloud Applications}, 
  year={2019},
  volume={7},
  number={},
  pages={147060-147080},
  abstract={Software systems are becoming graphical user intensive. They involve web technologies organized in the cloud platform which supports translation of services to a wider community. Such cloud applications are more vulnerable to misuse. Consequently, system development needs to focus on system security features in a comprehensive manner. Therefore, techniques that are based on test-driven development will be a good choice to use for the quality maintenance of such systems. We need checklists and mechanisms that provide identification and knowledge of best practices to maintain consistency in performing testing activities. We propose a test patterns-based technique which supports identification of test cases on the bases of specification and domain analysis of system under test. We provide a set of test patterns that support Test Driven Development (TDD) as well. We link misuse cases and security requirement to testing and provide test patterns for testing cloud applications. We consider threats associated with cloud applications and make use of case studies to evaluate and present results.},
  keywords={Testing;Security;Software;Cloud computing;Unified modeling language;Graphical user interfaces;Test pattern;TDD (test driven development);misuse case;test last development (TLD)},
  doi={10.1109/ACCESS.2019.2946315},
  ISSN={2169-3536},
  month={},}

@ARTICLE{1653662,
  author={Pei Hsia and Petry},
  journal={Computer}, 
  title={A Systematic Approach to Interactive Programming}, 
  year={1980},
  volume={13},
  number={6},
  pages={27-34},
  abstract={Designed for program development in an interactive environment, this framework can help create a new generation of programmers with an invaluable disciplined approach to software development.},
  keywords={Programming profession;Software engineering;Software design;Automatic programming;Programming environments;Production systems;Software quality},
  doi={10.1109/MC.1980.1653662},
  ISSN={1558-0814},
  month={June},}

@INPROCEEDINGS{4259241,
  author={Van Roijen, R. and Collins, C. and Ayala, J. and Barker, K. and Boiselle, H. and Catlett, S. and Dezfulian, K. and Logan, R. and Maxson, J. and Ramachandran, R. and Rawlins, B. and Ruegsegger, S. and Rust, T. and Shepard, J. and Singh, R.},
  booktitle={2007 IEEE/SEMI Advanced Semiconductor Manufacturing Conference}, 
  title={Reducing Time-to-Respond in a Modern Manufacturing Environment}, 
  year={2007},
  volume={},
  number={},
  pages={29-33},
  abstract={The complexity of modern manufacturing processes has sharply increased the number of steps affecting device and circuit performance. We discuss a number of critical steps, their control methodology and how to minimize the time to detect. Product test results and data-mining are used to identify critical steps and to determine which inline signals require most attention. The last section is devoted to optimizing the analysis of inline electrical signals and their application to tool control.},
  keywords={Implants;Manufacturing processes;Temperature control;Manufacturing automation;Rapid thermal annealing;Microelectronics;Circuit optimization;Circuit testing;Signal processing;Signal analysis;300mm manufacturing;SOI;Process control;Manufacturing automation},
  doi={10.1109/ASMC.2007.375075},
  ISSN={2376-6697},
  month={June},}

@INPROCEEDINGS{6166261,
  author={Edmondson, James and Gokhale, Aniruddha and Neema, Sandeep},
  booktitle={2011 IEEE International Conference on Service-Oriented Computing and Applications (SOCA)}, 
  title={Automating testing of service-oriented mobile applications with distributed knowledge and reasoning}, 
  year={2011},
  volume={},
  number={},
  pages={1-4},
  abstract={Automated testing of distributed, service-oriented applications, particularly mobile applications, is a hard problem due to challenges testers often must deal with, such as (1) heterogeneous platforms, (2) difficulty in introducing additional resources or backups of resources that fail during testing, and (3) lack of fine-grained control over test sequencing. To address these challenges, this paper describes an approach that combines portable operating system libraries with knowledge and reasoning, which together leverage the best features of centralized and decentralized testing infrastructures to support both heterogeneous systems and distributed control by reasoning on distributed testing events.},
  keywords={Testing;Cognition;Servers;Engines;Knowledge engineering;Real time systems;Conferences;test sequencing;distributed control in testing;portability;knowledge dissemination},
  doi={10.1109/SOCA.2011.6166261},
  ISSN={2163-2871},
  month={Dec},}

@INPROCEEDINGS{5767210,
  author={Mohammed, Rahima and Sahan, Ridvan and Xia, Yi and Pang, Ying-feng},
  booktitle={2011 27th Annual IEEE Semiconductor Thermal Measurement and Management Symposium}, 
  title={High performance air-cooled temperature margining thermal tools for silicon validation}, 
  year={2011},
  volume={},
  number={},
  pages={265-271},
  abstract={Thermal tools provide temperature margining capability by varying the case temperature at silicon thermal design power (TDP). They are used for process, voltage, temperature and frequency (PVTF) testing by Intel's post-silicon validation customers across servers, desktops, mobile and graphics segments. Thermal margining tools are widely used in silicon debug validation by varying the case temperature over a wide operating range of specifications of the Silicon to i) validate the silicon, ii) accelerate fault detection, and iii) reduce escapes and identify bugs. Thermal tool is controlled by a thermal controller to provide a temperature set-point based on the device under test's (DUT's) case or junction diode temperature. Air cooled thermal tool (AC-TT) employs a controller card to achieve the margining capability by running the tool's thermoelectric cooler (TEC), a Peltier device, within the optimal temperature range. AC-TT has an active heat sink design to remove the heat dissipated by the TEC and the silicon. Although AC-TT is expected to provide narrower range of margining capability due to the limitations of air cooling, they still can be an excellent solution for some specific thermal margining applications. Therefore, a new line of AC-TTs were developed for validation customers whose needs can be addressed without requiring costly controllers and noisy chillers while enhancing the user-experience. This paper presents the design improvement strategies implemented for developing the new line of CPU, Chipset and ASIC AC-TTs. Improved designs provide wider margining capability by using i) high performance active heat sink designs, ii) high power thermo-electric cooler (TEC), iii) cold plate designs compatible to keep out volume (KOV), iv) new choice of thermal interface material (TIM), and v) new retention design. This paper discusses the details of the design process and how multiple design strategies are implemented to finalize the design and to achieve the overall performance improvement while keeping the cost of the AC-TT low. The new line of AC-TT designs have performance improvement of 44% (~25C) for 130W CPU TT compared to existing CPU AC-TT, of 32% (~19C) for 60W chipset compared to existing chipset AC-TT, and of 41% (~8C) compared to existing 15W PCH (Peripheral Component Hub) AC-TT. Design strategies provided here can be easily adapted to develop future generation of low-cost CPU, chipset, and ASIC AC-TTs with a wider margining capability.},
  keywords={Heat sinks;Silicon;Cold plates;Heating;Heat transfer;Thermal resistance;Thermal tool;CPU;chipset;ASIC;retention design;CFD;TEC;air cooling;TIM;temperature margining},
  doi={10.1109/STHERM.2011.5767210},
  ISSN={1065-2221},
  month={March},}

@INPROCEEDINGS{10139331,
  author={Gerlin, Nicolas and Kaja, Endri and Vargas, Fabian and Lu, Li and Breitenreiter, Anselm and Chen, Junchao and Ulbricht, Markus and Gomez, Maribel and Tahiraga, Ares and Prebeck, Sebastian and Jentzsch, Eyck and Krstić, Miloš and Ecker, Wolfgang},
  booktitle={2023 26th International Symposium on Design and Diagnostics of Electronic Circuits and Systems (DDECS)}, 
  title={Bits, Flips and RISCs}, 
  year={2023},
  volume={},
  number={},
  pages={140-149},
  abstract={Electronic systems can be submitted to hostile environments leading to bit-flips or stuck-at faults and, ultimately, a system malfunction or failure. In safety-critical applications, the risks of such events should be managed to prevent injuries or material damage. This paper provides a comprehensive overview of the challenges associated with designing and verifying safe and reliable systems, as well as the potential of the RISC-V architecture in addressing these challenges.We present several state-of-the-art safety and reliability verification techniques in the design phase. These include a highly-automated verification flow, an automated fault injection and analysis tool, and an AI-based fault verification flow. Furthermore, we discuss core hardening and fault mitigation strategies at the design level. We focus on automated SoC hardening using model-driven development and resilient processing based on sensing and prediction for space and avionic applications.By combining these techniques with the inherent flexibility of the RISC-V architecture, designers can develop tailored solutions that balance cost, performance, and fault tolerance to meet the requirements of various safety-critical applications in different safety domains, such as avionics, automotive, and space. The insights and methodologies presented in this paper contribute to the ongoing efforts to improve the dependability of computing systems in safety-critical environments.},
  keywords={Computer architecture;Aerospace electronics;Reliability engineering;Safety;System-on-chip;Circuit faults;Vehicle dynamics;GNN;Hardening;Reliability;RISC-V;Safety;Verification},
  doi={10.1109/DDECS57882.2023.10139331},
  ISSN={2473-2117},
  month={May},}

@INPROCEEDINGS{8980532,
  author={Heilscher, Gerd and Kondzialka, Christoph and Chen, Shuo and Ebe, Falko and Hess, Sebastian and Lorenz, Heiko and Wening, Jens},
  booktitle={2019 IEEE 46th Photovoltaic Specialists Conference (PVSC)}, 
  title={Integration of Photovoltaic Systems into Smart Grids Demonstration of Solar-, Storage and E-Mobility Applications within a Secure Energy Information Network in Germany}, 
  year={2019},
  volume={},
  number={},
  pages={1541-1548},
  abstract={The integration of decentralized renewable energy systems into our distribution networks leads to a need of more detailed information about local network structure and state estimation down to the low voltage level [1]. This enforces the transformation of today's distribution networks into smart grids. Smart Meters with Smart Meter Gateways (iMSys) and Controllable Local Systems (CLS) are the essential new bricks of the future smart grid. In Germany the new law "Digitalization of the Energiewende"[2] sets up the rules for network operators to establish this secure energy information system based on the smart meter infrastructure. During the last two years the authors developed and demonstrated on laboratory and field level such a secure energy information system. The main innovation of the project is the direct and secure communication with decentralized energy systems such as photovoltaic inverters, battery storage systems, E-mobility charging stations or power to heat applications within this new smart meter infrastructure, which has been defined by technical rules from the German regulator for data security (BSI) [3]. The two-way communication is able to read measurement values from the field as well as change set points or activate curtailment of decentralized energy systems (see figure 1).},
  keywords={smart grids;distribution network;smart meter;decentralized energy systems;monitoring;control;data security;energy information system},
  doi={10.1109/PVSC40753.2019.8980532},
  ISSN={0160-8371},
  month={June},}

@INBOOK{7827467,
  author={Brooks, Tyson T.},
  booktitle={Cyber-Assurance for the Internet of Things}, 
  title={Cyber‐Assurance Through Embedded Security for the Internet of Things}, 
  year={2017},
  volume={},
  number={},
  pages={101-127},
  abstract={The Internet of Things (IoT) comprises billions of Internet‐connected devices (ICD) or "things", each of which can sense, communicate, compute, and potentially actuate and can have intelligence, multimodal interfaces, physical/virtual identities, and attributes. Cyber‐assurance is the justified confidence that networked systems are adequately secure to meet operational needs, even in the presence of attacks, failures, accidents, and unexpected events. The cyber‐assurance recognition strategy is to define only the service‐level interfaces and leave out domain‐specific implementation details. Once the recognition of a cyber‐attack has been identified from the recognition process, the fortification process takes place. Reestablishment is a means to return the ICDs to its operational condition after the cyber‐attack through remapping to a different route since the ICD was under attack. When the IoT technologies are used as part of mission critical systems, the IoT services should be survivable in order to support the important missions.},
  keywords={Embedded systems;Sensors;Protocols;Wireless sensor networks;Internet of Things;Authentication},
  doi={10.1002/9781119193784.ch2},
  ISSN={},
  publisher={IEEE},
  isbn={9781119193883},
  url={https://ieeexplore.ieee.org/document/7827467},}

@INPROCEEDINGS{6606719,
  author={Guana, Victor},
  booktitle={2013 35th International Conference on Software Engineering (ICSE)}, 
  title={Supporting maintenance tasks on transformational code generation environments}, 
  year={2013},
  volume={},
  number={},
  pages={1369-1372},
  abstract={At the core of model-driven software development, model-transformation compositions enable automatic generation of executable artifacts from models. Although the advantages of transformational software development have been explored by numerous academics and industry practitioners, adoption of the paradigm continues to be slow, and limited to specific domains. The main challenge to adoption is the fact that maintenance tasks, such as analysis and management of model-transformation compositions and reflecting code changes to model transformations, are still largely unsupported by tools. My dissertation aims at enhancing the field's understanding around the maintenance issues in transformational software development, and at supporting the tasks involved in the synchronization of evolving system features with their generation environments. This paper discusses the three main aspects of the envisioned thesis: (a) complexity analysis of model-transformation compositions, (b) system feature localization and tracking in model-transformation compositions, and (c) refactoring of transformation compositions to improve their qualities.},
  keywords={Maintenance engineering;Object oriented modeling;Analytical models;Complexity theory;Software;Games;Semantics;software maintenance;transformation composition;transformation complexity;transformation refactoring},
  doi={10.1109/ICSE.2013.6606719},
  ISSN={1558-1225},
  month={May},}

@INPROCEEDINGS{6227107,
  author={Devos, Nicolas and Ponsard, Christophe and Deprez, Jean-Christophe and Bauvin, Renaud and Moriau, Benedicte and Anckaerts, Guy},
  booktitle={2012 34th International Conference on Software Engineering (ICSE)}, 
  title={Efficient reuse of domain-specific test knowledge: An industrial case in the smart card domain}, 
  year={2012},
  volume={},
  number={},
  pages={1123-1132},
  abstract={While testing is heavily used and largely automated in software development projects, the reuse of test practices across similar projects in a given domain is seldom systematized and supported by adequate methods and tools. This paper presents a practical approach that emerged from a concrete industrial case in the smart card domain at STMicroelectronics Belgium in order to better address this kind of challenge. The central concept is a test knowledge repository organized as a collection of specific patterns named QPatterns. A systematic process was followed, first to gather, structure and abstract the test practices, then to produce and validate an initial repository, and finally to make it evolve later on Testers can then rely on this repository to produce high quality test plans identifying all the functional and nonfunctional aspects that have to be addressed, as well as the concrete tests that have to be developed within the context of a new project. A tool support was also developed and integrated in a traceable way into the existing industrial test environment. The approach was validated and is currently under deployment at STMicroelectronics Belgium.},
  keywords={Smart cards;Testing;Libraries;Software;Security;Concrete;patterns;test;generation;smartcard},
  doi={10.1109/ICSE.2012.6227107},
  ISSN={1558-1225},
  month={June},}

@INPROCEEDINGS{8327233,
  author={Chodarev, Sergej and Bačíková, Michaela},
  booktitle={2017 IEEE 14th International Scientific Conference on Informatics}, 
  title={Development of Oberon-0 using YAJCo}, 
  year={2017},
  volume={},
  number={},
  pages={122-127},
  abstract={YAJCo is a tool for the development of software languages based on an annotated language model. The model is represented by Java classes with annotations defining its mapping to concrete syntax. This approach to language definition enables the abstract syntax to be central point of the development process, instead of concrete syntax. In this paper a case study of Oberon-0 programming language development is presented. The study is based on the LTDA Tool Challenge and showcases details of abstract and concrete syntax definition using YAJCo, as well as implementation of name resolution, type checking, model transformation and code generation.},
  keywords={Syntactics;Tools;Grammar;Java;Generators;Analytical models;Abstract syntax;experience report;language development;Oberon-0;parser generator;YAJCo},
  doi={10.1109/INFORMATICS.2017.8327233},
  ISSN={},
  month={Nov},}

@INPROCEEDINGS{7338996,
  author={Borek, Marian and Stenzel, Kurt and Katkalov, Kuzman and Reif, Wolfgang},
  booktitle={2015 6th IEEE International Conference on Software Engineering and Service Science (ICSESS)}, 
  title={Abstracting security-critical applications for model checking in a model-driven approach}, 
  year={2015},
  volume={},
  number={},
  pages={11-14},
  abstract={Model checking at the design level makes it possible to find protocol flaws in security-critical applications automatically. But depending on the size of the application and especially on the abstraction of the application model, model checking may need a lot of resources, primarily time. To reduce the complexity, the application models are usually highly abstracted. But in a model-driven approach with automatic generation of runnable applications the application models need to be detailed and are often too complex to check in reasonable time. In this paper we describe an approach to handle this problem by using additional UML models to restrict the protocol runs, the attacker abilities and the numbers of participants. This makes model checking of large applications in our model-driven approach called SecureMDD possible without manual abstraction of the generated specifications. For model checking we use AVANTSSAR and show how the restrictions modeled within UML are translated. We demonstrate our approach with a smart card based electronic ticketing example.},
  keywords={Unified modeling language;Model checking;Security;Protocols;Complexity theory;Manuals;Smart cards;UML;model checking;security-critical systems;model-driven development;transformations;SecureMDD},
  doi={10.1109/ICSESS.2015.7338996},
  ISSN={2327-0594},
  month={Sep.},}

@INPROCEEDINGS{8088307,
  author={Gröning, Sven and Rosas, Christopher and Wietfeld, Christian},
  booktitle={2017 IEEE International Systems Engineering Symposium (ISSE)}, 
  title={Validating electric vehicle to grid communication systems based on model checking assisted test case generation}, 
  year={2017},
  volume={},
  number={},
  pages={1-8},
  abstract={In last decades software development processes changed in order to address increasing complexity within decreasing implementation time. Hence, new practices like Kanban, Extreme Programming or Agile Software Development emerged. Model-based development is one potential option, which is more and more used to cope these new demands. However, adapting testing processes to the needs is still an open topic. This paper describes how model checking assisted test case generation can be used to integrate testing in new software development processes, focusing on a protocol implementation for electric vehicle charging communication as a case study. Therefore, it describes certain extensions made in the COMmunication Protocol vaLidation Toolchain COMFLgTg in order to enable test case generation in TTCN-3 core language using counterexamples of SPIN model checker.},
  keywords={Unified modeling language;Protocols;Machine-to-machine communications;Model checking;Adaptation models;Grammar},
  doi={10.1109/SysEng.2017.8088307},
  ISSN={},
  month={Oct},}

@INPROCEEDINGS{1390775,
  author={Hicks, B.},
  booktitle={The 23rd Digital Avionics Systems Conference (IEEE Cat. No.04CH37576)}, 
  title={Transforming avionics architectures to support network centric warfare}, 
  year={2004},
  volume={2},
  number={},
  pages={8.E.3-81},
  abstract={Network centric warfare was applied to different layers in the military force structure to enable commanders and direct combatants to monopolize information to increase lethality and survivability. The flow of information, the amount, type, and other attributes to be discussed, heavily impact the aviation sector of military operations and acquisition. This work concentrates on the impact of NCW on avionics architectures and provides insight to the changes required of aircraft systems to fully utilize the NCW tenets. The NCW concepts are described along with the properties of information necessary for network centric operations.},
  keywords={Aerospace electronics;Information systems;Military aircraft;Privacy;Information security;Logistics;Business;Information resources;Radar;Electrooptic devices},
  doi={10.1109/DASC.2004.1390775},
  ISSN={},
  month={Oct},}

@ARTICLE{278309,
  author={},
  journal={IEEE Std 610.13-1993}, 
  title={IEEE Standard Glossary of Computer Languages}, 
  year={1993},
  volume={},
  number={},
  pages={i-},
  abstract={},
  keywords={Standards;IEEE Standards;Dictionaries;Computer languages;Standards organizations;Patents;Mathematical models},
  doi={10.1109/IEEESTD.1993.119224},
  ISSN={},
  month={},}

@INPROCEEDINGS{7965258,
  author={Mazinanian, Davood and Tsantalis, Nikolaos},
  booktitle={2017 IEEE/ACM 39th International Conference on Software Engineering Companion (ICSE-C)}, 
  title={CSSDev: Refactoring Duplication in Cascading Style Sheets}, 
  year={2017},
  volume={},
  number={},
  pages={63-66},
  abstract={Cascading Style Sheets (CSS) is a widely-used language for defining the presentation of structured documents and user interfaces. Despite its popularity, CSS still lacks adequate tool support for everyday maintenance tasks, such as debugging and refactoring. In this paper, we present CSSDEV, a tool suite for analyzing CSS code to detect refactoring opportunities.(https://youtu.be/lu3oITi1XrQ).},
  keywords={Cascading style sheets;Tools;HTML;Browsers;Crawlers;Maintenance engineering;Runtime;Cascading Style Sheets;Preprocessors;Refactoring},
  doi={10.1109/ICSE-C.2017.7},
  ISSN={},
  month={May},}

@INPROCEEDINGS{8071326,
  author={Mordvinov, Dmitry and Litvinov, Yurii and Bryksin, Timofey},
  booktitle={2017 20th Conference of Open Innovations Association (FRUCT)}, 
  title={TRIK studio: Technical introduction}, 
  year={2017},
  volume={},
  number={},
  pages={296-308},
  abstract={This paper presents TRIK Studio - an environment for visual (and textual) programming of robotic kits, which is used in educational organizations across Russia and Europe. First part of the article provides overview of the system - its purpose, features, differences from similar programming environments, general difficulties of robot programming and solutions proposed by TRIK Studio. Second part presents implementation details of TRIK Studio and its most interesting components. This article combines five fields of study: robotics, domain-specific visual modeling, education, formal methods and methods of program analysis. Main contribution of this article is detailed technical description of TRIK Studio as complex and successful open-source cross-platform robot programming environment written in C++/Qt, and first part of the article can also be interesting for teachers as it provides an overview of existing robot programming tools and related problems.},
  keywords={Visualization;Technological innovation;Semantics;Tools;Task analysis;Middleware;Pupils},
  doi={10.23919/FRUCT.2017.8071326},
  ISSN={2305-7254},
  month={April},}

@INPROCEEDINGS{10186634,
  author={Scheuer, Franz and Gambi, Alessio and Arcaini, Paolo},
  booktitle={2023 IEEE Intelligent Vehicles Symposium (IV)}, 
  title={STRETCH: Generating Challenging Scenarios for Testing Collision Avoidance Systems}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={Collision avoidance systems are fundamental for autonomous driving and need to be tested thoroughly to check whether they safely handle critical scenarios. Testing collision avoidance systems is generally done by means of scenario-based testing using simulators and comes with the main challenge of generating situations that are realistic but avoidable. In other words, driving scenarios must stress the collision avoidance functionalities while being representative. Existing crash databases and accident reports describe observed accidents and enable to (re)create realistic collisions in simulations; however, as those data sources focus on the impact, their data do not generally lead to avoidable collision scenarios. To address this issue, we propose STRETCH, which generates realistic, critical, and avoidable collision scenarios by extending focused collision descriptions using a multi-objective optimization algorithm. Thanks to STRETCH, developers and testers can automatically generate challenging test cases based on realistic crash scenarios.},
  keywords={Databases;Soft sensors;Vehicle crash testing;Data models;Collision avoidance;Autonomous vehicles;Stress;autonomous driving;avoidable collisions;search-based testing;collision avoidance systems;reactive planner},
  doi={10.1109/IV55152.2023.10186634},
  ISSN={2642-7214},
  month={June},}

@ARTICLE{5621965,
  author={Schwartz, Mischa},
  journal={IEEE Communications Magazine}, 
  title={X.25 Virtual Circuits - TRANSPAC IN France - Pre-Internet Data Networking [History of communications]}, 
  year={2010},
  volume={48},
  number={11},
  pages={40-46},
  abstract={The following article , by Remi Despres, is the second on the history of X.25 systems to appear in this column. As noted by Dr. Despres, the previous article focused on the Canadian Datapac system. Earlier articles on packet switching in this column have included one on the history of the Arpanet/Internet and one on early British packet switching systems. What makes this article particularly distinctive, aside, of course, from the fact that it focuses on the major contributions of French engineers to the development of packet switching as well as to X.25 standardization, is that it carefully outlines the reasons for the choice of connection-oriented virtual circuits for the Transpac network, as contrasted with datagram-based packet switching adopted for Arpanet. Interestingly, Dr. Despres notes that the idea of using virtual-circuit connection-oriented packet switching in the Transpac development came from the British packet switching activity. It is to be noted that early commercial packet switching networks in the United States, such as Tymnet and Telenet, also adopted the virtual circuit paradigm.},
  keywords={Packet switching;Virtual circuits;Protocols;Internet;History;Software},
  doi={10.1109/MCOM.2010.5621965},
  ISSN={1558-1896},
  month={November},}

@INPROCEEDINGS{9984994,
  author={Drusinsky, Doron and Michael, James Bret and Litton, Matthew},
  booktitle={2022 IEEE International Symposium on Software Reliability Engineering Workshops (ISSREW)}, 
  title={Machine-Learned Specifications for the Verification and Validation of Autonomous Cyberphysical Systems}, 
  year={2022},
  volume={},
  number={},
  pages={333-341},
  abstract={Machine learning classifiers can be used as speci-fications for runtime monitoring (RM), which in turn supports evaluating autonomous systems during design-time and detecting/responding to exceptional situations during system operation. In this paper we describe how the use of machine-learned specifications enhances the effectiveness of RM for verification and validation (V & V) of autonomous cyberphysical systems (CPSs). In addition, we show that the development of machine-learned specifications has a predictable cost, at less than $100 per specification, using 2022 cloud computing pricing. Finally, a key benefit of our approach is that developing specifications by training ML models brings the task of developing robust specifications from the realm of doctoral-level experts into the domain of system developers and engineers.},
  keywords={Training;Runtime;Systems operation;Pricing;Medical services;Machine learning;Cyber-physical systems;verification and validation;formal methods;machine learning;specifications;autonomous systems;cyberphysical systems},
  doi={10.1109/ISSREW55968.2022.00089},
  ISSN={},
  month={Oct},}

@INPROCEEDINGS{7872763,
  author={Akbar, Sabriansyah Rizqika and Kurniawan, Wijaya and Ichsan, Mochammad Hannats Hanafi and Arwani, Issa and Handono, Maystya Tri},
  booktitle={2016 International Conference on Advanced Computer Science and Information Systems (ICACSIS)}, 
  title={Pervasive device and service discovery protocol in XBee sensor network}, 
  year={2016},
  volume={},
  number={},
  pages={79-84},
  abstract={Internet of Things is a novel paradigm that combined microcomputer and wireless communication technology. IoT device will be considered as a pervasive and ubiquitous device that able to interact with it user and environment autonomously with minimum human intervention. At present, wireless technology already has pervasive features in the link and the network layer. They able to do the dynamic addressing, finding a neighbor and do the routing task such as in Xbee technology. In the future, pervasive sensing will support adaptive context-aware services that are not provided by the wireless sensor network protocol in the link and the network layer. Our research proposed pervasive device and service discovery protocol at the application level by creating a protocol in the smart sensor device and the smart sensor gateway. By implementing our protocol, the sensor network gateway is able to find each of the sensor network device and service descriptions and request the on-demand service to the smart sensor device. The protocol is implemented in two Arduino Uno integrated with XBee transceiver as the smart sensor device and the raspberry pi as the smart sensor gateway. Result shows, the gateway was able to find both device and service description in the smart sensor network with 4.13 seconds average time. The average round trip time for request and response data from the gateway is 0.201 seconds.},
  keywords={Logic gates;Intelligent sensors;Protocols;Machine-to-machine communications;Temperature sensors;Humidity;Wireless sensor networks;Internet of Things;pervasive discovery;sensor network},
  doi={10.1109/ICACSIS.2016.7872763},
  ISSN={},
  month={Oct},}

@INPROCEEDINGS{10298497,
  author={Bar-Sinai, Michael and Elyasaf, Achiya and Weiss, Gera and Weiss, Yeshayahu},
  booktitle={2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE)}, 
  title={Provengo: A Tool Suite for Scenario Driven Model-Based Testing}, 
  year={2023},
  volume={},
  number={},
  pages={2062-2065},
  abstract={We present Provengo, a comprehensive suite of tools designed to facilitate the implementation of Scenario-Driven Model-Based Testing (SDMBT), an innovative approach that utilizes scenarios to construct a model encompassing the user's perspective and the system's business value while also defining the desired outcomes. With the assistance of Provengo, testers gain the ability to effortlessly create natural user stories and seamlessly integrate them into a model capable of generating effective tests. The demonstration illustrates how SDMBT effectively addresses the bootstrapping challenge commonly encountered in model-based testing (MBT) by enabling incremental development, starting from simple models and gradually augmenting them with additional stories.},
  keywords={Testing;Software engineering;Business},
  doi={10.1109/ASE56229.2023.00146},
  ISSN={2643-1572},
  month={Sep.},}

@ARTICLE{10132586,
  author={Elyasaf, Achiya and Farchi, Eitan and Margalit, Oded and Weiss, Gera and Weiss, Yeshayahu},
  journal={IEEE Transactions on Software Engineering}, 
  title={Generalized Coverage Criteria for Combinatorial Sequence Testing}, 
  year={2023},
  volume={49},
  number={8},
  pages={4023-4034},
  abstract={We present a new model-based approach for testing systems that use sequences of actions and assertions as test vectors. Our solution includes a method for quantifying testing quality, a tool for generating high-quality test suites based on the coverage criteria we propose, and a framework for assessing risks. For testing quality, we propose a method that specifies generalized coverage criteria over sequences of actions, which extends previous approaches. Our publicly available tool demonstrates how to extract effective test suites from test plans based on these criteria. We also present a Bayesian approach for measuring the probabilities of bugs or risks, and show how this quantification can help achieve an informed balance between exploitation and exploration in testing. Finally, we provide an empirical evaluation demonstrating the effectiveness of our tool in finding bugs, assessing risks, and achieving coverage.},
  keywords={Testing;Behavioral sciences;Bayes methods;Computer bugs;Automata;Codes;Synchronization;Bayesian risk-Reduction;behavioral programming;combinatorial test design;model-based testing;sequence testing;test coverage;test generation;test optimization},
  doi={10.1109/TSE.2023.3279570},
  ISSN={1939-3520},
  month={Aug},}

@INPROCEEDINGS{9504763,
  author={Moghadam, Mahshid Helali and Hamidi, Golrokh and Borg, Markus and Saadatmand, Mehrdad and Bohlin, Markus and Lisper, Björn and Potena, Pasqualina},
  booktitle={2021 IEEE Congress on Evolutionary Computation (CEC)}, 
  title={Performance Testing Using a Smart Reinforcement Learning-Driven Test Agent}, 
  year={2021},
  volume={},
  number={},
  pages={2385-2394},
  abstract={Performance testing with the aim of generating an efficient and effective workload to identify performance issues is challenging. Many of the automated approaches mainly rely on analyzing system models, source code, or extracting the usage pattern of the system during the execution. However, such information and artifacts are not always available. Moreover, all the transactions within a generated workload do not impact the performance of the system the same way, a finely tuned workload could accomplish the test objective in an efficient way. Model-free reinforcement learning is widely used for finding the optimal behavior to accomplish an objective in many decision-making problems without relying on a model of the system. This paper proposes that if the optimal policy (way) for generating test workload to meet a test objective can be learned by a test agent, then efficient test automation would be possible without relying on system models or source code. We present a self-adaptive reinforcement learning-driven load testing agent, RELOAD, that learns the optimal policy for test workload generation and generates an effective workload efficiently to meet the test objective. Once the agent learns the optimal policy, it can reuse the learned policy in subsequent testing activities. Our experiments show that the proposed intelligent load test agent can accomplish the test objective with lower test cost compared to common load testing procedures, and results in higher test efficiency.},
  keywords={Analytical models;Automation;Transfer learning;Decision making;Reinforcement learning;Knowledge representation;Evolutionary computation;performance testing;load testing;workload generation;reinforcement learning;autonomous testing},
  doi={10.1109/CEC45853.2021.9504763},
  ISSN={},
  month={June},}

@INPROCEEDINGS{7329720,
  author={Sroka, Michal and Nagy, Roman and Fisch, Dominik},
  booktitle={2015 IEEE 19th International Conference on Intelligent Engineering Systems (INES)}, 
  title={Impact of mutation intensity on evolutionary test model learning}, 
  year={2015},
  volume={},
  number={},
  pages={271-276},
  abstract={Automation in the software testing process has significant impact on the overall software development in industry. The focus of this paper is on automation of test case design via model-based testing for automotive embedded software. A new method based on an evolutionary algorithm for acquiring the necessary test model automatically from sample test cases and additional sources of information was designed and this paper investigates the impact of mutation intensity on the evolutionary learning process.},
  keywords={Biological cells;Sociology;Statistics;Software;Testing;Evolutionary computation;Software algorithms},
  doi={10.1109/INES.2015.7329720},
  ISSN={},
  month={Sep.},}

@INPROCEEDINGS{6581637,
  author={Neufeldt, Holger and Stanzel, Stefan},
  booktitle={2013 14th International Radar Symposium (IRS)}, 
  title={An operational WAM in frankfurt airspace}, 
  year={2013},
  volume={2},
  number={},
  pages={561-566},
  abstract={Extending the capacity of Frankfurt airport with an additional runway to the north of the existing runway system, the surveillance capability in Frankfurt Terminal Maneuvering Area (TMA) had to be adapted as well. Based on their surveillance strategy, the DFS invested in multilateration technology to establish a Precision Approach Monitor (PAM) system and integrate it into the existing surveillance infrastructure. After a phase of thorough planning and preparation and an open tender process, Thales was contracted to implement the PAM FRA system which is now going into operation. This paper reports the successful implementation and testing of an operational WAM system in one of the most congested airspaces of the world. Transponder anomalies found as well as methods and strategies to achieve required performances are presented.},
  keywords={Surveillance;Airports;Accuracy;Radar tracking;Transponders;Receivers},
  doi={},
  ISSN={2155-5753},
  month={June},}

@INPROCEEDINGS{7081882,
  author={Laverdière, Marc-André and Berger, Bernhard J. and Merloz, Ettore},
  booktitle={2015 IEEE 22nd International Conference on Software Analysis, Evolution, and Reengineering (SANER)}, 
  title={Taint analysis of manual service compositions using Cross-Application Call Graphs}, 
  year={2015},
  volume={},
  number={},
  pages={585-589},
  abstract={We propose an extension over the traditional call graph to incorporate edges representing control flow between web services, named the Cross-Application Call Graph (CACG). We introduce a construction algorithm for applications built on the Jax-WS standard and validate its effectiveness on sample applications from Apache CXF and JBossWS. Then, we demonstrate its applicability for taint analysis over a sample application of our making. Our CACG construction algorithm accurately identifies service call targets 81.07% of the time on average. Our taint analysis obtains a F-Measure of 95.60% over a benchmark. The use of a CACG, compared to a naive approach, improves the F-Measure of a taint analysis from 66.67% to 100.00% for our sample application.},
  keywords={Web services;Benchmark testing;Java;Security;Manuals;Algorithm design and analysis;Androids},
  doi={10.1109/SANER.2015.7081882},
  ISSN={1534-5351},
  month={March},}

@INPROCEEDINGS{1342755,
  author={Andrews, J.H.},
  booktitle={Proceedings. 19th International Conference on Automated Software Engineering, 2004.}, 
  title={A case study of coverage-checked random data structure testing}, 
  year={2004},
  volume={},
  number={},
  pages={316-319},
  abstract={We study coverage-checked random unit testing (CRUT), the practice of repeatedly testing units on sequences of random function calls until given code coverage goals are achieved. Previous research has shown that this practice can be a useful complement to traditional testing methods. However, questions remained as to the breadth of its applicability. In this paper, we report on a case study in which we applied CRUT to the testing of two mature public-domain data structures packages. We show that CRUT helped in identifying faults, in debugging, in extracting and specifying actual behaviour, and in achieving greater assurance of the correctness of the debugged software},
  keywords={Computer aided software engineering;Data structures;Software testing;Packaging;Fault diagnosis;Software engineering;Automatic testing;Documentation;Computer science;Debugging},
  doi={10.1109/ASE.2004.1342755},
  ISSN={1938-4300},
  month={Sep.},}

@INPROCEEDINGS{9606984,
  author={Han, Zhao and Wang, Deyan and Rutsch, Gabriel and Li, Bowen and Prebeck, Sebastian Siegfried and Lopera, Daniela Sanchez and Devarajegowda, Keerthikumara and Ecker, Wolfgang},
  booktitle={2021 IFIP/IEEE 29th International Conference on Very Large Scale Integration (VLSI-SoC)}, 
  title={Aspect-Oriented Design Automation with Model Transformation}, 
  year={2021},
  volume={},
  number={},
  pages={1-6},
  abstract={Despite the high configurability of IPs and hardware generators, code modifications are still required to introduce aspect-oriented instrumentation to satisfy emerging design requirements such as on-chip debug and functional safety. These code modifications lead to escalated development, verification efforts and deteriorate the code reuse. This paper proposes a highly efficient aspect-oriented design automation approach that leverages graph-grammar-based model transformations. With the proposed approach, main design functionalities and aspect-oriented instrumentation are separately developed, automatically integrated and verified. To demonstrate the applicability, industrial SoCs were transformed to support on-chip debug. Experimental results confirm the efficiency of the approach. Further, reduced code is needed with the proposed automation approach, which also replaces the error-prone manual RTL coding. Finally, the transformation scripts are applicable to different SoCs, which promotes the overall code reuse.},
  keywords={Codes;Design automation;Automation;Instruments;Manuals;Very large scale integration;Hardware;Electronic Design Automation;Aspect-Oriented Programming;Model-Driven Architecture},
  doi={10.1109/VLSI-SoC53125.2021.9606984},
  ISSN={2324-8440},
  month={Oct},}

@INPROCEEDINGS{10154223,
  author={Frank, Alexander and Hommel, Wolfgang and Hopfner, Benedikt},
  booktitle={NOMS 2023-2023 IEEE/IFIP Network Operations and Management Symposium}, 
  title={An Intermediary Protocol Representation to Aid in Avionics Network Development}, 
  year={2023},
  volume={},
  number={},
  pages={1-5},
  abstract={Complex networked systems like aircraft or unmanned aerial vehicles (UAVs) often employ purpose-built network protocols. To eliminate design or implementation errors, general-purpose and domain-specific simulation/analysis tools are used. For newly developed protocols, these require dedicated configuration posing another source of errors and being cumbersome due to tool-specific protocol representations.As a first step towards alleviating this problem, we propose an Intermediary Protocol Representation (InPR) that can be generated programmatically, i.e., from existing machine-readable protocol specifications. An InPR can serve as the single basis for generating protocol specifications for other analysis tools, reducing the recurring implementation overhead.For demonstration purposes, we design and implement an InPR for protocols appearing in the aerospace industry and showcase how it can drive certain analysis tasks by itself or by generating configurations for other analysis tools.},
  keywords={Protocols;Codes;Aerospace electronics;Autonomous aerial vehicles;Task analysis;Aircraft;Aerospace industry;code generation;networking;avionics},
  doi={10.1109/NOMS56928.2023.10154223},
  ISSN={2374-9709},
  month={May},}

@INPROCEEDINGS{10730620,
  author={Song, Lei and Li, Jiaxin},
  booktitle={2024 3rd International Conference on Artificial Intelligence and Computer Information Technology (AICIT)}, 
  title={eBPF: Pioneering Kernel Programmability and System Observability - Past, Present, and Future Insights}, 
  year={2024},
  volume={},
  number={},
  pages={1-10},
  abstract={The eBPF (Extended Berkeley Packet Filter) is a state-of-the-art foundational programming infrastructure that occupies a pivotal position in the progression and deployment of modern operating systems and network technologies. With the profound advancement in the core technologies of the Linux kernel, the eBPF has exerted a significant and far-reaching impact on the overall domain development by virtue of its distinctive intrinsic characteristics. This research delves into the operational mechanics and fundamental technical attributes of eBPF technology, encompassing critical facets such as its kernel-level programming paradigm, the Just-In-Time (JIT) compilation mechanism, and the stringent security validation procedures. Additionally, through an exhaustive review and appraisal of scholarly literature pertaining to eBPF applications over recent years, the paper elucidates the practical accomplishments, benefits, and constraints of eBPF across various application domains. Finally, premised upon this foundation, this article charts out the existing challenges confronted by eBPF and proposes potential avenues for future research endeavors.},
  keywords={Technological innovation;Reviews;Source coding;Security management;Linux;Programming;Kernel;Observability;Information technology;Optimization;eBPF;linux;programmability;kernel extensions;linux kernel networking},
  doi={10.1109/AICIT62434.2024.10730620},
  ISSN={},
  month={Sep.},}

@ARTICLE{10688405,
  author={Lunnikivi, Henri and Hämäläinen, Roni and Hämäläinen, Timo D.},
  journal={IEEE Transactions on Very Large Scale Integration (VLSI) Systems}, 
  title={Keelhaul: Processor-Driven Chip Connectivity and Memory Map Metadata Validator for Large Systems-on-Chip}, 
  year={2024},
  volume={32},
  number={12},
  pages={2269-2280},
  abstract={The integration of large-scale systems-on-chip warrants thorough verification both at the level of the individual component and at the system level. In this article, we address the automated testing of system-level memory maps. The golden reference is the IEEE 1685/IP-XACT hardware description, which includes implementation agnostic definitions for the global memory map. The IP-XACT description is used as a specification for implementing the registers and memory regions in a register transfer-level (RTL) language, and for implementing the corresponding hardware-dependent software. The challenge is that hardware design changes might not always propagate to firmware and applications developers, which causes errors and faults. We present a method and a tool called Keelhaul which takes as input the CMSIS-SVD format commonly used for firmware development and generates automated software tests that attempt to access all available memory mapped input/output registers. During development of a large-scale research-focused multiprocessor system-on-chip, we ran a total of 32 automatically generated test suites per pipeline comprising 882 test cases for each of its two CPU subsystems. A total of 15 distinct issues were found by the tool in the lead-up to tapeout. Another research-focused SoC was validated posttapeout with 984 test cases generated for each core, resulting in the discovery of four distinct issues. Keelhaul can be used with any IP-XACT or CMSIS-SVD-based systems-on-chip that include processors for accessing implemented registers and memory regions.},
  keywords={Electronic ballasts;Registers;Software;Hardware;Generators;Computer architecture;Codes;CMSIS-SVD;IEEE-1685;IP-XACT;memory-mapped input/output (MMIO);rust;test generation;verification},
  doi={10.1109/TVLSI.2024.3454431},
  ISSN={1557-9999},
  month={Dec},}

@INPROCEEDINGS{8972261,
  author={Otte, Marcel and Rohjans, Sebastian and Andrén, Filip Pröstl and Strasser, Thomas I.},
  booktitle={2019 IEEE 17th International Conference on Industrial Informatics (INDIN)}, 
  title={Applying Machine Learning Concepts to Enhance the Smart Grid Engineering Process}, 
  year={2019},
  volume={1},
  number={},
  pages={1687-1693},
  abstract={The expansion of renewable energy sources, as an effort to reduce global warming and to guarantee a sustainable energy supply, forces the electrical energy systems into enhanced complexity through new requirements, actors, technological approaches or business models. This complexity is also noticed in the smart grid engineering process, resulting in increasing effort and costs. By applying machine learning concepts on the engineering process it is possible to decrease the work-effort and minimize tedious and error prone manual tasks. This work introduces three machine learning concepts and shows how they can improve the smart grid engineering process by applying a clustering approach to give recommendations of standards that are useful for the developed use case. According to their implementation-feasibility an evaluation based on the state-of-the-art is pursued. Furthermore, a tool prototype indicates current and future application possibilities of machine learning in the smart grid engineering process.},
  keywords={Renewable energy sources;Prototypes;Machine learning;Manuals;Tools;Smart grids;Complexity theory;Engineering process;machine Learning;smart grid;standardization;support systems},
  doi={10.1109/INDIN41052.2019.8972261},
  ISSN={2378-363X},
  month={July},}

@INPROCEEDINGS{10764815,
  author={Zhang, Man and Arcuri, Andrea and Teng, Piyun and Xue, Kaiming and Wang, Wenhao},
  booktitle={2024 39th IEEE/ACM International Conference on Automated Software Engineering (ASE)}, 
  title={Seeding and Mocking in White-Box Fuzzing Enterprise RPC APIs: An Industrial Case Study*}, 
  year={2024},
  volume={},
  number={},
  pages={2024-2034},
  abstract={Microservices is now becoming a promising architecture to build large-scale web services in industry. Due to the high complexity of enterprise microservices, industry has an urgent need to have a solution to enable automated testing of such systems. EvoMaster is an open-source fuzzer, equipped with the state-of-the-art techniques for supporting automated system-level testing of Web APIs. It has been assessed as the most performant tool in two recent empirical studies in terms of line coverage and fault detection. In this paper, we carried out an empirical experiment to investigate how to better apply the state-of-the-art academic prototype (i.e., EvoMaster) in industrial context. We extended the tool to handle seeding of existing industrial tests, and mocking of external services with their data handled as part of the input fuzzing. We studied two configurations of EvoMaster, using two time budgets, on 40 enterprise RPC-based APIs (involving 5.6 million lines of code for their core business logic) at Meituan. Results show that, compared to existing practice of manual system-level testing and tests produced by record and replay of online traffic, EvoMaster demonstrates clear additional benefits. EvoMaster with the best configuration is capable of covering up to 32.4% line coverage, covering more than 10% line coverage on 36 out of 40 (90%) case studies, and identifying on average 3520 potential faults in these 40 APIs. In addition, we also identified and discussed important challenges in fuzzing enterprise microservices that must be addressed in the future.CCS CONCEPTS• Software and its engineering → Search-based software engineering; Software verification and validation.},
  keywords={Industries;Fault diagnosis;Fault detection;Microservice architectures;Prototypes;Fuzzing;Logic;Testing;Software engineering;Glass box;Fuzzing;SBST;Microservices;Automated Test Case Generation},
  doi={},
  ISSN={2643-1572},
  month={Oct},}

@INPROCEEDINGS{7601515,
  author={Sroka, Michal and Fisch, Dominik and Nagy, Roman},
  booktitle={2016 IEEE 14th International Symposium on Intelligent Systems and Informatics (SISY)}, 
  title={Impact of crossover and mutation on reproduction in evolutionary test model learning}, 
  year={2016},
  volume={},
  number={},
  pages={39-44},
  abstract={Automation in the software test design process has a significant impact on the software testing process and therefore also on the overall software development in the industry. The focus of this paper is on the automation of test case design via model-based testing for automotive embedded software. A method based on an evolutionary algorithm for acquiring the necessary test model automatically from sample test cases and additional sources of information is briefly described. This paper further investigates the impact of reproduction configuration on the evolutionary learning method.},
  keywords={Biological cells;Sociology;Statistics;Software;Testing;Software algorithms;Evolutionary computation},
  doi={10.1109/SISY.2016.7601515},
  ISSN={1949-0488},
  month={Aug},}

@INPROCEEDINGS{4384854,
  author={Ferreira, Joao and Carvalho, Andre and Pimentel, Joao and Guedes, Marco and Furini, Francesco and Silva, Nuno},
  booktitle={2007 5th IEEE International Conference on Industrial Informatics}, 
  title={Modeling Engineering and Manufacturing Activity in Vehicle Development Process}, 
  year={2007},
  volume={2},
  number={},
  pages={675-680},
  abstract={Designing and consequent assembly of a new vehicle is a complex process as it requires close coordination and inputs from a number of disciplines in developing a number of systems and sub-systems in the vehicle that should fit within the confined vehicle space, function and provide the customers an acceptable combination of all relevant vehicle attributes. Understanding how these processes interact and how they are aligned with other while they should support the tasks involved in the conception of a new vehicle at a minimum time and cost. The first step to achieve this goal is the definition of a new UML profile (called VDML, Vehicle Development Modeling Language) based on the extension mechanics of UML (industry standard language) to assist business process description and consequent improvements achieved by the high level vision. To show the benefits of this new language applied to this specific business we model the engineer and manufacturing activity process using VDML},
  keywords={Automotive engineering;Manufacturing processes;Virtual manufacturing;Space vehicles;Unified modeling language;Process design;Assembly systems;Costs;Manufacturing industries;Standards development},
  doi={10.1109/INDIN.2007.4384854},
  ISSN={2378-363X},
  month={June},}

@INPROCEEDINGS{9282808,
  author={Jebbar, Oussama and Khendek, Ferhat and Toeroe, Maria},
  booktitle={2020 IEEE 20th International Conference on Software Quality, Reliability and Security (QRS)}, 
  title={Architecture for the Automation of Live Testing of Cloud Systems}, 
  year={2020},
  volume={},
  number={},
  pages={142-151},
  abstract={Live testing is performed in the production environment. In such environment, test activities have to be orchestrated properly to avoid interferences with normal usage traffic. Conducting live testing activities manually is error prone because of the size and the complexity of the system as well as the required complex orchestration of different tasks. Furthermore, it would be impossible to react to failures and contain them in due time without automation. Live testing requires a high level of automation. This automation comes with several challenges especially in contexts such as cloud and zero touch networks because of the diversity of the software composing them. In this paper we discuss the challenges of automating live testing for cloud systems. We propose an architecture that relies on a modeling framework to decouple the specification of testing activities from the platforms needed to conduct them. We propose a solution for conducting testing activities on a live system according to such a specification.},
  keywords={Automation;Computer architecture;Production;Complexity theory;Specification languages;Task analysis;Testing;live testing;cloud;UML Testing Profile;test architecture;automation},
  doi={10.1109/QRS51102.2020.00030},
  ISSN={},
  month={Dec},}

@INPROCEEDINGS{6912285,
  author={Wohlrab, Rebekka and de Gooijer, Thijmen and Koziolek, Anne and Becker, Steffen},
  booktitle={2014 IEEE 22nd International Requirements Engineering Conference (RE)}, 
  title={Experience of pragmatically combining RE methods for performance requirements in industry}, 
  year={2014},
  volume={},
  number={},
  pages={344-353},
  abstract={To meet end-user performance expectations, precise performance requirements are needed during development and testing, e.g., to conduct detailed performance and load tests. However, in practice, several factors complicate performance requirements elicitation: lacking skills in performance requirements engineering, outdated or unavailable functional specifications and architecture models, the specification of the system's context, lack of experience to collect good performance requirements in an industrial setting with very limited time, etc. From the small set of available non-functional requirements engineering methods, no method exists that alone leads to precise and complete performance requirements with feasible effort and which has been reported to work in an industrial setting. In this paper, we present our experiences in combining existing requirements engineering methods into a performance requirements method called PROPRE. It has been designed to require no up-to-date system documentation and to be applicable with limited time and effort. We have successfully applied PROPRE in an industrial case study from the process automation domain. Our lessons learned show that the stakeholders gathered good performance requirements which now improve performance testing.},
  keywords={Measurement;Context;Time factors;Documentation;Adaptation models;Throughput;Testing},
  doi={10.1109/RE.2014.6912285},
  ISSN={2332-6441},
  month={Aug},}

@INPROCEEDINGS{7755285,
  author={Jain, Manish and Gopalani, Dinesh},
  booktitle={2016 International Conference on Electrical, Electronics, and Optimization Techniques (ICEEOT)}, 
  title={Testing application security with aspects}, 
  year={2016},
  volume={},
  number={},
  pages={3161-3165},
  abstract={For the purpose of security of the computer systems, organizations now a days plan a lot of things like firewalls, network scanning tools, secure sockets layer (SSL) etc. However security bugs present at the application layer (code level) caused by unawareness or mistakes of the developers are usually ignored. Such security bugs can lead to unauthorized privileges on a computer system. For example most web applications connect back to databases which contain sensitive information. Malicious input can allow the attacker to alter the flow of the web application and provide unauthorized access to the confidential database. Hence proper security tests are required to be conducted in order to assess the security of applications. In this paper, we propose the use of Aspect Oriented Programming (AOP) for the purpose of security testing of Java applications. With the examples of fuzz testing and servlet testing using aspects, we will show how AOP can be used for detection of security bugs in Java applications by creeping inside the program without making any changes to the source code.},
  keywords={Security;Testing;Java;Databases;Computer bugs;HTML;Programming;Aspect Oriented Programming;Security Testing;Software Testing;Aspects;AOP},
  doi={10.1109/ICEEOT.2016.7755285},
  ISSN={},
  month={March},}

@INPROCEEDINGS{11029551,
  author={Al-Zuraiqi, Ahmad and Greer, Des},
  booktitle={2025 IEEE/ACM 6th International Workshop on Engineering and Cybersecurity of Critical Systems (EnCyCriS)}, 
  title={Static Analysis of IoT Firmware: Identifying Systemic Vulnerabilities with RMMIDL}, 
  year={2025},
  volume={},
  number={},
  pages={7-14},
  abstract={The unprecedented surge in Internet of Things (IoT) device deployment has brought forth significant security challenges, primarily arising from vulnerabilities within firmware that facilitate unauthorized access, data exfiltration, and network exploitation. This study undertakes a comprehensive static analysis of 1,520 IoT firmware samples using the Firmware Analysis and Comparison Tool (FACT) alongside metadata from the WikiDevi archive to systematically identify inherent security flaws. Among the key vulnerabilities discovered are improper handling of format strings (CWE-134, 10.07%), memory mismanagement issues (CWE-416, 10.06 %; CWE-415, 10.03 %), and the presence of exposed debugging interfaces (CWE-782, 10.07%). These results highlight enduring risks in critical domains such as healthcare and industrial IoT, often magnified by insecure coding practices and reliance on outdated software components. To address these systemic shortcomings, this study proposes the Risk Mitigation Modeling for IoT Development Lifecycle (RMMIDL), a secure-by-design framework that embeds proactive security measures throughout each phase of IoT development. RMMIDL offers a systematic and well-defined framework for addressing pervasive risks, enhancing the resilience of IoT ecosystems, and promoting the implementation of robust security measures. Furthermore, this study outlines prospective research directions, emphasizing the potential of integrating large language models (LLMs), broadening the scope of firmware datasets, and fostering industry-wide collaboration to drive advancements in IoT security.},
  keywords={Large language models;Ecosystems;Static analysis;Debugging;Internet of Things;Security;Object recognition;Microprogramming;Risk mitigation;Resilience;Internet of Things (IoT);Secure-by-Design;Vulnerable-by-Design;IoT Firmware Security;Firmware Analysis;Systemic Vulnerabilities;Risk Mitigation Modeling (RMMIDL);Large Language Models (LLMs)},
  doi={10.1109/EnCyCriS66464.2025.00008},
  ISSN={},
  month={May},}

@INPROCEEDINGS{7467285,
  author={Liu, Shaoying},
  booktitle={2015 Asia-Pacific Software Engineering Conference (APSEC)}, 
  title={Automatic Selection of System Functional Scenarios for Formal Specification Animation}, 
  year={2015},
  volume={},
  number={},
  pages={72-79},
  abstract={Functional scenario-based specification animation is a practical and effective technique for validating formal specifications but faces a scenario explosion problem. In this paper, we tackle this problem by proposing a new approach to selecting only consistent and meaningful functional scenarios in order to reduce the number of scenarios for animation. We define the concept of consistency for functional scenarios and describe how each of them can be automatically checked by means of a testing-based formal verification technique. We have applied the proposed technqiue to a railway card system to validate its applicability and present an example extracted from the application to illustrate how the proposed technqiues works in practice.},
  keywords={Software engineering;Animation;Testing;Formal Specification;specification animation;functional scenario;specification validation;specification verification},
  doi={10.1109/APSEC.2015.15},
  ISSN={1530-1362},
  month={Dec},}

@INPROCEEDINGS{9794130,
  author={Gerten, Michael C. and Marsh, Alexis L. and Lathrop, James I. and Cohen, Myra B. and Miner, Andrew S. and Klinge, Titus H.},
  booktitle={2022 IEEE/ACM 44th International Conference on Software Engineering (ICSE)}, 
  title={Inference and Test Generation Using Program Invariants in Chemical Reaction Networks}, 
  year={2022},
  volume={},
  number={},
  pages={1193-1205},
  abstract={Chemical reaction networks (CRNs) are an emerging distributed computational paradigm where programs are encoded as a set of abstract chemical reactions. CRNs can be compiled into DNA strands which perform the computations in vitro, creating a foundation for intelligent nanodevices. Recent research proposed a software testing framework for stochastic CRN programs in simulation, however, it relies on existing program specifications. In practice, specifications are often lacking and when they do exist, transforming them into test cases is time-intensive and can be error prone. In this work, we propose an inference technique called ChemFlow which extracts 3 types of invariants from an existing CRN model. The extracted invariants can then be used for test generation or model validation against program implementations. We applied ChemFlow to 13 CRN programs ranging from toy examples to real biological models with hundreds of reactions. We find that the invariants provide strong fault detection and often exhibit less flakiness than specification derived tests. In the biological models we showed invariants to developers and they confirmed that some of these point to parts of the model that are biologically incorrect or incomplete suggesting we may be able to use ChemFlow to improve model quality.},
  keywords={Software testing;Biological system modeling;Toy manufacturing industry;Stochastic processes;Chemical reactions;Nanoscale devices;Test pattern generators;chemical reaction networks;test generation;invariants;Petri nets},
  doi={10.1145/3510003.3510176},
  ISSN={1558-1225},
  month={May},}

@INPROCEEDINGS{1677379,
  author={Smidts, C.},
  booktitle={RAMS '06. Annual Reliability and Maintainability Symposium, 2006.}, 
  title={Research in software reliability engineering}, 
  year={2006},
  volume={},
  number={},
  pages={228-233},
  abstract={Our research has focused on development of an approach to predicting software reliability based on a systematic identification of software process failure modes and their likelihoods. A direct consequence of the approach and its supporting data collection efforts is the identification of weak areas in the software development process. A Bayes framework for the quantification of software process failure mode probabilities can be useful since it allows use of historical data that are only partially relevant to the software at hand. The approach has been applied in the context of a waterfall life-cycle and for failure modes related to the requirements phase},
  keywords={Software reliability;Reliability engineering;Programming;Risk management;Predictive models;Software tools;Software testing;Mechanical engineering;Computer science education;Educational programs},
  doi={10.1109/RAMS.2006.1677379},
  ISSN={0149-144X},
  month={Jan},}

@ARTICLE{1541696,
  author={Karayannis, F. and Serrat-Fernandez, J. and Baliosian, J. and Rubio-Loyola, J. and Vaxevanakis, K.G. and Pagomenos, G. and Zahariadis, T.B.},
  journal={IEEE Communications Magazine}, 
  title={In-field evaluation of a managed IP/MPLS over WDM provisioning solution}, 
  year={2005},
  volume={43},
  number={11},
  pages={S26-S33},
  abstract={This article demonstrates results and experiences gained in the area of multilayer internetworking, with emphasis on bandwidth on-demand provisioning as well as resource and restoration management. Behavioral characteristics and numerical results were obtained from a management. system prototype implemented and tested in an appropriately adapted commercial WDM environment enhanced with multivendor gigabit IP routers. The management solution, the testbed environment, and a representative evaluation scenario are presented as a means of explaining in detail the results that finally allow a global system assessment.},
  keywords={Multiprotocol label switching;Wavelength division multiplexing;Telecommunication network management;Testing;Optical scattering;Biomedical optical imaging;Asynchronous transfer mode;Information management;Inventory management;Synchronous digital hierarchy},
  doi={10.1109/MCOM.2005.1541696},
  ISSN={1558-1896},
  month={Nov},}

@INPROCEEDINGS{6984580,
  author={Oliveira, Pedro and Souza, Matheus and Braga, Ronyerison and Britto, Ricardo and Rabêlo, Ricardo Lira and Neto, Pedro Santos},
  booktitle={2014 IEEE 26th International Conference on Tools with Artificial Intelligence}, 
  title={Athena: A Visual Tool to Support the Development of Computational Intelligence Systems}, 
  year={2014},
  volume={},
  number={},
  pages={950-959},
  abstract={Computational Intelligence (CI) embraces techniques designed to address complex real-world problems in which traditional approaches are ineffective or infeasible. Some of these techniques are being used to solve several complex problems, such as the team allocation, building products portfolios in a software product line and test case selection/prioritization. However, despite the usefulness of these applications, the development of solutions based in CI techniques is not a trivial activity, since it involves the implementation/adaptation of algorithms to specific context and problems. This work presents Athena, a visual tool developed aiming at offering a simple approach to develop CI-based software systems. In order to do this, we proposed a drag-and-drop approach, which we called CI as a Service (CIaaS). Based on a preliminary study, we can state that Athena can help researchers to save time during the development of computational intelligence approaches.},
  keywords={Visualization;Productivity;Remuneration;Computational modeling;Algorithm design and analysis;Computational intelligence;Resource management;Computational Intelligence;Artificial Intelligence;Visual Programming;Tool;Service},
  doi={10.1109/ICTAI.2014.144},
  ISSN={2375-0197},
  month={Nov},}

@INPROCEEDINGS{9323015,
  author={Parrott, Chester and Carver, Doris},
  booktitle={2020 3rd International Conference on Data Intelligence and Security (ICDIS)}, 
  title={Lodestone: A Streaming Approach to Behavior Modeling and Load Testing}, 
  year={2020},
  volume={},
  number={},
  pages={109-116},
  abstract={It is evident that our technologically-dependent society rightly expects systems engineers to produce systems having increasing levels of security, performance, efficiency, and reliability. In addition, such systems must be able to handle sudden massive amounts of usage as well as withstand cyber-attacks such as Distributed Denial of Service (DDoS). As such, we must convolve academic and industrial data science approaches to provide theory, systems, and working technologies that can catalyze and propel engineers, developers, and technical professionals of various disciplines toward the ultimate goal of consistent delivery of quality systems. Tools and processes exist for improving the quality-oriented posture of the systems engineering industry; in practice, the most perpetual form of software testing continues to be the rote repetition of test cases through either manual testing or scripted automation of those same manual tests. We describe Lodestone: a real-time data science approach for generating workload in software systems. This real-time approach to load testing uses streaming log data to generate and dynamically update user behavior models, cluster them into similar behavior profiles, and instantiate distributed workload of software systems. We show that Lodestone outperforms Markov4JMeter on JMeter through a qualitative comparison of key feature parameters as well as experimentation based on shared data and models.},
  keywords={Testing;Measurement;Data models;Tools;Load modeling;Adaptation models;Security;Software quality;Software performance;System performance;Performance analysis;Software testing;System testing;Automatic testing;Automatic test pattern generation},
  doi={10.1109/ICDIS50059.2020.00021},
  ISSN={},
  month={June},}

@INPROCEEDINGS{4702753,
  author={Schavey, Todd and Duba, Shane},
  booktitle={2008 IEEE/AIAA 27th Digital Avionics Systems Conference}, 
  title={Streamlining IMA integration through model-driven methodologies}, 
  year={2008},
  volume={},
  number={},
  pages={1.B.3-1-1.B.3-5},
  abstract={Avionics systems integration is an inherently complex undertaking. In addition to ensuring that basic functionality is satisfied, the systems integrator must maximize the systempsilas flexibility and reliability while minimizing weight and cost of change. With the introduction of integrated modular architectures (IMA) based on open standards, many traditional integration issues have been greatly improved. However, additional integration responsibilities arise due to having a large number of functions developed by independent suppliers all sharing the same physical resources. This paper describes the benefits of incorporating model-driven methodologies and associated tools into the systems integration process through Model Driven Integration (MDI). It describes how a system model can not only streamline the additional responsibilities but also establish a highly productive systems development environment and allow for virtual integration. In addition, this paper discusses a number of side- benefits that grow out of having a modeling tool platform such as enhanced team communication and automation opportunities. The analysis of avionics architectures and the use of model-driven methodologies to increase IMA manageability is based upon the authors' experience in developing platform computing systems at GE Aviation. GE Aviation has developed open system IMA architectures for both commercial aircraft and military aircraft.},
  keywords={Aerospace electronics;Hardware;Cost function;Computer architecture;Humans;Biological system modeling;Manufacturing;Military aircraft;Muscles;Automotive engineering},
  doi={10.1109/DASC.2008.4702753},
  ISSN={2155-7209},
  month={Oct},}

@INPROCEEDINGS{7546576,
  author={Jain, Manish and Gopalani, Dinesh},
  booktitle={2016 Second International Conference on Computational Intelligence & Communication Technology (CICT)}, 
  title={Aspect Oriented Programming and Types of Software Testing}, 
  year={2016},
  volume={},
  number={},
  pages={64-69},
  abstract={Software testing is a process to determine that a software product satisfies the specified requirements. Software testing spans over all phases of the Software Development Life Cycle namely, requirement specification, analysis, designing, development, deployment and maintenance of a software. Software testing is important to point out the defects in the software and to ensure that the developed software works fine in the real environment with different operating systems, devices, browsers and concurrent users. Further software testing can be classified into various types based on the objective of testing, level at which the testing is performed, knowledge of the system or the degree of automation. In this paper, we examine the suitability of Aspect Oriented Programming (AOP) for the purpose of performing various types of software testing. AOP is a programming paradigm which modularizes the crosscutting concerns into units called aspects and separates them from the modules implementing the primary business logic. This leads to a system that is easier to understand and simpler to maintain. The basis of the idea behind using AOP for software testing is that aspects in AOP can be used to capture execution points within the program's modules and thus we can test components where we suspect bugs without even modifying the source code.},
  keywords={Computational intelligence;Communications technology;Aspect Oriented Programming;Types of Software Testing;Aspects;AOP},
  doi={10.1109/CICT.2016.22},
  ISSN={},
  month={Feb},}

@INPROCEEDINGS{10638902,
  author={Mosleh, Mogeeb A.A. and Ameen Al-Khulaidi, Nashwan and Gumaei, Abdu H. and Alsabry, Ayman and Musleh, Ali A. A.},
  booktitle={2024 4th International Conference on Emerging Smart Technologies and Applications (eSmarTA)}, 
  title={Classification and Evaluation Framework of Automated testing tools for agile software: Technical Review}, 
  year={2024},
  volume={},
  number={},
  pages={1-12},
  abstract={Test automation is crucial for agile software projects to enable frequent delivery of working software with cost and time and minimal bugs. However, selecting the right automated testing tool is considered challenging due to the wide range of such existing tools. Additionally, the challenges occur clearly due to several issues such as the programming code language, the categorization of the developed system, and the tester’s knowledge and skills. This paper aims to address this gap by proposing an evaluation framework for comparing and classifying the existing automated testing tools used in agile projects. The framework is developed based on an extensive literature review of existing agile testing methodologies and common commercial automation testing techniques. The key criteria for tool evaluation are identified to cover the main testing objective aspects such as test design support, testing interfaces, reporting capabilities, etc. These criteria are considered the core methodology for this study used to analyze and compare the popular open-source and commercial tools. The proposed evaluation framework provides agile practitioners with guidelines to assist in selecting the appropriate tools based on their specific project needs such as budget, timelines, and technical expertise. This study is considered a comparative evaluation of existing agile testing tools to highlight their key strengths and limitations. The findings of this study categorized the testing tools based on the interface, code, design, and report features. This research contributes to assisting the project developer and tester in selecting suitable tools for the adoption of automated testing tools in their agile software projects. It also identifies the direction for future work, such as integrations with modern development methodologies and technologies.},
  keywords={Codes;Automation;Reviews;Agile software development;Programming;Software;Software measurement;Object recognition;Testing;Guidelines;Agile methodology;agile testing;agile tools;agile evaluation model;Automated Testing},
  doi={10.1109/eSmarTA62850.2024.10638902},
  ISSN={},
  month={Aug},}

@INPROCEEDINGS{7473044,
  author={Zawistowski, Piotr},
  booktitle={2016 IEEE Symposium on Service-Oriented System Engineering (SOSE)}, 
  title={The Method of Measurement and Control Systems Design and Validation with Use of BRMS Systems}, 
  year={2016},
  volume={},
  number={},
  pages={324-332},
  abstract={Quality of software has always been a problem in every area of software use. Lack of software can cause problems during software execution and lead to different failures. Measurement and control systems (MCS) are such a group of software which use laboratory devices to obtain measurement data or to control e.g. production processes. Bugs in software can lead to variety of problems, from incorrect measurement data to device damages. For this reason it is important to deliver software of good quality. Software development should be considered a process which consists of a sequence of steps and is supported by a tool or set of tools that interoperate to make the work with the software easier. The problem is that there is no such approach nor tools for MCS systems. For this reason, a suitable method of design and validation of this group of systems has been developed. Moreover, the proper solution for supporting the proposed method has been implemented and tested. The solution consists of Business Rule Management System (BMRS) used for validating software execution data in an efficient way.},
  keywords={Software;Software measurement;Standards;Software engineering;Computer languages;Nickel;Control systems;BRMS;Drools;LabVIEW;measurement and control systems;software design;software quality assurance;software runtime validation},
  doi={10.1109/SOSE.2016.61},
  ISSN={},
  month={March},}

@INPROCEEDINGS{8942689,
  author={Yang, Yi-Chang and Jiang, Jehn-Ruey},
  booktitle={2019 IEEE Eurasia Conference on IOT, Communication and Engineering (ECICE)}, 
  title={Web-based Machine Learning Modeling in a Cyber-Physical System Construction Assistant}, 
  year={2019},
  volume={},
  number={},
  pages={478-481},
  abstract={The Cyber-Physical System (CPS) is critical for smart manufacturing of the Industry 4.0 vision. This study shows the design and implementation of machine learning modeling modules for a web-based CPS construction assistant, called PINE. The modules make easy the modeling of support vector classification (SVC), support vector regression (SVR), deep neural network (DNN), and convolutional neural network (CNN). They facilitate users to set modeling hyper-parameters and can generate source codes for the modeling. Examples are given to show how to use the modules to assist in training CNN models for an automated optical inspection (AOI) system.},
  keywords={Training;Neural networks;Static VAr compensators;Support vector machine classification;Machine learning;Cyber-physical systems;Soldering;smart manufacturing;machine learning;support vector machine;deep neural network;convolutional neural network},
  doi={10.1109/ECICE47484.2019.8942689},
  ISSN={},
  month={Oct},}

@INPROCEEDINGS{9921425,
  author={Binder, Christoph and Calà, Ambra and Vollmar, Jan and Neureiter, Christian and Lüder, Arndt},
  booktitle={2022 IEEE 27th International Conference on Emerging Technologies and Factory Automation (ETFA)}, 
  title={Towards Round-trip Engineering to evolve Complex Production Systems by utilizing AutomationML}, 
  year={2022},
  volume={},
  number={},
  pages={1-6},
  abstract={Additive manufacturing and product configurations will become increasingly important in future production systems. This trend brings great opportunities for manufacturing companies, but also inherits challenges to overcome. Resulting from this, a heterogeneous tool-landscape has emerged, where each of the single tools is addressing a particular aspect of the value-creation network. An example for such a tool specifically targeting the engineering of such flexible production systems according to the Reference Architecture Model Industrie 4.0 (RAMI 4.0) has been proposed with the RAMI Toolbox. However, as universal frameworks are too general to deal with all aspects of developing such complex systems, other possibilities for conflictfree engineering of the system need to be available. Thus, the main contribution of this paper deals with proposing a Round-trip Engineering (RTE) approach, that allows to export previously modeled flexible production systems according to the peculiarities of RAMI 4.0 and subsequently reload external elaborated results. Thereby, a major benefit is the application of Model-based Systems Engineering (MBSE), which ensures the traceability to the remaining system components. The chosen methodology for bidirectionally exchanging the engineering data is AutomationML, which allows to store exported information from RAMI 4.0 or import such stored information into it. The RTE-approach is thereby evaluated with a real-world case study, the Siemens Fischertechnik industrial plant model.},
  keywords={Industries;Production systems;Three-dimensional printing;Market research;Manufacturing;Industrial plants;Internet of Things;Reference Architecture Model Industrie 4.0 (RAMI 4.0);Model-based Systems Engineering (MBSE);Industrial Internet of Things (IIoT);AutomationML;Round-trip Engineering (RTE)},
  doi={10.1109/ETFA52439.2022.9921425},
  ISSN={},
  month={Sep.},}

@INPROCEEDINGS{11097838,
  author={Alshnakat, Anoud and Ahmadian, Amir M. and Balliu, Musard and Guanciale, Roberto and Dam, Mads},
  booktitle={2025 IEEE 38th Computer Security Foundations Symposium (CSF)}, 
  title={Securing P4 Programs by Information Flow Control}, 
  year={2025},
  volume={},
  number={},
  pages={284-299},
  abstract={Software-Defined Networking (SDN) has transformed network architectures by decoupling the control and data-planes, enabling fine-grained control over packet processing and forwarding. P4, a language designed for programming data-plane devices, allows developers to define custom packet processing behaviors directly on programmable network devices. This provides greater control over packet forwarding, inspection, and modification. However, the increased flexibility provided by P4 also brings significant security challenges, particularly in managing sensitive data and preventing information leakage within the data-plane. This paper presents a novel security type system for analyzing information flow in P4 programs that combines security types with interval analysis. The proposed type system allows the specification of security policies in terms of input and output packet bit fields rather than program variables. We formalize this type system and prove it sound, guaranteeing that well-typed programs satisfy noninterference. Our prototype implementation, TAP4S, is evaluated on several use cases, demonstrating its effectiveness in detecting security violations and information leakages.},
  keywords={Flow production systems;Prototypes;Process control;Programming;Network architecture;Inspection;Information leakage;Security;Cryptography;Software defined networking},
  doi={10.1109/CSF64896.2025.00031},
  ISSN={2374-8303},
  month={June},}

@INPROCEEDINGS{7430116,
  author={Marroquin, Alberto and Gonzalez, Douglas and Maag, Stephane},
  booktitle={2015 7th IEEE Latin-American Conference on Communications (LATINCOM)}, 
  title={Testing distributed systems with test cases dependencies architecture}, 
  year={2015},
  volume={},
  number={},
  pages={1-6},
  abstract={In this work, we present a novel distributed testing architecture based on a formal definition of test cases dependencies to test the conformance of distributed systems in a black box context. Utilizing the European Telecommunication Standards Institute, Test Description Language standard, we apply our approach to a real Internet Multimedia Subsystem (IMS)/ SIP (Session Initiation Protocol) test bed and perform the tests through two use cases. This crucial activity belongs to the conformance testing context. Which aims at stimulating the communication system under test (SUT) to detect errors and unexpected behaviors with regards to the standards. When handling distributed systems, a major difficulty arises when testing these, due to the joint and linked stimulation of distributed entities. The main reason for it is the correlation of verdicts obtained from these entities.},
  keywords={Testing;Protocols;Standards;Context;Synchronization;Correlation;Conformance Testing;Distributed Systems;Test Cases;SIP;TDL},
  doi={10.1109/LATINCOM.2015.7430116},
  ISSN={},
  month={Nov},}

@INPROCEEDINGS{8544381,
  author={Ernadote, Dominique},
  booktitle={2018 IEEE International Systems Engineering Symposium (ISSE)}, 
  title={A Framework for Descriptive Models Quality Assessment}, 
  year={2018},
  volume={},
  number={},
  pages={1-7},
  abstract={Numerous systems engineering projects use descriptive models structured by standard metamodels such as SysML, UML, or NAF. Model-based systems engineering is now so well established in the world of systems engineering that there is no longer questions about the usefulness of such modeling activities. However, it is still difficult to determine what is the current level of quality and how far the models are from the expected project objectives. This paper proposes a framework which reduces the pain of elaborating a quality process aside of the main systems engineering tasks so that a project manager can quickly be aware of the modeling progress statuses. The proposed framework is based on a modeling method which is itself modeled so that to allow a predefined set of modeling artefacts self-learning the quality aspects, and generating the appropriate dashboards at different levels of details.},
  keywords={Unified modeling language;Mathematical model;Quality assessment;Standards;Data models;Model Quality Assessment;MBSE;Model-Based Systems Engineering;Systems Engineering;Operations Research},
  doi={10.1109/SysEng.2018.8544381},
  ISSN={},
  month={Oct},}

@INPROCEEDINGS{8883668,
  author={Hăjmăȿan, Gheorghe and Mondoc, Alexandra and Creț, Octavian},
  booktitle={2019 Conference on Next Generation Computing Applications (NextComp)}, 
  title={Bytecode Heuristic Signatures for Detecting Malware Behavior}, 
  year={2019},
  volume={},
  number={},
  pages={1-6},
  abstract={For a long time, the most important approach for detecting malicious applications was the use of static, hash-based signatures. This approach provides a fast response time, has a low performance overhead and is very stable due to its simplicity. However, with the rapid growth in the number of malware, as well as their increased complexity in terms of polymorphism and evasion, the era of reactive security solutions started to fade in favor of new, proactive approaches such as behavior based detection. We propose a novel approach that uses an interpreter virtual machine to run proactive behavior heuristics from bytecode signatures, thus combining the advantages of behavior based detection with those of signatures. Based on our approximation, using this approach we succeeded to reduce by 85% the time required to update a behavior based detection solution to detect new threats, while continuing to benefit from the versatility of behavior heuristics.},
  keywords={Malware;Security;Prototypes;Instruction sets;Monitoring;Virtual machining;Computer languages;malware detection;behavior;bytecode;signature;heuristic;response time},
  doi={10.1109/NEXTCOMP.2019.8883668},
  ISSN={},
  month={Sep.},}

@INPROCEEDINGS{4382117,
  author={Ramanathan, S. and Lac, C.},
  booktitle={2007 IEEE International Symposium on Consumer Electronics}, 
  title={Use of fault tree analysis to improve residential gateway testing}, 
  year={2007},
  volume={},
  number={},
  pages={1-6},
  abstract={A residential gateway, heart of the strategy of most Telcos, is a centralized intelligent device between the operator's access network and the home's network. It terminates all external access networks and enables residential services to be delivered to the consumer. Besides a plethora of useful services, the growth in market depends upon the reputation of its resilience (availability, reliability and security). This emphasizes a near zero fault design and efficient testing should be taken care before its launch into the market. This paper deals with the analysis of failures, both from test and field data, aiming to increase the efficiency of laboratory testing. Using fault tree analysis, we study the faults that have passed through the testing phase and created failures in the customer premises. With the help of defined specifications, we have identified the zones in which testing in the laboratory needs to be improved.},
  keywords={Fault trees;Testing;Failure analysis;Laboratories;Heart;Intelligent networks;Home automation;Resilience;Availability;Data security},
  doi={10.1109/ISCE.2007.4382117},
  ISSN={2159-1423},
  month={June},}

@INPROCEEDINGS{5541042,
  author={Yan Huan and Miao Changyun and Wu Zhigang},
  booktitle={2010 International Conference On Computer Design and Applications}, 
  title={The design of IP telephony media gateway which is based on soft-switching technology}, 
  year={2010},
  volume={5},
  number={},
  pages={V5-379-V5-381},
  abstract={This paper presents a new type of design method for IP telephony gateway which is base on the soft-switch technology. It designs the hardware circuit with TMS320C5402 as its core and develops a telephone communication protocol which is based on UDP / IP. The Trunk Gateway supports MGCP protocol standards. Besides, it can fulfill lots of functions including audio processing, voice codec, and signaling tone generation and detection.},
  keywords={Telephony;Protocols;Design methodology;Hardware;Circuits;Communication standards;Standards development;Codecs;Signal processing;Signal generators;soft-switching;IP phones gateways;Media Gateway Controller (MGC);Media Gateway (MG);signaling;protocol},
  doi={10.1109/ICCDA.2010.5541042},
  ISSN={},
  month={June},}

@INPROCEEDINGS{9968371,
  author={Sarkar, Santonu and Stark, Katharina and Hoernicke, Mario},
  booktitle={IECON 2022 – 48th Annual Conference of the IEEE Industrial Electronics Society}, 
  title={Design of a Validator for Module Type Packages}, 
  year={2022},
  volume={},
  number={},
  pages={1-6},
  abstract={Modular plants build a production system by integrating a set of pre-designed modules. Integration of these modules, supplied by different vendors, is performed using various design tools during the engineering phase. The integration process must perform a rigorous validation of the correctness of a module specification (MTP). Otherwise, the integration process can fail without providing enough failure details. Consequently, Such a failure at the later stage can significantly impact implementation, testing, integration, and SAT. In this paper, we describe a validator tool, that allows a plant designer to define a set of invariants that must be satisfied so that an MTP can be deemed fit for integration. We have tested the validator on a set of MTPs and reported our findings. We expect that the use of such a validator can significantly reduce the possibility of introducing errors during the engineering phase.},
  keywords={Production systems;Design tools;Testing;MTP;Validation;Expert System;Invariant},
  doi={10.1109/IECON49645.2022.9968371},
  ISSN={2577-1647},
  month={Oct},}

@INPROCEEDINGS{8502653,
  author={Nezhad, Amir Soltani and Lukkien, Johan J. and Mak, Rudolf. H.},
  booktitle={2018 IEEE 23rd International Conference on Emerging Technologies and Factory Automation (ETFA)}, 
  title={Behavior-driven Development for Real-time Embedded Systems}, 
  year={2018},
  volume={1},
  number={},
  pages={59-66},
  abstract={Embedded systems are a class of computer systems that are typically characterized by a tight interaction with the physical environment. Various methodologies have been adopted for the development of such systems, ranging from traditional waterfall to modern agile techniques. One of the agile techniques that has recently attracted increasing attention is Behavior-Driven Development (BDD). BDD promotes the engagement of all stakeholders in every development iteration to minimize the misunderstanding between technical and non-technical stakeholders and, consequently, to speed up the development process and lower the costs. In this paper, we investigate the application of BDD to the development of embedded systems, especially focusing on the testing of timing requirements for real-time embedded software. In particular, we extend BDD with time-related concepts and propose an approach to generate test code for the verification of timing behavior of real-time embedded systems. Our approach offers more automation for the development of test code compared to existing BDD tools, thus minimizing the risk of timing faults and reducing development costs and time-to-market.},
  keywords={Logic gates;Testing;Timing;Embedded systems;Real-time systems;Stakeholders},
  doi={10.1109/ETFA.2018.8502653},
  ISSN={1946-0759},
  month={Sep.},}

@INPROCEEDINGS{5501495,
  author={Chatterjee, Shreeshankar},
  booktitle={2010 Seventh International Conference on Information Technology: New Generations}, 
  title={Modeling, Debugging, and Tuning QoE Issues in Live Stream-Based Applications - A Case Study with VoIP}, 
  year={2010},
  volume={},
  number={},
  pages={1044-1050},
  abstract={End-user acceptance criteria of live-stream based, interactive applications are different from traditional B2B or B2C applications. For example, if users sense disruptions in audio or video stream quality, they may quickly form a negative opinion. The Quality of Experience (QoE) in such live-stream applications is, thus, based on perception, and is open to subjective interpretations. QoE can be affected by hundreds of possible variables. QoE problems (QoE bugs), however, require an objective solution (fix in the product's code or tuning of product parameters). Understanding and debugging QoE bugs in such scenarios starts with designing relevant metrics and analysis tools. Thereafter, smart test-designs and strategies are required to gain insights into bottlenecks. This paper builds a discussion around these thoughts, and through a case study (sample QoE problem in a Voice over Internet Protocol (VoIP) application), collates some generic guidelines to investigate QoE bugs in live-streaming scenarios.},
  keywords={Debugging;Streaming media;Computer bugs;Collaboration;Delay;Information technology;Videoconference;Product codes;Testing;Internet telephony;VOIP;QoE;QoS;Media Streaming;VOIP Metric;Modeling},
  doi={10.1109/ITNG.2010.44},
  ISSN={},
  month={April},}

@INPROCEEDINGS{9812676,
  author={Thwe, May Myat and Belay, Zelalem Mihret and Jee, Eunkyoung and Bae, Doo-Hwan},
  booktitle={2022 17th Annual System of Systems Engineering Conference (SOSE)}, 
  title={Cybersecurity Vulnerability Identification in System-of-Systems using Model-based Testing}, 
  year={2022},
  volume={},
  number={},
  pages={317-322},
  abstract={When operationally and managerially independent constituent systems are integrated to form a System of Systems (SoS), cybersecurity vulnerabilities can be exploited by cyber threats that can break the security requirements of SoS due to its collaborative nature. Using model-based testing to generate test cases automatically can potentially aid in discovering vulnerabilities. However, security test case generation is time-consuming, error-prone, and labor-intensive; therefore, it is desirable to fully or partially automate security testing processes. This paper proposes the automatic test data generation using formal models presented as communicating sequential processes. We use the model-checking technique that generates counterexamples when the specified security properties are violated. Our approach then converted those counterexamples into executable test data by applying the conversion rule and defined mapping algorithm. We demonstrate our approach with an experiment using an operation of an air traffic control (ATC) system, a representative of SoS. We developed an agent simulation program to test the operation of the ATC by using the generated test data and evaluating it in terms of vulnerability identification. We incorporated four attack types, and our experimental results show that the security tests generated from the models can identify the known vulnerabilities in the ATC system.},
  keywords={Atmospheric modeling;Collaboration;Traffic control;Data models;Air traffic control;Computer security;Testing;Model-based testing;Vulnerability identification;Security testing;System of Systems;Air traffic control},
  doi={10.1109/SOSE55472.2022.9812676},
  ISSN={},
  month={June},}

@INPROCEEDINGS{4839257,
  author={Dechev, Damian and Stroustrup, Bjarne},
  booktitle={2009 16th Annual IEEE International Conference and Workshop on the Engineering of Computer Based Systems}, 
  title={Model-Based Product-Oriented Certification}, 
  year={2009},
  volume={},
  number={},
  pages={295-304},
  abstract={Future space missions such as the Mars Science Laboratory and Project Constellation suggest the engineering of some of the most complex man-rated software systems. The present process-oriented certification methodologies employed by NASA are becoming prohibitively expensive when applied to systems of such complexity. The process of software certification establishes the level of confidence in a software system in the context of its functional and safety requirements. Providing such certification evidence may require the application of a number of software development, analysis, and validation techniques. We define product-oriented certification as the process of measuring the system's reliability and efficiency based on the analysis of its design (expressed in models) and implementation (expressed in source code). In this work we introduce a framework for model-based product-oriented certification founded on the concept of source code enhancement and analysis. We describe a classification of the certification artifact types, the development and validation tools and techniques, the application domain-specific factors, and the levels of abstraction. We demonstrate the application of our certification platform by analyzing the process of model-based development of the parallel autonomic goals network, a critical component of the Jet Propulsion Laboratory's Mission Data System (MDS). We describe how we identify and satisfy seven critical certification artifacts in the process of model-driven development and validation of the MDS goal network. In the analysis of this process, we establish the relationship among the seven certification artifacts, the applied development and validation techniques and tools, and the level of abstraction of system design and development.},
  keywords={Certification;Laboratories;Software systems;Software safety;Space missions;Mars;NASA;Application software;Programming;Reliability;product-oriented certification;nonblocking synchronization;semantic enhancement;concurrent real-time systems},
  doi={10.1109/ECBS.2009.15},
  ISSN={},
  month={April},}

@ARTICLE{10500369,
  author={Pracner, Doni and Ward, Martin P. and Sukur, Nataša and Budimac, Zoran},
  journal={IEEE Access}, 
  title={Climbing the Hill to Understand the Code}, 
  year={2024},
  volume={12},
  number={},
  pages={56847-56859},
  abstract={Software maintenance takes up a disproportionately large amount of time in the modern software life cycle. One of the common problems is understanding the original code that is being restructured and improved and this is especially true with low-level code. This paper investigates the results and properties of an automated process that can raise the abstraction level of code from low-level operations to high-level structures. The process is made of independent components and can be adapted to different scenarios. The automated improvements implementation relies on the program transformation system FermaT and its catalogue of semantics-preserving transformations. The process uses hill climbing and a metric for the fitness function of the programs. This component was made to work on general inputs, without explicit knowledge of the type of origin of the program. The paper explores how different inputs are actually handled by the system, what are the properties and how these can be used for further improvements. Two main types of inputs are shown, x86 assembly and MicroJava bytecode. These two have many operational differences, and the translator tools introduce some more, but nonetheless, the same process can handle all of these and, on average, improve the Structure metric (a good approximation of the complexity of the code) by around 85%.},
  keywords={Codes;Semantics;Java;Assembly;Source coding;Security;Software maintenance;Life cycle assessment;Automated code transformation;bytecode;assembly;program comprehension},
  doi={10.1109/ACCESS.2024.3389500},
  ISSN={2169-3536},
  month={},}

@INPROCEEDINGS{9314622,
  author={Yeung, Yiksing and Lau, C. K.},
  booktitle={2020 IEEE International Conference on Power and Energy (PECon)}, 
  title={Exploring the Application of Phasor Measurement Units in the Distribution Network}, 
  year={2020},
  volume={},
  number={},
  pages={299-303},
  abstract={Phasor Measurement Units or PMU is a measurement device that produces synchronised measurement of phasor, frequency and rate of change of frequency from voltage and/or current signals based on a common time source that typically is the one provided by the Global Positioning System UTC-GPS [1]. It enables operators to monitor the network operating status in real time so as to take corresponding actions promptly and effectively while the network can be properly protected and supply can be restored automatically when fault incidents happen. PMU system can also provide the dedicated information to enhance the system protection scheme, supply reliability and asset utilisation so that the operators can operate the network smartly. This paper discusses system architecture design on the appropriate PMU technology for CLP Power's system, acquisition, installation, operation and performance evaluation of the PMU system in CLP Power's distribution network.},
  keywords={Phasor measurement units;Current measurement;State estimation;Voltage measurement;Global Positioning System;Synchronization;Substations;Phasor Measurement Unit;Distribution;Smart City;State Estimation},
  doi={10.1109/PECon48942.2020.9314622},
  ISSN={},
  month={Dec},}

@INPROCEEDINGS{6972271,
  author={Tokmakoff, Andrew and Sparrow, Ben and Turner, David and Lowe, Andrew},
  booktitle={2014 IEEE 10th International Conference on e-Science}, 
  title={AusPlots Rangelands Field Data Collection and Publication: Infrastructure for Ecological Monitoring}, 
  year={2014},
  volume={1},
  number={},
  pages={249-255},
  abstract={The TERN AusPlots Rangelands field data collection system has been developed to facilitate simple and efficient data collection by ecologists operating in the Australian outback. The infrastructure provides tooling for 'clean' data collection on mobile (tablet) devices, associated data storage in a cloud-based server infrastructure, facilities for data curation and management and interfaces with the Australian Ecological Knowledge and Observation System (ÆKOS) data repository for long-term data management and semantic enrichment. In this paper, we introduce the AusPlots Rangelands field data collection solution, providing a systems-level view and motivating its development through the discussion of key functional requirements. We provide an outline of the ÆKOS data repository and demonstrate that the combined system provides a unique end-to-end data collection, curation, archiving and publishing mechanism for ecological data.},
  keywords={Data collection;Vegetation;Databases;Servers;Soil;Protocols;Vegetation mapping;ecological data;mobile;data collection;data publishing},
  doi={10.1109/eScience.2014.55},
  ISSN={},
  month={Oct},}

@INPROCEEDINGS{10729496,
  author={Sathyakumar, Damodaran Chingleput},
  booktitle={2024 IEEE 2nd International Conference on Sensors, Electronics and Computer Engineering (ICSECE)}, 
  title={Enhancing web UX Test Coverage Through Model-Based Practices: State Machine Driven Test Auto-Generation}, 
  year={2024},
  volume={},
  number={},
  pages={82-93},
  abstract={Web front-end development is characterized by fast evolution, requiring frequent UX updates and feature releases. Such a dynamic environment poses challenges for traditional testing methodologies, which struggle with maintenance burdens and inadequate test coverage, leading to reduced release velocity as the codebase and requirements grow. This paper examines the shortcomings of current web and UX testing tools and processes, particularly in terms of achieving exhaustive coverage and maintenance. It highlights the discrepancy between perceived and actual coverage, that reduces confidence in deploying releases swiftly. Next, to mitigate the issues mentioned, it introduces the adoption of model-based testing (MBT) [1] using finite state machines (FSM) [2]. It includes a study of an autocomplete utility, demonstrating how the XState [3] framework facilitates the auto-generation of test suites, comprehensively, through FSM, thereby enhancing test coverage and reducing maintenance overhead. The paper concludes with insights into the benefits, limitations, and trade-offs encountered when implementing this approach, offering valuable lessons for practitioners seeking to improve web front-end testing practices.},
  keywords={Codes;Buildings;Automata;Maintenance;Sensors;Testing;Web;UX;UI;User Interface;Model Based Testing;XState;Testing;Finite State Machines;State Charts;Components;Front-end;Javascript;HTML;Verification;Validation;Frameworks;Software quality;Quality assurance;Software Tools},
  doi={10.1109/ICSECE61636.2024.10729496},
  ISSN={},
  month={Aug},}

@INPROCEEDINGS{9254687,
  author={Ghanem, Timothy and Zein, Samer},
  booktitle={2020 4th International Symposium on Multidisciplinary Studies and Innovative Technologies (ISMSIT)}, 
  title={A Model-based approach to assist Android Activity Lifecycle Development}, 
  year={2020},
  volume={},
  number={},
  pages={1-12},
  abstract={In Android app development, conforming to the activity lifecycle model is imperative to maintain app robustness and reliability as well as avoiding many issues tied to lifecycle state transitions, such as memory leaks, data preservation, and app crashes. Previous studies have shown that Android developers possess limited understanding and awareness of the activity lifecycle model and the current state-of-the-art Android app development tools and methods provide developers with little support during activity lifecycle development. In this study, we present an approach and a framework that provides a dynamic visual view for the activity lifecycle state transitions during implementation. The approach follows model-based development utilizing DSVL (Domain Specific Visual Language) and is implemented as a proof-of-concept Android Studio plugin. We evaluated our approach through experimentation by real Android developers. Initial results show that our approach can be useful and effective in assisting Android developers.},
  keywords={Visualization;Tools;Robustness;Data models;Computer crashes;model-based;model-driven;MDD;android lifecycle;android;android development;lifecycle development},
  doi={10.1109/ISMSIT50672.2020.9254687},
  ISSN={},
  month={Oct},}

@INPROCEEDINGS{10298415,
  author={Xiao, Mingxuan and Xiao, Yan and Dong, Hai and Ji, Shunhui and Zhang, Pengcheng},
  booktitle={2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE)}, 
  title={LEAP: Efficient and Automated Test Method for NLP Software}, 
  year={2023},
  volume={},
  number={},
  pages={1136-1148},
  abstract={The widespread adoption of DNNs in NLP software has highlighted the need for robustness. Researchers proposed various automatic testing techniques for adversarial test cases. However, existing methods suffer from two limitations: weak error-discovering capabilities, with success rates ranging from 0% to 24.6% for BERT-based NLP software, and time inefficiency, taking 177.8s to 205.28s per test case, making them challenging for time-constrained scenarios. To address these issues, this paper proposes LEAP, an automated test method that uses LEvy flight-based Adaptive Particle swarm optimization integrated with textual features to generate adversarial test cases. Specifically, we adopt Levy flight for population initialization to increase the diversity of generated test cases. We also design an inertial weight adaptive update operator to improve the efficiency of LEAP's global optimization of high-dimensional text examples and a mutation operator based on the greedy strategy to reduce the search time. We conducted a series of experiments to validate LEAP's ability to test NLP software and found that the average success rate of LEAP in generating adversarial test cases is 79.1%, which is 6.1% higher than the next best approach (PSOattack). While ensuring high success rates, LEAP significantly reduces time overhead by up to 147.6s compared to other heuristic-based methods. Additionally, the experimental results demonstrate that LEAP can generate more transferable test cases and significantly enhance the robustness of DNN-based systems.},
  keywords={Adaptation models;Scalability;Perturbation methods;Sociology;Software algorithms;Robustness;Statistics;NLP Software Testing;Particle Swarm Optimization},
  doi={10.1109/ASE56229.2023.00052},
  ISSN={2643-1572},
  month={Sep.},}

@INPROCEEDINGS{9985149,
  author={Datar, Advaita and Zare, Amey and A, Asia and Venkatesh, R and Kumar, Shrawan and Shrotri, Ulka},
  booktitle={2022 IEEE International Symposium on Software Reliability Engineering Workshops (ISSREW)}, 
  title={Automated Validation of Insurance Applications against Calculation Specifications}, 
  year={2022},
  volume={},
  number={},
  pages={55-60},
  abstract={Insurance companies rely on their Legacy Insurance System (LIS) to govern day-to-day operations. These LIS operate as per the company's business rules that are formally specified in Calculation Specification (CS) sheets. To meet ever-changing business demands, insurance companies are increasingly trans-forming their outdated LIS to modern Policy Administration Systems (PAS). Quality Assurance (QA) of such PAS involves manual validation of calculations' implementation against the corresponding CS sheets from the LIS. This manual QA approach is effort-intensive and error-prone, which may fail to detect inconsistencies in PAS implementations and ultimately result in monetary loss. To address this challenge, we propose a novel low-code/no-code technique to automatically validate PAS imple-mentation against CS sheets. Our technique has been evaluated on a digital transformation project of a large insurance company on 12 real-world calculations through 254 policies. The evaluation resulted in effort savings of approximately 92 percent against the conventional manual validation approach.},
  keywords={Quality assurance;Digital transformation;Conferences;Insurance;Manuals;Companies;Software reliability;Automated Validation;Formal Specification;Low-code/No-code;Insurance Applications},
  doi={10.1109/ISSREW55968.2022.00039},
  ISSN={},
  month={Oct},}

@INPROCEEDINGS{8666862,
  author={Zhang, Hailong and Liu, Xuan and Zheng, Ke},
  booktitle={2018 International Conference on Information Systems and Computer Aided Education (ICISCAE)}, 
  title={Design of Broadband PLC Conformance Testing System Based on TTCN-3}, 
  year={2018},
  volume={},
  number={},
  pages={35-41},
  abstract={Protocol conformance test plays a necessary and important role in guaranteeing that broadband PLC (Power Line Communication) devices to meet the standards and achieve interoperability. A broadband PLC Conformance Testing System is designed based on TTCN-3 framework. The implementation scheme for conformance testing platform is proposed and the module mechanisms as well as key techniques are described. Test suite are designed according to broadband PLC testing requirements, and finally test cases are executed based on testing environment constructed. The work is of significance for promoting the broadband PLC standardization and interoperability.},
  keywords={Protocols;Broadband communication;Testing;Software;Hardware;Physical layer;Decoding;Broadband PLC;Conformance Test;TTCN-3},
  doi={10.1109/ICISCAE.2018.8666862},
  ISSN={},
  month={July},}

@ARTICLE{10855999,
  author={Marand, Elaheh Azadi and Sheikhahmadi, Amir and Challenger, Moharram and Moradi, Parham and Khalilipour, Alireza},
  journal={IEEE Access}, 
  title={Recommender Systems for Unified Modeling Language and Vice Versa—A Systematic Literature Review}, 
  year={2025},
  volume={13},
  number={},
  pages={23426-23460},
  abstract={Recommender systems (RSs) are fundamental tools that address data redundancy and serve as intelligent supplements for tasks such as data retrieval and refinement by analyzing user behavior. Nowadays, RSs are utilized in various domains, ranging from filtering web news based on user preferences to recommending movies, music, books, and articles in e-commerce. Additionally, these systems are extensively employed to facilitate software engineering activities, including modeling. Modeling environments are enriched with RSs that assist in building models by providing recommendations based on previous solutions to similar problems within the same domain. Consequently, there is growing research interest in approaches that aid the modeling process. This paper presents a systematic literature review (SLR) that analyzes how recommender systems techniques are used to suggest UML diagrams, as well as the role of UML diagrams in describing recommender systems. In addition, it discusses methods for evaluating primary studies, the challenges that primary studies have addressed, and the domains of study that primary studies have targeted (based on an analysis of 4789 papers). We believe this study will guide researchers and professionals in identifying recommender system techniques for generating UML diagram suggestions and understanding the overall purpose of using UML diagrams. Furthermore, it may contribute to a broader understanding of the research process and inspire future research on recommender system techniques within other modeling languages. The results show that 45% of the studies use content-based techniques to suggest UML diagrams, with 77% of the recommendations being structural diagrams (such as class diagrams). On the other hand, to design the components of the proposed approaches (recommender systems), behavioral diagrams are generally used (53% on average), focusing on knowledge-based techniques (28% on average). Finally, the study shows that researchers use content-based (38%) and knowledge-based (41%) techniques to recommend design models. The analysis revealed that the following challenges were identified: 19 studies dealt with the cold start problem, 20 studies with sparsity issues, 11 studies with scalability concerns, 3 studies with diversity challenges, and 12 studies with other types of challenges.},
  keywords={Unified modeling language;Recommender systems;Systematic literature review;Software;Context modeling;Knowledge based systems;Software engineering;Data mining;Collaborative filtering;Termination of employment;Recommender system;unified modeling language (UML);systematic literature review (SLR)},
  doi={10.1109/ACCESS.2025.3535527},
  ISSN={2169-3536},
  month={},}

@INPROCEEDINGS{8973034,
  author={Moran, Kevin and Bernal-Cárdenas, Carlos and Linares-Vásquez, Mario and Poshyvanyk, Denys},
  booktitle={2018 IEEE/ACM 26th International Conference on Program Comprehension (ICPC)}, 
  title={Overcoming Language Dichotomies: Toward Effective Program Comprehension for Mobile App Development}, 
  year={2018},
  volume={},
  number={},
  pages={7-18},
  abstract={Mobile devices and platforms have become an established target for modern software developers due to performant hardware and a large and growing user base numbering in the billions. Despite their popularity, the software development process for mobile apps comes with a set of unique, domain-specific challenges rooted in program comprehension. Many of these challenges stem from developer difficulties in reasoning about different representations of a program, a phenomenon we define as a "language dichotomy". In this paper, we reflect upon the various language dichotomies that contribute to open problems in program comprehension and development for mobile apps. Furthermore, to help guide the research community towards effective solutions for these problems, we provide a roadmap of directions for future work.},
  keywords={Performance evaluation;Codes;Operating systems;Natural languages;Software;Mobile handsets;Hardware;Cognition;Mobile applications;Software development management;Program Comprehension;Mobile;Android;Natural Language;Code},
  doi={},
  ISSN={2643-7171},
  month={May},}

@INPROCEEDINGS{9586183,
  author={Mondelli, Andrea and Gazzillo, Paul and Solihin, Yan},
  booktitle={2021 58th ACM/IEEE Design Automation Conference (DAC)}, 
  title={SeMPE: Secure Multi Path Execution Architecture for Removing Conditional Branch Side Channels}, 
  year={2021},
  volume={},
  number={},
  pages={973-978},
  abstract={One prevalent source of side channel vulnerabilities is the secret-dependent behavior of conditional branches (SDBCB). The state-of-the-art solution relies on Constant-Time Expressions, which require high programming effort and incur high performance overheads. In this paper, we propose SeMPE, an architecture support to eliminate SDBCB without requiring much programming effort while incurring low performance overheads. When a secret-dependent branch is encountered, SeMPE fetches, executes, and commits both paths of the branch, preventing the adversary from inferring secret values from the branching behavior of the program. SeMPE outperforms code generated by FaCT, a constant-time expression language, by up to 18×.},
  keywords={Codes;Design automation;Programming;Hardware;Registers;side channel;conditional branch;multipath execution;microarchitecture},
  doi={10.1109/DAC18074.2021.9586183},
  ISSN={0738-100X},
  month={Dec},}

@INPROCEEDINGS{6575271,
  author={Mulkey, Nick and Liu, Brian and Medda, Alessio},
  booktitle={2013 8th International Conference on System of Systems Engineering}, 
  title={The Integrated Blast Effects Sensor Suite: A rapidly developed, complex, system of systems}, 
  year={2013},
  volume={},
  number={},
  pages={224-228},
  abstract={The need for rapid development of tactical system of systems solutions for military applications requires the use of system modeling techniques and simulation and validation methods to be applied throughout the lifecycle of the system. This combined approach of development and verification is preferred to traditional approaches for risk mitigation and cost effectiveness. This paper examines the Integrated Blast Effects Sensor Suite developed at the Georgia Tech Research Institute and its architecture as a complex system of systems.},
  keywords={Vehicles;Systems engineering and theory;Computer architecture;Databases;Complexity theory;Explosives;Data collection;System of System;SoS lifecycle;Fast Development;High Complexity},
  doi={10.1109/SYSoSE.2013.6575271},
  ISSN={},
  month={June},}

@INPROCEEDINGS{9226327,
  author={Haindl, Philipp and Plösch, Reinhold and Kömer, Christian},
  booktitle={2020 46th Euromicro Conference on Software Engineering and Advanced Applications (SEAA)}, 
  title={An Operational Constraint Language To Evaluate Feature-Dependent Non-Functional Requirements}, 
  year={2020},
  volume={},
  number={},
  pages={34-42},
  abstract={Features in a software system usually must satisfy different quality expectations, arising e.g., from their usage context or the long-term strategy of the manufacturer. As an example, the maintainability of the source code will likely be more important if the associated feature is frequently used by customers or if it has strategic value for the software manufacturer. Accordingly, features that process user-generated near real-time data will likely impose higher requirements towards performance efficiency than other features of the same application for maintaining the user profile. In order to practically approach these qualitative subtleties particularly in a DevOps context, we need an operational means to specify and automatically evaluate the fulfillment of these feature-dependent non-functional requirements, e.g., through quantitative constraints. However, the multitude of systems involved in DevOps and the heterogeneous data types of measures accruing on these systems hinder their effortless acquisition and automated evaluation.In this paper we present an operational constraint language for specifying and evaluating feature-dependent non-functional requirements quantitatively. Our language provides a compact set of time series operations, time filters, and comparison operators and allows to define metrical and ordinal threshold values. A comprehensive evaluation based on a large-scale software project with measures spanning the period over one year shows the performance and suitability of the approach for evaluating feature- dependent non-functional requirements specially in DevOps.},
  keywords={Time measurement;Time series analysis;Instruments;Complexity theory;Benchmark testing;Time factors;Software measurement;Constraint Languages;Non-Functional Requirements;Constraint Evaluation;Operational Software Quality},
  doi={10.1109/SEAA51224.2020.00017},
  ISSN={},
  month={Aug},}

@ARTICLE{10697116,
  author={Kessel, Marcus and Atkinson, Colin},
  journal={IEEE Software}, 
  title={N-Version Assessment and Enhancement of Generative AI: Differential GAI}, 
  year={2025},
  volume={42},
  number={2},
  pages={76-83},
  abstract={We propose a way of mitigating generative AI’s (GAI) inherent untrustworthiness by exploiting its ability to generate multiple versions of code and tests, facilitating comparative analysis across versions. Instead of relying on the quality of a single test or code module, this “Differential GAI” approach promotes more reliable quality evaluation through version diversity.},
  keywords={Codes;Software development management;Software engineering;Generative AI;Productivity;Semantics;Costs;Trusted computing},
  doi={10.1109/MS.2024.3469388},
  ISSN={1937-4194},
  month={March},}

@INPROCEEDINGS{6115389,
  author={Cruz, Tiago and Simões, Paulo and Almeida, João and Rodrigues, João and Monteiro, Edmundo and Bastos, Fernando and Laranjeira, Alexandre},
  booktitle={2011 IEEE 36th Conference on Local Computer Networks}, 
  title={How to provision and manage off-the-shelf SIP phones in domestic and SOHO environments}, 
  year={2011},
  volume={},
  number={},
  pages={42-49},
  abstract={Integrated services delivered over broadband connections are becoming the norm in domestic households, as it is the case with triple-play bundles which offer combined Voice, Television and Data services delivered using IP-based technologies and protocols. As a result, the usage of SIP-based (Session Initiation Protocol) VoIP devices has known a significant growth in domestic environments, either in the form of standalone (e.g. SIP telephones) or embedded devices (as it happens with some domestic gateways, which embed analog-to-SIP adaptors). For Internet Service Providers (ISPs), the provisioning and management of those devices is a challenge — especially standalone SIP phones, since most of them were exclusively designed for corporate LAN usage, not supporting adequate mechanisms for remote management over broadband access networks. In this paper we propose a framework which allows the integration of off-the-shelf SIP phones with the CWMP protocol suite, the prevailing standard for remote management of Customer Premises Devices (CPEs) in broadband access networks. This integration framework supports the vast majority of commercially available SIP phones whilst maintaining full compatibility with the original CWMP specification — thus allowing ISPs to reuse their CWMP management infrastructure to configure and provision off-the-shelf SIP telephones.},
  keywords={Data models;Logic gates;Servers;Protocols;Broadband communication;Local area networks;Runtime;CWMP;VoIP;SIP;Home Networks},
  doi={10.1109/LCN.2011.6115389},
  ISSN={0742-1303},
  month={Oct},}

@INPROCEEDINGS{4400351,
  author={Lengyel, Laszlo and Levendovszky, Tihamer and Mezei, Gergely and Vajk, Tamas and Charaf, Hassan},
  booktitle={EUROCON 2007 - The International Conference on "Computer as a Tool"}, 
  title={Practical Uses of Validated Model Transformation}, 
  year={2007},
  volume={},
  number={},
  pages={2200-2207},
  abstract={Model-based approaches in development are widely recognized as a potential way of increasing productivity in software engineering. Model-based development is driven by model transformations that attempt to bridge the large semantic gaps between high-level models and low-level languages. There is a demand for researching the ways in which model transformation can become more flexible, efficient, highly-configurable as well as validated. This paper addresses issues of visually defined metamodel-based model transformations that support validated model transformations. We introduce our model transformation framework, visual modeling and transformation system (VMTS), and a list of applications realized with VMTS on metamodel-based model transformation basis. Furthermore, a comprehensive comparison is given related to other model transformation approaches.},
  keywords={Algorithm design and analysis;Application software;Programming;Microwave integrated circuits;Productivity;Software engineering;System analysis and design;Automation;Informatics;Electronic mail;metamodel-based model transformation;graph rewriting;validated model transformation},
  doi={10.1109/EURCON.2007.4400351},
  ISSN={},
  month={Sep.},}

@INPROCEEDINGS{9953718,
  author={Sánchez M, Daniel E. and Vega L, Jeison A. and Rueda, Diego F. and Rodriguez F, Andres A.},
  booktitle={2022 Congreso Internacional de Innovación y Tendencias en Ingeniería (CONIITI)}, 
  title={Remote Monitoring of RF Amplifiers in HFC Networks: Voltage Drop Detection due to Power Blackouts}, 
  year={2022},
  volume={},
  number={},
  pages={1-6},
  abstract={Hybrid Fiber–Coaxial (HFC) networks are widely used around the world to deliver voice, video, and data services. These networks are made up of both passive and active elements, most of which are managed remotely by the network operation centers. However, RF amplifiers do not have a mechanism to notify metrics or alarms that allow network operators to segment and locate the point of failure in an efficient and timely manner. This means that subscribers first detect the fault before the network operator, translating into an increase in complaints to the call center. This paper proposes an electronic system for remote monitoring of RF amplifiers in an HFC network that detects voltage drops due to power outages and notifies an alarm to the technical support staff. The results show that the designed device efficiently detects the voltage drop and sends the alarm notification through a text message, contributing to improve response times to subscribers when this type of incident occurs.},
  keywords={Radio frequency;Optical fiber amplifiers;Measurement;Voltage;Hybrid fiber coaxial cables;Optical fiber networks;Power system reliability;voltage drop;HFC networks;power blackouts;remote monitoring;RF amplifiers},
  doi={10.1109/CONIITI57704.2022.9953718},
  ISSN={2539-4320},
  month={Oct},}

@INPROCEEDINGS{6616325,
  author={Zech, Philipp and Felderer, Michael and Farwick, Matthias and Breu, Ruth},
  booktitle={2013 IEEE Seventh International Conference on Software Security and Reliability Companion}, 
  title={A Concept for Language-Oriented Security Testing}, 
  year={2013},
  volume={},
  number={},
  pages={53-62},
  abstract={Today's ongoing trend towards intense usage of web service based applications in daily business and everybody's daily life poses new challenges for security testing. Additionally, such applications mostly not execute in their own runtime environment but instead are deployed in some data center, run alongside multiple other applications, and serve different purposes for sundry user domains with diverging security requirements. As a consequence, security testing also has to adapt to be able to meet the necessary requirements for each application in its domain and its specific security requirements. In addition, security testing needs to be feasible for both service providers and consumers. In our paper we identify drawbacks of existing security testing approaches and provide directions for meeting emerging challenges in future security testing approaches. We also introduce and describe the idea of language-oriented security testing, a novel testing approach building upon domain-specific languages and domain knowledge to meet future requirements in security testing.},
  keywords={Testing;Security;Business;Cloud computing;Automation;Security Testing;Domainspecific Language;Languageoriented Programming;Servicecentric Systems},
  doi={10.1109/SERE-C.2013.16},
  ISSN={},
  month={June},}

@INPROCEEDINGS{9212003,
  author={Meixner, Kristof and Kathrein, L. and Winkler, D. and Lüder, Arndt and Biffl, Stefan},
  booktitle={2020 25th IEEE International Conference on Emerging Technologies and Factory Automation (ETFA)}, 
  title={Efficient Test Case Generation from Product and Process Model Properties and Preconditions}, 
  year={2020},
  volume={1},
  number={},
  pages={859-866},
  abstract={In Cyber-Physical Production System (CPPS) engineering for discrete manufacturing, the definition of test cases is vital to ensure correct behavior of production processes and to test risky cases. Unfortunately, the definition of test cases requires know-how both from the CPPS engineering domain and on software test automation, and is time-consuming. In this paper, we investigate how domain experts can efficiently derive test cases for an assembly process step from process preconditions concerning product properties. We introduce the Test Case Derivation for PPR Models (TCD4PPR) method building on the Formalised Process Description and best practices from software testing. We evaluate the TCD4PPR method with an illustrative use case from industry in a feasibility study with domain experts at a large production systems engineering company for discrete manufacturing. The main result was that the domain experts found the TCD4PPR method efficient, usable, and useful. The evaluation results indicate that investing reasonable effort into modeling Product, Process, Resource (PPR) knowledge with preconditions can considerably reduce risks of untested production process behavior.},
  keywords={Software testing;Industries;Production systems;Conferences;Buildings;Companies;Software;Production Systems Engineering;Model-based Testing;Software Testing;Equivalence Class},
  doi={10.1109/ETFA46521.2020.9212003},
  ISSN={1946-0759},
  month={Sep.},}

@INPROCEEDINGS{8432020,
  author={Eberhardinger, Benedikt and Ponsar, Hella and Siegert, Gerald and Reif, Wolfgang},
  booktitle={2018 IEEE International Conference on Software Quality, Reliability and Security Companion (QRS-C)}, 
  title={Case Study: Adaptive Test Automation for Testing an Adaptive Hadoop Resource Manager}, 
  year={2018},
  volume={},
  number={},
  pages={513-518},
  abstract={Coping with adaptive software systems is one of the key challenges testing is currently faced with. In our previous work, we proposed to enable the test system itself to be adaptive to the system under test as a solution. The adaptation is built up on the concepts of a self-aware test automation enabling to use this information to sequence, instantiate, or update the test suite to the current situation. In our test framework the modeling language S# allows to use a run-time model to do so in a model-based testing approach. In this paper, we demonstrate how our concepts of adaptive, self-aware test automation are applied to a real world scenario: testing an adaptive resource manager of Hadoop. We show the steps necessary to implement the approach and discuss our experiences in this case study paper.},
  keywords={Adaptation models;Automation;Adaptive systems;Testing;Software systems;Yarn;Task analysis;Software Testing;Hadoop;Test Automation;Adaptive Testing;Adaptive Systems},
  doi={10.1109/QRS-C.2018.00092},
  ISSN={},
  month={July},}

@INPROCEEDINGS{7018469,
  author={Badreddin, Omar and Forward, Andrew and Lethbridge, Timothy C.},
  booktitle={2014 2nd International Conference on Model-Driven Engineering and Software Development (MODELSWARD)}, 
  title={A test-driven approach for developing software languages}, 
  year={2014},
  volume={},
  number={},
  pages={225-234},
  abstract={Test-Driven Development (TDD) is the practice of attempting to use the software you intend to write, before you write it. The premise is straightforward, but the specifics of applying it in different domains can be complex. In this paper, we provide aTDD approach for language development. The essence is to apply TDD at each of four levels of language processing, hence we call our approach Multi-Level TDD, or MLTDD. MLTDD can be applied to programming languages, preprocessors, domain specific languages, and transformation engines. MLTDD was used to build Umple, a model-oriented programming language available for Java, Ruby, and PHP. We present two case studies where this approach was implemented to develop two other domain specific languages.},
  keywords={Testing;Syntactics;Semantics;Java;Generators;Unified modeling language;Software;Test Driven Development;Model Oriented Programming Language;UML},
  doi={},
  ISSN={},
  month={Jan},}

@INPROCEEDINGS{10142518,
  author={Singh, Ajmer},
  booktitle={2023 7th International Conference on Intelligent Computing and Control Systems (ICICCS)}, 
  title={Taxonomy of Machine Learning Techniques in Test Case Generation}, 
  year={2023},
  volume={},
  number={},
  pages={474-481},
  abstract={machine learning empowers computer systems to automatically learn and improve their performance without being explicitly programmed for each specific task. It has revolutionized many industries and continues to drive advancements in artificial intelligence, enabling computers to perform complex tasks with greater efficiency and accuracy Machine learning algorithms use training data to learn patterns and make predictions or decisions about new data. It involves a range of techniques such as supervised learning, unsupervised learning, reinforcement learning, deep learning, and others. Machine learning (ML) can be applied to test case generation in several ways: Like Prioritization, Test case reduction, Test case minimization etc. Machine learning techniques can be beneficial in test case generation in several ways. This article reviews the research work on application of machine learning algorithms in test case generation.},
  keywords={Software testing;Adaptation models;Machine learning algorithms;Software algorithms;Machine learning;Manuals;Data models;Machine Learning;Software Testing;Test Case Generation.},
  doi={10.1109/ICICCS56967.2023.10142518},
  ISSN={2768-5330},
  month={May},}

@INPROCEEDINGS{9222867,
  author={Bicevskis, Janis and Bicevska, Zane and Nikiforova, Anastasija and Oditis, Ivo},
  booktitle={2020 15th Conference on Computer Science and Information Systems (FedCSIS)}, 
  title={Data Quality Model-based Testing of Information Systems}, 
  year={2020},
  volume={},
  number={},
  pages={595-602},
  abstract={This paper proposes a model-based testing approach by offering to use the data quality model (DQ-model) instead of the program’s control flow graph as a testing model. The DQ-model contains definitions and conditions for data objects to consider the data object as correct. The study proposes to automatically generate a complete test set (CTS) using a DQmodel that allows all data quality conditions to be tested, resulting in a full coverage of DQ-model. In addition, the possibility to check the conformity of the data to be entered and already stored in the database is ensured. The proposed alternative approach changes the testing process: (1) CTS can be generated prior to software development; (2) CTS contains not only input data, but also database content required for complete testing of the system; (3) CTS generation from DQ-model provides values against which the system can be further tested. If the test results correspond to the values obtained during CTS generation, the system under test shall be considered to have been tested according to DQ-model. Otherwise, the user can verify the cause of the differences that may occur due incorrect software, as well as an inaccurate specification.},
  keywords={Systematics;Databases;Data integrity;Switches;Data models;Software;Software reliability;complete test set;data quality model;information system;model-based testing;symbolic execution.},
  doi={10.15439/2020F25},
  ISSN={},
  month={Sep.},}

@INPROCEEDINGS{8116506,
  author={Jimenez, Ivo and Hamedian, Sina and Lofstead, Jay and Maltzahn, Carlos and Mohror, Kathryn and Arpaci-Dusseau, Remzi and Arpaci-Dusseau, Andrea and Ricci, Robert},
  booktitle={2017 IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS)}, 
  title={Demo abstract: PopperCI: Automated reproducibility validation}, 
  year={2017},
  volume={},
  number={},
  pages={952-953},
  abstract={In this demo we illustrate the usage of PopperCI [1], a continous integration (CI) service for experiments hosted at UC Santa Cruz that allows researchers to automate the end-to-end execution and validation of experiments. PopperCI assumes that experiments follow Popper [2], a convention for implementing experiments and writing articles following a DevOps approach that has been proposed recently.},
  keywords={Tools;Measurement;Runtime;Software;Conferences;Laboratories;Guidelines},
  doi={10.1109/INFCOMW.2017.8116506},
  ISSN={},
  month={May},}

@INPROCEEDINGS{4031279,
  author={Langlois, Benoit and Exertier, Daniel and Bonnet, Stephane},
  booktitle={2006 10th IEEE International Enterprise Distributed Object Computing Conference Workshops (EDOCW'06)}, 
  title={Performance Improvement of MDD Tools}, 
  year={2006},
  volume={},
  number={},
  pages={19-19},
  abstract={From first to mature versions of Model-Driven Development (MDD) tools, there is a gap, as for any other software applications. All functional requirements must be met, including qualities of services, at the risk of seeing MDD tools rejected by users. In this paper, we focus on performance, especially for large-scale developments. After an overview of methodological elements, we give a list of reusable practices on performance. We conclude by a set of observations and stakes in order to understand where efforts must be applied during the development process.},
  keywords={Model driven engineering;Software tools;Scalability;Unified modeling language;Aircraft;Bridges;Software performance;Application software;Quality of service;Large-scale systems},
  doi={10.1109/EDOCW.2006.54},
  ISSN={},
  month={Oct},}

@INBOOK{10952302,
  author={Kaur, Sharanpreet and Singh, Satwinder},
  booktitle={Agile Software Development: Trends, Challenges and Applications}, 
  title={Improving the Quality of Open Source Software}, 
  year={2023},
  volume={},
  number={},
  pages={309-323},
  abstract={Summary <p>This study aims at development of generating metrics based code smells prediction to improve the software quality assurance by working at preventive maintenance level. In order to do so, Refactoring is the best solution for identification of smelly areas in the code to reveal the portions which demands patching. It not only increases the life of code but eventually increases the quality of software in long run, where versions of a software are launched one after the other. The empirical model development considered Deep learning based neural network technique for establishing the association between code smells and metrics in the source code of Eclipse which is a Java based application contributing efficiently on the open source platform. A statistical analysis was pre applied on the set of code smells and metrics for finding the connection between the both. Later on, Multi Layer Perceptron model development on four versions of Eclipse has been made. Subsequently Area Under Curve (ROC) has been generated for class &amp; method level code smells. The value of ROC in predicting code smells pointed towards the fact that Neural Network Multi Layer Perceptron model perform fair to good in determining the presence of code smells based on software metrics in Eclipse. Therefore from the results obtained it is concluded that smelly classes are predicted efficiently by software metric based code smells prediction model. The present study will be beneficial to the software development community to locate the refactoring areas and providing resources for testing. The results of empirical study also guide the development community by providing information relative to code smells and its types. The aim of this study is to provide statistical proof of linkage between metrics and code smells. The software metric based prediction model of code smells and its type's aids in development of prediction model based upon metrics values to improve the overall quality of software.</p>},
  keywords={Codes;Software;Maintenance;Predictive models;Software maintenance;Preventive maintenance;Software measurement;Visualization;Systematic literature review;Source coding},
  doi={10.1002/9781119896838.ch16},
  ISSN={},
  publisher={Wiley},
  isbn={9781119896821},
  url={https://ieeexplore.ieee.org/document/10952302},}

@INPROCEEDINGS{5533639,
  author={Piho, Gunnar and Tepandi, Jaak and Parman, Marko and Perkins, David},
  booktitle={The 33rd International Convention MIPRO}, 
  title={From archetypes-based domain model of clinical laboratory to LIMS software}, 
  year={2010},
  volume={},
  number={},
  pages={1179-1184},
  abstract={We present our approach for developing a laboratory information management system (LIMS) software by combining Björners software triptych methodology (from domain models via requirements to software) with Arlow and Neustadt archetypes and archetype patterns based initiative. The fundamental hypothesis is that through this Archetypes Based Development (ABD) approach to domains, requirements and software, it is possible to improve the software development process as well as to develop more dependable software. We use ADB in developing LIMS software for the Clinical and Biomedical Proteomics Group (CBPG), University of Leeds.},
  keywords={Laboratories;Abstracts;Programming;Production facilities;Software engineering;Proteomics;Information management;Software systems;Application software;Customer relationship management},
  doi={},
  ISSN={},
  month={May},}

@INPROCEEDINGS{9594375,
  author={Darwesh, Darbaz Nawzad and Annighöfer, Björn and Lehmann, Matthias},
  booktitle={2021 IEEE/AIAA 40th Digital Avionics Systems Conference (DASC)}, 
  title={A qualification effort assessment framework for development processes of safety-critical system functions}, 
  year={2021},
  volume={},
  number={},
  pages={1-10},
  abstract={This paper proposes an evaluation framework for quantifying the impact on the qualification effort using the Selective Middleware approach for safety-critical avionics functions. The qualification process of safety-critical avionics functions is troublesome and time-consuming. Most of the effort is spent on the qualification process to meet the qualification means or so-called objectives defined by recommended standards and guidelines. Therefore, tools or automation approaches are usually used to minimize the effort in the qualification process. In this research, the platform development approach called Selective Middleware (SMW) is investigated. It provides an efficient development approach for deploying a platform management middleware used for safety-critical avionics functions on IMA. The SMW approach provides a knowledge-based toolchain that automatically derives the platform management middleware instance from a manually designed high-level system architecture model. In addition to the platform management software, the required design and verification artifacts are also automatically generated. Therefore, this paper investigates to what extent the SMW approach reduces the qualification effort considering DO-178C, ARP4754A, and DO-297. To this object, a detailed assessment framework is defined in terms that captures and evaluates the impact on relevant effort reduction in each aircraft function life cycle process. The assessment framework is applied to the SMW approach concerning the guidelines, and a detailed and quantified analysis is performed.},
  keywords={Knowledge based systems;Systems architecture;Aerospace electronics;Tools;Middleware;Task analysis;Aircraft;assessment framework;safety-critical avionics functions;model-based development;software development effort estimation;fault-tolerant;integrated modular avionics;qualification;certification},
  doi={10.1109/DASC52595.2021.9594375},
  ISSN={2155-7209},
  month={Oct},}

@ARTICLE{5525316,
  author={Schavey, Todd and Duba, Shane},
  journal={IEEE Aerospace and Electronic Systems Magazine}, 
  title={Streamlining IMA integration through model-driven methodologies}, 
  year={2010},
  volume={25},
  number={6},
  pages={21-24},
  abstract={Avionics systems integration is an inherently complex undertaking. In addition to ensuring that basic functionality is satisfied, the systems integrator must maximize the system's flexibility and reliability while minimizing weight and cost of change. With the introduction of integrated modular architectures (IMA) based on open standards, many traditional integration issues have been greatly improved. However, additional integration responsibilities arise due to having a large number of functions developed by independent suppliers all sharing the same physical resources.},
  keywords={Aerospace electronics;Cost function},
  doi={10.1109/MAES.2010.5525316},
  ISSN={1557-959X},
  month={June},}

@INPROCEEDINGS{9843626,
  author={Khan, Shaheer and Mukherji, Deep and Lawler, Christopher and Kruger, Andrew and Voskanian, Vicken and Schellpfeffer, Maria and Alibay, Farah and Hwangpo, Nari and Weise, Tim},
  booktitle={2022 IEEE Aerospace Conference (AERO)}, 
  title={Flight Rule Design, Implementation, Verification, and Validation for the Psyche Mission}, 
  year={2022},
  volume={},
  number={},
  pages={1-11},
  abstract={NASA Jet Propulsion Lab (JPL)'s upcoming mission Psyche will begin its journey to the asteroid (16) Psyche in late 2022 in an effort to better understand its origins and, in turn, better understand our own. Operating the spacecraft safely will require the dedicated efforts of a small team that understands the spacecraft's operational constraints, as well as a set of powerful spacecraft models designed to catch command errors that can pose risks to mission success. One of the responsibilities of the operations team is to ensure adherence to a set of Flight Rules written by spacecraft and instrument experts that are designed to mitigate these risks. Psyche's innovations in Flight Rule design principles and advancements in the tools and processes used to implement and check Flight Rules are discussed. A comparison of Psyche's approach to Flight Rules to other JPL missions will provide lessons learned for future missions that must perform constraint checking during operations. Flight Rule development faces several major challenges. First, Flight Rule developers must work with Subject Matter Experts (SME) to write the rules in a way that captures the intent of the constraint in a straightforward, enforceable manner. Second, software developers must correctly interpret Flight Rules into code so that it meets the original intent of the SME. Finally, a means must be provided for SMEs to validate Flight Rule implementations without requiring them to understand the underlying software. Innovative processes intended to efficiently close the loop between stakeholders and software developers are described, such as the use of test-driven development to provide stakeholders with easy-to-review implementations. New guidelines for Flight Rule writing, designed to address these challenges, are described for future missions to adopt and build upon. All missions must perform detailed constraint checking, so a comparison of Psyche's approach to some of these items to the approaches taken by other missions such as Dawn, M2020, and Europa Clipper is done, specifically to examine SME-developer communication, tools used, and development process. Lessons learned from this comparison are be provided. Psyche Mission System has a variety of new and heritage tools that improve the Flight Rule validation and checking process. Psyche developed a powerful, new tool called RandSEQ and made significant improvements to OctopusJam, two valuable tools that aid the development of Flight Rule unit tests. Advancements in the models and processes for performing sequence validation with SEQuence GENerator (SEQGEN), the primary, high-heritage tool used for automated Flight Rule checks on Psyche, are described. The development of new software and the advancements to existing software put Psyche at the forefront of Flight Rule technology.},
  keywords={Space vehicles;Technological innovation;NASA;Writing;Propulsion;Software;Solar system},
  doi={10.1109/AERO53065.2022.9843626},
  ISSN={1095-323X},
  month={March},}

@ARTICLE{531663,
  author={},
  journal={IEEE Std 743-1995}, 
  title={IEEE Standard Equipment Requirements and Measurement Techniques for Analog Transmission Parameters for Telecommunications}, 
  year={1996},
  volume={},
  number={},
  pages={i-},
  abstract={Performance requirements for test equipment that measures the analog transmission parameters of subscriber loops, message trunks, PBX trunks, and ties lines are specified. Requirements for these measurements with DS1 bit stream access are also provided. The measurement of loss, noise, and impulse noise on non-loaded cable pairs used for digital subscriber lines is also addressed.},
  keywords={Communication standards},
  doi={10.1109/IEEESTD.1996.81076},
  ISSN={},
  month={},}

@INPROCEEDINGS{10190372,
  author={Considine, Breandan and Albion, Nicholas and Si, Xujie},
  booktitle={2023 IEEE/ACM 5th International Workshop on Bots in Software Engineering (BotSE)}, 
  title={Idiolect: A Reconfigurable Voice Coding Assistant}, 
  year={2023},
  volume={},
  number={},
  pages={14-18},
  abstract={This paper presents Idiolect, an open source 1 IDE plugin for voice coding and a novel approach to building bots that allows for users to define custom commands on-the-fly. Unlike traditional chatbots, Idiolect does not pretend to be an omniscient virtual assistant but rather a reconfigurable voice programming system that empowers users to create their own commands and actions dynamically, without rebuilding or restarting the application. We offer an experience report describing the tool itself, illustrate some example use cases, and reflect on several lessons learned during the tool’s development.},
  keywords={Virtual assistants;Semantics;Speech recognition;Programming;Chatbots;Software;Encoding;speech recognition;voice programming;bots},
  doi={10.1109/BotSE59190.2023.00011},
  ISSN={},
  month={May},}

@INPROCEEDINGS{9340228,
  author={Nikiforova, Anastasija and Bicevskis, Janis and Bicevska, Zane and Oditis, Ivo},
  booktitle={2020 7th International Conference on Internet of Things: Systems, Management and Security (IOTSMS)}, 
  title={Data Quality Model-based Testing of Information Systems: the Use-case of E-scooters}, 
  year={2020},
  volume={},
  number={},
  pages={1-8},
  abstract={The paper proposes a data quality model-based testing methodology aimed at improving testing methodology of information systems (IS) using previously proposed data quality model. The solution supposes creation of a description of the data to be processed by IS and the data quality requirements used for the development of the tests, followed by performing an automated test of the system on the generated tests verifying the correctness of data to be entered and stored in the database. The generation of tests for all possible data quality conditions creates a complete set of tests that verify the operation of the IS under all possible data quality conditions. The proposed solution is demonstrated by the real example of the system dealing with e-scooters. Although the proposed solution is demonstrated by applying it to the system that is already in use, it can also be used when developing a new system.},
  keywords={Databases;Data integrity;Syntactics;Data models;Software;Testing;Information systems;complete test set;data quality model;e-scooters;Internet of Things;IoT;Internet of Vehicles;model-based testing;symbolic execution},
  doi={10.1109/IOTSMS52051.2020.9340228},
  ISSN={},
  month={Dec},}

@INPROCEEDINGS{6193584,
  author={Gohil, Komal and Alapati, Nagalakshmi and Joglekar, Sunil},
  booktitle={3rd International Conference on Advances in Recent Technologies in Communication and Computing (ARTCom 2011)}, 
  title={Towards behavior driven operations (BDOps)}, 
  year={2011},
  volume={},
  number={},
  pages={262-264},
  abstract={Modern Enterprise Software Systems entail many challenges such as availability, scalability, complexity and providing business agility. Ensuring the systems to be up and running for 24 × 7 has become a mandate for operations. Agile development has been adopted to keep pace with the demands of business and IT. Test Driven Development (TDD) and Behavior Driven Development (BDD) are practices, which enable agile development. So far the agile approach has been limited to development. For ensuring business to be truly agile, we need to take forward the agile approach to operations. In this paper, we discuss the behavior driven approach for operations specifically on the core sub-systems like infrastructure provisioning, deployment and monitoring. We share our explorations and experiments with Behavior Driven Monitoring (BDM) and how the same can be adopted for infrastructure provisioning and deployment. We used Cucumber-Nagios to detect behavior of an enterprise application. We close this paper with a note on the benefits to busmess and IT showing its relevance to DevOps, Continuous Delivery and Cloud Computing.},
  keywords={Behavior Driven Development;Behavior Driven Operations;Behavior Driven Monitoring;Cucumber-Nagios;Behavior Driven Infrastructure},
  doi={10.1049/ic.2011.0095},
  ISSN={},
  month={Nov},}

@ARTICLE{6768354,
  author={McGee, Andrew R. and Vasireddy, S. Rao and Johnson, K. Jeffrey and Chandrashekhar, Uma and Richman, Steven H. and El-Sayed, Mohamed},
  journal={Bell Labs Technical Journal}, 
  title={Dynamic virtual private networks}, 
  year={2002},
  volume={6},
  number={2},
  pages={116-135},
  abstract={Modifications to a virtual private network's (VPN's) topology, security, service provisioning options, or quality of service (QoS) typically require an end-user request to their service provider, whose personnel currently perform the VPN management. This process incurs more provisioning delay and is more costly than user self-provisioning. This paper presents a new service approach and dynamic virtual private network (D-VPN) technology that marries VPNs with directory enabled networking and Web-based subscriber service selection. It places VPN management into the hands of the user to produce instantaneous results, lowering service-provider operations costs, and subsequently reducing the cost to the end user. The paper also describes the target architecture and framework as well as the initial types of services that could be supported by D-VPN technology.3},
  keywords={},
  doi={10.1002/bltj.9},
  ISSN={1538-7305},
  month={},}

@INPROCEEDINGS{6595796,
  author={Diepenbeck, Melanie and Soeken, Mathias and Große, Daniel and Drechsler, Rolf},
  booktitle={2013 8th International Workshop on Automation of Software Test (AST)}, 
  title={Towards automatic scenario generation from coverage information}, 
  year={2013},
  volume={},
  number={},
  pages={82-88},
  abstract={Nowadays, the design of software systems is pushed towards agile development practices. One of its most fundamental approaches is Test Driven Development (TDD). This procedure is based on test cases which are incrementally written prior to the implementation. Recently, Behavior Driven Development (BDD) has been introduced as an extension of TDD, in which natural language scenarios are the starting point for the test cases. This description offers a ubiquitous communication mean for both the software developers and stakeholders. Following the BDD methodology thoroughly, one would expect 100 % code coverage, since code is only written to make the test cases pass. However, as we show in an empirical study this expectation is not valid in practice. It becomes even worse in the process of development, i.e. the coverage decreases over time. To close the coverage gap, we sketch an algorithm that generates BDD-style scenarios based on uncovered code.},
  keywords={Data structures;Boolean functions;Natural languages;Software;Testing;Unified modeling language;Context},
  doi={10.1109/IWAST.2013.6595796},
  ISSN={},
  month={May},}

@ARTICLE{10225251,
  author={Torres, Adriano and Costa, Pedro and Amaral, Luis and Pastro, Jonata and Bonifácio, Rodrigo and d'Amorim, Marcelo and Legunsen, Owolabi and Bodden, Eric and Dias Canedo, Edna},
  journal={IEEE Transactions on Software Engineering}, 
  title={Runtime Verification of Crypto APIs: An Empirical Study}, 
  year={2023},
  volume={49},
  number={10},
  pages={4510-4525},
  abstract={Misuse of cryptographic (crypto) APIs is a noteworthy cause of security vulnerabilities. For this reason, static analyzers were recently proposed for detecting crypto API misuses. They differ in strengths and weaknesses, and they might miss bugs. Motivated by the inherent limitations of static analyzers, this article reports on a study of runtime verification (RV) as a dynamic-analysis-based alternative for crypto API misuse detection. RV monitors program runs against formal specifications; it was shown to be effective and efficient for amplifying the bug-finding ability of software tests. We focus on the popular JCA crypto API and write 22 RV specifications based on expert-validated rules in a static analyzer. We monitor these specifications while running tests in five benchmarks. Lastly, we compare the accuracy of our RV-based approach, RVSec, with those of three state-of-the-art crypto API misuses detectors: CogniCrypt, CryptoGuard, and CryLogger. Results show that RVSec has higher accuracy in four benchmarks and is on par with CryptoGuard in the fifth. Overall, RVSec achieves an average ${\boldsymbol{F}}_{1}$    F   1     measure of 95%, compared with 83%, 78%, and 86% for CogniCrypt, CryptoGuard, and CryLogger, respectively. We highlight the strengths and limitations of these tools and show that RV is effective for detecting crypto API misuses. We also discuss how static and dynamic analysis can complement each other for detecting crypto API misuses.},
  keywords={Cryptography;Codes;Ciphers;Runtime;Benchmark testing;Monitoring;Software;Security vulnerability;crypto API misuse;runtime verification},
  doi={10.1109/TSE.2023.3301660},
  ISSN={1939-3520},
  month={Oct},}

@ARTICLE{10878985,
  author={Lahti, Sakari and Hämäläinen, Timo D.},
  journal={IEEE Access}, 
  title={High-Level Synthesis for FPGAs—A Hardware Engineer’s Perspective}, 
  year={2025},
  volume={13},
  number={},
  pages={28574-28593},
  abstract={The recent decades have witnessed unprecedented advances in the complexity of digital hardware systems, yet their design methods are still mostly based on manual register-transfer level (mRTL) languages such as VHDL and Verilog, introduced in the 1980s. While allowing exact system description, these languages have low productivity and require special expertise. High-level synthesis (HLS) promises to increase the productivity of hardware design by allowing system description from abstract, timeless source code, which is synthesized into optimized RTL code by an HLS tool according to technological constraints. However, HLS is still seen as somewhat immature technology with a non-consolidated offering of tools with varying features. Furthermore, the quality of results (QoR) of HLS is seen to be worse than with mRTL methods. This study sheds light on the status of HLS today. The emphasis is on field-programmable gate arrays (FPGAs) that allow fast development cycles. The study briefly covers the history of HLS, describes the HLS design flow, and lists the benefits and remaining challenges. The offering of current commercial and academic HLS tools is surveyed along with their features. A literature survey covering academic articles published between 2017 and 2024 on the QoR and productivity of HLS is presented. The results show that a gap of some margin still exists between the QoR of the HLS and mRTL methods. However, in productivity, HLS clearly outcompetes mRTL. Based on the study, several recommendations are made for HLS tool developers to close the QoR gap and accelerate the adoption of the method.},
  keywords={Field programmable gate arrays;Hardware;Productivity;Clocks;Surveys;Codes;Software;Resource management;Mathematical models;C++ languages;Design tools;field programmable gate arrays;high-level synthesis;productivity;quality-of-results},
  doi={10.1109/ACCESS.2025.3540320},
  ISSN={2169-3536},
  month={},}

@INPROCEEDINGS{9282802,
  author={Wang, Rui and Artho, Cyrille and Kristensen, Lars Michael and Stolz, Volker},
  booktitle={2020 IEEE 20th International Conference on Software Quality, Reliability and Security (QRS)}, 
  title={Multi-objective Search for Model-based Testing}, 
  year={2020},
  volume={},
  number={},
  pages={130-141},
  abstract={This paper presents a search-based approach relying on multi-objective reinforcement learning and optimization for test case generation in model-based software testing. Our approach considers test case generation as an exploration versus exploitation dilemma, and we address this dilemma by implementing a particular strategy of multi-objective multi-armed bandits with multiple rewards. After optimizing our strategy using the jMetal multi-objective optimization framework, the resulting parameter setting is then used by an extended version of the Modbat tool for model-based testing. We experimentally evaluate our search-based approach on a collection of examples, such as the ZooKeeper distributed service and PostgreSQL database system, by comparing it to the use of random search for test case generation. Our results show that test cases generated using our search-based approach can obtain more predictable and better state/transition coverage, find failures earlier, and provide improved path coverage.},
  keywords={Software testing;Software algorithms;Software quality;Tools;Software reliability;Security;Optimization;model-based testing;test case generation;bandit-based methods;multi-objective optimization;genetic algorithm;search-based software testing},
  doi={10.1109/QRS51102.2020.00029},
  ISSN={},
  month={Dec},}

@INPROCEEDINGS{776011,
  author={Hodge, Y. and Bajpay, P. and Chao, C.-W. and Grammer, G. and Kan, H. and Nadle, D.},
  booktitle={IEEE GLOBECOM 1998 (Cat. NO. 98CH36250)}, 
  title={AT&T service maintenance platform for next century}, 
  year={1998},
  volume={6},
  number={},
  pages={3757-3762 vol.6},
  abstract={With rapid deployment of new services and increasing competitive pressure have come new challenges in the telecommunications management arena. This paper presents an evolved service maintenance platform intended to streamline, simplify and automate network management operations. A unified Business Maintenance Platform (BMP) for AT&T voice and data services is a key enabler for supporting AT&T continuous commitment to quality of service (QoS). The BMP is critical to the seamless and cost effective integration of voice, data and frame relay services and provides a flexible platform to encompass local, ATM, wireless services and new services in the future.},
  keywords={Quality of service;Costs;Frame relay;Asynchronous transfer mode;Customer service;SONET;Availability;Chaos;Laboratories;Pressing},
  doi={10.1109/GLOCOM.1998.776011},
  ISSN={},
  month={Nov},}

@INPROCEEDINGS{7338253,
  author={Rodriguez-Echeverria, Roberto and Macias, Fernando},
  booktitle={2015 ACM/IEEE 18th International Conference on Model Driven Engineering Languages and Systems (MODELS)}, 
  title={A statistical analysis approach to assist model transformation evolution}, 
  year={2015},
  volume={},
  number={},
  pages={226-235},
  abstract={Model Driven Engineering (MDE) is essentially based in metamodel definition, model edition and the specification of model transformations (MT) among these. In many cases the development, evolution and adaptation of these transformations is still carried out without the support of proper methods and tools to reduce the effort and related costs to these activities. In this work, a novel model testing approach specifically designed to assist the engineer in model transformation evolution is presented. A statistical analysis of the actual behavior of the transformations is performed by means of the computation of well-known information extraction metrics. In order to assist the MT adaptation, a detailed interpretation of the possible results of those metrics is also presented. And finally, the results of applying this approach on a Model-Driven Reverse Engineering (MDRE) scenario defined in the context of the MIGRARIA project are discussed.},
  keywords={Adaptation models;Concrete;Contracts;Testing;Context modeling;Measurement;Unified modeling language;Model Transformation;Model Transformation Evolution;Model Transformation Testing;Testing Oracle},
  doi={10.1109/MODELS.2015.7338253},
  ISSN={},
  month={Sep.},}

@INPROCEEDINGS{10555775,
  author={Oliver, Philip and Dietrich, Jens and Anslow, Craig and Homer, Michael},
  booktitle={2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR)}, 
  title={CrashJS: A NodeJS Benchmark for Automated Crash Reproduction}, 
  year={2024},
  volume={},
  number={},
  pages={75-87},
  abstract={Software bugs often lead to software crashes, which cost US companies upwards of $2.08 trillion annually. Automated Crash Reproduction (ACR) aims to generate unit tests that successfully reproduce a crash. The goal of ACR is to aid developers with debugging, providing them with another tool to locate where a bug is in a program. The main approach ACR currently takes is to replicate a stack trace from an error thrown within a program. Currently, ACR has been developed for C, Java, and Python, but there are no tools targeting JavaScript programs. To aid the development of JavaScript ACR tools, we propose CrashJS: a benchmark dataset of 453 Node.js crashes from several sources. CrashJS includes a mix of real-world and synthesised tests, multiple projects, and different levels of complexity for both crashes and target programs.},
  keywords={Java;Target tracking;Computer bugs;Benchmark testing;Computer crashes;Software;Complexity theory;Automated Crash Reproduction;Benchmark;Data Collection;Dataset;Software Testing;Test Generation},
  doi={},
  ISSN={2574-3864},
  month={April},}

@BOOK{10163446,
  author={Santana, David and Malik, Amit},
  booktitle={Cloud Computing Demystified for Aspiring Professionals: Hone your skills in AWS, Azure, and Google cloud computing and boost your career as a cloud engineer},
  year={2023},
  volume={},
  number={},
  pages={},
  abstract={Gain in-depth knowledge of cloud computing concepts and apply them to accelerate your career in any cloud engineering roleKey FeaturesGet to grips with key cloud computing concepts, cloud service providers, and best practicesExplore demonstrations for cloud computing models using real-world examplesAdopt the self-paced learning strategy and get industry-ready for cloud engineering rolesPurchase of the print or Kindle book includes a free eBook in the PDF formatBook DescriptionIf you want to upskill yourself in cloud computing domains to thrive in the IT industry, then you’ve come to the right place. Cloud Computing Demystified for Aspiring Professionals helps you to master cloud computing essentials and important technologies offered by cloud service providers needed to succeed in a cloud-centric job role. This book begins with an overview of transformation from traditional to modern-day cloud computing infrastructure, and various types and models of cloud computing. You’ll learn how to implement secure virtual networks, virtual machines, and data warehouse resources including data lake services used in big data analytics — as well as when to use SQL and NoSQL databases and how to build microservices using multi-cloud Kubernetes services across AWS, Microsoft Azure, and Google Cloud. You'll also get step-by-step demonstrations of infrastructure, platform, and software cloud services and optimization recommendations derived from certified industry experts using hands-on tutorials, self-assessment questions, and real-world case studies. By the end of this book, you'll be ready to successfully implement cloud computing standardized concepts, services, and best practices in your workplace.What you will learnGain insights into cloud computing essentials and public, private, hybrid, and multi-cloud deployment modelsExplore core cloud computing services such as IaaS, PaaS, and SaaSDiscover major public cloud providers such as AWS, Microsoft, and GoogleUnlock the power of IaaS, PaaS, and SaaS with AWS, Azure, and GCPCreate secure networks, containers, Kubernetes, compute, databases, and API services on cloudDevelop industry-based cloud solutions using real-world examplesGet recommendations on exam preparation for cloud accreditationsWho this book is forThe book is for aspiring cloud engineers, as well as college graduates, IT enthusiasts, and beginner-level cloud practitioners looking to get into cloud computing or transforming their career and upskilling themselves in a cloud engineering role in any industry. A basic understanding of networking, database development, and data analysis concepts and experience in programming languages such as Python and C# will help you get the most out of this book.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781803230573},
  url={https://ieeexplore.ieee.org/document/10163446},}

@ARTICLE{11024158,
  author={Zhang, Chuan and You, You and Wang, Naigang and Park, Jongsun and Zhang, Li},
  journal={IEEE Journal on Emerging and Selected Topics in Circuits and Systems}, 
  title={Generative AI Through CAS Lens: An Integrated Overview of Algorithmic Optimizations, Architectural Advances, and Automated Designs}, 
  year={2025},
  volume={15},
  number={2},
  pages={149-185},
  abstract={Generative artificial intelligence (GenAI) has emerged as a pivotal focus in global innovation agendas, revealing transformative potential that extends beyond technological applications to reshape diverse societal domains. Given the fundamental dependency of GenAI deployment on circuits and systems (CAS), a co-evolutionary approach integrating both technological paradigms becomes imperative. This synergistic framework confronts three interrelated challenges: 1) developing deployment-ready GenAI algorithms, 2) engineering implementation-efficient CAS architectures, and 3) leveraging GenAI for autonomous CAS designs - each representing critical innovations vectors. Given the rapid advancement of GenAI-CAS technologies, a comprehensive synthesis has become an urgent priority across academia and industry. Consequently, this timely review systematically analyzes current advancements, provides integrative perspectives, and identifies emerging research trajectories. This review endeavors to serve both AI and CAS communities, thereby catalyzing an innovation feedback loop: GenAI-optimized CAS architectures in turn accelerate GenAI evolution through algorithm-hardware co-empowerment.},
  keywords={Computational modeling;Optimization;Circuits and systems;Artificial intelligence;Hardware;Biological system modeling;Computer architecture;Adaptation models;Integrated circuit modeling;Generative adversarial networks;Generative artificial intelligence (GenAI);circuits and systems (CAS);algorithms;architectures;autonomous design},
  doi={10.1109/JETCAS.2025.3575272},
  ISSN={2156-3365},
  month={June},}

@BOOK{10163159,
  author={Sharma, Sourabh},
  booktitle={Modern API Development with Spring and Spring Boot: Design highly scalable and maintainable APIs with REST, gRPC, GraphQL, and the reactive paradigm},
  year={2021},
  volume={},
  number={},
  pages={},
  abstract={A developer's guide to designing, testing, and securing production-ready modern APIs with the help of practical ideas to improve your application’s functionalityKey FeaturesBuild resilient software for your enterprises and customers by understanding the complete API development life cycleOvercome the challenges of traditional API design by adapting to a new and evolving culture of modern API developmentUse Spring and Spring Boot to develop future-proof scalable APIsBook DescriptionThe philosophy of API development has evolved over the years to serve the modern needs of enterprise architecture, and developers need to know how to adapt to these modern API design principles. Apps are now developed with APIs that enable ease of integration for the cloud environment and distributed systems. With this Spring book, you'll discover various kinds of production-ready API implementation using REST APIs and explore async using the reactive paradigm, gRPC, and GraphQL. You'll learn how to design evolving REST-based APIs supported by HATEOAS and ETAGs and develop reactive, async, non-blocking APIs. After that, you'll see how to secure REST APIs using Spring Security and find out how the APIs that you develop are consumed by the app's UI. The book then takes you through the process of testing, deploying, logging, and monitoring your APIs. You'll also explore API development using gRPC and GraphQL and design modern scalable architecture with microservices. The book helps you gain practical knowledge of modern API implementation using a sample e-commerce app. By the end of this Spring book, you'll be able to develop, test, and deploy highly scalable, maintainable, and developer-friendly APIs to help your customers to transform their business.What you will learnUnderstand RESTful API development, its design paradigm, and its best practicesBecome well versed in Spring's core components for implementing RESTful web servicesImplement reactive APIs and explore async API developmentApply Spring Security for authentication using JWT and authorization of requestsDevelop a React-based UI to consume APIsImplement gRPC inter-service communicationDesign GraphQL-based APIs by understanding workflows and toolingGain insights into how you can secure, test, monitor, and deploy your APIsWho this book is forThis book is for inexperienced Java programmers, comp science, or coding boot camp graduates who have knowledge of basic programming constructs, data structures, and algorithms in Java but lack the practical web development skills necessary to start working as a developer. Professionals who've recently joined a startup or a company and are tasked with creating real-world web APIs and services will also find this book helpful. This book is also a good resource for Java developers who are looking for a career move into web development to get started with the basics of web service development.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781800562875},
  url={https://ieeexplore.ieee.org/document/10163159},}

@INPROCEEDINGS{9183509,
  author={Schiewe, Alexander and Ruck, Andreas and Duffley, Gordon and Butson, Christopher R. and Krüger, Jens},
  booktitle={2017 IEEE 10th Workshop on Software Engineering and Architectures for Realtime Interactive Systems (SEARIS)}, 
  title={VisAnalyticsKit: User Logging for Mobile Visualization Applications}, 
  year={2017},
  volume={},
  number={},
  pages={1-8},
  abstract={This paper introduces a full-stack self-hosted user logging system for mobile devices, available to the open-source community. It excels through minimal integration and administration efforts for developers or researchers who use our system. VisAnalyticsKit features a W3C conform provenance data model and two-way data replication, rarely found among other analytics services. It facilitates pushing data from mobile devices to back-end servers and pulling data from data centers to the users' or analysts' devices. This allows replaying and reviewing previously captured log sessions in the sense of What You See Now, Is What I Saw Then (WYSNIWIST). Further, we evaluate our system by instrumenting a mobile visualization application, publicly available on the App Store.},
  keywords={Servers;Open source software;Mobile handsets;Databases;Data visualization;Tools;W3C;H.5.2 [Information Interfaces and Presentation]: User Interfaces—Evaluation/methodology;I.3.6 [Computer Graphics]: Methodology and Techniques—Interaction techniques, D.1.m [Programming Techniques]: Miscellaneous—;K.6.2 [Management of Computing and Information Systems]: Installation Management—Performance and usage measurement},
  doi={10.1109/SEARIS41720.2017.9183509},
  ISSN={2328-7829},
  month={March},}

@BOOK{10745312,
  author={McDonald, Malcolm},
  booktitle={Grokking Web Application Security},
  year={2024},
  volume={},
  number={},
  pages={},
  abstract={When you launch an application on the web, every hacker in the world has access to it. Are you sure your web apps can stand up to the most sophisticated attacks? Trying to teach yourself about web security from the internet can feel like walking into a huge disorganized library—one where you can never find what you need, and the wrong advice might endanger your application! You need a single, all-in-one guide to securing your apps against all the attacks they can and will face. You need Grokking Web Application Security. This brilliantly illustrated and clearly written guide delivers detailed coverage on:  Browser security, including sandboxing, the same-origin policy, and cookie security Securing web servers with input validation, escaping of output, and defense in depth A development process that prevents security bugs Browser vulnerabilities, from cross-site scripting and cross-site request forgery, to clickjacking Network vulnerabilities, such as man-in-the-middle attacks, SSL-stripping, and DNS poisoning Authentication vulnerabilities, such as brute forcing of credentials with single sign-on or multi-factor authentication Authorization vulnerabilities, such as broken access control and session jacking How to use encryption in web applications Injection attacks, command execution attacks, and remote code execution attacks Malicious payloads that can be used to attack XML parsers and file upload functions  Grokking Web Application Security teaches you how to build web apps that are ready and resilient to any attack. It’s laser-focused on what the working programmer needs to know about web security. In it, you’ll find practical recommendations for both common and not-so-common vulnerabilities—everything from SQL injection to cross-site scripting inclusion attacks. You’ll learn what motivates hackers, discover the latest tools for identifying issues, and set up a development lifecycle that catches problems early. Read it cover to cover for a comprehensive overview of web security, and dip in as a reference whenever you need to tackle a specific vulnerability.},
  keywords={CSRF;bugs;encryption;clickjacking;vulnerabilities;attacks;frontend;backend;DNS poisoning;SSL-stripping;man-in-the-middle;network;attacks;browser;hackers;sandboxing},
  doi={},
  ISSN={},
  publisher={Manning},
  isbn={9781633438262},
  url={https://ieeexplore.ieee.org/document/10745312},}

@BOOK{10163381,
  author={Ghita, Catalin},
  booktitle={Kickstart Modern Android Development with Jetpack and Kotlin: Enhance your applications by integrating Jetpack and applying modern app architectural concepts},
  year={2022},
  volume={},
  number={},
  pages={},
  abstract={Explore modern Android development in Kotlin 1.6.10 with this condensed hands-on guide to building reliable apps using libraries such as Compose, ViewModel, Hilt, Retrofit, Flow, and moreKey FeaturesExplore Jetpack libraries and other modern technologies for Android developmentImprove the architectural design of your Android appsEnhance the quality of your Android projects’ code bases and applications using the latest librariesBook DescriptionWith Jetpack libraries, you can build and design high-quality, robust Android apps that have an improved architecture and work consistently across different versions and devices. This book will help you understand how Jetpack allows developers to follow best practices and architectural patterns when building Android apps while also eliminating boilerplate code. Developers working with Android and Kotlin will be able to put their knowledge to work with this condensed practical guide to building apps with the most popular Jetpack libraries, including Jetpack Compose, ViewModel, Hilt, Room, Paging, Lifecycle, and Navigation. You'll get to grips with relevant libraries and architectural patterns, including popular libraries in the Android ecosystem such as Retrofit, Coroutines, and Flow while building modern applications with real-world data. By the end of this Android app development book, you'll have learned how to leverage Jetpack libraries and your knowledge of architectural concepts for building, designing, and testing robust Android applications for various use cases.What you will learnIntegrate popular Jetpack libraries such as Compose, ViewModel, Hilt, and Navigation into real Android apps with KotlinApply modern app architecture concepts such as MVVM, dependency injection, and clean architectureExplore Android libraries such as Retrofit, Coroutines, and FlowIntegrate Compose with the rest of the Jetpack libraries or other popular Android librariesWork with other Jetpack libraries such as Paging and Room while integrating a real REST API that supports paginationTest Compose UI and the application logic through unit testsWho this book is forThis book is for junior and intermediate-level Android developers looking to level up their Android development skills to develop high-quality apps using Jetpack libraries and other cutting-edge technologies. Beginners with knowledge of Android development fundamentals will also find this book useful. Familiarity with Kotlin is assumed.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781801818216},
  url={https://ieeexplore.ieee.org/document/10163381},}

@INPROCEEDINGS{10589222,
  author={Ragel, Ritz Kevin C. and Balahadia, Francis F.},
  booktitle={2023 IEEE 15th International Conference on Humanoid, Nanotechnology, Information Technology, Communication and Control, Environment, and Management (HNICEM)}, 
  title={Visual Test Framework: Enhancing Software Test Automation with Visual Artificial Intelligence and Behavioral Driven Development}, 
  year={2023},
  volume={},
  number={},
  pages={1-5},
  abstract={Software test automation plays a crucial role in today's software needs. Traditional test automation approaches often rely on script-based testing, which can be time-consuming, error-prone, and challenging to maintain. In recent years, Visual AI (Artificial Intelligence) and Behavior Driven Development (BDD) have emerged as valuable approaches to enhance the efficiency and effectiveness of software testing. Visual AI leverages machine learning algorithms to identify visual differences in Graphical User Interfaces (GUI), accelerating the test automation process and improving accuracy. BDD focuses on defining and automating system behavior from the end-user perspective, fostering collaboration between stakeholders and eliminating the need for specialized technical expertise. This paper explores the integration of Visual AI and BDD in software test automation, aiming to evaluate its impact on test creation, execution, and maintenance. The research employs a mixed-methods approach, including surveys and evaluations of the Visual Test Framework integrated with Visual AI tools. The findings provide valuable insights into the benefits and challenges of integrating Visual AI and BDD in software test automation, highlighting their potential to streamline testing processes, improve accuracy, and eliminate technical barriers. This paper recommends the usage of Visual AI with BDD},
  keywords={Surveys;Software testing;Visualization;Automation;Accuracy;Software;Stakeholders;Visual AI;Behavioral Driven Development (BDD);Graphical User Interface (GUI);Software Quality;Software Testing;Test Automation;Framework;Applitools},
  doi={10.1109/HNICEM60674.2023.10589222},
  ISSN={2770-0682},
  month={Nov},}

@INPROCEEDINGS{6229805,
  author={Schiller, Todd W. and Lucia, Brandon},
  booktitle={2012 Second International Workshop on Developing Tools as Plug-Ins (TOPI)}, 
  title={Playing cupid: The IDE as a matchmaker for plug-ins}, 
  year={2012},
  volume={},
  number={},
  pages={1-6},
  abstract={We describe a composable, data-driven, plug-in ecosystem for IDEs. Inspired by Unix's and Windows Power-Shell's pipeline communication models, each plug-in declares data-driven capabilities. Developers can then seamlessly mix, match, and combine plug-in capabilities to produce new insight, without modifying the plug-ins. We formalize the architecture using the polymorphic lambda calculus, with special types for source and source locations; the type system prevents nonsensical plug-in combinations, and helps to inform the design of new tools and plug-ins. To illustrate the power of the formalism, we describe several synergies between existing plug-ins (and tools) made possible by the ecosystem.},
  keywords={Java;Pipelines;Biological system modeling;Contracts;Cloning;Ecosystems;Debugging},
  doi={10.1109/TOPI.2012.6229805},
  ISSN={2327-0772},
  month={June},}

@ARTICLE{6772080,
  author={Aprille, T. J. and Gupta, D. V. and St. Amand, P. G.},
  journal={The Bell System Technical Journal}, 
  title={D4 Digital Channel Bank Family: Dataport — Channel units for digital data system subrates}, 
  year={1982},
  volume={61},
  number={9},
  pages={2721-2740},
  abstract={The single-channel dataports are a series of D4 channel units that convert the digital signal derived from one T-facility time slot by the D4 common circuits to an appropriate format at speeds of 64, 9.6, 4.8, or 2.4 kb/s for use in the Digital Data System (DDS). They come in two formats, the first being the DDS bipolar format for 64 kb/s and the second, for the remaining three speeds, being an EIA RS-449 format. Their error-correction feature ensures 10−8 error-rate performance for a 10−3 error-rate transmission channel. Advances in large-scale integration (LSI) technology have allowed the packaging of all the digital circuit functions needed into the space of a single channel unit. An on-board power converter unit generates the additional current required by the dataports over that needed by regular analog channel units. The local loop side of each channel unit uses integrated technology to achieve signal equalization and timing recovery. Standard DDS remote maintenance features are provided. The dataport channel units are easily installed and removed; they supply economical digital transmission.},
  keywords={},
  doi={10.1002/j.1538-7305.1982.tb03449.x},
  ISSN={0005-8580},
  month={Nov},}

@BOOK{10251221,
  author={Avdi, Mark and Lam, Leo},
  booktitle={AWS CDK in Practice: Unleash the power of ordinary coding and streamline complex cloud applications on AWS},
  year={2023},
  volume={},
  number={},
  pages={},
  abstract={Uncover the secrets of building maintainable, extensible, and virtually indestructible cloud applications on AWS with Cloud Development Kit (CDK) Purchase of the print or Kindle book includes a free PDF eBookKey FeaturesBuild complex cloud applications with the revolutionary AWS CDKGain practical knowledge of AWS CDK to leverage the powerful toolset of AWSEmploy practical exercises & architectural design patterns for developing modern serverless applicationBook DescriptionAs cloud applications are becoming more complex, multiple tools and services have emerged to cater to the challenges of running reliable solutions. Although infrastructure as code, containers, and orchestration tools, such as Kubernetes, have proved to be efficient in solving these challenges, AWS CDK represents a paradigm shift in building easily developed, extended, and maintained applications. With AWS CDK in Practice, you’ll start by setting up basic day-to-day infrastructure while understanding the new prospects that CDK offers. You’ll learn how to set up pipelines for building CDK applications on the cloud that are long-lasting, agile, and maintainable. You’ll also gain practical knowledge of container-based and serverless application development. Furthermore, you’ll discover how to leverage AWS CDK to build cloud solutions using code instead of configuration files. Finally, you’ll explore current community best practices for solving production issues when dealing with CDK applications. By the end of this book, you’ll have practical knowledge of CDK, and you’ll be able to leverage the power of AWS with code that is simple to write and maintain using AWS CDK.What you will learnTurn containerized web applications into fully managed solutionsExplore the benefits of building DevOps into everyday code with AWS CDKUncover the potential of AWS services with CDKCreate a serverless-focused local development environmentSelf-assemble projects with CI/CD and automated live testingBuild the complete path from development to production with AWS CDKBecome well versed in dealing with production issues through best practicesWho this book is forThis book is for traditional full stack developers looking to explore the new world of Infrastructure as Code and serverless applications, solutions architects seeking to define their services with AWS CDK, and DevOps specialists searching for a better management technique to configure files. Readers should not be new to coding and must have experience in web development in languages such as Python, JS, Typescript, Java, etc. along with a basic understanding of how web applications are developed.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781801815277},
  url={https://ieeexplore.ieee.org/document/10251221},}

@INBOOK{5396736,
  author={Xiu, Liming},
  booktitle={VLSI Circuit Design Methodology Demystified: A Conceptual Taxonomy}, 
  title={CellBased ASIC Design Methodology}, 
  year={2008},
  volume={},
  number={},
  pages={73-188},
  abstract={<P>This chapter contains sections titled: <UL> <LI> <P>What are the major tasks and personnel required in a chip design project?</P> </LI> <LI> <P>What are the major steps in ASIC chip construction?</P> </LI> <LI> <P>What is the ASIC design flow?</P> </LI> <LI> <P>What are the two major aspects of ASIC design flow?</P> </LI> <LI> <P>What are the characteristics of good design flow?</P> </LI> <LI> <P>What is the role of market research in an ASIC project?</P> </LI> <LI> <P>What is the optimal solution of an ASIC project?</P> </LI> <LI> <P>What is system-level study of a project?</P> </LI> <LI> <P>What are the approaches for verifying design at the system level?</P> </LI> <LI> <P>What is register-transfer-level (RTL) system-level description?</P> </LI> <LI> <P>What are methods of verifying design at the register-transfer-level?</P> </LI> <LI> <P>What is a test bench?</P> </LI> <LI> <P>What is code coverage?</P> </LI> <LI> <P>What is functional coverage?</P> </LI> <LI> <P>What is bug rate convergence?</P> </LI> <LI> <P>What is design planning?</P> </LI> <LI> <P>What are hard macro and soft macro?</P> </LI> <LI> <P>What is hardware description language (HDL)?</P> </LI> <LI> <P>What is register-transfer-level (RTL) description of hardware?</P> </LI> <LI> <P>What is standard cell? What are the differences among standard cell, gate-array, and sea-of-gate approaches?</P> </LI> <LI> <P>What is an ASIC library?</P> </LI> <LI> <P>What is logic synthesis?</P> </LI> <LI> <P>What are the optimization targets of logic synthesis?</P> </LI> <LI> <P>What is schematic or netlist?</P> </LI> <LI> <P>What is the gate count of a design?</P> </LI> <LI> <P>What is the purpose of test insertion during logic synthesis?</P> </LI> <LI> <P>What is the most commonly used model in VLSI circuit testing?</P> </LI> <LI> <P>What are controllability and observability in a digital circuit?</P> </LI> <LI> <P>What is a testable circuit?</P> </LI> <LI> <P>What is the aim of scan insertion?</P> </LI> <LI> <P>What is fault coverage? What is defect part per million (DPPM)?</P> </LI> <LI> <P>Why is design for testability important for a product's financial success?</P> </LI> <LI> <P>What is chip power usage analysis?</P> </LI> <LI> <P>What are the major components of CMOS power consumption?</P> </LI> <LI> <P>What is power optimization?</P> </LI> <LI> <P>What is VLSI physical design?</P> </LI> <LI> <P>What are the problems that make VLSI physical design so challenging?</P> </LI> <LI> <P>What is floorplanning?</P> </LI> <LI> <P>What is the placement process?</P> </LI> <LI> <P>What is the routing process?</P> </LI> <LI> <P>What is a power network?</P> </LI> <LI> <P>What is clock distribution?</P> </LI> <LI> <P>What are the key requirements for constructing a clock tree?</P> </LI> <LI> <P>What is the difference between time skew and length skew in a clock tree?</P> </LI> <LI> <P>What is scan chain?</P> </LI> <LI> <P>What is scan chain reordering?</P> </LI> <LI> <P>What is parasitic extraction?</P> </LI> <LI> <P>What is delay calculation?</P> </LI> <LI> <P>What is back annotation?</P> </LI> <LI> <P>What kind of signal integrity problems do place and route tools handle?</P> </LI> <LI> <P>What is cross-talk delay?</P> </LI> <LI> <P>What is cross-talk noise?</P> </LI> <LI> <P>What is IR drop?</P> </LI> <LI> <P>What are the major netlist formats for design representation?</P> </LI> <LI> <P>What is gate-level logic verification before tapeout?</P> </LI> <LI> <P>What is equivalence check?</P> </LI> <LI> <P>What is timing verification?</P> </LI> <LI> <P>What is design constraint?</P> </LI> <LI> <P>What is static timing analysis (STA)?</P> </LI> <LI> <P>What is simulation approach on timing verification?</P> </LI> <LI> <P>What is the logical-effort-based timing closure approach?</P> </LI> <LI> <P>What is physical verification?</P> </LI> <LI> <P>What are design rule check (DRC), design verification (DV), and geometry verification (GV)?</P> </LI> <LI> <P>What is schematic verification (SV) or layout versus schematic (LVS)?</P> </LI> <LI> <P>What is automatic test pattern generation (ATPG)?</P> </LI> <LI> <P>What is tapeout?</P> </LI> <LI> <P>What is yield?</P> </LI> <LI> <P>What are the qualities of a good IC implementation designer?</P> </LI> </UL> </P>},
  keywords={},
  doi={10.1002/9780470199114.ch4},
  ISSN={},
  publisher={IEEE},
  isbn={9780470199107},
  url={https://ieeexplore.ieee.org/document/5396736},}

@INPROCEEDINGS{1607414,
  author={White, J. and Schmidt, D.C.},
  booktitle={13th Annual IEEE International Symposium and Workshop on Engineering of Computer-Based Systems (ECBS'06)}, 
  title={FireAnt: a tool for reducing enterprise product line architecture deployment, configuration, and testing costs}, 
  year={2006},
  volume={},
  number={},
  pages={2 pp.-508},
  abstract={Product-line architectures (PLA)s are a paradigm for developing software families by customizing and composing reusable artifacts, rather than handcrafting software from scratch. Extensive testing is required to develop reliable PLAs. Each PLA may have hundreds of valid variants that can be constructed from the architecture's components. It is crucial that each of these variants be thoroughly tested to ensure the quality of these applications on multiple OS platforms and hardware configurations. Setting up test environments and running tests can become extremely complex and expensive as the number of variants and the complexity of their deployment and configuration increases. Once a variant is deemed ready for deployment and configuration in a production environment, it is crucial that these activities be done identically to the tested configurations and upholds the assumptions of the component developers. Rapidly setting up numerous distributed test environments and ensuring that they are deployed and configured correctly is hard. This poster paper presents FireAnt, which is a tool for the model-driven development (MDD) of PLA deployment plans},
  keywords={Costs;Programmable logic arrays;Computer architecture;Software reusability;Software testing;Application software;Hardware;Production;Software packages;Packaging},
  doi={10.1109/ECBS.2006.43},
  ISSN={},
  month={March},}

@INPROCEEDINGS{10175397,
  author={Györgyi, Csaba and Laki, Sándor and Schmid, Stefan},
  booktitle={2023 IEEE 9th International Conference on Network Softwarization (NetSoft)}, 
  title={Toward Highly Reliable Programmable Data Planes: Verification of P4 Code Generation}, 
  year={2023},
  volume={},
  number={},
  pages={1-5},
  abstract={Data plane programming gained much attention in the past years, having a fast-growing community both in academia and industry. Many tools have emerged to simplify and/or help the development of reliable data plane programs, including fuzzing, formal verification, and different code generators. However, even the tools themselves must be verified to meet the most stringent dependability requirements. In this paper, we investigate various tools and methods to verify code generators leveraging P4 through the example of P4RROT (an open source code generator focusing on the application layer). We show that our approach is efficient and can indeed successfully find bugs. We identify two bugs and propose reusable ideas, such as the use of ghost code.},
  keywords={Industries;Codes;Source coding;Computer bugs;Focusing;Programming;Fuzzing;code generation;in-network computing;data plane verification;P4},
  doi={10.1109/NetSoft57336.2023.10175397},
  ISSN={2693-9789},
  month={June},}

@INPROCEEDINGS{781322,
  author={Harbison, S.P.},
  booktitle={Proceedings 1999 Design Automation Conference (Cat. No. 99CH36361)}, 
  title={System-level hardware/software trade-offs}, 
  year={1999},
  volume={},
  number={},
  pages={258-259},
  abstract={Operating systems and development tools can impose overly general requirements that prevent an embedded system from achieving its hardware performance entitlement. It is time for embedded processor designers to become more involved with system software and tools.},
  keywords={Hardware;Software systems;System software;Operating systems;Computer architecture;Systolic arrays;Assembly;Real time systems;Embedded software;Digital signal processing},
  doi={10.1109/DAC.1999.781322},
  ISSN={},
  month={June},}

@INPROCEEDINGS{6614743,
  author={Dorn, Christoph and Egyed, Alexander},
  booktitle={2013 6th International Workshop on Cooperative and Human Aspects of Software Engineering (CHASE)}, 
  title={Towards collaboration-centric pattern-based software development support}, 
  year={2013},
  volume={},
  number={},
  pages={109-112},
  abstract={Software engineering activities tend to be loosely coupled to allow for flexibly reacting to unforeseen development complexity, requirements changes, and progress delays. This flexibility comes a the price of hidden dependencies among design and code artifacts that make it difficult or even impossible to assess change impact. Incorrect change propagation subsequently results in costly errors. This position paper proposes a novel approach based on monitoring engineering activities for subsequent high-level pattern detection. Patterns of (i) collaboration structures, (ii) temporal action sequences, and (iii) artifact consistency constraints serve as input to recommendation and automatic reconfiguration algorithms for ultimately avoiding and correcting artifact inconsistencies.},
  keywords={Software;Collaboration;Computer architecture;Unified modeling language;Uncertainty;Adaptation models;Software engineering;monitoring;pattern detection;software engineering;recommendation;collaboration structures},
  doi={10.1109/CHASE.2013.6614743},
  ISSN={},
  month={May},}

@INBOOK{9953544,
  author={Chapple, Mike and Seidl, David},
  booktitle={(ISC)2 SSCP Systems Security Certified Practitioner Official Practice Tests}, 
  title={Answers to Review Questions}, 
  year={2022},
  volume={},
  number={},
  pages={197-282},
  abstract={},
  keywords={},
  doi={},
  ISSN={},
  publisher={Wiley},
  isbn={9781119852087},
  url={https://ieeexplore.ieee.org/document/9953544},}

@INBOOK{9558067,
  author={Beck, Gene},
  booktitle={Grid Parity The Art of Financing Renewable Energy Projects in the U.S.}, 
  title={Appendix 1}, 
  year={2014},
  volume={},
  number={},
  pages={363-504},
  abstract={Grid Parity provides an in-depth examination of the knowledge, insights, and techniques that are essential to success in financing renewable energy projects. An energy project finance expert with 35 years of experience in capital asset financing, the author provides a comprehensive overview of how to finance renewable energy projects in America today. He explores all components of “the deal” including tax, accounting, legal, regulatory, documentation, asset management and legislative drivers to this dynamic growth sector. Filled with case studies, the book provides a thorough examination of what it takes to compete in the green-energy marketplace.},
  keywords={},
  doi={},
  ISSN={},
  publisher={River Publishers},
  isbn={9788770223157},
  url={https://ieeexplore.ieee.org/document/9558067},}

@INPROCEEDINGS{9449436,
  author={Lisboa, Luciano A. C.},
  booktitle={15th International Conference on Developments in Power System Protection (DPSP 2020)}, 
  title={Formal methods to power-system automation based on Petri Nets}, 
  year={2020},
  volume={},
  number={},
  pages={1-6},
  abstract={In this work, after surveying the current available methods and formats for specification of functional requirements of PAC systems used by the industry and analysing their advantages and disadvantages, a formal method approach based on Petri Nets is proposed to power-system automation.},
  keywords={Automation;Formal languages;Petri Nets;Power transmission control;Substations},
  doi={10.1049/cp.2020.0145},
  ISSN={},
  month={March},}

@INPROCEEDINGS{10764962,
  author={Xiong, Yiheng and Su, Ting and Wang, Jue and Sun, Jingling and Pu, Geguang and Su, Zhendong},
  booktitle={2024 39th IEEE/ACM International Conference on Automated Software Engineering (ASE)}, 
  title={General and Practical Property-based Testing for Android Apps}, 
  year={2024},
  volume={},
  number={},
  pages={53-64},
  abstract={Finding non-crashing functional bugs for Android apps is challenging for both manual testing and automated GUI testing techniques. This paper introduces and designs a general and practical testing technique based on the idea of property-based testing for finding such bugs. Specifically, our technique incorporates (1) a property description language (PDL) to allow specifying desired app properties, and (2) two exploration strategies as the input generators for effectively validating the properties. We implemented our technique as a tool named Kea and evaluated it on 124 historical bugs from eight real-world, popular Android apps. Our evaluation shows that our PDL can specify all the app properties violated by these historical bugs, demonstrating its generability for finding functional bugs. Kea successfully found 66 (68.0%) and 92 (94.8%) of the 97 historical bugs in scope under the two exploration strategies, demonstrating its practicability. Moreover, Kea found 25 new functional bugs on the latest versions of these eight apps, given the specified properties. To date, all these bugs have been confirmed, and 21 have been fixed. In comparison, prior state-of-the-art techniques found only 13 (13.4%) historical bugs and 1 new bug. We have made all the artifacts publicly available at https://github.com/ecnusse/Kea.CCS CONCEPTS• Software and its engineering → Software testing and debugging.},
  keywords={Software testing;Computer bugs;Manuals;Debugging;Software;Generators;Testing;Software engineering;Graphical user interfaces;Property-based testing;Android app testing;Non-crashing functional bugs},
  doi={},
  ISSN={2643-1572},
  month={Oct},}

@INPROCEEDINGS{10765618,
  author={Goluža, Tomislav},
  booktitle={2024 International Workshop on Fiber Optics in Access Networks (FOAN)}, 
  title={Passive Optical Network Testing and Deployment at Telekom Slovenije}, 
  year={2024},
  volume={},
  number={},
  pages={1-4},
  abstract={Fixed access networks have evolved from legacy copper infrastructure to fibre. The deployment of fibre is crucial for the development of a gigabit society. The majority of networks currently in use are based on a point-to-multipoint topology. By deploying new Passive Optical Network (PON) technologies, providers can offer their customers a better service and provide a baseline for new applications that require high bandwidth and lower latency. For every new technology that is being deployed, there are some key milestones that need to be crossed. Two of the most important factors are standardisation and interoperability. Vendor locking is not a viable option in current deployments; vendors must cooperate to provide interoperability. Providers must also play an active role in this process, as their influence on vendors is significant. It is crucial that vendor proprietary solutions are not confirmed and deployed. The Broadband Forum introduced the BBF.247 ONU Certification program, based on the ONU Conformance Test Plan document TP-247. In conjunction with standardised test procedures, it is incumbent upon the provider to undertake real-case test scenarios with their own specific configuration. A significant aspect of the standardisation of the Gigabit Passive Optical Network (GPON) is the ONU Management and Control Interface (OMCI), which is the source of the majority of interoperability issues. Providers are constructing their optical distribution network for future use, with the objective of deploying all new PON technologies over the same network. In order to deploy new technology, it is necessary to either swap or upgrade existing active equipment. XGS-PON technology, which is a successor to GPON with a symmetrical speed of 10 Gbit/s, is being deployed using a comboPON solution. Concurrently, Wi-Fi 7 is being developed on residential gateways, which represents a significant driver for the extensive deployment of XGS-PON. The advent of 25 Gigabit Symmetric Passive Optical Network (25GS-PON) and 50-Gigabit-capable passive optical networks (50G-PON) will necessitate the consideration of coexistence strategies by service providers. The key decision to be made is whether to retain legacy GPON in the network and combine it with new technologies or to transition completely to XGS-PON and beyond.},
  keywords={Optical fibers;Bandwidth;Logic gates;Optical network units;Passive optical networks;Topology;Interoperability;Wireless fidelity;Standards;Testing;PON;ONU;OLT;interoperability;coexistence;25GS-PON;50G-PON},
  doi={10.1109/FOAN63517.2024.10765618},
  ISSN={2378-8488},
  month={Oct},}

@INPROCEEDINGS{10188640,
  author={Menon, Pottayil Harisanker and Woods, Walt},
  booktitle={2023 IEEE Security and Privacy Workshops (SPW)}, 
  title={Corpus-wide Analysis of Parser Behaviors via a Format Analysis Workbench}, 
  year={2023},
  volume={},
  number={},
  pages={209-218},
  abstract={As the number of parsers written for a data format grows, the number of interpretations of that format's specification also grows. Often, these interpretations differ in subtle, hard-to-determine ways that can result in parser differentials – where one input passed to two parsing programs results in two semantically different behaviors. For example, two widely-used HTTP parsers have been shown to process packet headers differently, allowing for the exfiltration of private files. To help find, diagnose, and mitigate the risks of parser differentials, we present the Format Analysis Workbench (FAW), a collection of tools for collecting information on large numbers of parser/input interactions and analyzing those interactions to detect and explain differentials. This tool suite supports any number of file formats through a flexible configuration, allows for processing to be scaled horizontally, and can be run offline. It has been used for results including the analysis of more than 1 million PDF files and unifying parser behaviors across these files to identify a gold standard of validity across multiple parsers. The included statistical tools have been used to identify the root causes of parser rendering differentials, including mislabeled non-embedded fonts. Tools for instrumenting existing parsers are also included, such as PolyTracker, allowing for the analysis of blind spots which might be used to craft differentials for other parsers, or to exfiltrate large quantities of data. Through allowing users to characterize parser behaviors at scale against large corpuses of inputs, the FAW helps to mitigate security risks arising from parser behaviors by making it tractable to resolve examples of differentials back to their behavioral causes.},
  keywords={Privacy;Instruments;Conferences;Rendering (computer graphics);Behavioral sciences;Grammar;Security;parser-differential;parsing;instrumentation;pdf},
  doi={10.1109/SPW59333.2023.00024},
  ISSN={2770-8411},
  month={May},}

@BOOK{10162934,
  author={Ostrowski, Adrian and Gaczkowski, Piotr},
  booktitle={Software Architecture with C++: Design modern systems using effective architecture concepts, design patterns, and techniques with C++20},
  year={2021},
  volume={},
  number={},
  pages={},
  abstract={Apply business requirements to IT infrastructure and deliver a high-quality product by understanding architectures such as microservices, DevOps, and cloud-native using modern C++ standards and featuresKey FeaturesDesign scalable large-scale applications with the C++ programming languageArchitect software solutions in a cloud-based environment with continuous integration and continuous delivery (CI/CD)Achieve architectural goals by leveraging design patterns, language features, and useful toolsBook DescriptionSoftware architecture refers to the high-level design of complex applications. It is evolving just like the languages we use, but there are architectural concepts and patterns that you can learn to write high-performance apps in a high-level language without sacrificing readability and maintainability. If you're working with modern C++, this practical guide will help you put your knowledge to work and design distributed, large-scale apps. You'll start by getting up to speed with architectural concepts, including established patterns and rising trends, then move on to understanding what software architecture actually is and start exploring its components. Next, you'll discover the design concepts involved in application architecture and the patterns in software development, before going on to learn how to build, package, integrate, and deploy your components. In the concluding chapters, you'll explore different architectural qualities, such as maintainability, reusability, testability, performance, scalability, and security. Finally, you will get an overview of distributed systems, such as service-oriented architecture, microservices, and cloud-native, and understand how to apply them in application development. By the end of this book, you'll be able to build distributed services using modern C++ and associated tools to deliver solutions as per your clients' requirements.What you will learnUnderstand how to apply the principles of software architectureApply design patterns and best practices to meet your architectural goalsWrite elegant, safe, and performant code using the latest C++ featuresBuild applications that are easy to maintain and deployExplore the different architectural approaches and learn to apply them as per your requirementSimplify development and operations using application containersDiscover various techniques to solve common problems in software design and developmentWho this book is forThis software architecture C++ programming book is for experienced C++ developers looking to become software architects or develop enterprise-grade applications. },
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781789612462},
  url={https://ieeexplore.ieee.org/document/10162934},}

@INBOOK{9663077,
  author={Martins, Luiz Eduardo G. and Gorschek, Tony},
  booktitle={Requirements Engineering for Safety-Critical Systems}, 
  title={4 Agile Requirements Engineering}, 
  year={2021},
  volume={},
  number={},
  pages={37-54},
  abstract={Safety-Critical Systems (SCS) are increasingly present in people’s daily activities. In the means of transport, in medical treatments, in industrial processes, in the control of air, land, maritime traffic, and many other situations, we use and depend on SCS. The requirements engineering of any system is crucial for the proper development of the same, and it becomes even more relevant for the development of SCS. Requirements Engineering is a discipline that focuses on the development of techniques, methods, processes, and tools that assist in the design of software and systems, covering the activities of elicitation, analysis, modeling and specification, validation, and management of requirements. The complete specification of system requirements establishes the basis for its architectural design. It offers a description of the functional and quality aspects that should guide the implementation and system evolution. In this book, we discuss essential elements of requirements engineering applied to SCS, such as the relationship between safety/hazard analysis and requirements specification, a balance between conservative and agile methodologies during SCS development, the role of requirements engineering in safety cases, and requirements engineering maturity model for SCS. This book provides relevant insights for professionals, students, and researchers interested in improving the quality of the SCS development process, making system requirements a solid foundation for improving the safety and security of future systems.},
  keywords={},
  doi={},
  ISSN={},
  publisher={River Publishers},
  isbn={9788770224260},
  url={https://ieeexplore.ieee.org/document/9663077},}

@INBOOK{5732852,
  author={Etemad, Kamran and Lai, Ming-Yee},
  booktitle={WiMAX Technology and Network Evolution}, 
  title={Overview of WiMAX Network Architecture and Evolution}, 
  year={2010},
  volume={},
  number={},
  pages={147-177},
  abstract={<P>This chapter contains sections titled: <UL> <LI> <P>Introduction</P> </LI> <LI> <P>WiMAX Basic Network Reference Model</P> </LI> <LI> <P>WiMAX Network Roadmap: Release 1.0, 1.5, 1.6, and 2.0</P> </LI> <LI> <P>Overview of Major Features in Release 1.0</P> </LI> <LI> <P>Overview of Major Features in Release 1.5</P> </LI> <LI> <P>Major Features in Network Release 1.6</P> </LI> <LI> <P>Comparison of Mobile WiMAX and 3GPP/SAE Network Architecture</P> </LI> <LI> <P>Summary</P> </LI> </UL> </P>},
  keywords={WiMAX;network architecture;service provider's working group;network working group;residential gateways},
  doi={10.1002/9780470633021.ch6},
  ISSN={},
  publisher={IEEE},
  isbn={9780470633014},
  url={https://ieeexplore.ieee.org/document/5732852},}

@ARTICLE{11095787,
  author={Zhang, Yunming and Ye, Dengpan and Shen, Sipeng and Wang, Jun and Xie, Caiyun},
  journal={IEEE Transactions on Information Forensics and Security}, 
  title={StyleMark: Robust Style Watermarking for Artworks Against Black-Box Zero-Shot Style Transfer}, 
  year={2025},
  volume={20},
  number={},
  pages={7867-7880},
  abstract={Zero-shot style transfer (ZSST) enables the rendering of real-world natural images into the painting styles of arbitrary artworks without requiring fine-tuning on unseen artistic styles. This low-cost and efficient approach to artistic re-creation promotes the dissemination and communication of art. However, misuse of unauthorized artistic style images for ZSST may infringe on the copyrights of artists. One countermeasure is robust watermarking, which tracks image propagation by embedding copyright watermarks into carriers. Unfortunately, the stylized image generated by ZSST lose the structural and semantic information of the original style image, hindering end-to-end robust tracking by watermarks. To fill this gap, we propose StyleMark, the first robust watermarking method for black-box ZSST, which can be seamlessly applied to artistic style images achieving precise attribution of artistic styles after ZSST, without compromising the social usability of artworks. Specifically, we propose a new style watermark network that adjusts the mean activations of style features through multi-scale watermark embedding, thereby planting watermark traces into the shared style feature space of style images. Furthermore, we design a distribution squeeze loss, which constrain content statistical feature distortion, forcing the reconstruction network to focus on integrating style features with watermarks, thus optimizing the intrinsic watermark distribution. Finally, based on solid end-to-end training, StyleMark mitigates the optimization conflict between robustness and watermark invisibility through decoder fine-tuning under random noise. Experimental results demonstrate that StyleMark exhibits significant robustness against black-box ZSST and common pixel-level distortions, maintains high watermark decoding accuracy under complex multi-stage processing scenarios, and securely defending against malicious adaptive attacks.},
  keywords={Watermarking;Decoding;Robustness;Distortion;Copyright protection;Closed box;Training;Noise;Rendering (computer graphics);Semantics;Deep watermark;style transfer;image processing;copyright protection},
  doi={10.1109/TIFS.2025.3592521},
  ISSN={1556-6021},
  month={},}

@BOOK{10443736,
  author={Holmes, Joel},
  booktitle={Shipping Go: Develop, deliver, discuss, design, and go again},
  year={2023},
  volume={},
  number={},
  pages={},
  abstract={Build and upgrade an automated software delivery pipeline that supports containerization, integration testing, semantic versioning, automated deployment, and more. In Shipping Go you will learn how to:  Develop better software based on feedback from customers Create a development pipeline that turns feedback into features Reduce bugs with pipeline automation that validates code before it is deployed Establish continuous testing for exceptional code quality Serverless, container-based, and server-based deployments Scale your deployment in a cost-effective way Deliver a culture of continuous improvement  Shipping Go is a hands-on guide to shipping Go-based software. Author Joel Holmes shows you the easy way to set up development pipelines, fully illustrated with practical examples in the powerful Go language. You’ll put continuous delivery and continuous integration into action, and discover instantly useful guidance on automating your team’s build and reacting with agility to customer demands. Your new pipelines will ferry your projects through production and deployment, and also improve your testing, code quality, and production applications.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Manning},
  isbn={9781617299506},
  url={https://ieeexplore.ieee.org/document/10443736},}

@INBOOK{8686312,
  author={Janevski, Toni},
  booktitle={QoS for Fixed and Mobile Ultra-Broadband}, 
  title={Broadband QoS Parameters, KPIs, and Measurements}, 
  year={2019},
  volume={},
  number={},
  pages={221-259},
  abstract={Broadband and ultra‐broadband access networks provide capabilities for delivery of various services toward end‐users, which can have different quality of service (QoS) and quality of experience (QoE) requirements. The relationship between network layer QoS and the obtained QoE is strongly dependent on the given service and its application. The QoS parameters are defined to be used by service and network providers, to manage and improve the offering of their services. The main purpose of QoS parameters in an operational network is to be measured and then compared with reference values. There are two types of QoS parameters regarding measurement methods: objective QoS parameters and subjective QoS parameters. This chapter defines several important QoS parameters for time‐division multiplexing‐based points of interconnection, and their possible thresholds values. Such key performance indicators should be measurable and realistic to ensure effective interconnection regulation and mitigation of eventual disputes.},
  keywords={Quality of service;Streaming media;Delays;Internet of Things;Broadband communication;Real-time systems},
  doi={10.1002/9781119470519.ch7},
  ISSN={},
  publisher={Wiley},
  isbn={9781119470496},
  url={https://ieeexplore.ieee.org/document/8686312},}

@BOOK{10162936,
  author={Gilbert, John and Price, Ed},
  booktitle={Software Architecture Patterns for Serverless Systems: Architecting for innovation with events, autonomous services, and micro frontends},
  year={2021},
  volume={},
  number={},
  pages={},
  abstract={A professional's guide to solving complex problems while designing modern softwareKey FeaturesLearn best practices for designing enterprise-grade software systems from a seasoned CTODeeper your understanding of system reliability, maintainability, and scalabilityElevate your skills to a professional level by learning the most effective software design patterns and architectural conceptsBook DescriptionAs businesses are undergoing a digital transformation to keep up with competition, it is now more important than ever for IT professionals to design systems to keep up with the rate of change while maintaining stability. This book takes you through the architectural patterns that power enterprise-grade software systems and the key architectural elements that enable change (such as events, autonomous services, and micro frontends), along with showing you how to implement and operate anti-fragile systems. First, you’ll divide up a system and define boundaries so that your teams can work autonomously and accelerate innovation. You’ll cover low-level event and data patterns that support the entire architecture, while getting up and running with the different autonomous service design patterns. Next, the book will focus on best practices for security, reliability, testability, observability, and performance. You’ll combine all that you've learned and build upon that foundation, exploring the methodologies of continuous experimentation, deployment, and delivery before delving into some final thoughts on how to start making progress. By the end of this book, you'll be able to architect your own event-driven, serverless systems that are ready to adapt and change so that you can deliver value at the pace needed by your business.What you will learnExplore architectural patterns to create anti-fragile systems that thrive with changeFocus on DevOps practices that empower self-sufficient, full-stack teamsBuild enterprise-scale serverless systemsApply microservices principles to the frontendDiscover how SOLID principles apply to software and database architectureCreate event stream processors that power the event sourcing and CQRS patternDeploy a multi-regional system, including regional health checks, latency-based routing, and replicationExplore the Strangler pattern for migrating legacy systemsWho this book is forThis book is for software architects who want to learn more about different software design patterns and best practices. This isn’t a beginner’s manual – you’ll need an intermediate level of programming proficiency and software design to get started. You’ll get the most out of this software design book if you already know the basics of the cloud, but it isn’t a prerequisite.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781800200739},
  url={https://ieeexplore.ieee.org/document/10162936},}

@INBOOK{5238155,
  author={Sherif, Mostafa Hashem},
  booktitle={Managing Projects in Telecommunication Services}, 
  title={Standards and Innovation in Telecommunication Services}, 
  year={2006},
  volume={},
  number={},
  pages={19-41},
  abstract={<P>This chapter contains sections titled: <UL> <LI> <P>The Two Dimensions of Telecommunication Projects</P> </LI> <LI> <P>Innovation in Telecommunication Services</P> </LI> <LI> <P>Phasic Relation Between Equipment and Services</P> </LI> <LI> <P>Standardization for Telecommunication Services</P> </LI> <LI> <P>Summary</P> </LI> </UL> </P>},
  keywords={},
  doi={10.1002/9780470047682.ch2},
  ISSN={},
  publisher={IEEE},
  isbn={9780470047675},
  url={https://ieeexplore.ieee.org/document/5238155},}

@BOOK{10251333,
  author={Gershkovich, Serge and Graziano, Kent},
  booktitle={Data Modeling with Snowflake: A practical guide to accelerating Snowflake development using universal data modeling techniques},
  year={2023},
  volume={},
  number={},
  pages={},
  abstract={Discover how Snowflake’s unique objects and features can be used to leverage universal modeling techniques through real-world examples and SQL recipes Purchase of the print or Kindle book includes a free PDF eBookKey FeaturesLearn core modeling techniques tied to practical examples using native Snowflake architectureAdopt a universal modeling language to communicate business value to functional teamsGo beyond physical modeling with SQL recipes to transform and shape your Snowflake dataBook DescriptionThe Snowflake Data Cloud is one of the fastest-growing platforms for data warehousing and application workloads. Snowflake's scalable, cloud-native architecture and expansive set of features and objects enables you to deliver data solutions quicker than ever before. Yet, we must ensure that these solutions are developed using recommended design patterns and accompanied by documentation that’s easily accessible to everyone in the organization. This book will help you get familiar with simple and practical data modeling frameworks that accelerate agile design and evolve with the project from concept to code. These universal principles have helped guide database design for decades, and this book pairs them with unique Snowflake-native objects and examples like never before – giving you a two-for-one crash course in theory as well as direct application. By the end of this Snowflake book, you’ll have learned how to leverage Snowflake’s innovative features, such as time travel, zero-copy cloning, and change-data-capture, to create cost-effective, efficient designs through time-tested modeling principles that are easily digestible when coupled with real-world examples.What you will learnDiscover the time-saving benefits and applications of data modelingLearn about Snowflake’s cloud-native architecture and its featuresUnderstand and apply modeling techniques using Snowflake objectsUniversal modeling concepts and language through Snowflake objectsGet comfortable reading and transforming semistructured dataLearn directly with pre-built recipes and examplesLearn to apply modeling frameworks from Star to Data VaultWho this book is forThis book is for developers working with SQL who are looking to build a strong foundation in modeling best practices and gain an understanding of where they can be effectively applied to save time and effort. Whether you’re an ace in SQL logic or starting out in database design, this book will equip you with the practical foundations of data modeling to guide you on your data journey with Snowflake. Developers who’ve recently discovered Snowflake will be able to uncover its core features and learn to incorporate them into universal modeling frameworks.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781837632787},
  url={https://ieeexplore.ieee.org/document/10251333},}

@BOOK{10162828,
  author={Wen, Robert and Koehnemann, Harry},
  booktitle={SAFe® for DevOps Practitioners: Implement robust, secure, and scaled Agile solutions with the Continuous Delivery Pipeline},
  year={2022},
  volume={},
  number={},
  pages={},
  abstract={Discover how the DevOps approach with Scaled Agile Framework helps you develop and deliver high-quality, secured solutions with a reduced risk of production failures with this step-by-step guideKey FeaturesExplore the five elements of the CALMR approach to avoid product development challengesUse value stream management to introduce systems thinking and flow for product developmentDemonstrate how the CD pipeline combines practices and technology to optimize your value streamPurchase of the print or Kindle book includes a free eBook in the PDF formatBook DescriptionProduct development and release faces overlapping challenges due to the combined pressure of delivering high-quality products in shorter time-to-market cycles, along with maintaining proper operation and ensuring security in a complex high-tech environment. This calls for new ways of overcoming these challenges from design to development, to release, and beyond. SAFe® for DevOps Practitioners helps you use a DevOps approach with the Scaled Agile Framework and details how value streams help you resolve these challenges using examples and use cases. The book begins by explaining how the CALMR approach makes DevOps effective in resolving product development roadblocks. Next, you’ll learn to apply value stream management to establish a value stream that enables product development flow, measure its effectiveness through appropriate feedback loops, and find ways of improving it. Finally, you’ll get to grips with implementing a continuous delivery pipeline that optimizes the value stream through four phases during release on demand. This book complements the latest SAFe DevOps courses, and you’ll find it useful while studying for the SAFe DevOps Practitioner (SDP) certification. By the end of this DevOps book, you’ll have gained a clear understanding of how to achieve continuous execution and release on demand using DevOps and SAFe.What you will learnUnderstand the important elements of the CALMR approachDiscover how to organize around value using value stream mappingMeasure your value stream using value stream metricsImprove your value stream with continuous learningUse continuous exploration to design high-quality and secure featuresPrevent rework and build in quality using continuous integrationAutomate delivery with continuous deploymentMeasure successful outcomes with Release on DemandWho this book is forThis book is for IT professionals such as DevOps and DevSecOps practitioners, SREs, and managers who are interested in implementing DevOps practices using the Scaled Agile Framework (SAFe) approach. Basic knowledge of DevOps and agile software development lifecycle and methodology will be helpful.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781803237435},
  url={https://ieeexplore.ieee.org/document/10162828},}

@BOOK{10163385,
  author={Chatterjee, Subhajit and Deshpande, Swapneel and Been, Henry and Gaag, Maik van der},
  booktitle={Designing and Implementing Microsoft DevOps Solutions AZ-400 Exam Guide: Prepare for the certification exam and successfully apply Azure DevOps strategies with practical labs},
  year={2022},
  volume={},
  number={},
  pages={},
  abstract={Written by Microsoft MVPs and Azure experts, this comprehensive guide comes with self-study exercises to help you understand the concepts better and move closer to becoming a skilled Azure DevOps engineerKey FeaturesExplore a step-by-step approach to designing and creating a successful DevOps environmentUnderstand how to implement continuous integration and continuous deployment pipelines on AzureIntegrate and implement security, compliance, containers, and databases in your DevOps strategiesBook DescriptionThe AZ-400 Designing and Implementing Microsoft DevOps Solutions certification helps DevOps engineers and administrators get to grips with practices such as continuous integration and continuous delivery (CI/CD), containerization, and zero downtime deployments using Azure DevOps Services. This new edition is updated with advanced topics such as site reliability engineering (SRE), continuous improvement, and planning your cloud transformation journey. The book begins with the basics of CI/CD and automated deployments, and then moves ahead to show you how to apply configuration management and Infrastructure as Code (IaC) along with managing databases in DevOps scenarios. As you make progress, you’ll explore fitting security and compliance with DevOps and find out how to instrument applications and gather metrics to understand application usage and user behavior. This book will also help you implement a container build strategy and manage Azure Kubernetes Services. Lastly, you’ll discover quick tips and tricks to confidently apply effective DevOps practices and learn to create your own Azure DevOps organization. By the end of this DevOps book, you'll have gained the knowledge needed to ensure seamless application deployments and business continuity.What you will learnGet acquainted with Azure DevOps Services and DevOps practicesDiscover how to efficiently implement CI/CD processesBuild and deploy a CI/CD pipeline with automated testing on AzureIntegrate security and compliance in pipelinesUnderstand and implement Azure Container ServicesEffectively close the loop from production back to developmentApply continuous improvement strategies to deliver innovation at scaleWho this book is forThe book is for anyone looking to prepare for the AZ-400 certification exam. Software developers, application developers, and IT professionals who want to implement DevOps practices for the Azure cloud will also find this book helpful. Familiarity with Azure DevOps basics, software development, and development practices is recommended but not necessary.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781803230283},
  url={https://ieeexplore.ieee.org/document/10163385},}

@INBOOK{8040815,
  author={Pang, Sauming},
  booktitle={Successful Service Design for Telecommunications: A comprehensive guide to design and implementation}, 
  title={Glossary}, 
  year={2009},
  volume={},
  number={},
  pages={329-333},
  abstract={},
  keywords={},
  doi={10.1002/9780470741207.gloss},
  ISSN={},
  publisher={Wiley},
  isbn={9780470740828},
  url={https://ieeexplore.ieee.org/document/8040815},}

@BOOK{10803968,
  author={Cronin, Irena},
  booktitle={Decoding Large Language Models: An exhaustive guide to understanding, implementing, and optimizing LLMs for NLP applications},
  year={2024},
  volume={},
  number={},
  pages={},
  abstract={Explore the architecture, development, and deployment strategies of large language models to unlock their full potentialKey FeaturesGain in-depth insight into LLMs, from architecture through to deploymentLearn through practical insights into real-world case studies and optimization techniquesGet a detailed overview of the AI landscape to tackle a wide variety of AI and NLP challengesPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionEver wondered how large language models (LLMs) work and how they're shaping the future of artificial intelligence? Written by a renowned author and AI, AR, and data expert, Decoding Large Language Models is a combination of deep technical insights and practical use cases that not only demystifies complex AI concepts, but also guides you through the implementation and optimization of LLMs for real-world applications. You’ll learn about the structure of LLMs, how they're developed, and how to utilize them in various ways. The chapters will help you explore strategies for improving these models and testing them to ensure effective deployment. Packed with real-life examples, this book covers ethical considerations, offering a balanced perspective on their societal impact. You’ll be able to leverage and fine-tune LLMs for optimal performance with the help of detailed explanations. You’ll also master techniques for training, deploying, and scaling models to be able to overcome complex data challenges with confidence and precision. This book will prepare you for future challenges in the ever-evolving fields of AI and NLP. By the end of this book, you’ll have gained a solid understanding of the architecture, development, applications, and ethical use of LLMs and be up to date with emerging trends, such as GPT-5.What you will learnExplore the architecture and components of contemporary LLMsExamine how LLMs reach decisions and navigate their decision-making processImplement and oversee LLMs effectively within your organizationMaster dataset preparation and the training process for LLMsHone your skills in fine-tuning LLMs for targeted NLP tasksFormulate strategies for the thorough testing and evaluation of LLMsDiscover the challenges associated with deploying LLMs in production environmentsDevelop effective strategies for integrating LLMs into existing systemsWho this book is forIf you’re a technical leader working in NLP, an AI researcher, or a software developer interested in building AI-powered applications, this book is for you. To get the most out of this book, you should have a foundational understanding of machine learning principles; proficiency in a programming language such as Python; knowledge of algebra and statistics; and familiarity with natural language processing basics.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781835081808},
  url={https://ieeexplore.ieee.org/document/10803968},}

@ARTICLE{10848093,
  author={Proença, Jorge and Figueiredo, Ricardo and Pereira, Vasco and Cruz, Tiago and Simões, Paulo},
  journal={IEEE Access}, 
  title={Contributions to the Virtualization of the BNG}, 
  year={2025},
  volume={13},
  number={},
  pages={18713-18733},
  abstract={The Border Network Gateway (BNG) assumes a pivotal role within the telecommunications network operator domain, particularly in broadband access services, by establishing and managing subscribers’ connections. It serves as a crucial component, being responsible for various essential services that allow subscribers’ connections and help optimizing infrastructure resources, including enforcing QoS policies and implementing Access Control Lists to safeguard against unauthorized access. Similarly to other elements within the operators’ infrastructure, the BNG has faced escalating challenges due to the surge in bandwidth demand, rapid subscriber growth, and the introduction of novel services with unique requirements. As a single point of failure for subscriber-operator access, it becomes crucial to enhance management flexibility, simplify the device, and improve overall resilience. Consequently, there has been a growing interest among operators in virtualizing the BNG, following a trajectory which is similar to the virtualization of other telecommunications networks devices, such as the residential gateway. This paper examines the case for virtualizing the BNG, starting with an analysis of the underlying motivations driving this virtualization effort. Next, it provides an overview of the State-of-Art, highlighting the current proposals and advances related with the Virtual Border Network Gateway (vBNG) concept. Furthermore, it proposes a reference vBNG architecture, also exploring the seamless integration with a virtual Customers’ Premise Equipment infrastructure. Finally, a practical Proof-of-Concept is presented and used to assess the feasibility of the vBNG concept.},
  keywords={Logic gates;Protocols;Hardware;Software;Resilience;Computer architecture;Complexity theory;Quality of service;Central office;Broadband communication;Border network gateway virtualization;broadband networks;network function virtualization;software defined networks;telecommunications network operator},
  doi={10.1109/ACCESS.2025.3532548},
  ISSN={2169-3536},
  month={},}

@BOOK{10443738,
  author={Roestenburg, Raymond and Williams, Rob and Lopez-Sancho, Francisco and Bakker, Robertus},
  booktitle={Akka in Action, Second Edition},
  year={2023},
  volume={},
  number={},
  pages={},
  abstract={Akka solves the big problems of distributed systems, from multithreading and concurrency to scalability and failure. Learn how to use it effectively. In Akka in Action, Second Edition you will learn how to:  Create basic programs with Akka Typed Work with clusters to build robust, fault-tolerant programs Use Akka with Kubernetes Build microservices with Akka Create and maintain distributed state with strong consistency guarantees Employ actor-based concurrency and parallelism Test Akka software   Akka in Action, Second Edition teaches you to use Akka Typed to solve common problems of distributed systems. You’ll learn how to bring together all of Akka’s moving parts to design and implement highly scalable and maintainable software. Extensively revised by Akka contributor Francisco López-Sancho Abraham, this new edition demonstrates Akka’s complex concepts through engaging hands-on examples. Discover the power of the Actor Model, how Akka works with Kubernetes, and how to utilize Akka modules to create microservices that are reliable and fault tolerant.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Manning},
  isbn={9781617299216},
  url={https://ieeexplore.ieee.org/document/10443738},}

@INPROCEEDINGS{11063525,
  author={Dang, Duc-Hanh and Be, Trong-Nghia and Dinh, Minh-Hai and Le, Hoang-Lan},
  booktitle={2024 16th International Conference on Knowledge and System Engineering (KSE)}, 
  title={On Automatic Test Case Generation From Use Case Specification}, 
  year={2024},
  volume={},
  number={},
  pages={01-06},
  abstract={Automatically generating test cases from use case specifications ensures consistency, enhances software quality, and provides comprehensive test coverage. However, automating this task is challenging due to the ambiguity of natural language use case specifications. This paper proposes a method to generate both test scenarios and test data automatically from use case specifications written in FRSL. The method involves two main steps: transforming the use case specification into a class model using an Acceleo transformation, and converting the class model into a filmstrip model. An OCL model finder is then used to identify valid snapshots, yielding the corresponding test scenarios. We have implemented a support tool using FRSL and the OCL tool USE and illustrated the method with a running example. Our contributions include (1) a method to automatically generate test cases from FRSL use case specifications, (2) a technique to transform these specifications into a filmstrip model, and (3) a support tool for the proposed method. Our method has been successfully applied to several case studies, demonstrating its potential applicability in practice.},
  keywords={Knowledge engineering;Unified modeling language;Natural languages;Transforms;Software quality;Systems engineering and theory;Use Case Specification;UML/OCL;Test case Generation;Model Transformation;},
  doi={10.1109/KSE63888.2024.11063525},
  ISSN={2694-4804},
  month={Nov},}

@ARTICLE{10982240,
  author={Baker, Nathan and Campbell, Ethan and Wilson, Andrew and Wirthlin, Michael},
  journal={IEEE Transactions on Nuclear Science}, 
  title={Post-Irradiation Fault Injection for Complex FPGA Designs}, 
  year={2025},
  volume={},
  number={},
  pages={1-1},
  abstract={Fault injection is a widely used method of assessing the reliability of FPGA designs operating in harsh radiation environments. While typically conducted before radiation testing, fault injection can also be performed after radiation testing to gain additional insight into an FPGA design’s behavior during a radiation test. In this work, fault injection was performed on a fault-mitigated FPGA soft processor design based on configuration memory (CRAM) upsets observed during a neutron radiation test. During the neutron radiation test, the locations and timestamps of configuration memory (CRAM) upsets were logged along with the timestamps of design failures. A variety of statistical post-irradiation fault injection experiments “replayed” these recorded CRAM upsets to reproduce the design’s behavior in the beam and better understand vulnerabilities in the mitigated design. Using this approach, the majority of the failures seen in the radiation beam (92%) were successfully replicated through fault injection. This method identified the specific CRAM upsets responsible for design failures, estimated the probability that such CRAM upsets cause design failures, and uncovered failures caused by the latent effects of previously scrubbed CRAM upsets. Additionally, this approach identified differences in design failure when the same CRAM upset playback is applied to different FPGA devices and boards. These findings demonstrate that post-irradiation fault injection can provide new insights into the failure mechanisms of complex FPGA designs beyond what is possible with radiation testing alone.},
  keywords={Field programmable gate arrays;Testing;Circuit faults;Fault diagnosis;Reliability engineering;Reliability;Binary sequences;Radiation effects;Single event upsets;Object recognition;FPGA;fault injection;radiation testing;soft processor;RISC-V;fault tolerance;Single Event Upset (SEU)},
  doi={10.1109/TNS.2025.3566344},
  ISSN={1558-1578},
  month={},}

@BOOK{10163281,
  author={Ramakani, Arun},
  booktitle={End-to-End Automation with Kubernetes and Crossplane: Develop a control plane-based platform for unified infrastructure, services, and application automation},
  year={2022},
  volume={},
  number={},
  pages={},
  abstract={A complete journey to automating infrastructure provisioning and cloud-native application deploymentKey FeaturesLeverage Crossplane and Kubernetes for a unified automation experience of infrastructure and appsBuild a modern self-service infrastructure platform abstracting recipes and in-house policiesClear guidance on trade-offs to manage Kubernetes configuration and ecosystem toolsBook DescriptionIn the last few years, countless organizations have taken advantage of the disruptive application deployment operating model provided by Kubernetes. With Crossplane, the same benefits are coming to the world of infrastructure provisioning and management. The limitations of Infrastructure as Code with respect to drift management, role-based access control, team collaboration, and weak contract make people move towards a control-plane-based infrastructure automation, but setting it up requires a lot of know-how and effort. This book will cover a detailed journey to building a control-plane-based infrastructure automation platform with Kubernetes and Crossplane. The cloud-native landscape has an overwhelming list of configuration management tools that can make it difficult to analyze and choose. This book will guide cloud-native practitioners to select the right tools for Kubernetes configuration management that best suit the use case. You'll learn about configuration management with hands-on modules built on popular configuration management tools such as Helm, Kustomize, Argo, and KubeVela. The hands-on examples will be patterns that one can directly use in their work. By the end of this book, you'll be well-versed with building a modern infrastructure automation platform to unify application and infrastructure automation.What you will learnUnderstand the context of Kubernetes-based infrastructure automationGet to grips with Crossplane concepts with the help of practical examplesExtend Crossplane to build a modern infrastructure automation platformUse the right configuration management tools in the Kubernetes environmentExplore patterns to unify application and infrastructure automationDiscover top engineering practices for infrastructure platform as a productWho this book is forThis book is for cloud architects, platform engineers, infrastructure or application operators, and Kubernetes enthusiasts who want to simplify infrastructure and application automation. A basic understanding of Kubernetes and its building blocks like Pod, Deployment, Service, and Namespace is needed before you can get started with this book.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781801818254},
  url={https://ieeexplore.ieee.org/document/10163281},}

@INPROCEEDINGS{10689900,
  author={Singh, Ashish Kumar and Sharma, Disha and Dhawan, Manan and Kumar, Shivansh and Chattaraj, Amrita and Gour, Chhavin},
  booktitle={2024 IEEE Recent Advances in Intelligent Computational Systems (RAICS)}, 
  title={Amazon Kindle Review Sentiment Analysis}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={In the digital age, the exponential growth of user-generated content, particularly in online platforms like the Kindle Store, has prompted the need for efficient sentiment analysis methods. This study explores sentiment analysis within Kindle Store book reviews using machine learning techniques to extract insights, aiding stakeholders in decision-making processes. Focusing on sentiment classification-positive, negative, or neutral-the research assesses the efficacy of Logistic Regression and Random Forest models in analyzing sentiments. Key preprocessing steps address data cleaning, noise reduction, and class imbalance, ensuring readiness for model training. Stakeholders, including authors, publishers, and readers, seek insights from sentiment analysis for marketing strategies, book reception evaluation, and informed book selection. The contemporary issue lies in automating sentiment analysis due to the vast volume of unstructured textual data, diverse sentiments, and the need for precise analysis. Challenges encompass diverse sentiments, varying review quality, absence of standardized structures, and biases in the data. The tasks involve data collection, annotation.feature extraction, model selection, evaluation, optimization, deployment, and iterative improvement. Successful completion of these tasks leads to a robust sentiment analysis framework for Kindle Store reviews, contributing to the discourse on sentiment analysis methodologies and their broader applications beyond e-commerce.},
  keywords={Training;Sentiment analysis;Electronic publishing;Reviews;User-generated content;Data models;Stakeholders;Logistic Regression;Data Analysis;Data Science;Machine Learning;Random Forest Algorithm;Cross Validation},
  doi={10.1109/RAICS61201.2024.10689900},
  ISSN={2769-5565},
  month={May},}

@INPROCEEDINGS{7980399,
  author={Contan, Andrei and Miclea, Liviu and Dehelean, Catalin},
  booktitle={2017 14th International Conference on Engineering of Modern Electric Systems (EMES)}, 
  title={Automated testing framework development based on social interaction and communication principles}, 
  year={2017},
  volume={},
  number={},
  pages={136-139},
  abstract={The speed of development of the IT industry as well as the computational power which are increasing exponentially, create great competitiveness in the process of development but also in the launching of software products on the market. Automated testing comes to help with these challenges by trying to increase the speed of development by offering fast feedback and trustworthy quality by means of repeated runs of the implemented tests. This isn't a problem just on a technical level, but also on a social level, especially in the area of communication and understanding the requirements of the client. This work presents the implementation of an automated testing framework which also addresses the social problems. BDD or “Behavior Driven Development” includes an approach which would like to line up the area of client requests to the technical area, offering a uniform platform of collaboration and development. The implementation of this principle is applied in an MVP (Minimum Viable Product) type project which is meant to demonstrate the technical solution which may draw together, both socially and communication wise, the business teams and the technical implementation teams.},
  keywords={Testing;Software;Business;Documentation;Collaboration;Automation;Libraries;testing process;BDD;automated testing;Gherkin language},
  doi={10.1109/EMES.2017.7980399},
  ISSN={},
  month={June},}

@BOOK{10162258,
  author={Bonocore, Giuseppe},
  booktitle={Hands-On Software Architecture with Java: Learn key architectural techniques and strategies to design efficient and elegant Java applications},
  year={2022},
  volume={},
  number={},
  pages={},
  abstract={Build robust and scalable Java applications by learning how to implement every aspect of software architectureKey FeaturesUnderstand the fundamentals of software architecture and build production-grade applications in JavaMake smart architectural decisions with comprehensive coverage of various architectural approaches from SOA to microservicesGain an in-depth understanding of deployment considerations with cloud and CI/CD pipelinesBook DescriptionWell-written software architecture is the core of an efficient and scalable enterprise application. Java, the most widespread technology in current enterprises, provides complete toolkits to support the implementation of a well-designed architecture. This book starts with the fundamentals of architecture and takes you through the basic components of application architecture. You'll cover the different types of software architectural patterns and application integration patterns and learn about their most widespread implementation in Java. You'll then explore cloud-native architectures and best practices for enhancing existing applications to better suit a cloud-enabled world. Later, the book highlights some cross-cutting concerns and the importance of monitoring and tracing for planning the evolution of the software, foreseeing predictable maintenance, and troubleshooting. The book concludes with an analysis of the current status of software architectures in Java programming and offers insights into transforming your architecture to reduce technical debt. By the end of this software architecture book, you'll have acquired some of the most valuable and in-demand software architect skills to progress in your career.What you will learnUnderstand the importance of requirements engineering, including functional versus non-functional requirementsExplore design techniques such as domain-driven design, test-driven development (TDD), and behavior-driven developmentDiscover the mantras of selecting the right architectural patterns for modern applicationsExplore different integration patternsEnhance existing applications with essential cloud-native patterns and recommended practicesAddress cross-cutting considerations in enterprise applications regardless of architectural choices and application typeWho this book is forThis book is for Java software engineers who want to become software architects and learn everything a modern software architect needs to know. The book is also for software architects, technical leaders, vice presidents of software engineering, and CTOs looking to extend their knowledge and stay up to date with the latest developments in the field of software architecture.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781800206144},
  url={https://ieeexplore.ieee.org/document/10162258},}

@INPROCEEDINGS{10988923,
  author={Arcuri, Andrea and Poth, Alexander and Rrjolli, Olsi},
  booktitle={2025 IEEE Conference on Software Testing, Verification and Validation (ICST)}, 
  title={Introducing Black-Box Fuzz Testing for REST APIs in Industry: Challenges and Solutions}, 
  year={2025},
  volume={},
  number={},
  pages={382-393},
  abstract={REST APIs are widely used in industry, in all different kinds of domains. An example is Volkswagen AG, a German automobile manufacturer. Established testing approaches for REST APIs are time consuming, and require expertise from professional test engineers. Due to its cost and importance, in the scientific literature several approaches have been proposed to automatically test REST APIs. The open-source, search-based fuzzer EVOMASTER is one of such tools proposed in the academic literature. However, how academic prototypes can be integrated in industry and have real impact to software engineering practice requires more investigation. In this paper, we report on our experience in using EVOMASTER at Volkswagen. We share our learnt lessons, and identify real-world research challenges that need to be solved.},
  keywords={Industries;Costs;Closed box;Technology transfer;Prototypes;Fuzzing;Automobiles;Faces;Software engineering;Software development management;SBST;REST;API;black-box;industry;fuzzing},
  doi={10.1109/ICST62969.2025.10988923},
  ISSN={2159-4848},
  month={March},}

@BOOK{9453331,
  author={McRuer, Duane T. and Graham, Dunstan and Ashkenas, Irving},
  booktitle={Aircraft Dynamics and Automatic Control},
  year={1974},
  volume={},
  number={},
  pages={},
  abstract={Aeronautical engineers concerned with the analysis of aircraft dynamics and the synthesis of aircraft flight control systems will find an indispensable tool in this analytical treatment of the subject. Approaching these two fields with the conviction that an understanding of either one can illuminate the other, the authors have summarized selected, interconnected techniques that facilitate a high level of insight into the essence of complex systems problems. These techniques are suitable for establishing nominal system designs, for forecasting off-nominal problems, and for diagnosing the root causes of problems that almost inevitably occur in the design process. A complete and self-contained work, the text discusses the early history of aircraft dynamics and control, mathematical models of linear system elements, feedback system analysis, vehicle equations of motion, longitudinal and lateral dynamics, and elementary longitudinal and lateral feedback control. The discussion concludes with such topics as the system design process, inputs and system performance assessment, and multi-loop flight control systems.Originally published in 1974.The Princeton Legacy Library uses the latest print-on-demand technology to again make available previously out-of-print books from the distinguished backlist of Princeton University Press. These editions preserve the original texts of these important books while presenting them in durable paperback and hardcover editions. The goal of the Princeton Legacy Library is to vastly increase access to the rich scholarly heritage found in the thousands of books published by Princeton University Press since its founding in 1905.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Princeton University Press},
  isbn={9781400855988},
  url={https://ieeexplore.ieee.org/document/9453331},}

@INPROCEEDINGS{404523,
  author={Parrella, E.L. and Sin-Min Chang},
  booktitle={Proceedings Seventh Annual IEEE International ASIC Conference and Exhibit}, 
  title={Four channel DS1 framer}, 
  year={1994},
  volume={},
  number={},
  pages={445-448},
  abstract={A four channel DS1 framer chip has been developed for deployment in multichannel T1 systems, SONET add-drop multiplexers, T3 multiplexes, and ATM over T1 applications. Area reduction was realized through the use of a high speed clock, permitting use of shared resources and construction of simple arbiters for single port RAM. For further gate reduction, a state-machine based framing algorithm utilizing RAM as next state memory was developed.<>},
  keywords={Clocks;SONET;Costs;Buffer storage;Switches;Jitter;Application specific integrated circuits;Read-write memory;Add-drop multiplexers;Asynchronous transfer mode},
  doi={10.1109/ASIC.1994.404523},
  ISSN={},
  month={Sep.},}

@ARTICLE{4392504,
  author={},
  journal={IEEE Unapproved Draft Std P2600/D30b, Nov 2007}, 
  title={IEEE Standard for Information Technology: Hardcopy Device and System Security}, 
  year={2007},
  volume={},
  number={},
  pages={1-172},
  abstract={This standard defines security requirements (all aspects of security including but not limited to authentication, authorization, privacy, integrity, device management, physical security, and information security) for manufacturers, users, and others on the selection, installation, configuration, and usage of hardcopy devices (HCDs) and systems, including printers, copiers, and multifunction devices (MFDs), and the computer systems that support these devices. This standard identifies security exposures for these HCDs and systems, and instructs manufacturers and software developers on appropriate security capabilities to include in their devices and systems, and instructs users on appropriate ways to use these security capabilities.},
  keywords={Standards;Security;IEEE Standards;Best practices;Patents;Licenses;Terminology;2600-2008;all-in-one;copier;facsimile;fax;hardcopy device;HCD;information security;MFD;MFP;multifunction device;multifunction product;printer;scanner},
  doi={},
  ISSN={},
  month={Dec},}

@BOOK{10251231,
  author={Sandilands, David and Kersten, Nigel},
  booktitle={Puppet 8 for DevOps Engineers: Automate your infrastructure at an enterprise scale},
  year={2023},
  volume={},
  number={},
  pages={},
  abstract={Harness the power of this popular declarative configuration management tool to automate your infrastructure configuration Purchase of the print or Kindle book includes a free PDF eBookKey FeaturesUnderstand the core concepts and best approaches to the latest version of the Puppet languageLearn the key components of the Puppet platform and see how they deploy and apply Puppet code to infrastructureDiscover approaches to collaborative working by using the right structure of code and deploymentBook DescriptionAs DevOps and platform engineering drive the demand for robust internal development platforms, the need for infrastructure configuration tools has never been greater. Puppet, a powerful configuration management tool, is widely used by leading enterprises and boasts a thriving open source community. This book provides a comprehensive explanation of both the Puppet language and the platform. It begins by helping you grasp the basic concepts and approach of Puppet as a stateful language, and then builds up to explaining how to structure Puppet code to scale and allow flexibility and collaboration among teams. As you advance, you’ll find out how the Puppet platform allows the management and reporting of infrastructure configuration. The book also shows you how the platform can be integrated with other tooling, such as ServiceNow and Splunk. The concluding chapters help you implement Puppet to fit in heavily regulated and audited environments as well as modern hybrid cloud environments. By the end of this book, you’ll have gained a solid understanding of the capabilities of both the Puppet language and platform, and you will have learned how to structure and scale Puppet to create a platform to provide enterprise-grade infrastructure configuration.What you will learnFind out how to structure Puppet code and data to scale and be secureDiscover the core components of the Puppet platform and how to achieve performanceGet to grips with classifying infrastructure and deploying code for different environmentsUnderstand how Bolt can provide procedural orchestration alongside Puppet codeUse Puppet’s integrations and Forge modules that allow Puppet to integrate with other systemsAdopt approaches to adoption to ensure your Puppet implementation will succeed in regulated environments, the cloud, and with change controlWho this book is forThis book is for DevOps engineers looking to automate infrastructure with Puppet as a configuration management tool. It will allow both beginners and current Puppet users to understand the full power of the Puppet language and platform. A basic understanding of Unix system administration and Windows systems and core development concepts such as revision control tools like git, virtualization, testing, and coding tooling like vi or Visual Studio code is a prerequisite.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781803235455},
  url={https://ieeexplore.ieee.org/document/10251231},}

@INPROCEEDINGS{10344209,
  author={Uelschen, Michael and Schaarschmidt, Marco and Budde, Jannis},
  booktitle={2023 ACM/IEEE 26th International Conference on Model Driven Engineering Languages and Systems (MODELS)}, 
  title={Rapid-Prototyping and Early Validation of Software Models through Uniform Integration of Hardware}, 
  year={2023},
  volume={},
  number={},
  pages={250-260},
  abstract={Model-driven software engineering (MDSE) uses software models to make the complexity of cyber-physical and mechatronic systems (CPMS) manageable. For the validation of CPMS software models, closed-loop simulations are typically used. Since the system's environment is part of the simulation, the software model is directly affected by the surroundings, which enables a more realistic evaluation. In early development phases, the expected target hardware platform for these software models is usually not considered, although such CPMS have a strong hardware dependency. This paper outlines a novel approach to couple these software models with hardware systems to improve the quality of these models and shorten the development cycle. The presented method allows the evaluation of functional and non-functional requirements. For this, a new in-the-loop concept is introduced where the hardware access is transparently performed using a remote procedure call mechanism. Moreover, the achieved modeling language and tool independence makes the novel approach suitable for various applications. The provided evaluation is based on two distinct modeling languages and tools to demonstrate the feasibility and performance of the new concept.},
  keywords={Mechatronics;Software;Hardware;Model driven engineering;Complexity theory;Software engineering;model-driven software engineering;model-in-the-loop;cyber-physical systems;remote procedure call},
  doi={10.1109/MODELS58315.2023.00019},
  ISSN={},
  month={Oct},}

@BOOK{10163175,
  author={Edenfield, Orrin and Corcoran, Edward},
  booktitle={Microsoft Power BI Data Analyst Certification Guide: A comprehensive guide to becoming a confident and certified Power BI professional},
  year={2022},
  volume={},
  number={},
  pages={},
  abstract={Gain the knowledge and skills needed to become a certified Microsoft Power BI data analyst and get the most out of Power BIKey FeaturesGet the skills you need to pass the PL-300 certification exam with confidenceCreate and maintain robust reports and dashboards to enable a data-driven enterpriseTest your new BI skills with the help of practice questionsBook DescriptionMicrosoft Power BI enables organizations to create a data-driven culture with business intelligence for all. This guide to achieving the Microsoft Power BI Data Analyst Associate certification will help you take control of your organization's data and pass the exam with confidence. From getting started with Power BI to connecting to data sources, including files, databases, cloud services, and SaaS providers, to using Power BI’s built-in tools to build data models and produce visualizations, this book will walk you through everything from setup to preparing for the certification exam. Throughout the chapters, you'll get detailed explanations and learn how to analyze your data, prepare it for consumption by business users, and maintain an enterprise environment in a secure and efficient way. By the end of this book, you'll be able to create and maintain robust reports and dashboards, enabling you to manage a data-driven enterprise, and be ready to take the PL-300 exam with confidence.What you will learnConnect to and prepare data from a variety of sourcesClean, transform, and shape your data for analysisCreate data models that enable insight creationAnalyze data using Microsoft Power BI's capabilitiesCreate visualizations to make analysis easierDiscover how to deploy and manage Microsoft Power BI assetsWho this book is forThis book is for data analysts and BI professionals who want to become more competent in Microsoft Power BI. Although the content in this book will help you pass the PL-300 exam, there are plenty of other practical applications beyond exam preparation in the chapters. No prior experience with Power BI is needed.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781803241326},
  url={https://ieeexplore.ieee.org/document/10163175},}

@BOOK{9100572,
  author={Wrobel, Sharon and Wrobel, Leo},
  booktitle={Disaster Recovery Planning for Communications and Critical Infrastructure},
  year={2009},
  volume={},
  number={},
  pages={},
  abstract={Addressing the vulnerabilities in today's critical infrastructure to natural disasters and terrorism, this practical book describes what you should be doing to protect your infrastructure before the unthinkable happens. You learn how to maintain command and control in any disaster, and how to predict the probability of those disasters. Written by two highly regarded experts in the field, this one-of-a-kind book shows you how to simplify risk assessments and emergency response procedures to disasters affecting our critical national and local infrastructure. This practical resource helps you: Understand the latest technologies that help assure word gets out quickly after an act of terrorism, a severe weather occurance, or other destructive event occurs; Set up procedures for 4Ciù (Command, Control, Communications, Computers and intelligence); Assure that critical public services such as 911 centers will survive a catastrophic event; Learn the basics of what a good emergency response plan should contain for critical infrastructure providers; Create step-by-step plans and templates for assessing vulnerability in hospitals, government agencies, police and fire departments, EMT centers, water supplies, power grids, telecommunication networks, large business enterprises, and more; Develop safeguards and standards for critical infrastructure systems and write first alertù procedures; Discover ways to have seismic, weather and other alerts delivered to your telephone, wireless phone, blackberry or email, almost as they happen!},
  keywords={},
  doi={},
  ISSN={},
  publisher={Artech},
  isbn={9781596934696},
  url={https://ieeexplore.ieee.org/document/9100572},}

@ARTICLE{10251853,
  author={Vu, Henry and Fertig, Tobias and Braun, Peter},
  journal={Journal of Web Engineering}, 
  title={Model-Driven Integration Testing of Hypermedia Systems}, 
  year={2019},
  volume={18},
  number={4–6},
  pages={381-407},
  abstract={The proper design of Representational State Transfer (REST) APIs is not trivial because developers have to deal with a flood of recommendations and best practices, especially the proper application of the hypermedia constraint requires some decent experience. Furthermore, testing RESTful APIs is a missing topic within literature. Especially hypermedia testing is not mentioned at all. Manual hypermedia testing is time-consuming and hard to maintain. Testing a hypermedia API requires many test cases that have similar structure, especially when different user roles and error cases are considered. In order to tackle this problem, we proposed a Model-Driven Testing (MDT) approach for hypermedia systems using the metamodel within our existing Model Driven Software Development (MDSD) approach. This work discusses challenges and results of hypermedia testing for RESTful APIs using MDT techniques that were discovered within our research. MDT allows white-box testing, hence covering complete program structure and behavior of the generated application. By doing this, we are able to achieve a high automated test coverage. Moreover, any runtime behavior deviated from the metamodel reveals bugs within the generators.},
  keywords={Navigation;Source coding;Crawlers;Restful API;Hypermedia;Data models;Generators;REST;Integration Testing;RESTful API;Hypermedia Testing;MDSD;MDE;MDT;Model-Driven Testing},
  doi={10.13052/jwe1540-9589.18465},
  ISSN={1544-5976},
  month={June},}

@INPROCEEDINGS{10939419,
  author={M, Logesh.},
  booktitle={2024 International Conference on Distributed Systems, Computer Networks and Cybersecurity (ICDSCNC)}, 
  title={Design and Control of Medical Prosthetic Hand Using Electromyography for Easy Hand Gesture}, 
  year={2024},
  volume={},
  number={},
  pages={1-7},
  abstract={Losing a finger can have a profound impact on a person's mental and physical well-being, making it an incredibly difficult situation. In order to address this challenge, a microcontroller or computer can process verbal commands from the user, analyzing and translating them into actions that the prosthetic hand can perform, such as gripping, holding, releasing, and fine motor skills. This technology has the potential to greatly improve the quality of life for amputees, as it will make the prosthetic hand more natural and intuitive to use. To achieve this, the design of the hand is inspired by the human musculoskeletal system, allowing for flexible and lifelike movements in both a prosthetic hand and a humanoid robot hand. A bioinspired controller was created using 3D printing technology to operate a five-fingered robot hand and its mechanical parts. Additionally, sensors were utilized to detect muscle impulses and control hand movements.},
  keywords={Hands;Mechanical sensors;Service robots;Psychology;Robot sensing systems;Motors;Electromyography;Prosthetic hand;Robots;Faces;Prosthetic hand;Electromyography;Robot;Arm},
  doi={10.1109/ICDSCNC62492.2024.10939419},
  ISSN={},
  month={Sep.},}

@BOOK{10163604,
  author={Ping, David},
  booktitle={The Machine Learning Solutions Architect Handbook: Create machine learning platforms to run solutions in an enterprise setting},
  year={2022},
  volume={},
  number={},
  pages={},
  abstract={Build highly secure and scalable machine learning platforms to support the fast-paced adoption of machine learning solutionsKey FeaturesExplore different ML tools and frameworks to solve large-scale machine learning challenges in the cloudBuild an efficient data science environment for data exploration, model building, and model trainingLearn how to implement bias detection, privacy, and explainability in ML model developmentBook DescriptionWhen equipped with a highly scalable machine learning (ML) platform, organizations can quickly scale the delivery of ML products for faster business value realization. There is a huge demand for skilled ML solutions architects in different industries, and this handbook will help you master the design patterns, architectural considerations, and the latest technology insights you’ll need to become one. You’ll start by understanding ML fundamentals and how ML can be applied to solve real-world business problems. Once you've explored a few leading problem-solving ML algorithms, this book will help you tackle data management and get the most out of ML libraries such as TensorFlow and PyTorch. Using open source technology such as Kubernetes/Kubeflow to build a data science environment and ML pipelines will be covered next, before moving on to building an enterprise ML architecture using Amazon Web Services (AWS). You’ll also learn about security and governance considerations, advanced ML engineering techniques, and how to apply bias detection, explainability, and privacy in ML model development. And finally, you'll get acquainted with AWS AI services and their applications in real-world use cases. By the end of this book, you’ll be able to design and build an ML platform to support common use cases and architecture patterns like a true professional. What you will learnApply ML methodologies to solve business problemsDesign a practical enterprise ML platform architectureImplement MLOps for ML workflow automationBuild an end-to-end data management architecture using AWSTrain large-scale ML models and optimize model inference latencyCreate a business application using an AI service and a custom ML modelUse AWS services to detect data and model bias and explain modelsWho this book is forThis book is for data scientists, data engineers, cloud architects, and machine learning enthusiasts who want to become machine learning solutions architects. You’ll need basic knowledge of the Python programming language, AWS, linear algebra, probability, and networking concepts before you get started with this handbook. },
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781801070416},
  url={https://ieeexplore.ieee.org/document/10163604},}

@BOOK{10559419,
  author={Ayeva, Kamon and Kasampalis, Sakis},
  booktitle={Mastering Python Design Patterns: Craft essential Python patterns by following core design principles},
  year={2024},
  volume={},
  number={},
  pages={},
  abstract={Explore Python design patterns such as observer, proxy, throttling, dependency injection, and anti-patterns to develop efficient and scalable applications Key FeaturesMaster essential design principles to build robust software architecture with the latest features in Python 3.10Apply proven design patterns to solve complex problems efficientlyUnderstand anti-patterns to avoid common pitfalls in Python programmingPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionAs software systems become increasingly complex, maintaining code quality, scalability, and efficiency can be a daunting challenge. Mastering Python Design Patterns is an essential resource that equips you with the tools you need to overcome these hurdles and create robust, scalable applications. The book delves into design principles and patterns in Python, covering both classic and modern patterns, and showing you how to apply them to solve daily challenges as a Python developer or architect. This new edition covers creational, structural, behavioral, and architectural patterns, including concurrency, asynchronous, and performance patterns. You'll explore how these patterns are relevant to various domains, such as event handling, concurrency, distributed systems, and testing. Whether you're working on user interfaces (UIs), web apps, APIs, data pipelines, or AI models, this book equips you with the knowledge to build robust and maintainable software. The book also presents Python anti-patterns, helping you avoid common pitfalls and ensuring your code remains clean and efficient. By the end of this book, you'll be able to confidently apply classic and modern Python design patterns to build robust, scalable applications.What you will learnMaster fundamental design principles and SOLID conceptsBecome familiar with Gang of Four (GoF) patterns and apply them effectively in PythonExplore architectural design patterns to architect robust systemsDelve into concurrency and performance patterns for optimized codeDiscover distributed systems patterns for scalable applicationsGet up to speed with testing patterns to ensure code reliability and maintainabilityDevelop modular, decoupled systems and manage dependencies efficientlyWho this book is forWith a focus on intermediate and advanced Python programmers, this book offers valuable insights into the best practices for software design, backed by real-world examples and decades of experience. The book is also an excellent resource for software architects and team leaders who want to improve code quality and maintainability across their projects. Prior Python proficiency, including syntax, data structures, and OOP will help you get the most out of this book. },
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781837637652},
  url={https://ieeexplore.ieee.org/document/10559419},}

@BOOK{10559423,
  author={Gouigoux, Jean-Philippe and Tamzalit, Dalila},
  booktitle={Enterprise Architecture with .NET: Expert-backed advice for information system design, down to .NET and C# implementation},
  year={2024},
  volume={},
  number={},
  pages={},
  abstract={Write applications in C#/.NET that will stand the test of time, evolving with the information systems they belong to and the services they interoperate with by using standards and solid business-related architecture rulesKey FeaturesLearn the principles of business-aligned software architectureRelate theory to several well-known architecture frameworksApply the knowledge you gain to create a .NET application with a standard-based APIPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionThe software development domain continues to grow exponentially, and information systems have become the backbone of most industries, including non-digital-native ones. However, technical debt, coupling, and a high level of maintenance - sometimes bringing IT systems to a complete halt – continue to present a problem. The software industry has to still apply standards-based, modular, and repeatable approaches that exist in other industries. This book demonstrates such methods in action, particularly business/IT alignment principles. As you progress, you’ll cover advanced concepts and theories currently researched in academia. Then, you’ll be guided toward a practical framework to transfer these approaches to actual software architecture. Finally, a dedicated section will help you apply the knowledge you gain to a sample application in .NET where API design, dependency management, and code writing will be explained in detail to relate to the business-alignment principles explained at the beginning. Throughout the book, you’ll get equipped with the skills to create modular, long-living applications that serve your users better. By the end of this .NET book, you’ll not only have learned new concepts but also gained the ability to apply them immediately to your upcoming software endeavors. What you will learnComprehend the main problems in real-world software developmentUnderstand what business alignment meansCreate a four-layer map of an information systemBecome proficient in SOLID, C4, and domain-driven design (DDD) architectureGet up to speed with semantics, APIs, and standards for better interoperabilityInclude BPM, MDM, and BRMS in information systemsDesign an application with strict responsibility separationWho this book is forThis book is for software architects who want to have an in-depth understanding of how their applications will be used and how they can fight technical debt as well as design software to keep it working even when business requirements evolve. If your previous software designs experienced progressive loss of performance and the capacity to evolve, this book is for you.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781835081471},
  url={https://ieeexplore.ieee.org/document/10559423},}

@INPROCEEDINGS{10638613,
  author={Ansari, Saba Gholizadeh and Prasetya, I. S. W. B. and Dastani, Mehdi and Dignum, Frank and Keller, Gabriele},
  booktitle={2024 IEEE Conference on Software Testing, Verification and Validation (ICST)}, 
  title={EmoSTL: Formal Spatial-Temporal Verification of Emotion Specifications in Computer Games}, 
  year={2024},
  volume={},
  number={},
  pages={13-24},
  abstract={As the game industry continues to evolve in pop-ularity, testing the experience of players becomes crucial for attracting and retaining players in the highly competitive market. However, the absence of automated methods for articulating and verifying player experience (PX) specifications led us to introduce EmoSTL, a specialized language that extends Linear Temporal Logic with spatial and time-interval expressions, enabling the capture of complex temporal and spatial aspects of players' emotions and their experiences within games. We conducted a user study to collect suggestive PX requirements for a game under test to assess the capabilities of EmoSTL. Findings reveal that the language formalizes 92 percent of the set PX requirements, and with runtime verification, several PX design issues are iden-tified in the game. Moreoever, EmoSTL performance evaluation demonstrates its linear execution time, showcasing the language potential usage in automated PX testing of games.},
  keywords={Software testing;Performance evaluation;Industries;Video games;Runtime;Games;Logic;formal verification;player experience testing;emotional experience;play testing},
  doi={10.1109/ICST60714.2024.00011},
  ISSN={2159-4848},
  month={May},}

@BOOK{10745288,
  author={Bahree, Amit},
  booktitle={Generative AI in Action},
  year={2024},
  volume={},
  number={},
  pages={},
  abstract={Generative AI can transform your business by streamlining the process of creating text, images, and code. This book will show you how to get in on the action! Generative AI in Action is the comprehensive and concrete guide to generative AI you’ve been searching for. It introduces both AI’s fundamental principles and its practical applications in an enterprise context—from generating text and images for product catalogs and marketing campaigns, to technical reporting, and even writing software. Inside, author Amit Bahree shares his experience leading Generative AI projects at Microsoft for nearly a decade, starting well before the current GPT revolution. Inside Generative AI in Action you will find:  A practical overview of of generative AI applications Architectural patterns, integration guidance, and best practices for generative AI The latest techniques like RAG, prompt engineering, and multi-modality The challenges and risks of generative AI like hallucinations and jailbreaks How to integrate generative AI into your business and IT strategy  Generative AI in Action is full of real-world use cases for generative AI, showing you where and how to start integrating this powerful technology into your products and workflows. You’ll benefit from tried-and-tested implementation advice, as well as application architectures to deploy GenAI in production at enterprise scale.},
  keywords={prompt engineering;model fine tuning;enterprise;safety;ethics;integration;RAG;multi-modality;LLMs;hallucinations;jailbreaks;architectural patterns;ChatGPT;Bard;Copilot},
  doi={},
  ISSN={},
  publisher={Manning},
  isbn={9781633436947},
  url={https://ieeexplore.ieee.org/document/10745288},}

@INPROCEEDINGS{10911161,
  author={Haripriya, K and Sreevibha, G and Kumar, J Naveen and Reddy, S Sri Sai Sakethram and Nithish, T Sai},
  booktitle={2024 IEEE 4th International Conference on ICT in Business Industry & Government (ICTBIG)}, 
  title={CodeGuardian: Improving Software Quality with Automated CI-Driven Analysis}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  abstract={CodeGuardian is an advanced static code analyzer designed with much to improve the processes of software development. This is by automating performance and quality of code intelligence. A tool of this nature typically has an intuitive interface that works in combination with a large backend to drive a complete solution for managing code efficiently. Core features include project management, static code analysis, smooth integration of the continuous integration/ continuous deployment pipeline, and storage of extensive analysis results. In the backend, it can be used with several static analysis tools to scan code for any issues, and is able to notify users about key activities and findings. The front end should be an intuitive, responsive user interface that shall support easier project management. This view of extended analysis reports provides clear visualizations so that the monitoring of code quality trends becomes much easier.},
  keywords={Visualization;Computer languages;Codes;Pipelines;Project management;Static analysis;Software quality;User interfaces;Standards;Testing;static code analysis;code quality;performance;backend functionalities;project management;CI/CD pipeline integration;visualizations},
  doi={10.1109/ICTBIG64922.2024.10911161},
  ISSN={},
  month={Dec},}

@BOOK{10522532,
  author={Kruger, Joel and Beal, Helen},
  booktitle={Embracing DevOps Release Management: Strategies and tools to accelerate continuous delivery and ensure quality software deployment},
  year={2024},
  volume={},
  number={},
  pages={},
  abstract={Unlock the power of DevOps release management to elevate your software development with early quality checks, testing, automation, and QA integration, reshaping your software delivery life cycle for excellenceKey FeaturesUnderstand the SDLC and the most popular release management modelsLearn what makes DevOps unique and how CI/CD pipelines enforce good DevOps release managementDrive a culture-driven release management initiative in your organization that breaks down silosPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionAt the core of software development lies the imperative of swiftly and reliably releasing new features and updates, emphasizing the vital role of release management in the DevOps methodology. Discover how software development teams can elevate their processes by incorporating quality checks and shifting left, moving testing, automation, and QA procedures much earlier into the SDLC. However, release management is still tasked with application monitoring, overseeing infrastructure components, and managing change orders and schedules. This book offers insights into the essence of DevOps Release Management, illuminating its nuances and providing basic strategies for its implementation. You’ll explore how CI/CD pipelines enforce good DevOps release management and master techniques to optimize them. You’ll also learn how to foster a culture of cross-functional product development that minimizes waste and maximizes value to the customer. By the end of the book, you’ll have gained a comprehensive understanding of DevOps release management, its benefits, and practical implementation strategies. Equipped with this knowledge, you’ll be able to assess your own development processes and identify areas for improvement, ultimately leading to increased efficiency, collaboration, and value creation.What you will learnDiscover the significance and anatomy of the SDLCUnderstand the history of release management and how various models workGrasp DevOps release management and basic strategies to implement itConstruct optimized CI/CD pipelines capable of early issue detectionImplement the shift-left approach to enhance value delivery to customers at record speedFoster a culture of cross-functional collaboration in your teamMake DevOps release management pragmatic and accessibleOvercome common pitfalls in DevOps release managementWho this book is forThis book is a comprehensive introduction for those who are new to DevOps release management, but it's also valuable for DevOps engineers and release managers looking to enhance their skills and knowledge. If you’re looking to adopt key practices to shift left, this book will enable you to build high-quality products in record time.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781835466940},
  url={https://ieeexplore.ieee.org/document/10522532},}

@BOOK{10162302,
  author={Lukavský, Jan},
  booktitle={Building Big Data Pipelines with Apache Beam: Use a single programming model for both batch and stream data processing},
  year={2022},
  volume={},
  number={},
  pages={},
  abstract={Implement, run, operate, and test data processing pipelines using Apache BeamKey FeaturesUnderstand how to improve usability and productivity when implementing Beam pipelinesLearn how to use stateful processing to implement complex use cases using Apache BeamImplement, test, and run Apache Beam pipelines with the help of expert tips and techniquesBook DescriptionApache Beam is an open source unified programming model for implementing and executing data processing pipelines, including Extract, Transform, and Load (ETL), batch, and stream processing. This book will help you to confidently build data processing pipelines with Apache Beam. You’ll start with an overview of Apache Beam and understand how to use it to implement basic pipelines. You’ll also learn how to test and run the pipelines efficiently. As you progress, you’ll explore how to structure your code for reusability and also use various Domain Specific Languages (DSLs). Later chapters will show you how to use schemas and query your data using (streaming) SQL. Finally, you’ll understand advanced Apache Beam concepts, such as implementing your own I/O connectors. By the end of this book, you’ll have gained a deep understanding of the Apache Beam model and be able to apply it to solve problems.What you will learnUnderstand the core concepts and architecture of Apache BeamImplement stateless and stateful data processing pipelinesUse state and timers for processing real-time event processingStructure your code for reusabilityUse streaming SQL to process real-time data for increasing productivity and data accessibilityRun a pipeline using a portable runner and implement data processing using the Apache Beam Python SDKImplement Apache Beam I/O connectors using the Splittable DoFn APIWho this book is forThis book is for data engineers, data scientists, and data analysts who want to learn how Apache Beam works. Intermediate-level knowledge of the Java programming language is assumed.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781800566569},
  url={https://ieeexplore.ieee.org/document/10162302},}

@BOOK{9100252,
  author={Dias, Dileeka and Kularatna, Nihal},
  booktitle={Essentials of Modern Telecommunications Systems},
  year={2004},
  volume={},
  number={},
  pages={},
  abstract={In today's competitive and fast-changing telecom industry, most professionals find themselves in the difficult situation of having to sacrifice keeping on top of the latest technology because they are striving to meet another round of high-pressure deadlines. Essentials of Modern Telecommunications Systems offers you a solution to this problem, helping you quickly coming up to speed with the latest advances in your field. By cutting out arcane mathematics and management-speak jargon, it focuses on the essentials you need for rapidly understanding and mastering the latest implementation and development techniques. It provides the complete systems picture from semiconductors to end-to-end networking. Covered are such cutting-edge technologies as Next Generation Wireless, VoIP, optical transmission, and digital subscriber loop, as well as other broadband technologies. Special attention is given to systems software, especially applications software for 3G products. Throughout the book, the emphasis is on techniques you can use on the job for modeling, testing, and system management. By relating advanced technology to current technology already deployed, this book lets you quickly go from the familiar and the everyday to the cutting edge and the future.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Artech},
  isbn={9781580538848},
  url={https://ieeexplore.ieee.org/document/9100252},}

@INBOOK{8044359,
  author={Flanagan, William A.},
  booktitle={VoIP and Unified Communications: Internet Telephony and the Future Voice Network}, 
  title={Index}, 
  year={2011},
  volume={},
  number={},
  pages={277-298},
  abstract={},
  keywords={},
  doi={10.1002/9781118166048.index},
  ISSN={},
  publisher={Wiley},
  isbn={9781118166017},
  url={https://ieeexplore.ieee.org/document/8044359},}

@INPROCEEDINGS{10920179,
  author={Beringhoff, Felix and Greenyer, Joel and Roesener, Christian and Tichy, Matthias},
  booktitle={2024 IEEE 27th International Conference on Intelligent Transportation Systems (ITSC)}, 
  title={Realizing Scenario-Based Verification Tests of Automated Vehicles With an Ai-Controlled Surrounding Vehicle in a Practice-Relevant Context}, 
  year={2024},
  volume={},
  number={},
  pages={4196-4203},
  abstract={Scenario-based testing is seen as a key for the verification and validation of automated vehicles (AV). In a test scenario, the AV is tested under pre-defined traffic conditions. However, realizing those traffic conditions is challenging due to the autonomously behaving AV. The AV decides on its own, whether it gets into the test scenario conditions at all, e.g., by deciding on which lane it wants to drive or at which velocity it is driving. In order to influence the AV's decision making in a required way to realize a test scenario condition, we implement a novel AI method for controlling a surrounding vehicle (SV) of the AV. The AI-controlled SV (AISV) consists of a reinforcement learning (RL) agent which is trained to, e.g., nudge the AV into changing lanes. In contrast to current common practice, this approach does not require the manual tailoring of triggers and actions for controlling the SV. In this paper, we report on a working design of the RL framework and experiments regarding three different training scopes for the RL agent. We distinguish specialized agents which are trained to reach a single scenario condition and two sorts of generalized agents which shall be capable of reaching a set of scenario conditions. The results show that specialized agents perform the best with a success rate of up to 100 %. However, the generalized agents perform better in realizing scenario conditions which are not known to the agent from training. We also report on an implementation of the approach in a hardware-in-the-loop-simulation (HIL) test bench used in industrial practice and discuss a first try-out.},
  keywords={Training;Symbiosis;Roads;XML;Prototypes;Reinforcement learning;Manuals;Artificial intelligence;Intelligent transportation systems;Testing},
  doi={10.1109/ITSC58415.2024.10920179},
  ISSN={2153-0017},
  month={Sep.},}

@ARTICLE{4067154,
  author={},
  journal={IEEE Std P1175.2/D12.2, Jul 2006}, 
  title={IEEE Draft Recommended Practice for Case Tool Interconnection-Characterization of Interconnections}, 
  year={2006},
  volume={},
  number={},
  pages={},
  abstract={},
  keywords={Standards;IEEE Standards;Computer aided software engineering;Behavioral sciences;Syntactics;Software;Standards organizations},
  doi={},
  ISSN={},
  month={},}

@INPROCEEDINGS{9796461,
  author={Liu, Yu and Yandrapally, Rahulkrishna and Kalia, Anup K. and Sinha, Saurabh and Tzoref-Brill, Rachel and Mesbah, Ali},
  booktitle={2022 IEEE/ACM International Conference on Automation of Software Test (AST)}, 
  title={CRAWLABEL: Computing Natural-Language Labels for UI Test Cases}, 
  year={2022},
  volume={},
  number={},
  pages={103-114},
  abstract={End-to-end test cases that exercise the application under test via its user interface (UI) are known to be hard for developers to read and understand; consequently, diagnosing failures in these tests and maintaining them can be tedious. Techniques for computing natural-language descriptions of test cases can help increase test readability. However, so far, such techniques have been developed for unit test cases; they are not applicable to end-to-end test cases. In this paper, we focus on the problem of computing naturallanguage labels for the steps of end-to-end UI test cases for web applications. We present two techniques that apply natural-language processing to information available in the browser document object model (DOM). The first technique is an instance of a supervised approach in which labeling-relevant DOM attributes are ranked via manual analysis and fed into label computation. However, supervised approach requires a training dataset. So we propose the second technique, which is unsupervised: it leverages probabilistic context-free grammar learning to compute dominant DOM attributes automatically. We implemented these techniques, along with two simpler baseline techniques, in a tool called CRAWLABEL (available as a plugin to Crawljax, a state-of-the-art UI test-generation tool for web applications) and evaluated their effectiveness on open-source web applications. Our results indicate that the supervised approach can achieve precision, recall, and Fl-score of 83.38, 60.64, and 66.40, respectively. The unsupervised approach, although less effective, is competitive, achieving scores of 72.37, 58.12, and 59.77. We highlight key results and discuss the implications of our findings.},
  keywords={Training;Automation;Computational modeling;Manuals;User interfaces;Probabilistic logic;Grammar},
  doi={10.1145/3524481.3527229},
  ISSN={},
  month={May},}

@INBOOK{5238094,
  author={Sherif, Mostafa Hashem},
  booktitle={Managing Projects in Telecommunication Services}, 
  title={Index}, 
  year={2006},
  volume={},
  number={},
  pages={239-247},
  abstract={<P>No abstract.</P>},
  keywords={Indexes},
  doi={10.1002/9780470047682.index},
  ISSN={},
  publisher={IEEE},
  isbn={9780470047675},
  url={https://ieeexplore.ieee.org/document/5238094},}

@INBOOK{5273158,
  author={Kaplan, Steven M.},
  booktitle={Wiley Electrical and Electronics Engineering Dictionary}, 
  title={A}, 
  year={2004},
  volume={},
  number={},
  pages={1-53},
  abstract={},
  keywords={Dictionaries},
  doi={10.1109/9780470547151.ch1},
  ISSN={},
  publisher={IEEE},
  isbn={9780470547151},
  url={https://ieeexplore.ieee.org/document/5273158},}

@INPROCEEDINGS{5463662,
  author={Palviainen, Marko},
  booktitle={2010 Third International Conference on Software Testing, Verification, and Validation Workshops}, 
  title={A Dynamic Behaviour and Reliability Evaluation Method for Applications That Are Based on Asynchronous Processing Nodes}, 
  year={2010},
  volume={},
  number={},
  pages={309-318},
  abstract={Many embedded and distributed applications are based on processing nodes that perform parallel processing tasks. Unfortunately, it is difficult to evaluate the overall behaviour of this kind of applications because the overall behaviour consists of 1) the execution-paths of asynchronous processing nodes and of 2) messages that either activate or deactivate processing nodes to perform parallel processing tasks. In order to facilitate behaviour and reliability evaluation of applications doing parallel processing, we developed a method that: 1) is capable of composing an overall representation for parallel behaviours and recognizing both the defined use cases and undetermined behaviours from this representation and 2) supports calculation of use case-specific reliability values for components. In this paper, we describe the method, present a ComponentBee tool that implements the method and supports behaviour and reliability evaluation of multithreaded Java applications, and finally demonstrate the use of the method with a case study.},
  keywords={Parallel processing;Application software;Software measurement;Java;Software systems;Software testing;Performance evaluation;Concurrent computing;Predictive models;behaviour evaluation;reliability evaluation;ComponentBee;parallel processing},
  doi={10.1109/ICSTW.2010.45},
  ISSN={},
  month={April},}

@INPROCEEDINGS{8432197,
  author={Rana, Rakesh and Lagercrantz, Tommy and Staron, Miroslaw},
  booktitle={2018 IEEE International Conference on Software Architecture Companion (ICSA-C)}, 
  title={Building an Effective Software Issues Scorecard: An Action Research Report from the Automotive Domain}, 
  year={2018},
  volume={},
  number={},
  pages={136-143},
  abstract={A large number of mature software companies use data and analytic for status monitoring of their projects and to help improve their decision making at different levels within the organization. Dashboards or scorecards also provide common platform for different stakeholders to access information they need for tracking the status of projects of their interest. Further data from software issues database can provide real and observable indicators to track the quality of given product during its development and testing. The study presented here reports on distinct and evolution of information needs of different stakeholder groups interested in tracking such data. The action research report documents the evolution of software issues scorecard as it is extended to meet information need of specific user groups. A roadmap for future into how such scorecard can be made more effective is also presented.},
  keywords={Software;Testing;Stakeholders;Monitoring;Automotive engineering;Companies;Automobiles;software issue;scorecard;software development;action research;information need;defect database},
  doi={10.1109/ICSA-C.2018.00042},
  ISSN={},
  month={April},}

@BOOK{10848483,
  author={Chow, Jason (Tsz Shun)},
  booktitle={Software Architecture with Kotlin: Combine various architectural styles to create sustainable and scalable software solutions},
  year={2024},
  volume={},
  number={},
  pages={},
  abstract={Develop innovative architectural styles by analyzing and merging various approaches, focusing on making trade-offs and mitigating risks to solve real-world problemsKey FeaturesLearn how to analyze and dissect various architectural styles into building blocksCombine existing ideas with your own to create custom solutionsMake informed decisions by navigating trade-offs and compromisesPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionSoftware Architecture with Kotlin explores the various styles of software architecture with a focus on using the Kotlin programming language. The author draws on their 20+ years of industry experience in developing large-scale enterprise distributed systems to help you grasp the principles, practices, and patterns that shape the architectural landscape of modern software systems. The book establishes a strong foundation in software architecture, explaining key concepts such as architectural qualities and principles, before teaching you how architectural decisions impact the quality of a system, such as scalability, reliability, and extendability. The chapters address modern architecture topics such as microservices, serverless, and event-driven architectures, providing insights into the challenges and trade-offs involved in adopting these architectural styles. You’ll also discover practical tools that’ll help you make informed decisions and mitigate risks. All architectural patterns in this book are demonstrated using Kotlin. By the end of this book, you’ll have gained practical expertise by using real-world examples, along with a solid understanding of Kotlin, to become a more proficient and impactful software architect.What you will learnMaster the fundamental principles of architecture and designExplore common architectural styles and their applicable scenariosAnalyze, break down, compare, and design architectural styles to solve practical problemsReason, negotiate, and make difficult choices in the absence of ideal solutionsMitigate risks when making compromises and trade-offsCreate scalable, sustainable, maintainable, and extendable software systemsUse the Kotlin programming language to achieve your architectural goalsWho this book is forThis book is for developers with basic Kotlin knowledge seeking a deeper understanding of architecture, Kotlin Android developers who are starting to get involved in backend development, and Java developers transitioning to Kotlin. It's also ideal for software architects who are less experienced in Kotlin and want to enhance their skills, as well as those who enjoy discussing and exploring unique architectural concepts.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781835464960},
  url={https://ieeexplore.ieee.org/document/10848483},}

@INPROCEEDINGS{9062726,
  author={Coon, Ethan T. and Elwasif, Wael R. and Pillai, Himanshu and Thornton, Peter E. and Painter, Scott L.},
  booktitle={2019 IEEE/ACM Parallel Applications Workshop, Alternatives To MPI (PAW-ATM)}, 
  title={Exploring the Use of Novel Programming Models in Land Surface Models}, 
  year={2019},
  volume={},
  number={},
  pages={1-10},
  abstract={A wide range of programming models are currently under rapid development to meet the needs of application devel- opers looking to work on more complex machines. These models fill a variety of roles. Some look to abstract supercomputer architecture, including both processors and memory, to present a strategy for portable performance across a wide range of machines. Others look to expose concurrency by explicitly con- structing task-driven dependency graphs that allow a scheduler to find parallelism. Here we explore the implications for application codes of adopting two such programming models, Kokkos and Legion, one from each class of models. We specifically focus on the software design implications on refactoring existing applications, rather than the performance and performance tuning of these models. We identify a strategy for refactoring the Energy Exas- cale Earth System Model's Land Surface Model, an extremely complex code for climate applications, and prototype a series of mini-apps that explore the adoption of Kokkos and Legion. In doing this, we identify commonalities across the models, leading to a series of conclusions about application software design and refactoring for the adoption of novel programming models. Specifically, we find that refactoring efforts to abstract physics algorithms from data structures enable the use of a variety of programming models. With this refactoring done, we find that, at least in the case of Kokkos and Legion, these types of programming models are sufficiently mature for active use by even small application software development teams.},
  keywords={Computational modeling;Biological system modeling;Programming;Mathematical model;Earth;Atmospheric modeling;Computer architecture;Software testing;Parallel processing;Distributed processing;Runtime;Coprocessors;Multitasking},
  doi={10.1109/PAW-ATM49560.2019.00006},
  ISSN={},
  month={Nov},}

@BOOK{10251235,
  author={Grover, Vikas and Verma, Ishu and Rajagopalan, Praveen},
  booktitle={Achieving Digital Transformation Using Hybrid Cloud: Design standardized next-generation applications for any infrastructure},
  year={2023},
  volume={},
  number={},
  pages={},
  abstract={Accelerate your career growth by building dynamic applications that function across all environments and cloud types Purchase of the print or Kindle book includes a free PDF eBookKey FeaturesLearn hybrid cloud architecture from experienced cloud and telco architectsAdapt and deploy emerging technologies like AI and ML in a standardized and secure mannerMaster communication between Kubernetes clusters and managementBook DescriptionHybrid cloud technology can be leveraged by organizations aiming to build next-gen applications while safeguarding prior technological investments. This book will help you explore different hybrid cloud architectural patterns, whether designing new projects or migrating legacy applications to the cloud. You'll learn about the key building blocks of hybrid cloud enabling you to deploy, manage, and secure applications and data while porting the workloads between environments without rebuilding. Further, you’ll explore Kubernetes, GitOps, and Layer 3/7 services to reduce operational complexity. You'll also learn about nuances of security and compliance in hybrid cloud followed by the economics of hybrid cloud. You’ll gain a deep understanding of the concepts with use cases from telecom 5G and industrial manufacturing, giving you a glimpse into real industry problems resolved by hybrid cloud, and unlocking millions of dollars of opportunities for enterprises. By the end of this book, you'll be well-equipped to design and develop efficient hybrid cloud strategies, lead conversations with senior IT and business executives, and succeed in hybrid cloud implementation or transformation opportunities.What you will learnDesign and build a foundation for hybrid cloud platformLeverage Kubernetes, containers, and GitOps for hybrid cloudUse architectural pattern blueprints to deliver applications on hybrid cloudEnable communication between applications hosted on different cloudsRollout zero-touch provisioning and monitoring in a hybrid architectureEnhance stability and scale up or down without rebuilding appsUnderstand principles of hybrid cloud security for application stackDesign cost-optimized systems based on the economics of hybrid cloudWho this book is forThis book is for cloud architects, developers, and DevOps engineers, responsible for delivering modern applications and deploying resources anywhere. Professionals aspiring to implement distributed and cloud solutions will also benefit from reading this book. Basic understanding of VM, containers, CI/CD and familiarity with public cloud and edge is a must.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781837634156},
  url={https://ieeexplore.ieee.org/document/10251235},}

@INPROCEEDINGS{8452099,
  author={Steffens, Andreas and Lichter, Horst and Döring, Jan Simon},
  booktitle={2018 IEEE/ACM 4th International Workshop on Rapid Continuous Software Engineering (RCoSE)}, 
  title={Designing a Next-Generation Continuous Software Delivery System: Concepts and Architecture}, 
  year={2018},
  volume={},
  number={},
  pages={1-7},
  abstract={Continuous Integration and Continuous Delivery are established practices in modern agile software development. The DevOps movement adapted theses practices and places the deployment pipeline at its heart as one of the main requirements to automate the software development process and to deliver and operate software in a more robust way with higher quality. Over the time a lot of systems and tools has been developed to implement the deployment pipeline and to support continuous delivery. But software development is complex, its process even more and due to the individual organization of software vendors no real all-in-one solution for CD exists. Literature identified a lot of challenges when adopting CD and DevOps in an organization. This paper presents a conceptual model and fundamental design decisions for a new generation of software delivery systems tackling some of these issues. Our approach focuses on two specific challenges for adopting CD. The first is the lack of flexibility and maintainability of software delivery systems. The second is the insufficient user support to model and manage delivery processes and pipelines. We introduce an automated mechanism to ease the effort for developers and other stakeholders. Based on these results this paper introduces an architectural proposal for a next-generation continuous software delivery system.},
  keywords={Software;Pipelines;Logic gates;Computer architecture;Tools;Computational modeling;Next generation networking;Continuous Delivery;Continuous Integration;DevOps;Software Architecture;Microservices;Domain Driven Design;Domain Model;Software Delivery;Continuous Software Engineering;Software Design},
  doi={},
  ISSN={},
  month={May},}

@BOOK{10163040,
  author={Sangbin Park, Calvin and Adithya, Lalit and Gleske, Sam and Miranda, Tracy},
  booktitle={Jenkins Administrator's Guide: Install, manage, and scale a CI/CD build and release system to accelerate your product life cycle},
  year={2021},
  volume={},
  number={},
  pages={},
  abstract={Build and manage a production Jenkins instance, complete with CI/CD pipelines using GitHub and Docker Hub, Jenkins Configuration as Code, Shared Libraries, Script Security, and optimization guidesKey FeaturesSet up production-grade Jenkins and CI/CD pipelines with GitHub and Docker Hub integrationsManage, protect, and upgrade a production Jenkins instance regardless of its size and the number of usersScale a Jenkins instance using advanced optimization tips, tricks, and best practicesBook DescriptionJenkins is a renowned name among build and release CI/CD DevOps engineers because of its usefulness in automating builds, releases, and even operations. Despite its capabilities and popularity, it's not easy to scale Jenkins in a production environment. Jenkins Administrator's Guide will not only teach you how to set up a production-grade Jenkins instance from scratch, but also cover management and scaling strategies. This book will guide you through the steps for setting up a Jenkins instance on AWS and inside a corporate firewall, while discussing design choices and configuration options, such as TLS termination points and security policies. You’ll create CI/CD pipelines that are triggered through GitHub pull request events, and also understand the various Jenkinsfile syntax types to help you develop a build and release process unique to your requirements. For readers who are new to Amazon Web Services, the book has a dedicated chapter on AWS with screenshots. You’ll also get to grips with Jenkins Configuration as Code, disaster recovery, upgrading plans, removing bottlenecks, and more to help you manage and scale your Jenkins instance. By the end of this book, you’ll not only have a production-grade Jenkins instance with CI/CD pipelines in place, but also knowledge of best practices by industry experts. What you will learnSet up a production-grade Jenkins instance on AWS and on-premisesCreate continuous integration and continuous delivery (CI/CD) pipelines triggered by GitHub pull request eventsUse Jenkins Configuration as Code to codify a Jenkins setupBackup and restore configurations and plan for disaster recoveryPlan, communicate, execute, and roll back upgrade scenariosIdentify and remove common bottlenecks in scaling JenkinsUse Shared Libraries to develop helper functions and create new DSLsWho this book is forThis book is for both new Jenkins administrators and advanced users who want to optimize and scale Jenkins. Jenkins beginners can follow the step-by-step directions, while advanced readers can join in-depth discussions on Script Security, removing bottlenecks, and other interesting topics. Build and release CI/CD DevOps engineers of all levels will also find new and useful information to help them run a production-grade Jenkins instance following industry best practices.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781838828295},
  url={https://ieeexplore.ieee.org/document/10163040},}

@INPROCEEDINGS{10508648,
  author={García, Josue Capistran and Hernández, Jorge Octavio Ocharán and Arriaga, Juan Carlos Peréz and Riaño, Hector Javier Limón},
  booktitle={2023 Mexican International Conference on Computer Science (ENC)}, 
  title={Advances in Web API testing: A Systematic Mapping Study}, 
  year={2023},
  volume={},
  number={},
  pages={1-8},
  abstract={Web APIs serving as an intermediary for communication between distributed systems has increased recently. It has become critical to test these APIs to ensure their functionality and quality thoroughly. This study aims to systematically map the literature to analyze the techniques, methods, artifacts, and strategies employed during the testing phase of web APIs. Utilizing a systematic mapping study approach (SMS), we identified 42 studies that outlined various tests applicable to these APIs. Further, our analysis uncovered numerous methods, techniques, and strategies. Types of test artifacts such as the API specification, test cases, or test matrices were also found. Finally, testing activities were identified through approaches presented by each study’s authors, in which test results were applied and analyzed. The findings will establish the basis for the development of a testing guide, which in turn will support professionals who need to test APIs and who lack specific knowledge on how to carry out this activity due to the lack of standards.},
  keywords={Computer science;Systematics;Reviews;Standards;Testing;Web APIs;software testing;quality assurance;testing process;systematic mapping study;systematic review},
  doi={10.1109/ENC60556.2023.10508648},
  ISSN={2332-5712},
  month={Sep.},}

@INPROCEEDINGS{1214600,
  author={Gohar, I. and Mirza, A.},
  booktitle={IEEE Students Conference, ISCON '02. Proceedings.}, 
  title={Voice over asynchronous transfer modc(ATM)}, 
  year={2002},
  volume={2},
  number={},
  pages={11-12},
  abstract={},
  keywords={Asynchronous transfer mode;Software testing;Software performance;Transportation;Educational institutions;Shape;System testing;Fault detection;Speech analysis;Spine},
  doi={10.1109/ISCON.2002.1214600},
  ISSN={},
  month={Aug},}

@BOOK{10162701,
  author={Rappl, Florian and Schöttner, Lothar},
  booktitle={The Art of Micro Frontends: Build websites using compositional UIs that grow naturally as your application scales},
  year={2021},
  volume={},
  number={},
  pages={},
  abstract={Apply your experience of web development with HTML and JavaScript to build micro frontends for large-scale web projects using frameworks such as React and popular web tooling such as Node.js with Express or webpackKey FeaturesCut through the complexities of designing a monolithic web architecture using micro frontend architectureExplore architecture patterns for building large-scale applicationsLearn how to build, test, and secure your micro frontends efficientlyBook DescriptionMicro frontend is a web architecture for frontend development borrowed from the idea of microservices in software development, where each module of the frontend is developed and shipped in isolation to avoid complexity and a single point of failure for your frontend. Complete with hands-on tutorials, projects, and self-assessment questions, this easy-to-follow guide will take you through the patterns available for implementing a micro frontend solution. You’ll learn about micro frontends in general, the different architecture styles and their areas of use, how to prepare teams for the change to micro frontends, as well as how to adjust the UI design for scalability. Starting with the simplest variants of micro frontend architectures, the book progresses from static approaches to fully dynamic solutions that allow maximum scalability with faster release cycles. In the concluding chapters, you'll reinforce the knowledge you’ve gained by working on different case studies relating to micro frontends. By the end of this book, you'll be able to decide if and how micro frontends should be implemented to achieve scalability for your user interface (UI).What you will learnUnderstand how to choose the right micro frontend architectureDesign screens for compositional UIsCreate a great developer experience for micro frontend solutionsAchieve enhanced user experiences with micro frontendsIntroduce governance and boundary checks for managing distributed frontendsBuild scalable modular web applications from scratch or by migrating an existing monolithWho this book is forThis book is for software/solution architects or (mostly lead) developers as well as web developers and frontend engineers. Beginner-level knowledge of HTML and CSS along with a solid understanding of JavaScript programming and its ecosystem, including Node.js and NPM, is assumed.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781800565609},
  url={https://ieeexplore.ieee.org/document/10162701},}

@BOOK{10740988,
  author={Intorf, Dylan and Storey, Dylan and Doorn, Kendrick van},
  booktitle={Apache Airflow Best Practices: A practical guide to orchestrating data workflow with Apache Airflow},
  year={2024},
  volume={},
  number={},
  pages={},
  abstract={Confidently orchestrate your data pipelines with Apache Airflow by applying industry best practices and scalable strategiesKey FeaturesUnderstand the steps for migrating from Airflow 1.x to 2.x and explore the new features and improvements in version 2.xLearn Apache Airflow workflow authoring through real-world use casesUncover strategies to operationalize your Airflow instance and pipelines for resilient operations and high throughputPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionData professionals face the monumental task of managing complex data pipelines, orchestrating workflows across diverse systems, and ensuring scalable, reliable data processing. This definitive guide to mastering Apache Airflow, written by experts in engineering, data strategy, and problem-solving across tech, financial, and life sciences industries, is your key to overcoming these challenges. It covers everything from the basics of Airflow and its core components to advanced topics such as custom plugin development, multi-tenancy, and cloud deployment. Starting with an introduction to data orchestration and the significant updates in Apache Airflow 2.0, this book takes you through the essentials of DAG authoring, managing Airflow components, and connecting to external data sources. Through real-world use cases, you’ll gain practical insights into implementing ETL pipelines and machine learning workflows in your environment. You’ll also learn how to deploy Airflow in cloud environments, tackle operational considerations for scaling, and apply best practices for CI/CD and monitoring. By the end of this book, you’ll be proficient in operating and using Apache Airflow, authoring high-quality workflows in Python for your specific use cases, and making informed decisions crucial for production-ready implementation.What you will learnExplore the new features and improvements in Apache Airflow 2.0Design and build data pipelines using DAGsImplement ETL pipelines, ML workflows, and other advanced use casesDevelop and deploy custom plugins and UI extensionsDeploy and manage Apache Airflow in cloud environments such as AWS, GCP, and AzureDescribe a path for the scaling of your environment over timeApply best practices for monitoring and maintaining AirflowWho this book is forThis book is for data engineers, developers, IT professionals, and data scientists who want to optimize workflow orchestration with Apache Airflow. It's perfect for those who recognize Airflow’s potential and want to avoid common implementation pitfalls. Whether you’re new to data, an experienced professional, or a manager seeking insights, this guide will support you. A functional understanding of Python, some business experience, and basic DevOps skills are helpful. While prior experience with Airflow is not required, it is beneficial.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781805129332},
  url={https://ieeexplore.ieee.org/document/10740988},}

@BOOK{10280593,
  author={Vitale, Thomas},
  booktitle={Cloud Native Spring in Action: With Spring Boot and Kubernetes},
  year={2022},
  volume={},
  number={},
  pages={},
  abstract={Build and deliver production-grade cloud-native apps with Spring framework and Kubernetes.  In Cloud Native Spring in Action you’ll learn:  Cloud native best practices and design patterns Build and test cloud native apps with Spring Boot and Spring Cloud Handle security, resilience, and scalability in imperative and reactive applications Configure, deploy, and observe applications on Kubernetes Continuous delivery and GitOps to streamline your software lifecycle   Cloud Native Spring in Action is a practical guide to building applications that are designed for cloud environments. You’ll learn effective Spring and Kubernetes cloud development techniques that you can immediately apply to enterprise-grade applications. Follow a detailed and complete cloud native system from first concept right through to production and deployment, learning best practices, design patterns, and little-known tips and tricks for pain-free cloud native development. Including coverage of security, continuous delivery, and configuration, this hands-on guide is the perfect primer for navigating the increasingly complex cloud landscape.},
  keywords={cloud;native;spring;boot;docker;monitor;deploy;kubernetes;manage;framework;scalability},
  doi={},
  ISSN={},
  publisher={Manning},
  isbn={9781617298424},
  url={https://ieeexplore.ieee.org/document/10280593},}

@INBOOK{6542510,
  author={Cambron, G. Keith},
  booktitle={Global Networks: Engineering, Operations and Design}, 
  title={Front Matter}, 
  year={2013},
  volume={},
  number={},
  pages={i-xxxviii},
  abstract={The prelims comprise:   Half-Title Page   Title Page   Copyright Page   Dedication Page   Table of Contents   List of Figures   About the Author   Foreword   Preface   Acknowledgments   List of Acronyms   ]]>},
  keywords={},
  doi={10.1002/9781118394519.fmatter},
  ISSN={},
  publisher={IEEE},
  isbn={9781118394526},
  url={https://ieeexplore.ieee.org/document/6542510},}

@INPROCEEDINGS{10684641,
  author={de Moura, Thiago Santos and de Lima Mendes, Francisco Igor and Alves, Everton L. G. and Da Silva Neto, Ismael Raimundo and Baptista, Cláudio de Souza},
  booktitle={2024 IEEE 24th International Conference on Software Quality, Reliability and Security (QRS)}, 
  title={An Automatic Approach for Uniquely Discovering Actionable Elements for Systematic GUI Testing in Web Applications}, 
  year={2024},
  volume={},
  number={},
  pages={13-24},
  abstract={Automated GUI testing can play a crucial role in uncovering faults within web applications. In this context, scriptless testing can streamline the process by automatically generating and executing test cases that explore GUI elements. However, discovering non-repetitive actionable elements within complex web pages remains a challenging task. Often, testers need to employ manual strategies to facilitate element discovery and avoid redundancies. In this paper, we introduce a novel automatic approach, Unique Actionable Element Search (UAES), designed to provide a way to semantically differentiate elements in web applications. By leveraging common HTML properties and semantically expressive locators, UAES streamlines the process of discovering and distinguishing actionable elements, enhancing the efficiency and accuracy of systematic GUI testing. We conducted two empirical studies involving four open-source and twenty industrial web applications. They compared the results of our approach to those obtained using explicit markups included by experienced testers. The results indicate that our approach discovered 94.25% of the elements manually identified by the testers, while also uncovering a significant number of elements (46.86% of all elements discovered) that the testers overlooked.},
  keywords={Systematics;Accuracy;Web pages;Termination of employment;Software quality;Manuals;Software reliability;actionable elements;GUI testing;systematic testing},
  doi={10.1109/QRS62785.2024.00012},
  ISSN={2693-9177},
  month={July},}

@INBOOK{5396935,
  author={},
  booktitle={A Guide to the Wireless Engineering Body of Knowledge (WEBOK)}, 
  title={Fundamental Knowledge}, 
  year={2009},
  volume={},
  number={},
  pages={201-219},
  abstract={<P>This chapter contains sections titled: <UL> <LI> <P>Introduction</P> </LI> <LI> <P>Contents</P> </LI> <LI> <P>Electrical Engineering Basics for Wireless Communications</P> </LI> <LI> <P>Signal Processing and Communication Systems</P> </LI> <LI> <P>RF Engineering</P> </LI> <LI> <P>Instruments And Measurements [Wit02]</P> </LI> <LI> <P>Communication Networks</P> </LI> <LI> <P>Other Communication Systems</P> </LI> <LI> <P>General Engineering Management and Economics</P> </LI> <LI> <P>Key References</P> </LI> </UL> </P>},
  keywords={},
  doi={10.1002/9780470439128.ch7},
  ISSN={},
  publisher={IEEE},
  isbn={9780470439111},
  url={https://ieeexplore.ieee.org/document/5396935},}

@INPROCEEDINGS{6549304,
  author={Iyer, Ramkumar and Chandramouleeswaran, Balasundaram},
  booktitle={International Conference on Software Engineering and Mobile Application Modelling and Development (ICSEMA 2012)}, 
  title={Best practices and case study for open source middleware migration: Egate to apache camel migration}, 
  year={2012},
  volume={},
  number={},
  pages={1-7},
  abstract={Many legacy systems use message oriented middleware to communicate between themselves. Message oriented middleware are considered to be the most effective technology for enterprise integration. There are a lot of proprietary middleware solutions in the market that involve huge licensing costs, difficult maintenance procedures and niche skill sets. The usage of open source middleware to replace these proprietary solutions in a cost effective manner is an idea that can now bear fruition due to the relative maturity of such solutions. The use of open source reduces licensing cost, enables the developers to have greater insight into the working of the system and avail of the wide spread community support for such systems.},
  keywords={Middleware;open source;Apache Camel;Egate;enterprise migration},
  doi={10.1049/ic.2012.0140},
  ISSN={},
  month={Dec},}

@INPROCEEDINGS{5381646,
  author={Wieczorek, Sebastian and Stefanescu, Alin and Schieferdecker, Ina},
  booktitle={2009 Testing: Academic and Industrial Conference - Practice and Research Techniques}, 
  title={Model-Based Integration Testing of Enterprise Services}, 
  year={2009},
  volume={},
  number={},
  pages={56-60},
  abstract={The success of service-oriented architectures (SOA) depends on faultless and seamless service integration. Formal modeling of global communication protocols between services enables a model-based integration testing (MBIT) approach. In this paper we present an MBIT approach based on SAP proprietary choreography models called message choreography models (MCM). We explain how MBIT fits into the SAP testing methodology for SOA and give some insights into the experience we gained from the work.},
  keywords={Service oriented architecture;Enterprise resource planning;Automatic testing;Software testing;Protocols;Application software;Global communication;Unified modeling language;Java;Fault detection;Model-based Testing;Integration Testing;Service Oriented Architecture},
  doi={10.1109/TAICPART.2009.11},
  ISSN={},
  month={Sep.},}

@BOOK{10280556,
  author={Verburg, Martijn and Clark, Jason and Evans, Benjamin},
  booktitle={The Well-Grounded Java Developer, Second Edition},
  year={2022},
  volume={},
  number={},
  pages={},
  abstract={Understanding Java from the JVM up gives you a solid foundation to grow your expertise and take on advanced techniques for performance, concurrency, containerization, and more. In The Well-Grounded Java Developer, Second Edition you will learn:  The new Java module system and why you should use it Bytecode for the JVM, including operations and classloading Performance tuning the JVM Working with Java’s built-in concurrency and expanded options Programming in Kotlin and Clojure on the JVM Maximizing the benefits from your build/CI tooling with Maven and Gradle Running the JVM in containers Planning for future JVM releases   The Well-Grounded Java Developer, Second Edition introduces both the modern innovations and timeless fundamentals you need to know to become a Java master. Authors Ben Evans, Martijn Verburg, and Jason Clark distill their decades of experience as Java Champions, veteran developers, and key contributors to the Java ecosystem into this clear and practical guide. You’ll discover how Java works under the hood and learn design secrets from Java’s long history. Each concept is illustrated with hands-on examples, including a fully modularized application/library and creating your own multithreaded application.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Manning},
  isbn={9781617298875},
  url={https://ieeexplore.ieee.org/document/10280556},}

@INPROCEEDINGS{5457806,
  author={Bakera, Marco and Wagner, Christian and Margaria, Tiziana and Vassev, Emil and Hinchey, Mike and Steffen, Bernhard},
  booktitle={2010 Seventh IEEE International Conference and Workshops on Engineering of Autonomic and Autonomous Systems}, 
  title={Extracting Component-Oriented Behaviour for Self-Healing Enabling}, 
  year={2010},
  volume={},
  number={},
  pages={152-161},
  abstract={Rich and multifaceted domain specific specification languages like the Autonomic System Specification Language (ASSL) help to design reliable systems with self-healing capabilities. The GEAR game-based Model Checker has been used successfully to investigate in depth properties of the ESA ExoMars Rover. We show here how to enable GEAR's game-based verification techniques for ASSL via systematic model extraction from a behavioral subset of the language, and illustrate it on a description of the Voyager II space mission. This way, we close the gap between the design-time and the run-time techniques provided in the SHADOWS platform for self-healing of concurrency, performance, and functional issues.},
  keywords={Gears;Specification languages;Runtime;Concurrent computing;Software systems;Aerospace industry;Aerospace electronics;Conferences;Design engineering;Software engineering},
  doi={10.1109/EASe.2010.23},
  ISSN={2168-1872},
  month={March},}

@INPROCEEDINGS{10456864,
  author={Arrieta, Aitor and Sagardui, Goiuria and Agirre, Aitor and Afzal, Wasif and Ali, Shaukat},
  booktitle={2023 26th Euromicro Conference on Digital System Design (DSD)}, 
  title={DevOps for Cyber-Physical Systems: Objectives, Results and Lessons Learned from the Adeptness H2020 Project}, 
  year={2023},
  volume={},
  number={},
  pages={184-189},
  abstract={While most large web-based software systems (e.g., Amazon, Google) release a new software version every almost a minute, in the context of Cyber-Physical Systems (CPSs), this is still far. However, the software of CPSs needs to evolve while these are in operation to fix bugs, add new functionalities, carry out refactoring activities and deal with unforeseen situations that were discovered while the CPS was operating. In the last three years, the Adeptness project has been developing in a solution to help speedup the software release of CPSs that are in operation while guaranteeing their reliability. In this paper, we summarize the objectives, results and lessons learned from this H2020 project.},
  keywords={DevOps;Computer bugs;Agile software development;Software quality;Cyber-physical systems;Software systems;Software reliability;Cyber-Physical Systems;DevOps;Uncertainty},
  doi={10.1109/DSD60849.2023.00035},
  ISSN={2771-2508},
  month={Sep.},}

@ARTICLE{4152663,
  author={},
  journal={IEEE Unapproved Std P487/D7 Feb 2007}, 
  title={Unapproved IEEE Draft Recommended Practice for the Protection of Wire-Line Communication Facilities Serving Electric Supply Locations}, 
  year={2007},
  volume={},
  number={},
  pages={},
  abstract={},
  keywords={IEEE Standards;Standards;Safety;High-voltage techniques;Communications technology;Power transformer insulation;Optical fiber cables},
  doi={},
  ISSN={},
  month={},}

@BOOK{10163077,
  author={Dent, Chris},
  booktitle={Mastering PowerShell Scripting: Automate and manage your environment using PowerShell 7.1},
  year={2021},
  volume={},
  number={},
  pages={},
  abstract={This complete guide takes you on a tour of PowerShell from the basics to its advanced functionality, helping you automate your tedious and time-consuming system admin tasksKey FeaturesAutomate complex tasks, manipulate data, and secure your environmentWork with dual code for PowerShell 7 and Windows PowerShell to maintain compatibility with older versionsSee PowerShell in action, from learning the fundamentals to creating classes, scripts, and modulesBook DescriptionPowerShell scripts offer a convenient way to automate various tasks, but working with them can be daunting. Mastering PowerShell Scripting takes away the fear and helps you navigate through PowerShell's capabilities.This extensively revised edition includes new chapters on debugging and troubleshooting and creating GUIs (online chapter). Learn the new features of PowerShell 7.1 by working with parameters, objects, and .NET classes from within PowerShell 7.1. This comprehensive guide starts with the basics before moving on to advanced topics, including asynchronous processing, desired state configuration, using more complex scripts and filters, debugging issues, and error-handling techniques. Explore how to efficiently manage substantial amounts of data and interact with other services using PowerShell 7.1. This book will help you to make the most of PowerShell's automation features, using different methods to parse data, manipulate regular expressions, and work with Windows Management Instrumentation (WMI).What you will learnOptimize code with functions, switches, and looping structuresTest and debug your scripts as well as raising and catching errorsWork with objects and operators to test and manipulate dataParse and manipulate different data typesUse jobs, runspaces, and runspace pools to run code asynchronouslyWrite .NET classes with ease within PowerShellCreate and implement regular expressions in PowerShell scriptsMake use of advanced techniques to define and restrict the behavior of parametersWho this book is forThis book is for system administrators who want to automate and speed up their processes using PowerShell and Windows PowerShell. You’ll need to know the basics of operating systems, but beginners with no prior experience with PowerShell will have no trouble following along. },
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781800208575},
  url={https://ieeexplore.ieee.org/document/10163077},}

@INPROCEEDINGS{7503833,
  author={João, Ricardo and Marinheiro, Rui and Assunçõo, Pedro and Cruz, Luís},
  booktitle={2016 IEEE International Conference on Communications Workshops (ICC)}, 
  title={A flexible monitor for assessing 3D video QoE in real-time}, 
  year={2016},
  volume={},
  number={},
  pages={480-485},
  abstract={With the evolution of 3D technology, 3D IPTV services may prove to be a common service widely distributed by operators. So it is important that they have the necessary means to easily and inexpensively monitor the Quality of Experience (QoE) of this new service. Deployment of 3D video QoE monitors anywhere in the network will enable operators to adapt their service and network infrastructure in order to guarantee a desired QoE level, e.g., in scenarios where 3D IPTV streaming is offered to users with multi-homed equipment and simultaneous access to the network by means of heterogeneous smartcells in the customer premises.},
  keywords={Monitoring;Streaming media;Three-dimensional displays;Real-time systems;Mathematical model;Artificial neural networks;Computational modeling;3D video;G.1050;monitor;video-plus-depth;MVC},
  doi={10.1109/ICCW.2016.7503833},
  ISSN={},
  month={May},}

@INBOOK{6078760,
  author={Jacobs, Stuart},
  booktitle={Engineering Information Security: The Application of Systems Engineering Concepts to Achieve Information Assurance}, 
  title={Index}, 
  year={2011},
  volume={},
  number={},
  pages={663-700},
  abstract={<P>No abstract.</P>},
  keywords={},
  doi={10.1002/9780470947913.index},
  ISSN={},
  publisher={IEEE},
  isbn={9780470947838},
  url={https://ieeexplore.ieee.org/document/6078760},}

@INPROCEEDINGS{11068385,
  author={Ancona, Davide and Avola, Stefano and Ferrando, Angelo and Baglietto, Pierpaolo and Ter Beek, Maurice H. and Parodi, Andrea and Camera, Giancarlo and Pinasco, Matteo},
  booktitle={2025 55th Annual IEEE/IFIP International Conference on Dependable Systems and Networks - Supplemental Volume (DSN-S)}, 
  title={Integrating Testing with Runtime Verification for Mission-Critical Distributed Control Systems}, 
  year={2025},
  volume={},
  number={},
  pages={153-159},
  abstract={Verification of safety properties of mission-critical Distributed Control Systems (DCS) is challenging, especially when depending on a dynamically varying number of distributed components interacting via the system’s Integration Layer (IL). In such cases, complementing testing with Runtime Verification (RV) can help detect non-systematic errors earlier and reduce time-to-production. We adopt RV to test the IL of a real-world mission-critical railway control system, based on a Message-oriented Middleware (MoM) implementing a publish-subscribe communication protocol, with critical requirements on message uniqueness and order. These requirements are formalized in RML (Runtime Monitoring Language) and compiled into a monitor which verifies them dynamically. Performance measurements on real-world scenario parameters show that our approach can complement testing in the Continuous Integration (CI) cycle.},
  keywords={Protocols;Runtime;Instruments;Pipelines;Mission critical systems;Publish-subscribe;Rail transportation;Monitoring;Method of moments;Testing},
  doi={10.1109/DSN-S65789.2025.00056},
  ISSN={2833-292X},
  month={June},}

@INBOOK{5273129,
  author={Kaplan, Steven M.},
  booktitle={Wiley Electrical and Electronics Engineering Dictionary}, 
  title={C}, 
  year={2004},
  volume={},
  number={},
  pages={88-162},
  abstract={},
  keywords={Dictionaries},
  doi={10.1109/9780470547151.ch3},
  ISSN={},
  publisher={IEEE},
  isbn={9780470547151},
  url={https://ieeexplore.ieee.org/document/5273129},}

@ARTICLE{4040128,
  author={},
  journal={IEEE Std P1175.2/D11.2}, 
  title={Unapproved IEEE Recommended Practice for CASE Tool Interconnection - Characterization of Interconnections (Superseded by P1175.2_D12.2)}, 
  year={2006},
  volume={},
  number={},
  pages={},
  abstract={},
  keywords={Standards;IEEE Standards;Computer aided software engineering;Behavioral sciences;Syntactics;Software;Standards organizations},
  doi={},
  ISSN={},
  month={},}

@INPROCEEDINGS{9169459,
  author={},
  booktitle={2020 IEEE International Conference on Blockchain and Cryptocurrency (ICBC)}, 
  title={ICBC 2020 Keynotes}, 
  year={2020},
  volume={},
  number={},
  pages={18-49},
  abstract={Provides an abstract for each of the keynote presentations and may include a brief professional biography of each presenter. The complete presentations were not made available for publication as part of the conference proceedings.},
  keywords={},
  doi={10.1109/ICBC48266.2020.9169459},
  ISSN={},
  month={May},}

@BOOK{10163219,
  author={Boyle, Matthew},
  booktitle={Domain-Driven Design with Golang: Use Golang to create simple, maintainable systems to solve complex business problems},
  year={2022},
  volume={},
  number={},
  pages={},
  abstract={Understand the concept of Domain-driven design and build two DDD systems from scratch that can be showcased as part of your portfolioKey FeaturesExplore Domain-driven design as a timeless concept and learn how to apply it with GoBuild a domain-driven monolithic application and a microservice from scratchLeverage patterns to make systems scalable, resilient, and maintainableBook DescriptionDomain-driven design (DDD) is one of the most sought-after skills in the industry. This book provides you with step-by-step explanations of essential concepts and practical examples that will see you introducing DDD in your Go projects in no time. Domain-Driven Design with Golang starts by helping you gain a basic understanding of DDD, and then covers all the important patterns, such as bounded context, ubiquitous language, and aggregates. The latter half of the book deals with the real-world implementation of DDD patterns and teaches you how to build two systems while applying DDD principles, which will be a valuable addition to your portfolio. Finally, you’ll find out how to build a microservice, along with learning how DDD-based microservices can be part of a greater distributed system. Although the focus of this book is Golang, by the end of this book you’ll be able to confidently use DDD patterns outside of Go and apply them to other languages and even distributed systems.What you will learnGet to grips with domains and the evolution of Domain-driven designWork with stakeholders to manage complex business needsGain a clear understanding of bounded context, services, and value objectsGet up and running with aggregates, factories, repositories, and servicesFind out how to apply DDD to monolithic applications and microservicesDiscover how to implement DDD patterns on distributed systemsUnderstand how Test-driven development and Behavior-driven development can work with DDDWho this book is forThis book is for intermediate-level Go developers who are looking to ensure that they not only write maintainable code, but also deliver great business value. If you have a basic understanding of Go and are interested in learning about Domain-driven design, or you’ve explored Domain-driven design before but never in the context of Go, then this book will be helpful.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781804619261},
  url={https://ieeexplore.ieee.org/document/10163219},}

@INPROCEEDINGS{554009,
  author={Tanir, O.},
  booktitle={Wescon/96}, 
  title={Specification driven design of complex systems}, 
  year={1996},
  volume={},
  number={},
  pages={327-332},
  abstract={This paper examines the benefits of applying a specification driven approach and presents a framework for environments that can support the related design activities. The paper also outlines interrelated advanced topics such as intermediate architecture languages, model libraries and model verification issues.},
  keywords={Costs;Hardware;Design engineering;Audio systems;Switches;Software performance;Software design;Libraries;Testing},
  doi={10.1109/WESCON.1996.554009},
  ISSN={1095-791X},
  month={Oct},}

@INPROCEEDINGS{7352573,
  author={Bandung, Yoanes and Subekti, Luki Bangun and Gondokaryono, Yudi Satria},
  booktitle={2015 International Conference on Electrical Engineering and Informatics (ICEEI)}, 
  title={A design of teacher portal for supporting teacher's internet literacy web based solution for teacher learning}, 
  year={2015},
  volume={},
  number={},
  pages={618-623},
  abstract={There are a lot of valuable materials available in the Internet to support learning and self-improvement. But in some developing countries like Indonesia, Internet penetration rate is still low and people still don't comprehend on how to get benefit from it. In this research, a web based solution to support teacher's Internet literacy is designed. This solution is a teacher portal system which provides featured contents and files management system for teacher. Some application modules have been developed to support this system those are web-based portal, file management system, chatting system, and content aggregator module as the main component of the system builder.},
  keywords={Servers;Internet;Portals;Synchronization;Feature extraction;Computer architecture;Education;Internet literacy;teacher portal;files management;information and communication technology},
  doi={10.1109/ICEEI.2015.7352573},
  ISSN={2155-6830},
  month={Aug},}

@INPROCEEDINGS{9320327,
  author={Leal, Lucas and Montecchi, Leonardo and Ceccarelli, Andrea and Martins, Eliane},
  booktitle={2020 IEEE 25th Pacific Rim International Symposium on Dependable Computing (PRDC)}, 
  title={Using Metamodels to Improve Model-Based Testing of Service Orchestrations}, 
  year={2020},
  volume={},
  number={},
  pages={130-139},
  abstract={Online model-based testing is one of the most suitable techniques to assess the proper behavior of service orchestrations. However, the diverse panorama in terms of modeling languages and test case generation tools is a limitation to widespread adoption. We advocate that the application of Model-Driven Engineering principles as meta-modeling and model transformation can cope with this problem, improving the interoperability of artifacts in the test case generation process, thus bringing benefits in case of agile development processes, where system and technology evolution is frequent. In this paper, we present our contribution to this idea, introducing i) a reference metamodel, which stores the business process behavior and the information to generate input models for testing tools, and ii) transformations from orchestration languages towards testing tools. The proposed approach is implemented in a testing framework and evaluated on a case study where multiple orchestrations are expressed in two languages. Also, the paper presents how test cases are appropriately generated and successfully executed, starting from an orchestration model as a consequence of successful transformations.},
  keywords={Testing;Unified modeling language;Tools;Service-oriented architecture;Business;Runtime;Software;Model-Driven Engineering;SOA;Meta-modeling;Model-Based Testing},
  doi={10.1109/PRDC50213.2020.00024},
  ISSN={2473-3105},
  month={Dec},}

@INPROCEEDINGS{1552877,
  author={Fan, Y. and Kendall, E.A.},
  booktitle={IEEE International Conference on e-Business Engineering (ICEBE'05)}, 
  title={A hybrid dialogue strategy for speech-enabled mobile commerce}, 
  year={2005},
  volume={},
  number={},
  pages={110-117},
  abstract={Designing a dialogue strategy for speech-enabled mobile commerce is a significant challenge due to the context. This paper introduces a hybrid dialogue strategy to overcome the inflexibility of application-directed interactions while avoiding the significant recognition difficulty of a full mixed-initiative style. The system uses N-gram grammars to govern the recognition at the request segment of a dialogue, and employs an application-directed strategy at the clarification discourse segment. The paper also details generating a corpus for the N-gram grammar through a case-based reasoning approach, and constructing application-directed dialogues with decision trees. Our preliminary testing indicates the strategy is a feasible and effective solution for voice-enabling mobile commerce applications},
  keywords={Business;Natural languages;Computer networks;Speech;Cities and towns;Mobile computing;Decision trees;Testing;Standards development;Control systems},
  doi={10.1109/ICEBE.2005.6},
  ISSN={},
  month={Oct},}

@ARTICLE{4101803,
  author={Ring, Steven J.},
  journal={IEEE Transactions on Aerospace and Electronic Systems}, 
  title={A Distributed Intelligence Automatic Test System for PATRIOT}, 
  year={1977},
  volume={AES-13},
  number={3},
  pages={264-272},
  abstract={An automatic test system supporting high volume production testing of diverse state-of-the-art electronic assemblies is described. The test complex consists of a centralized computer system communicating to a network of satellite stations, each structured as "Intelligent Test Centers" dedicated to a particular family of assemblies (e.g., analog, digital, microwave). Allocation of resources and tasks have been distributed for optimum efficiency of production testing. This paper describes the organization and characteristics of the test system. Test center operation is explained with emphasis given to unique man-machine interactive features designed for on-line generation, examination, and maintenance of Unit-Under-Test (UUT) programs. Details are presented of the test language, RATEL, used for UUT programming. Other aspects that are discussed include test data and UUT program characteristics.},
  keywords={Intelligent systems;Automatic testing;System testing;Electronic equipment testing;Assembly systems;Production systems;Analog computers;Computer networks;Artificial satellites;Intelligent networks},
  doi={10.1109/TAES.1977.308394},
  ISSN={1557-9603},
  month={May},}

@BOOK{10162956,
  author={Kumar, A B Vijay},
  booktitle={Supercharge Your Applications with GraalVM: Hands-on examples to optimize and extend your code using GraalVM's high performance and polyglot capabilities},
  year={2021},
  volume={},
  number={},
  pages={},
  abstract={Understand the internals and architecture of GraalVM with the help of hands-on experiments and gain deep knowledge that you can apply to improve your application's performance, interoperability, and throughput.Key FeaturesGenerate faster and leaner code with minimum computing resources for high performanceCompile Java applications faster than ever to a standalone executable called native imagesCreate high-performance polyglot applications that are compatible across various JVM and non-JVM languagesBook DescriptionGraalVM is a universal virtual machine that allows programmers to compile and run applications written in both JVM and non-JVM languages. It improves the performance and efficiency of applications, making it an ideal companion for cloud-native or microservices-based applications. This book is a hands-on guide, with step-by-step instructions on how to work with GraalVM. Starting with a quick introduction to the GraalVM architecture and how things work under the hood, you'll discover the performance benefits of running your Java applications on GraalVM. You'll then learn how to create native images and understand how AOT (ahead-of-time) can improve application performance significantly. The book covers examples of building polyglot applications that will help you explore the interoperability between languages running on the same VM. You'll also see how you can use the Truffle framework to implement any language of your choice to run optimally on GraalVM. By the end of this book, you'll not only have learned how GraalVM is beneficial in cloud-native and microservices development but also how to leverage its capabilities to create high-performing polyglot applications.What you will learnGain a solid understanding of GraalVM and how it works under the hoodWork with GraalVM's high performance optimizing compiler and see how it can be used in both JIT (just-in-time) and AOT (ahead-of-time) modesGet to grips with the various optimizations that GraalVM performs at runtimeUse advanced tools to analyze and diagnose performance issues in the codeCompile, embed, run, and interoperate between languages using Truffle on GraalVMBuild optimum microservices using popular frameworks such as Micronaut and Quarkus to create cloud-native applicationsWho this book is forThis book is for JVM developers looking to optimize their application's performance. You'll also find this book useful if you're a JVM developer looking to explore options to develop polyglot applications using tools from the Python, R, Ruby, or Node.js ecosystem. A solid understanding of software development concepts and prior experience working with programming languages is necessary to get started.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781800566231},
  url={https://ieeexplore.ieee.org/document/10162956},}

@BOOK{9106157,
  author={Elbert, Bruce},
  booktitle={The Satellite Communication Ground Segment and Earth Station Handbook},
  year={2000},
  volume={},
  number={},
  pages={},
  abstract={From international telephone network gateways to direct broadcast home receivers, today's broad range of ground systems and devices require satellite communication engineers and business managers to have a broad and sound understanding of the design and operating principles of earth stations and ground control facilities. The book is the first to explore the delivery end of the satellite link and its relationship to delivery of services. Authored by a leading authority in the field, the book provides you with the knowledge you need to devise your own approach to implementing and managing earth stations and the overall ground segment. You find practical guidance in an array of critical areas, including: preparing requirements, performing preliminary analyses, reviewing hardware designs, managing the introduction of the overall ground segment, and more. In over 400 pages with nearly 200 illustrations, this valuable book covers dozens of essential topics, including: Earth station and ground segment design philosophy and principles; GEO and non-GEO satellite orbit configurations; Video, data, and digital audio, and multimedia transmission; Uplink power control, RF Filtering, decoding, and Internet interfaces; Major earth station configurations and antennas; Building design, environmental concerns, and O&M principles; Supplier negotiations, cost evaluation, and support services. Plus, you find informative case studies, insightful ideas for future-proofing earth stations, and much more!},
  keywords={},
  doi={},
  ISSN={},
  publisher={Artech},
  isbn={9781580533973},
  url={https://ieeexplore.ieee.org/document/9106157},}

@BOOK{9101136,
  author={Lehpamer, Harvey},
  booktitle={Introduction to Power Utility Communications},
  year={2016},
  volume={},
  number={},
  pages={},
  abstract={This timely new book is a cutting edge resource for engineers involved in the electric utility industry. This one-of-a-kind resource explores the planning, design, and deployment of communications networks, including fiber, microwave, RF, and Ethernet in electric utility spaces as related to Smart Grid. Readers are presented with an introduction to power utility communications, providing a thorough overview of data transmission media, electrical grid, and power grid modernization. Communication fundamentals and fiber-optic radio system design are also covered. Network performance and reliability considerations are discussed including channel protection, system latency, and cyber and grid security. Clear examples and calculations are presented to demonstrate reliability and availability measures for fiber-optic systems.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Artech},
  isbn={9781630810078},
  url={https://ieeexplore.ieee.org/document/9101136},}

@INBOOK{7406343,
  author={Jacobs, Stuart},
  booktitle={Engineering Information Security: The Application of Systems Engineering Concepts to Achieve Information Assurance}, 
  title={Index}, 
  year={2016},
  volume={},
  number={},
  pages={725-752},
  abstract={No abstract.},
  keywords={},
  doi={10.1002/9781119104728.index},
  ISSN={},
  publisher={IEEE},
  isbn={9781119104711},
  url={https://ieeexplore.ieee.org/document/7406343},}

@INPROCEEDINGS{8449448,
  author={Kröher, Christian and El-Sharkawy, Sascha and Schmid, Klaus},
  booktitle={2018 IEEE/ACM 40th International Conference on Software Engineering: Companion (ICSE-Companion)}, 
  title={KernelHaven – An Experimentation Workbench for Analyzing Software Product Lines}, 
  year={2018},
  volume={},
  number={},
  pages={73-76},
  abstract={Systematic exploration of hypotheses is a major part of any empirical research. In software engineering, we often produce unique tools for experiments and evaluate them independently on different data sets. In this paper, we present KernelHaven as an experimentation workbench supporting a significant number of experiments in the domain of static product line analysis and verification. It addresses the need for extracting information from a variety of artifacts in this domain by means of an open plug-in infrastructure. Available plug-ins encapsulate existing tools, which can now be combined efficiently to yield new analyses. As an experimentation workbench, it provides configuration-based definitions of experiments, their documentation, and technical services, like parallelization and caching. Hence, researchers can abstract from technical details and focus on the algorithmic core of their research problem. KernelHaven supports different types of analyses, like correctness checks, metrics, etc., in its specific domain. The concepts presented in this paper can also be transferred to support researchers of other software engineering domains. The infrastructure is available under Apache 2.0: https://github.com/KernelHaven. The plug-ins are available under their individual licenses.},
  keywords={Data mining;Pipelines;Data models;Feature extraction;Tools;Analytical models;Software engineering;Software product line analysis;variability extraction;static analysis;empirical software engineering},
  doi={},
  ISSN={2574-1934},
  month={May},}

@BOOK{10540160,
  author={Dent, Chris},
  booktitle={Mastering PowerShell Scripting: Automate repetitive tasks and simplify complex administrative tasks using PowerShell},
  year={2024},
  volume={},
  number={},
  pages={},
  abstract={Use PowerShell to save time and reduce the potential for human error by writing scripts that automate the execution of tasks Purchase of the print or Kindle book includes a free PDF eBook.Key FeaturesExplores PowerShell as a programming languageTake advantage of the features built into the PowerShell language in day-to-day automationAutomation of complex tasks, data manipulation, and environment securityBook DescriptionMastering PowerShell Scripting, Fifth Edition, can help you overcome any fears and become proficient in navigating PowerShell's capabilities. This edition includes new chapters on debugging, troubleshooting, and creating GUIs. You will learn about the latest features of PowerShell 7.3, including working with parameters, objects, and .NET classes. The book covers basic and advanced topics, such as asynchronous processing, desired state configuration, managing large amounts of data, interacting with other services, and working with regular expressions and Windows Management Instrument (WMI). Discover how to efficiently use PowerShell's automation features and error-handling techniques for more complex scripts and filters. Starting with foundational knowledge, this extensive guide progresses to advanced concepts like using complex scripts and filters, asynchronous processing, desired state configuration, debugging, and error-handling techniques. You will learn how to effectively handle large data sets and interact with external services using PowerShell 7.3. Additionally, you'll discover how to fully utilize PowerShell's automation capabilities, including parsing data, manipulating regular expressions, and working with WMI using various methods.What you will learnCreate scripts that can be run on different systemsPowerShell is highly extensible and can integrate with other programming languagesDiscover the powerful command-line interface that enables users to perform various operations with easeCreate reusable scripts and functions in PowerShellUtilize PowerShell for various purposes, including system administration, automation, and data processingIntegrate PowerShell with other technologies such as .NET, COM, and WMIWork with common data formats such as XML, JSON, and CSV in PowerShellCreate custom PowerShell modules and cmdlets to extend its functionalityWho this book is forThis book is for system administrators who want to automate and speed up their processes using PowerShell and Windows PowerShell. You'll need to know the basics of operating systems, but beginners with no prior experience with PowerShell will have no trouble following along.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781805124153},
  url={https://ieeexplore.ieee.org/document/10540160},}

@INPROCEEDINGS{10674071,
  author={Sathyakumar, Damodaran Chingleput},
  booktitle={2024 IEEE 4th International Conference on Software Engineering and Artificial Intelligence (SEAI)}, 
  title={Practical Workflows to Engineer Scalable Presentation Platforms for Modern Web Applications}, 
  year={2024},
  volume={},
  number={},
  pages={165-174},
  abstract={This abstract presents the concept of presentation engineering, a critical facet of software engineering focused on the user experience of web applications. As web and native app distinctions blur, presentation engineering's significance in software architecture grows. Modern web applications, including e-commerce and social networking, demand advanced presentation platforms to manage content-heavy and interaction-rich services. This paper introduces presentation platform engineering, highlighting its necessity and exploring best practices for building and operating scalable systems. It details engineering workflows that address the complexities of contemporary web development, enabling the delivery of engaging content and interactions effectively at scale. Insights into the trade-offs involved in these workflows are also discussed. The findings are drawn from empirical research, including developer experience surveys and release velocity assessments, within large-scale organizations like eBay Inc. This study aims to enhance understanding and improve practices within the evolving domain of presentation engineering.},
  keywords={Surveys;Visualization;Software architecture;Social networking (online);Friction;Buildings;Organizations;Component;Web;UX;Platform;Practices;Pre-sentation Engineering;Presentation platforms;Platform engi-neering;User interface;Engineering workflows},
  doi={10.1109/SEAI62072.2024.10674071},
  ISSN={},
  month={June},}

@INPROCEEDINGS{9843780,
  author={Mukherji, Shubhodeep and Khan, Shaheer and Voskanian, Vicken and Su, Laura},
  booktitle={2022 IEEE Aerospace Conference (AERO)}, 
  title={Feedback-Directed Random Sequence Generation for Verifying Spacecraft Flight Rule Violations}, 
  year={2022},
  volume={},
  number={},
  pages={1-8},
  abstract={The Psyche: Journey to a Metal World mission will be launched in 2022 to study the largest metal asteroid in the main asteroid belt, (16) Psyche. The spacecraft will perform a Mars Flyby in 2023 and enter (16) Psyche's orbit in 2026. Throughout the mission, safely operating the spacecraft will require abiding by a set of flight rules that can be defined at any point in the mission lifecycle. These flight rules ensure that the spacecraft is operating within allowed regimes and a discrete event simulation tool, SEQGEN, will be used to model all command sequences prior to uplink. One of SEQGEN's responsibilities is to determine if a command sequence violates any flight rules. For each flight rule, the necessary logic to determine if a violation has occurred is implemented in the SEQGEN adaptation, which is maintained by the mission. This adaptation must be tested thoroughly to ensure that the flight rule logic was interpreted and implemented correctly. This work describes a tool, RandSEQ, that autogenerates a suite of flight rule violating test sequences for each flight rule implemented in SEQGEN, and can be used by any mission using SEQGEN. The Psyche SEQGEN adaptation is still being developed, but so far, RandSEQ has been used to generate 532 test cases, that can be reviewed by various stakeholders, for 40 flight rules. The ability to autogenerate these test cases has significantly reduced the amount of time required to implement and test each flight rule.},
  keywords={Space vehicles;Schedules;Metals;Writing;Software;Solar system;Stakeholders},
  doi={10.1109/AERO53065.2022.9843780},
  ISSN={1095-323X},
  month={March},}

@BOOK{10769348,
  author={Miller, Richard H. and Johnson, Jeff},
  booktitle={UX for Enterprise ChatGPT Solutions: A practical guide to designing enterprise-grade LLMs},
  year={2024},
  volume={},
  number={},
  pages={},
  abstract={Create engaging AI experiences by mastering ChatGPT for business and leveraging user interface design practices, research methods, prompt engineering, the feeding lifecycle, and moreKey FeaturesLearn in-demand design thinking and user research techniques applicable to all conversational AI platformsMeasure the quality and evaluate ChatGPT from a customer’s perspective for optimal user experienceSet up and use your secure private data, documents, and materials to enhance your ChatGPT modelsPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionMany enterprises grapple with new technology, often hopping on the bandwagon only to abandon it when challenges emerge. This book is your guide to seamlessly integrating ChatGPT into enterprise solutions with a UX-centered approach. UX for Enterprise ChatGPT Solutions empowers you to master effective use case design and adapt UX guidelines through an engaging learning experience. Discover how to prepare your content for success by tailoring interactions to match your audience’s voice, style, and tone using prompt-engineering and fine-tuning. For UX professionals, this book is the key to anchoring your expertise in this evolving field. Writers, researchers, product managers, and linguists will learn to make insightful design decisions. You’ll explore use cases like ChatGPT-powered chat and recommendation engines, while uncovering the AI magic behind the scenes. The book introduces a and feeding model, enabling you to leverage feedback and monitoring to iterate and refine any Large Language Model solution. Packed with hundreds of tips and tricks, this guide will help you build a continuous improvement cycle suited for AI solutions. By the end, you’ll know how to craft powerful, accurate, responsive, and brand-consistent generative AI experiences, revolutionizing your organization’s use of ChatGPT.What you will learnAlign with user needs by applying design thinking to tailor ChatGPT to meet customer expectationsHarness user research to enhance chatbots and recommendation enginesTrack quality metrics and learn methods to evaluate and monitor ChatGPT's quality and usabilityEstablish and maintain a uniform style and tone with prompt engineering and fine-tuningApply proven heuristics by monitoring and assessing the UX for conversational experiences with trusted methodsRefine continuously by implementing an ongoing process for chatbot and feedingWho this book is forThis book is for user experience designers, product managers, and product owners of business and enterprise ChatGPT solutions who are interested in learning how to design and implement ChatGPT-4 solutions for enterprise needs. You should have a basic-to-intermediate level of understanding in UI/UX design concepts and fundamental knowledge of ChatGPT-4 and its capabilities.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781835463802},
  url={https://ieeexplore.ieee.org/document/10769348},}

@ARTICLE{6781158,
  author={Battaglia, Patricia A. and Byers, Charles C. and Guth, Leslie A. and Holliday, Albert and Spinelli, Claudio and Tong, Jason J.},
  journal={Bell Labs Technical Journal}, 
  title={Modular platform vision and strategy}, 
  year={2004},
  volume={9},
  number={1},
  pages={121-142},
  abstract={A platform is a set of hardware, software, and process building blocks that can form the basis of many different products. The goals of platforms are to reuse and/or share assets wherever possible, to save on development costs, to spread fixed development and production costs across the largest volumes, and to offer highly integrated solutions, all while maintaining critical differentiation of products. Using platforms correctly can produce substantial life-cycle cost benefits and can lead to enhanced supplier management and customer satisfaction. This paper will begin by describing the general concept of platforms. It will then consider their economic benefits to development teams, suppliers, and customers. Next, it will discuss the role of industry standards in forming the basis of a platform offer. It will consider the changing architectures of telecommunications networks, along with the contribution of platforms to these changes. Finally, the paper will outline an example set of platform building blocks and their requirements, along with some case studies of how to combine these building blocks into products.},
  keywords={},
  doi={10.1002/bltj.20009},
  ISSN={1538-7305},
  month={},}

@BOOK{10769352,
  author={Khedher, Omar},
  booktitle={Mastering OpenStack: Implement the latest techniques for designing and deploying an operational, production-ready private cloud},
  year={2024},
  volume={},
  number={},
  pages={},
  abstract={Design and manage a powerful OpenStack cloud with practical insights from real-world examplesKey FeaturesSimplify the architecture complexity of the OpenStack ecosystem with new container and networking optionsApply best practices to operate and manage large OpenStack deployments with confidenceDesign and implement hybrid cloud setups using OpenStack and public cloudsPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionOpenStack provides flexibility and control for designing and deploying robust and scalable cloud infrastructures, which has led to it having one of the largest open source communities in the cloud market. This book delves deep into the OpenStack architecture, dissecting each component to guide you in architecting your cloud with precision. From essential components to cutting-edge services, this book offers a step-by step approach, ensuring you grasp the fundamentals before exploring the latest advancements. This updated edition guides you through the deployment process, integrating secure best practices inspired by the DevSecOps philosophy. You’ll also explore the Antelope release, covering new services such as container management and software-defined networking (SDN). The book outlines best practices for running and managing fault-tolerant, secure, monitored, and high-performing setups. In the last part, it navigates the convergence of public and private clouds, covering hybrid models through use cases of managing Kubernetes-based applications in OpenStack private and public clouds. By the end of the book, you’ll be well versed in the latest OpenStack advancements, ready to lead your organization on a successful cloud journey.What you will learnExplore the latest design patterns in the OpenStack ecosystemImplement DevSecOps practices for agile and secure deployment managementEnsure resilience, fault tolerance, and performance in your cloud setupStay up to date with OpenStack networking and storage advancementsMaster operational best practices for managing a large-scale cloud setupDiscover logging and monitoring options for your cloudGet acquainted with new services such as SDN and containersUnderstand how to extend OpenStack's capabilities through a hybrid modelWho this book is forThis book is for OpenStack administrators, cloud and enterprise architects, and system and DevOps engineers looking to launch a private cloud with OpenStack. If you’re a cloud advisor, consultant, or evangelist, you’ll benefit from the expert insights, and if you’re a software developer or system operator aiming to accelerate your development cycle and agility, this book will help you bridge the gap between these roles. Basic knowledge of OpenStack, along with a prior understanding of systems, virtualization, and networking, is recommended.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781835466858},
  url={https://ieeexplore.ieee.org/document/10769352},}

@BOOK{9100514,
  author={Frey, Robert},
  booktitle={Successful Proposal Strategies for Small Businesses: Using Knowledge Management to Win Government, Private-Sector, and International Contracts, Sixth Edition},
  year={2012},
  volume={},
  number={},
  pages={},
  abstract={Here's your one-stop-shop for winning new business! The new, Sixth Edition of this perennial bestseller updates and expands all previous editions, making this volume the most exhaustive and definitive proposal strategy resource. Directly applicable for businesses of all sizes, Successful Proposal Strategies provides extensive and important context, field-proven approaches, and in-depth techniques for business success with the Federal Government, the largest buyer of services and products in the world. This popular book and its companion CD-ROM are highly accessible, self-contained desktop references developed to be informative, highly practical, and easy to use. Small companies with a viable service or product learn how to gain and keep a customer 's attention, even when working with only a few employees. Offering a greatly expanded linkage of proposals to technical processes and directions, the Sixth Edition includes a wealth of new material, adding important chapters on cost building and price volume, the criticality of business culture and investments in proposal success, the proposal solution development process, and developing key conceptual graphics. CD-ROM Download Included: Features useful proposal templates in Adobe Acrobat, platform-independent format, HTML pointers to Small Business Web Sites, a comprehensive, fully searchable listing Proposal and Contract Acronyms, and a sample architecture for a knowledge base or proposal library.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Artech},
  isbn={9781608074754},
  url={https://ieeexplore.ieee.org/document/9100514},}

@INPROCEEDINGS{873944,
  author={Bakar, B.A. and Janowski, T.},
  booktitle={Proceedings Sixth IEEE International Conference on Engineering of Complex Computer Systems. ICECCS 2000}, 
  title={Automated result verification with AWK}, 
  year={2000},
  volume={},
  number={},
  pages={188-198},
  abstract={The goal of result-verification is to prove that one execution run of a program satisfies its specification. Compared with implementation-verification, result-verification has a larger scope for applications in practice, gives more opportunities for automation and, based on the execution record not the implementation, is particularly suitable for complex systems. This paper proposes a technical framework to apply this technique in practice. We show how to write formal result-based specifications, how to generate a verifier program to check a given specification and to carry out result-verification according to the generated program. The execution result is written as a text file, the verifier is written in AWK (special-purpose language for text processing) and verification is done automatically by the AWK interpreter given the verifier and the execution result as inputs.},
  keywords={Text processing;Application software;Automation;Formal specifications;Automatic programming;Law;Legal factors;Humans;Error correction},
  doi={10.1109/ICECCS.2000.873944},
  ISSN={},
  month={Sep.},}

@BOOK{10162707,
  author={Beattie, Tim and Hepburn, Mike and O’Connor, Noel and Spring, Donal and Doria, Ilaria},
  booktitle={DevOps Culture and Practice with OpenShift: Deliver continuous business value through people, processes, and technology},
  year={2021},
  volume={},
  number={},
  pages={},
  abstract={A practical guide to making the best use of the OpenShift container platform based on the real-life experiences, practices, and culture within Red Hat Open Innovation LabsKey FeaturesLearn how modern software companies deliver business outcomes that matter by focusing on DevOps culture and practicesAdapt Open Innovation Labs culture and foundational practices from the Open Practice LibraryImplement a metrics-driven approach to application, platform, and product, understanding what to measure and how to learn and pivotBook DescriptionDevOps Culture and Practice with OpenShift features many different real-world practices - some people-related, some process-related, some technology-related - to facilitate successful DevOps, and in turn OpenShift, adoption within your organization. It introduces many DevOps concepts and tools to connect culture and practice through a continuous loop of discovery, pivots, and delivery underpinned by a foundation of collaboration and software engineering. Containers and container-centric application lifecycle management are now an industry standard, and OpenShift has a leading position in a flourishing market of enterprise Kubernetes-based product offerings. DevOps Culture and Practice with OpenShift provides a roadmap for building empowered product teams within your organization. This guide brings together lean, agile, design thinking, DevOps, culture, facilitation, and hands-on technical enablement all in one book. Through a combination of real-world stories, a practical case study, facilitation guides, and technical implementation details, DevOps Culture and Practice with OpenShift provides tools and techniques to build a DevOps culture within your organization on Red Hat's OpenShift Container Platform.What you will learnImplement successful DevOps practices and in turn OpenShift within your organizationDeal with segregation of duties in a continuous delivery worldUnderstand automation and its significance through an application-centric viewManage continuous deployment strategies, such as A/B, rolling, canary, and blue-greenLeverage OpenShift’s Jenkins capability to execute continuous integration pipelinesManage and separate configuration from static runtime softwareMaster communication and collaboration enabling delivery of superior software products at scale through continuous discovery and continuous deliveryWho this book is forThis book is for anyone with an interest in DevOps practices with OpenShift or other Kubernetes platforms. This DevOps book gives software architects, developers, and infra-ops engineers a practical understanding of OpenShift, how to use it efficiently for the effective deployment of application architectures, and how to collaborate with users and stakeholders to deliver business-impacting outcomes.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781800206502},
  url={https://ieeexplore.ieee.org/document/10162707},}

@INPROCEEDINGS{11081450,
  author={Ivanković, Marko and Rimanić, Luka and Budiselić, Ivan and Petrović, Goran and Fraser, Gordon and Just, René},
  booktitle={2025 IEEE/ACM International Conference on Automation of Software Test (AST)}, 
  title={What Types of Automated Tests do Developers Write?}, 
  year={2025},
  volume={},
  number={},
  pages={80-90},
  abstract={Software testing is a widely adopted quality assurance technique that assesses whether a software system meets a given specification. The overall goal of software testing is to develop effective tests that capture desired program behaviors and reveal defects. Automated software testing is an essential part of modern software development processes, in particular those that focus on continuous integration and deployment. Existing test classifications (e.g., unit vs. integration vs. system tests) and testing best practices offer general conceptual frameworks, but instantiating these conceptual models requires a definition of what is considered a unit, or even a test. These conceptual models are rarely explicated in the literature or documentation which makes interpretation and generalization of results (e.g., comparisons between unit and integration testing efficacy) difficult. Additionally, comparatively little is known about how developers operationalize software testing in modern industrial contexts, how they write and automate software tests, and how well those tests fit into existing classifications. Since software engineering processes have substantially evolved, it is time to revisit and refine test classifications to support future research on software testing efficacy and best practices. This is especially important with the advent of AI-generated test code, where those classifications may be used to automatically classify the types of generated tests or to formulate the desired test output.This paper presents a novel test classification framework, developed using insights and data on what types of tests developers write in practice. The data was collected in an industrial setting at Google and involves tens of thousands of developers and tens of millions of tests. The developed classification framework is precise enough that it can be encoded in an automated analysis. We describe our proof-of-concept implementation and report on the development approach and costs. We also report on the results of applying the automated classification to all tests in Google’s repository and on what types of automated tests developers write.},
  keywords={Software testing;Industries;Codes;Pipelines;Documentation;Continuous integration;Software systems;Internet;Best practices;Software engineering;software testing;test automation;test classification},
  doi={10.1109/AST66626.2025.00015},
  ISSN={2833-9061},
  month={April},}

@BOOK{10280026,
  author={Birchall, Chris},
  booktitle={Re-Engineering Legacy Software},
  year={2016},
  volume={},
  number={},
  pages={},
  abstract={As a developer, you may inherit projects built on existing codebases with design patterns, usage assumptions, infrastructure, and tooling from another time and another team. Fortunately, there are ways to breathe new life into legacy projects so you can maintain, improve, and scale them without fighting their limitations. Re-Engineering Legacy Software is an experience-driven guide to revitalizing inherited projects. It covers refactoring, quality metrics, toolchain and workflow, continuous integration, infrastructure automation, and organizational culture. You'll learn techniques for introducing dependency injection for code modularity, quantitatively measuring quality, and automating infrastructure. You'll also develop practical processes for deciding whether to rewrite or refactor, organizing teams, and convincing management that quality matters. Core topics include deciphering and modularizing awkward code structures, integrating and automating tests, replacing outdated build systems, and using tools like Vagrant and Ansible for infrastructure automation.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Manning},
  isbn={9781617292507},
  url={https://ieeexplore.ieee.org/document/10280026},}

@INPROCEEDINGS{8250775,
  author={Johnson, Jiya and Jai, Aarathi Elizabeth},
  booktitle={2017 International Conference on Intelligent Computing and Control Systems (ICICCS)}, 
  title={Netact based test-automation framework development for IMS CMREPO}, 
  year={2017},
  volume={},
  number={},
  pages={518-522},
  abstract={IP Multimedia Subsystem (IMS) is an architectural framework to provide VoLte and other multimedia services. It is based on ETSI standards like SIP for interfaces between architectural elements.[9] IMS(IP Multimedia subsysytem) was originally designed by the wireless standards body 3rd Generation Partnership Project (3GPP). IMS(IP Multimedia subsysytem) is the key element in the 3G architecture that makes it possible to provide cellular access to all the services that the Internet provides. It is considered as a bridge between[9] cellular network and internet. The introduction of Configuration Management (CM) Repository Server (CMRepo Server) is an important prerequisite for the mass roll out of network elements (NEs), such as, Call Session Control Function (CSCF) and Home Subscriber Server (HSS). It is also required to pre-administer important parameters of the NEs that are required for IMS functionality. Thus, CMRepo Server provides the central CM system to manage multiple NEs. NetAct functions as the central CM system for managing online changeable parameters (class D and class E parameters). Management of other parameters (class A, B, and C parameters) is done through the customization procedure, which is time consuming as well as complex. Management of these parameters in simplified with the introduction of CMRepo Server. NetAct is an OSS Platform. This environment giving access to statistic, performance monitoring, configuration management, user management, fault management and all OSS aspect for the overall subsystem. All tools which is available in NetAct mostly developed by Java platform. Using the Robot framework tool automating NetAct test cases in IMS(IP Multimedia subsysytem).},
  keywords={Robots;Servers;Testing;Protocols;Multimedia communication;Logic gates;IMS;Robot framework;Netact;VOLTE;CMRepo},
  doi={10.1109/ICCONS.2017.8250775},
  ISSN={},
  month={June},}

@BOOK{10251247,
  author={Bischoff, Benjamin and Thomas, Peter},
  booktitle={Writing API Tests with Karate: Enhance your API testing for improved security and performance},
  year={2023},
  volume={},
  number={},
  pages={},
  abstract={Unlock the full potential of Karate with this comprehensive guide to effortlessly setup, write, run, optimize, and report test results Purchase of the print or Kindle book includes a free PDF eBookKey FeaturesExplore the capabilities of the Karate framework for effective software testingLearn how to integrate Karate tests into projects and CI/CD pipelinesExplore lesser-known Karate modules such as Karate UI and Karate GatlingBook DescriptionSoftware in recent years is moving away from centralized systems and monoliths to smaller, scalable components that communicate with each other through APIs. Testing these communication interfaces is becoming increasingly important to ensure the security, performance, and extensibility of the software. A powerful tool to achieve safe and robust applications is Karate, an easy-to-use, and powerful software testing framework. In this book, you’ll work with different modules of karate to get tailored solutions for modern test challenges. You’ll be exploring interface testing, UI testing as well as performance testing. By the end of this book, you’ll be able to use the Karate framework in your software development lifecycle to make your APIs and applications robust and trustworthy.What you will learnUnderstand the basic concepts of Karate and its functionalityIntegrate and use it effectively to solve your testing needsExtend Karate and customize its functionality for your use casesExplore different testing methods and their use casesRun your tests to check the software development lifecycleGet to know more about Karate’s UI and performance test approachesWho this book is forThis book is for QA engineers and developers who are familiar with APIs and want to make them safer and more secure by applying automated tests with the help of a lightweight and modern framework - Karate.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781837631056},
  url={https://ieeexplore.ieee.org/document/10251247},}

@INPROCEEDINGS{10188631,
  author={Moriconi, Florent and Neergaard, Axel Ilmari and Georget, Lucas and Aubertin, Samuel and Francillon, Aurélien},
  booktitle={2023 IEEE Security and Privacy Workshops (SPW)}, 
  title={Reflections on Trusting Docker: Invisible Malware in Continuous Integration Systems}, 
  year={2023},
  volume={},
  number={},
  pages={219-227},
  abstract={Continuous integration (CI) is a widely adopted methodology for supporting software development. It provides automated generation of artifacts (e.g., binaries, container images) which are then deployed in production. However, to which extent should you trust the generated artifacts even if the source code is clean of malicious code? Revisiting the famous compiler backdoor from Ken Thompson, we show that a container-based CI system can be compromised without leaving any trace in the source code. Therefore, detecting such malware is challenging or even impossible with common practices such as peer review or static code analysis. We detail multiple ways to do the initial infection process. Then, we show how to persist during CI system updates, allowing long-term compromise. We detail possible malicious attack payloads such as sensitive data extraction or backdooring production software. We show that infected CI systems can be remotely controlled using covert channels to update attack payload or adapt malware to mitigation strategies. Finally, we propose a proof of concept implementation tested on GitLab CI and applicable to major CI providers.},
  keywords={Privacy;Source coding;Conferences;Production;Containers;Malware;Reflection;continuous-integration;malware;docker;self-hosting},
  doi={10.1109/SPW59333.2023.00025},
  ISSN={2770-8411},
  month={May},}

@BOOK{10522546,
  author={Mathieu, Laurent},
  booktitle={Mastering AWS Security: Strengthen your cloud environment using AWS security features coupled with proven strategies},
  year={2024},
  volume={},
  number={},
  pages={},
  abstract={Explore the depths of AWS security and learn how to design, implement, and maintain a secure cloud environment using state-of-the-art AWS technology Key FeaturesDive into AWS security concepts and technologies that can be applied for diverse use casesDesign and deploy secure AWS environments based on modern architectural principlesElevate your AWS security expertise with advanced techniques for automation and continuous improvementPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionIf you’re trying to navigate the complex world of AWS security and fortify your organizational cloud environment, then this book is for you. Written by an accomplished cybersecurity and AWS cloud consultant, Mastering AWS Security will help you understand and master the complexities of AWS security. This book offers an in-depth and practical exploration of AWS security concepts, features, and services, focusing on how they apply to modern cloud-based application environments. As you progress, you’ll gain a thorough introduction to the art of security automation and DevSecOps. You’ll learn how to automate security tasks, integrate security into your development process, and maintain a high level of security as your applications evolve and scale. Emphasizing continuous monitoring and improvement, this book will teach you how to set up monitoring systems, interpret security data, and make informed decisions to enhance your security over time. Through real-world case studies, you’ll learn how to tackle the challenges and find solutions for securing AWS environments. By the end of this book, you’ll confidently secure your AWS environments, and stay up to date with the latest security trends and updates in the AWS ecosystem.What you will learnDiscover AWS IAM, access control models, and the principle of least privilegeGet to grips with VPC network security strategies and tools to protect and isolate your critical assetsLeverage and orchestrate AWS security services tailored to your environmentImplement encryption and data protection best practices in key AWS servicesExplore best practices to secure microservices and serverless architectures on AWSImplement security strategies for multi-tenant architecturesMaster the art of security automation and DevSecOps toolingWho this book is forThis comprehensive guide is for cloud architects, engineers, DevOps professionals, and AWS enthusiasts. Cybersecurity professionals who want to learn AWS security to protect their applications, data, and infrastructure from threats, ensure compliance with regulations, and build trust with customers, will also find this book useful. },
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781805121718},
  url={https://ieeexplore.ieee.org/document/10522546},}

@INPROCEEDINGS{1471346,
  author={Verdonck, C. and Vandenameele, D.},
  booktitle={Proceedings of the 27th European Solid-State Circuits Conference}, 
  title={A single chip configurable network processor with built in ADSL-modem in 0.18 µm CMOS}, 
  year={2001},
  volume={},
  number={},
  pages={109-112},
  abstract={The SEA ASIC integrates a complete Discrete Multi-Tone (DMT) ADSL modem with an Asynchronous Transfer Mode (ATM) switch, an IEEE 802.3 Ethernet based packet switch and an ARM microcontroller into a single 0.18 µm CMOS chip. The device is a cost-effective platform for a complete range of Alcatel Customer Premises ADSL Equipment.},
  keywords={Intelligent networks;CMOS process;Hardware;Application specific integrated circuits;Local area networks;Modems;Wide area networks;Switches;Ethernet networks;Protocols},
  doi={},
  ISSN={},
  month={Sep.},}

@BOOK{11031165,
  author={Katsis, Charalampos and Bertino, Elisa},
  booktitle={The Zero-trust Paradigm: Concepts, Architectures and Applications},
  year={2025},
  volume={},
  number={},
  pages={},
  abstract={Existing measures aimed at securing network perimeters have demonstrated insufficiency in preventing breaches within an organization’s infrastructure. This inadequacy stems from the escalating resource capabilities of adversaries and the increasing sophistication of multi-step attack strategies, rendering breaches feasible. Zero Trust Architecture (ZTA), also known as perimeter-less security, is a recent paradigm that challenges the conventional notion of network security by considering both internal and external networks as potentially compromised and that threats exist at all times in the network. The notion of ZTA has been introduced as a fine-grained defense approach. It assumes that no entities outside and inside the protected system can be trusted and, therefore, requires articulated and high coverage deployment of security controls. However, ZTA is a complex notion that does not have a single design solution, rather, it consists of numerous interconnected concepts and processes that need to be assessed prior to deciding on a solution. In this monograph, the authors cover the principles and architectural foundations of ZTA following the guidelines by NIST, and provide a detailed analysis of ZTA proposed by research and industry. The monograph also describes an approach for the automatic generation of Zero Trust (ZT) policies based on application communication requirements, network topology, and organizational information. This approach was designed to meet a critical need of ZTA, that is, the generation and implementation of a large number of fine-grained policies. Finally, the monograph discusses several research directions, including the incorporation of threat intelligence into ZT networks and the use of large language models.},
  keywords={},
  doi={},
  ISSN={},
  publisher={now},
  isbn={9781638285731},
  url={https://ieeexplore.ieee.org/document/11031165},}

@BOOK{10162212,
  author={Bashir, Imran},
  booktitle={Mastering Blockchain: A technical reference guide to the inner workings of blockchain, from cryptography to DeFi and NFTs},
  year={2023},
  volume={},
  number={},
  pages={},
  abstract={Demystify one of the most disruptive modern technologies and gain an understanding of distributed ledger technology, consensus protocols, smart contracts, DApps, blockchain scalability, privacy, security, and more. Purchase of the print or Kindle book includes a free eBook in PDF format.Key FeaturesStudy new blockchains, including Polkadot, Solana, and dive into the architecture of Ethereum’s execution and consensus layerExplore distributed ledger technologies such as Ethereum, Bitcoin, Hyperledger Fabric, and QuorumGet to grips with Solidity, Web3, NFTs, DeFi, and smart contract developmentBook DescriptionBlockchain is the backbone of cryptocurrencies, with applications in finance, government, media, and more. With a legacy of providing technologists with executable insights, this new edition of Mastering Blockchain is thoroughly revised and updated according to the latest blockchain research. With new chapters on decentralized finance, decentralized identity, blockchain privacy, scalability, security, and bonus online content exploring alternative blockchains, this is an unmissable read for everyone who wants to gain a deep understanding of blockchain. Although this book covers the basics, including blockchain's technical underpinnings, cryptography, and consensus protocols, it doesn’t shy away from advanced topics and practical expertise, such as decentralized application (DApp) development using smart contracts. Throughout the book, you’ll explore blockchain solutions beyond cryptocurrencies, such as the Internet of Things (IoT) with blockchain, enterprise blockchains, and tokenization, and gain insight into the future scope of this fascinating and disruptive technology. By the end of this blockchain book, you will have gained a thorough understanding of the various facets of blockchain and understand the potential of this technology in diverse real-world scenarios.What you will learnGrasp the mechanisms behind Bitcoin, Ethereum, and other blockchain protocolsUnderstand cryptography and its usage in blockchainBecome familiar with blockchain consensus algorithms and develop smart contracts and DApps using Solidity, Remix, Truffle, and GanacheSolve issues relating to scalability, privacy, and security in blockchainExplore enterprise blockchainsDelve into emerging trends like decentralized and self-sovereign identity, DeFi, NFTs, and MetaverseExplore various applications, research topics, and future directions of blockchainWho this book is forThis book is for blockchain enthusiasts from all backgrounds, including business executives who want to leverage new platforms and students who want to a textbook exploring this fascinating technology. It is also a useful reference guide for blockchain development professionals who want to build fast and highly secure transactional applications. Basic knowledge in any programming language will come in handy.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781803230214},
  url={https://ieeexplore.ieee.org/document/10162212},}

@ARTICLE{4197854,
  author={},
  journal={IEEE Unapproved Draft Std P487/D8, Apr 2007}, 
  title={Unapproved IEEE Draft Recommended Practice for the Protection of Wire-Line Communication Facilities Serving Electric Supply Locations (Revision of IEEE Std 487-2000)}, 
  year={2007},
  volume={},
  number={},
  pages={},
  abstract={},
  keywords={IEEE Standards;Standards;Safety;Power transformer insulation;High-voltage techniques;Communications technology;Optical fiber cables},
  doi={},
  ISSN={},
  month={},}

@BOOK{10163624,
  author={Fraser, Simon and Ziadé, Tarek},
  booktitle={Python Microservices Development: Build efficient and lightweight microservices using the Python tooling ecosystem},
  year={2021},
  volume={},
  number={},
  pages={},
  abstract={Use Python microservices to craft applications that are built as small standard units using proven best practices and avoiding common errorsKey FeaturesBecome well versed with the fundamentals of building, designing, testing, and deploying Python microservicesIdentify where a monolithic application can be split, how to secure it, and how to scale it once ready for deploymentUse the latest framework based on asynchronous programming to write effective microservices with PythonBook DescriptionThe small scope and self-contained nature of microservices make them faster, cleaner, and more scalable than code-heavy monolithic applications. However, building microservices architecture that is efficient as well as lightweight into your applications can be challenging due to the complexity of all the interacting pieces. Python Microservices Development, Second Edition will teach you how to overcome these issues and craft applications that are built as small standard units using proven best practices and avoiding common pitfalls. Through hands-on examples, this book will help you to build efficient microservices using Quart, SQLAlchemy, and other modern Python tools In this updated edition, you will learn how to secure connections between services and how to script Nginx using Lua to build web application firewall features such as rate limiting. Python Microservices Development, Second Edition describes how to use containers and AWS to deploy your services. By the end of the book, you’ll have created a complete Python application based on microservices.What you will learnExplore what microservices are and how to design themConfigure and package your code according to modern best practicesIdentify a component of a larger service that can be turned into a microserviceHandle more incoming requests, more effectivelyProtect your application with a proxy or firewallUse Kubernetes and containers to deploy a microserviceMake changes to an API provided by a microservice safely and keep things workingIdentify the factors to look for to get started with an unfamiliar cloud providerWho this book is forThis book is for developers who want to learn how to build, test, scale, and manage Python microservices. Readers will require basic knowledge of the Python programming language, the command line, and HTTP-based application principles. No prior experience of writing microservices in Python is assumed.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781801079372},
  url={https://ieeexplore.ieee.org/document/10163624},}

@INBOOK{5273677,
  author={Bhatnagar, P. K.},
  booktitle={Engineering Networks for Synchronization, CCS 7, and ISDN: Standards, Protocols, Planning and Testing}, 
  title={Testing in the ISDN}, 
  year={1997},
  volume={},
  number={},
  pages={437-456},
  abstract={This chapter contains sections titled:   Introduction   Layer 1 Tests   Layer 2 and Layer 3 Tests   Maintenance of ISDN Access    This chapter contains sections titled:   References   },
  keywords={},
  doi={10.1109/9780470544570.ch14},
  ISSN={},
  publisher={IEEE},
  isbn={9780470544570},
  url={https://ieeexplore.ieee.org/document/5273677},}

@INBOOK{10013982,
  author={Chapple, Mike and Seidl, David},
  booktitle={(ISC)2 CISSP Certified Information Systems Security Professional Official Practice Tests}, 
  title={Answers}, 
  year={2021},
  volume={},
  number={},
  pages={311-455},
  abstract={},
  keywords={},
  doi={},
  ISSN={},
  publisher={Wiley},
  isbn={9781119793151},
  url={https://ieeexplore.ieee.org/document/10013982},}

@ARTICLE{7752755,
  author={},
  journal={ISO/IEC/IEEE FDIS P24765 FDIS, October 2016}, 
  title={ISO/IEC/IEEE Draft Systems and Software Engineering - Vocabulary}, 
  year={2016},
  volume={},
  number={},
  pages={1-554},
  abstract={Consistent with ISO vocabulary standards, each technical committee is responsible for standard terminology in its area of specialization. This International Standard provides a common vocabulary applicable to all systems and software engineering work falling within the scope of ISO/IEC JTC 1/SC 7, Systems and software engineering, and the IEEE Computer Society Systems and Software Engineering Standards Committee (IEEE-CS S2ESC).The scope of each concept defined has been chosen to provide a definition that is suitable for general application. In those circumstances where a restricted application is concerned, a more specific definition might be needed.Terms have been excluded if they were considered to be parochial to one group or organization; company proprietary or trademarked; multi-word terms whose meaning could be inferred from the definitions of the component words; terms whose meaning in the information technology (IT) field could be directly inferred from their common English dictionary meaning.},
  keywords={IEEE Standards;ISO Standards;IEC Standards;Software engineering;Terminology;Dictionaries},
  doi={},
  ISSN={},
  month={Jan},}

@INPROCEEDINGS{10471429,
  author={Wurzinger, Jurgen Gert and Petschnik, Harald and Santiago, Prabhu},
  booktitle={2023 IEEE International Transportation Electrification Conference (ITEC-India)}, 
  title={Protecting Passengers and Data: The Importance of a Consistent Process from TARA to Testing}, 
  year={2023},
  volume={},
  number={},
  pages={1-2},
  abstract={The automotive industry's rapid transition to software-defined vehicles (SDVs) presents a complex cybersecurity landscape. With nearly every vehicle function based on software, the attack surface has grown exponentially. With each line of code added, the potential for vulnerabilities grows. In addition, modern vehicles communicate via numerous interfaces, including the Internet, Bluetooth, diagnostic ports and more. This diverse connectivity creates a maze of potential entry points for cyber attackers. In addition, the interconnectedness of vehicle components means that a breach in one area can have far-reaching consequences, impacting critical functions and occupant safety. To effectively address these challenges, the use of new methods and technologies for early threat detection is essential. Traditional approaches to cybersecurity are no longer sufficient. A proactive stance is required to defend against cyber threats from the outset. In addition to identifying threats, it is equally important to test the effectiveness of countermeasures. This includes simulating potential attacks to determine if they could realistically occur and if mitigation strategies are robust enough to neutralize them.},
  keywords={Electric potential;Codes;Bluetooth;Transportation;Threat assessment;Software;Internet},
  doi={10.1109/ITEC-India59098.2023.10471429},
  ISSN={},
  month={Dec},}

@INBOOK{8044562,
  author={},
  booktitle={LTE and the Evolution to 4G Wireless: Design and Measurement Challenges}, 
  title={List of Acronyms}, 
  year={2013},
  volume={},
  number={},
  pages={601-611},
  abstract={},
  keywords={},
  doi={10.1002/9781118799475.oth1},
  ISSN={},
  publisher={Wiley},
  isbn={9781119967927},
  url={https://ieeexplore.ieee.org/document/8044562},}

@BOOK{10162286,
  author={Artasanchez, Alberto},
  booktitle={AWS for Solutions Architects: Design your cloud infrastructure by implementing DevOps, containers, and Amazon Web Services},
  year={2021},
  volume={},
  number={},
  pages={},
  abstract={Apply cloud design patterns to overcome real-world challenges by building scalable, secure, highly available, and cost-effective solutionsKey FeaturesApply AWS Well-Architected Framework concepts to common real-world use casesUnderstand how to select AWS patterns and architectures that are best suited to your needsEnsure the security and stability of a solution without impacting cost or performanceBook DescriptionOne of the most popular cloud platforms in the world, Amazon Web Services (AWS) offers hundreds of services with thousands of features to help you build scalable cloud solutions; however, it can be overwhelming to navigate the vast number of services and decide which ones best suit your requirements. Whether you are an application architect, enterprise architect, developer, or operations engineer, this book will take you through AWS architectural patterns and guide you in selecting the most appropriate services for your projects. AWS for Solutions Architects is a comprehensive guide that covers the essential concepts that you need to know for designing well-architected AWS solutions that solve the challenges organizations face daily. You'll get to grips with AWS architectural principles and patterns by implementing best practices and recommended techniques for real-world use cases. The book will show you how to enhance operational efficiency, security, reliability, performance, and cost-effectiveness using real-world examples. By the end of this AWS book, you'll have gained a clear understanding of how to design AWS architectures using the most appropriate services to meet your organization's technological and business requirements.What you will learnRationalize the selection of AWS as the right cloud provider for your organizationChoose the most appropriate service from AWS for a particular use case or projectImplement change and operations managementFind out the right resource type and size to balance performance and efficiencyDiscover how to mitigate risk and enforce security, authentication, and authorizationIdentify common business scenarios and select the right reference architectures for themWho this book is forThis book is for application and enterprise architects, developers, and operations engineers who want to become well-versed with AWS architectural patterns, best practices, and advanced techniques to build scalable, secure, highly available, and cost-effective solutions in the cloud. Although existing AWS users will find this book most useful, it will also help potential users understand how leveraging AWS can benefit their organization.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781789539141},
  url={https://ieeexplore.ieee.org/document/10162286},}

@INBOOK{8045719,
  author={},
  booktitle={Internet Protocol-based Emergency Services}, 
  title={Architectures}, 
  year={2013},
  volume={},
  number={},
  pages={103-192},
  abstract={},
  keywords={Emergency services;IP networks;3GPP;Protocols;Internet;WiMAX},
  doi={10.1002/9781119993858.ch3},
  ISSN={},
  publisher={Wiley},
  isbn={9781118652473},
  url={https://ieeexplore.ieee.org/document/8045719},}

@BOOK{10162858,
  author={Teter, Joshua Alan and Tobin, Ben},
  booktitle={Technical Program Manager's Handbook: Empowering managers to efficiently manage technical projects and build a successful career path},
  year={2022},
  volume={},
  number={},
  pages={},
  abstract={Explore the different aspects of the technical program manager’s role in the tech world and get ready to advance your career across the Big Five tech companiesKey FeaturesUncover the secret to becoming a successful technical program managerLearn some of the system design principles and architectural concepts necessary for a TPMGet up and running with a wide range of foundational program management topicsBook DescriptionThe technical program manager (TPM) is a relatively new role born out of the need of the tech industry to have a specialized practitioner who speaks both tech and business and leverages this bilingual talent to get results that no one else can. This book dives into what makes a TPM tick. You’ll find out which project and program management skills will help you shine and how you can apply your technical skills for effective results. This book looks at the TPM role across the Big Five tech companies (Amazon, Google, Microsoft, Apple, and Meta) to help you discern the most effective skills to be successful no matter which company you work for. Are you already a well-performing TPM looking to see what’s next? This book identifies the career paths for a TPM at the Big Five to help you decide the next step for you. By the end of this book, you’ll have a clear understanding of how to be a TPM, along with a breakdown of the necessary technical and program management skills to develop a clear roadmap for your career.What you will learnInvestigate why a TPM is an important role in the tech industryUnderstand the purpose and uniqueness of the TPM roleDiscover what makes a successful TPMNavigate project management with your unique technical skillsExplorer the career opportunities available for a TPMCompare the TPM role and responsibilities across the Big Five tech leadersWho this book is forThis TPM book is for aspiring and established technical program managers in the tech industry. To get the most out of this book, you should have a basic understanding of the project management life cycle and be comfortable with technical concepts as we dive into basic system design and architecture landscapes in context to the TPM role and expectations.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781804613160},
  url={https://ieeexplore.ieee.org/document/10162858},}

@INPROCEEDINGS{4766483,
  author={Sarkar, Santonu and Panayappan, Arun},
  booktitle={TENCON 2008 - 2008 IEEE Region 10 Conference}, 
  title={Formal architecture modeling of business application- software maintenance case study}, 
  year={2008},
  volume={},
  number={},
  pages={1-6},
  abstract={Maintenance of complex business applications is challenging for software services industry. The maintenance team inherits the software with little design and implementation knowledge. The client-facing team gathers an ad-hoc architectural description of some sort and communicates the same to the geographically distributed maintenance team through informal box and line diagrams. This information is poorly understood, and the underlying architectural constraints are never enforced. This paper proposes a type system to model the architecture of a complex enterprise IT system using Acme architecture description language and reports a modeling approach to capture various architectural design decisions architects perform as a part of the architecture review. An initial field-study to evaluate the usefulness of such modeling has been encouraging.},
  keywords={Computer architecture;Software maintenance;Application software;Computer industry;Architecture description languages;Software design;Documentation;Logic design;Software development management;Programming},
  doi={10.1109/TENCON.2008.4766483},
  ISSN={2159-3450},
  month={Nov},}

@INPROCEEDINGS{186190,
  author={Godon, F. and Al-Khalili, D. and Inkol, R.},
  booktitle={Third Annual IEEE Proceedings on ASIC Seminar and Exhibit}, 
  title={Multi circular buffer controller chip for advanced ESM system}, 
  year={1990},
  volume={},
  number={},
  pages={P14/5.1-P14/5.4},
  abstract={A 90 K transistor 1.5 mu m CMOS integrated circuit that operates at a data transfer rate of 20 MHz and implements an array of variable size circular buffers mapped into a high-speed RAM through physical and virtual addressing techniques is discussed. The device is fully programmable with the capability of single and block data transfers. The target application is an advanced multiprocessor ESM system.<>},
  keywords={Control systems;Buffer storage;Random access memory;Read-write memory;Pulse measurements;Counting circuits;Very large scale integration;Computer architecture;Registers;Logic arrays},
  doi={10.1109/ASIC.1990.186190},
  ISSN={},
  month={Sep.},}

@INBOOK{5273145,
  author={Kaplan, Steven M.},
  booktitle={Wiley Electrical and Electronics Engineering Dictionary}, 
  title={R}, 
  year={2004},
  volume={},
  number={},
  pages={623-675},
  abstract={},
  keywords={Dictionaries},
  doi={10.1109/9780470547151.ch18},
  ISSN={},
  publisher={IEEE},
  isbn={9780470547151},
  url={https://ieeexplore.ieee.org/document/5273145},}

@BOOK{10280248,
  author={Nicieja, Kamil},
  booktitle={Writing Great Specifications: Using Specification by Example and Gherkin},
  year={2017},
  volume={},
  number={},
  pages={},
  abstract={Writing Great Specifications is an example-rich tutorial that teaches you how to write good Gherkin specification documents that take advantage of the benefits of specification by example. },
  keywords={},
  doi={},
  ISSN={},
  publisher={Manning},
  isbn={9781617294105},
  url={https://ieeexplore.ieee.org/document/10280248},}

@INPROCEEDINGS{6075019,
  author={Balaretnaraja, Dhananjeyan and Weerawarana, Shahani},
  booktitle={2011 International Conference on Advances in ICT for Emerging Regions (ICTer)}, 
  title={A framework for managing persistence in distributed systems}, 
  year={2011},
  volume={},
  number={},
  pages={9-13},
  abstract={Enterprise applications today have acquired the need to be distributed due various demanding reasons. Such systems are developed with focus on distributed concerns than on the application logic. This diverted the developers from the functional requirement of the system and burdened them with the responsibility of developing and maintaining code related to distributed concerns. The main intention of this research is to facilitate development of distributed systems without any consideration for distributed concerns. We suggest a way where the application is initially designed without them and later enabled by integrating the framework proposed in this research. We confine our interest in separating persistence and replication among other distributed concerns. The motivation for this research comes by recognizing the fact that such a framework drastically reduces the code and complexity involved to make a distributed application resilient to failures and thereby to minimize the effort necessary to debug, deploy and maintain.},
  keywords={Peer to peer computing;Robustness;Indexing;Servers;distributed computing;group communication framework;peer to peer overlay;JGroups;FreePastry},
  doi={10.1109/ICTer.2011.6075019},
  ISSN={},
  month={Sep.},}

@INBOOK{9663076,
  author={Martins, Luiz Eduardo G. and Gorschek, Tony},
  booktitle={Requirements Engineering for Safety-Critical Systems}, 
  title={Requirements Engineering for Safety-Critical Systems}, 
  year={2021},
  volume={},
  number={},
  pages={i-xiv},
  abstract={Safety-Critical Systems (SCS) are increasingly present in people’s daily activities. In the means of transport, in medical treatments, in industrial processes, in the control of air, land, maritime traffic, and many other situations, we use and depend on SCS. The requirements engineering of any system is crucial for the proper development of the same, and it becomes even more relevant for the development of SCS. Requirements Engineering is a discipline that focuses on the development of techniques, methods, processes, and tools that assist in the design of software and systems, covering the activities of elicitation, analysis, modeling and specification, validation, and management of requirements. The complete specification of system requirements establishes the basis for its architectural design. It offers a description of the functional and quality aspects that should guide the implementation and system evolution. In this book, we discuss essential elements of requirements engineering applied to SCS, such as the relationship between safety/hazard analysis and requirements specification, a balance between conservative and agile methodologies during SCS development, the role of requirements engineering in safety cases, and requirements engineering maturity model for SCS. This book provides relevant insights for professionals, students, and researchers interested in improving the quality of the SCS development process, making system requirements a solid foundation for improving the safety and security of future systems.},
  keywords={},
  doi={},
  ISSN={},
  publisher={River Publishers},
  isbn={9788770224260},
  url={https://ieeexplore.ieee.org/document/9663076},}

@INPROCEEDINGS{10628955,
  author={De Petrillo, Erica and Mussbacher, Gunter},
  booktitle={2024 IEEE 32nd International Requirements Engineering Conference Workshops (REW)}, 
  title={FeatureLanguage: Automatic Generation of Application Backend for Model-Based Programming Course Projects}, 
  year={2024},
  volume={},
  number={},
  pages={281-290},
  abstract={University programs in software engineering or computer science increasingly include foundational courses in model-driven engineering. Building a substantial application through a term-long group project is one hands-on, practical way to learn the concepts taught in these courses. While learning by example can be very beneficial, providing students of these model-based programming courses with solutions in the form of complete working applications can be a real challenge due to time and resource constraints. In this paper, we argue that by specifying high-level requirements using our FeatureLanguage, we can completely generate the backend (i.e., Controller and Model) of a Model-View-Controller (MVC) application suitable for a university-level course. The proposed FeatureLanguage is an extension of a domain model with a specification of the different features the application should be able to accommodate as well as the constraints that need to be enforced. First, we discuss the FeatureLanguage, followed by an explanation of the different transformations from the FeatureLanguage to the backend code. We demonstrate that the complete backend can be generated and compare a generated MVC application with its handwritten counterpart. We argue that it is also feasible to completely generate a Controller test suite following a behaviour-driven development approach as well as the frontend of the MV C application, which we will explore in future work.},
  keywords={Productivity;Codes;Programming;Model driven engineering;Time factors;Security;Requirements engineering;Model-Driven Engineering;Domain Model;Model-Based Programming;MVC Application;Model Transformation},
  doi={10.1109/REW61692.2024.00043},
  ISSN={2770-6834},
  month={June},}

@ARTICLE{4519427,
  author={},
  journal={IEEE Unapproved Draft Std P11073-00101/D03, Sep 2007}, 
  title={Health Informatics - Point-Of-Care Medical Device Communication - Technical Report - Guidelines for the Use of RF Wireless Technology}, 
  year={2008},
  volume={},
  number={},
  pages={},
  abstract={The following Guidance document addresses the use of radio frequency 1 (RF) wireless technology for the transport of medical data both to and from point-of-care (PoC) medical devices. The context of such wireless medical data transport can range from home- or mobile-based healthcare to in hospital ambulatory and stationary situations. The intent of the guidance document is to be global with respect to wireless spectrum and equipment, although working group participation and expertise have favored detail of scenarios from the US. At the time of this Guidance document several applicable RF wireless technologies exist with a range of capabilities and characteristics, and in different stages of maturity, standardization, and adoption in healthcare. It is recognized that RF technologies are rapidly evolving, and new options may become available (or sufficiently established) after the publication of this Guidance document. The recommendations, therefore, avoid being overly prescriptive and instead attempt to assist medical device manufacturers, wireless equipment manufacturers, healthcare providers, government agencies and any other end-user of this document to make reasonable judgments regarding performance and practical implementation of wireless solutions. The Guidance document defines specific use cases to estimate, compare, and contrast performance of known technologies operating on wireless personal area (WPAN), wireless local area (WLAN), wireless metropolitan area (WMAN), and wireless wide area (WWAN) networks. Major considerations are 1) the quality-of-service (QoS) requirements (reliability, latency, priority, bandwidth) associated with the data being transported, 2) the expected performance (power, link range, throughput, link establishment and maintenance) of the wireless technology, and 3) the specific needs and resources of the end user. Related issues include network architecture, EMI/EMC, coexistence with other data streams, security, cost, power consumption, and technology configurability. Performance summaries for specific wireless technologies that support defined use cases are not intended as an endorsement of optimal solution because different needs, resources, sizes, and environments cannot be comprehensively addressed. This overview document is meant to be a foundation and reference for several follow-on IEEE 11073.3.5.x standards that will profile specific classes of off-the-shelf RF wireless technologies for medical data transport. Importantly, this guidance document is not envisioned to be periodically updated, but instead will act as a source of information for the follow-on IEEE 11073-0305.x standards that will supplant it. Periodic updates will be performed on the IEEE 11073- 0305.x standards only.},
  keywords={Wireless communication;Communication system security;Radio frequency;Standards;Medical devices;Performance evaluation;Patents},
  doi={},
  ISSN={},
  month={},}

@INPROCEEDINGS{7018532,
  author={Ulrich, Andreas and Jell, Sylvia and Votintseva, Anjelika and Kull, Andres},
  booktitle={2014 2nd International Conference on Model-Driven Engineering and Software Development (MODELSWARD)}, 
  title={The ETSI Test Description Language TDL and its application}, 
  year={2014},
  volume={},
  number={},
  pages={601-608},
  abstract={The wide-scale introduction of model-based testing techniques in an industrial context faces many obstacles. One of the obstacles is the existing methodology gap between informally described test purposes and formally defined test descriptions used as the starting point for test automation. The provision of an explicit test description becomes increasingly essential when integrating complex, distributed systems and providing support for conformance and interoperability tests of such systems. The upcoming ETSI standard on the Test Definition Language (TDL) covers this gap. It allows describing scenarios on a higher abstraction level than programming or scripting languages. Furthermore, TDL can be used as an intermediate representation of tests generated from other sources, e.g. simulators, test case generators, or logs from previous test runs. TDL is based on a meta-modelling approach that expresses its abstract syntax. Deploying this design approach, individual concrete syntaxes of TDL can be designed for different application domains. The paper provides an overview of TDL and discusses its application on a use case from the rail domain.},
  keywords={Testing;Unified modeling language;Concrete;Telecommunication standards;Syntactics;Semantics;Abstracts;Model-based Testing;Domain-Specific Languages;Meta-modelling;Rail Application},
  doi={},
  ISSN={},
  month={Jan},}

@BOOK{10769217,
  author={Staveley, Confidence and Romeo, Christopher},
  booktitle={API Security for White Hat Hackers: Uncover offensive defense strategies and get up to speed with secure API implementation},
  year={2024},
  volume={},
  number={},
  pages={},
  abstract={Become an API security professional and safeguard your applications against threats with this comprehensive guide Key FeaturesGain hands-on experience in testing and fixing API security flaws through practical exercisesDevelop a deep understanding of API security to better protect your organization's dataIntegrate API security into your company's culture and strategy, ensuring data protectionPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionAPIs have evolved into an essential part of modern applications, making them an attractive target for cybercriminals. Written by a multi-award-winning cybersecurity leader , this comprehensive guide offers practical insights into testing APIs, identifying vulnerabilities, and fixing them. With a focus on hands-on learning, this book guides you through securing your APIs in a step-by-step manner. You'll learn how to bypass authentication controls, circumvent authorization controls, and identify vulnerabilities in APIs using open-source and commercial tools. Moreover, you'll gain the skills you need to write comprehensive vulnerability reports and recommend and implement effective mitigation strategies to address the identified vulnerabilities. This book isn't just about hacking APIs; it's also about understanding how to defend them. You'll explore various API security management strategies and understand how to use them to safeguard APIs against emerging threats. By the end of this book, you'll have a profound understanding of API security and how to defend against the latest threats. Whether you're a developer, security professional, or ethical hacker, this book will ensure that your APIs are secure and your organization's data is protected.What you will learnImplement API security best practices and industry standardsConduct effective API penetration testing and vulnerability assessmentsImplement security measures for API security managementUnderstand threat modeling and risk assessment in API securityGain proficiency in defending against emerging API security threatsBecome well-versed in evasion techniques and defend your APIs against themIntegrate API security into your DevOps workflowImplement API governance and risk management initiatives like a proWho this book is forIf you’re a cybersecurity professional, web developer, or software engineer looking to gain a comprehensive understanding of API security, this book is for you. The book is ideal for those who have beginner to advanced-level knowledge of cybersecurity and API programming concepts. Professionals involved in designing, developing, or maintaining APIs will also benefit from the topics covered in this book. },
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781800569355},
  url={https://ieeexplore.ieee.org/document/10769217},}

@ARTICLE{7361678,
  author={},
  journal={IEEE Std 487-2015 (Revision of IEEE Std 487-2007) - Redline}, 
  title={IEEE Standard for the Electrical Protection of Communications Facilities Serving Electric Supply Locations -- General Considerations - Redline}, 
  year={2015},
  volume={},
  number={},
  pages={1-324},
  abstract={General considerations are presented for the electrical protection of telecommunications facilities serving electric supply locations. This standard contains material that is common to the IEEE 487(TM) family of standards (i.e., dot-series) including fundamental protection theory; basic electrical protection philosophy, concepts, and designs; protection apparatus; service types; reliability; service performance objective (SPO) classifications; and transmission considerations. In general, special protective measures, handling procedures, and administrative procedures are necessary to provide electrical protection against damage to telecommunications facilities and equipment, maintain reliability of service, and ensure the safety of personnel.},
  keywords={IEEE Standards;Electricity supply industry;Voltage control;Power transmission lines;Power stations;electric supply locations;high-voltage tower;IEEE 487(TM);power stations;protection;wire-line telecommunications},
  doi={},
  ISSN={},
  month={July},}

@INBOOK{5273147,
  author={Kaplan, Steven M.},
  booktitle={Wiley Electrical and Electronics Engineering Dictionary}, 
  title={B}, 
  year={2004},
  volume={},
  number={},
  pages={54-87},
  abstract={},
  keywords={Dictionaries},
  doi={10.1109/9780470547151.ch2},
  ISSN={},
  publisher={IEEE},
  isbn={9780470547151},
  url={https://ieeexplore.ieee.org/document/5273147},}

@INPROCEEDINGS{8051361,
  author={Terber, Matthias},
  booktitle={2017 43rd Euromicro Conference on Software Engineering and Advanced Applications (SEAA)}, 
  title={Function-Oriented Decomposition for Reactive Embedded Software}, 
  year={2017},
  volume={},
  number={},
  pages={288-295},
  abstract={Due to C's overwhelming dominance in industry, reactive embedded applications usually rely on conventional sequential programming. Adopted approaches favor event-driven paradigms which prevent function-oriented code decomposition in particular. This encourages the violation of fundamental software engineering principles. The reactive programming paradigm is proposed as a general solution. However, most reactive languages cannot keep up with C's practical advantages. It appears, that the subfamily of synchronous languages provides promising features but real-world deployments and evaluations are rarely reported in literature. On this account, we make two major contributions in this paper. First, we elaborate how the lack of function-oriented software decomposition manifests in a real-life industrial application. Second, we provide a corresponding re-implementation which illustrates the deployment and discusses the gained engineering benefits provided by the third-party, synchronous-reactive programming language Céu. We believe that our work generally reveals a practicable way of improving embedded software quality in industrial applications.},
  keywords={Programming;Logic gates;Software;Software engineering;Heating systems;Mirrors;Switches;synchronous reactive programming;Céu;software decomposition;software quality},
  doi={10.1109/SEAA.2017.42},
  ISSN={},
  month={Aug},}

@BOOK{10163667,
  author={Camargo, Julio Cesar Bueno de},
  booktitle={OPNsense Beginner to Professional: Protect networks and build next-generation firewalls easily with OPNsense},
  year={2022},
  volume={},
  number={},
  pages={},
  abstract={Work with one of the most efficient open-source FreeBSD-based firewall and routing solutions to secure your network with easeKey FeaturesLearn end-to-end OPNsense firewall implementation and managementDefend against attacks by leveraging third-party plugins such as Nginx and SenseiGrasp hands-on examples and labs to become proficient with OPNsense firewallBook DescriptionOPNsense is one of the most powerful open source firewalls and routing platforms available. With OPNsense, you can now protect networks using features that were only previously available to closed source commercial firewalls. This book is a practical guide to building a comprehensive network defense strategy using OPNsense. You’ll start with the basics, understanding how to install, configure, and protect network resources using native features and additional OPNsense plugins. Next, you’ll explore real-world examples to gain in-depth knowledge of firewalls and network defense. You’ll then focus on boosting your network defense, preventing cyber threats, and improving your knowledge of firewalling using this open source security platform. By the end of this OPNsense book, you’ll be able to install, configure, and manage the OPNsense firewall by making the most of its features.What you will learnUnderstand the evolution of OPNsenseGet up and running with installing and setting up OPNsenseBecome well-versed with firewalling concepts and learn their implementation and practicesDiscover how to apply web browsing controls and website protectionLeverage Sensei to implement next-generation firewall featuresExplore the command-line interface (CLI) and learn the most relevant FreeBSD commandsWho this book is forThis OPNsense firewall book is for system administrators, network administrators, network security professionals, and enthusiasts who wish to build and manage an enterprise-grade firewall using OPNsense. A basic understanding of how a firewall works will be helpful to make the most of this book.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781801814058},
  url={https://ieeexplore.ieee.org/document/10163667},}

@BOOK{10522560,
  author={Wijaya, Adi and Vilares, António},
  booktitle={Data Engineering with Google Cloud Platform: A guide to leveling up as a data engineer by building a scalable data platform with Google Cloud},
  year={2024},
  volume={},
  number={},
  pages={},
  abstract={Become a successful data engineer by building and deploying your own data pipelines on Google Cloud, including making key architectural decisionsKey FeaturesGet up to speed with data governance on Google CloudLearn how to use various Google Cloud products like Dataform, DLP, Dataplex, Dataproc Serverless, and DatastreamBoost your confidence by getting Google Cloud data engineering certification guidance from real exam experiencesPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionThe second edition of Data Engineering with Google Cloud builds upon the success of the first edition by offering enhanced clarity and depth to data professionals navigating the intricate landscape of data engineering. Beyond its foundational lessons, this new edition delves into the essential realm of data governance within Google Cloud, providing you invaluable insights into managing and optimizing data resources effectively. Furthermore, this book helps you stay ahead of the curve by guiding you through the latest technological advancements in the Google Cloud ecosystem. You’ll cover essential aspects, from exploring Cloud Composer 2 to the evolution of Airflow 2.5. Additionally, you’ll explore how to work with cutting-edge tools like Dataform, DLP, Dataplex, Dataproc Serverless, and Datastream to perform data governance on datasets. By the end of this book, you'll be equipped to navigate the ever-evolving world of data engineering on Google Cloud, from foundational principles to cutting-edge practices.What you will learnLoad data into BigQuery and materialize its outputFocus on data pipeline orchestration using Cloud ComposerFormulate Airflow jobs to orchestrate and automate a data warehouseEstablish a Hadoop data lake, generate ephemeral clusters, and execute jobs on the Dataproc clusterHarness Pub/Sub for messaging and ingestion for event-driven systemsApply Dataflow to conduct ETL on streaming dataImplement data governance services on Google CloudWho this book is forData analysts, IT practitioners, software engineers, or any data enthusiasts looking to have a successful data engineering career will find this book invaluable. Additionally, experienced data professionals who want to start using Google Cloud to build data platforms will get clear insights on how to navigate the path. Whether you're a beginner who wants to explore the fundamentals or a seasoned professional seeking to learn the latest data engineering concepts, this book is for you.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781835085363},
  url={https://ieeexplore.ieee.org/document/10522560},}

@BOOK{10162581,
  author={Leung, Adrian},
  booktitle={Rapid Application Development with AWS Amplify: Full stack web development on Amazon Web Servics},
  year={2021},
  volume={},
  number={},
  pages={},
  abstract={Get to grips with the AWS Amplify framework and use it to build scalable cloud-native progressive web apps with React and cross-platform mobile apps with React Native in TypeScriptKey FeaturesExplore the capabilities of AWS Amplify with popular app frameworks for both web and mobile app platformsBuild your first cloud-native web and mobile applications using AWS AmplifyLeverage AWS Amplify to design GraphQL APIs for your web and mobile applicationsBook DescriptionAWS Amplify is a modern toolkit that includes a command line interface (CLI); libraries for JS, iOS, and Android programming; UI component libraries for frameworks like React, Angular, and Vue.js for web development, and React Native and Flutter for mobile development. You'll begin by learning how to build AWS Amplify solutions with React and React Native with TypeScript from scratch, along with integrating it with existing solutions. This book will show you the fastest way to build a production-ready minimum viable product (MVP) within days instead of years. You'll also discover how to increase development speed without compromising on quality by adopting behavior-driven development (BDD) and Cypress for end-to-end test automation, as well as the Amplify build pipeline (DevOps or CI/CD pipeline) to ensure optimal quality throughout continuous test automation and continuous delivery. As you advance, you'll work with React to determine how to build progressive web apps (PWAs) with Amplify and React Native for cross-platform mobile apps. In addition to this, you'll find out how to set up a custom domain name for your new website and set up the AWS Amplify Admin UI for managing the content of your app effectively. By the end of this AWS book, you'll be able to build a full-stack AWS Amplify solution all by yourself.What you will learnBuild React and React Native apps with Amplify and TypeScriptExplore pre-built Amplify UI components for rapid prototypingAdd user management with Amplify authentication to your appUse Amplify GraphQL to create a blog postDiscover how to upload photos to Amplify StorageEnable DevOps with the Amplify pipeline for your appGet to grips with BDD and test automation with Cypress and CucumberSet up a custom domain name for your website and manage app content with the Amplify Admin UIWho this book is forThis book is for developers and tech companies looking to develop cloud-native products rapidly with the AWS ecosystem. Web and mobile developers with little-to-no experience in TypeScript programming will also find this book helpful. Although no prior experience with AWS or TypeScript is required, basic familiarity with modern frameworks such as React and React Native is useful.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781800201446},
  url={https://ieeexplore.ieee.org/document/10162581},}

@INPROCEEDINGS{7323164,
  author={De, Tran Cao},
  booktitle={2015 3rd International Conference on Model-Driven Engineering and Software Development (MODELSWARD)}, 
  title={Applying model-driven development to environment monitoring System}, 
  year={2015},
  volume={},
  number={},
  pages={577-584},
  abstract={Environmental monitoring is critical in understanding whether the quality of our environment is getting better or worse. Information gathered by using an environmental monitoring system is important to make decisions. Vietnam is a vulnerable country of climate change. Specially, in the South of Vietnam, the Mekong delta is known as the region getting the most impact of sea level rise in Vietnam. That leads to a lot of problems making the worst effects to residents in the area, who are mainly still very poor. On the other hand, Vietnam is going on industrialization process that makes a strong effect on the environment. To deal with these challenges, different projects of environment management have been proposed and implemented and many monitoring systems have been built in those projects. Those systems are basically sensor networks with high cost in developing and maintaining. They are related to modern technology such as cloud, communication mobile and wireless. They provide the data for large community for different purposes. Therefore, building such a system is normally a long term project that requires an incremental and modular development for a complex system. This paper, on one hand, represents some common characteristics of an environment monitoring system that requires more study to develop a formal model and a methodology for their specifications, implementations and verification. On the other hand, we would like to adapt the formal model approach proposed for Intelligent Transport Systems (ITS) to an environmental monitoring system. The framework of Baobab is also introduced as an example for transformation from model to code.},
  keywords={Unified modeling language;Cities and towns;Environmental monitoring;Hardware;Mobile communication;Adaptation models;Climate change;Environment Management;Environment Monitoring System;Formal Method;Model-Driven Development;Sensor System},
  doi={},
  ISSN={},
  month={Feb},}

@INBOOK{10830650,
  author={Rahman, Abdul and Redino, Christopher and Nandakumar, Dhruv and Cody, Tyler and Shetty, Sachin and Radke, Dan},
  booktitle={Reinforcement Learning for Cyber Operations: Applications of Artificial Intelligence for Penetration Testing}, 
  title={Index}, 
  year={2025},
  volume={},
  number={},
  pages={253-256},
  abstract={},
  keywords={},
  doi={10.1002/9781394206483.index},
  ISSN={},
  publisher={IEEE},
  isbn={9781394206469},
  url={https://ieeexplore.ieee.org/document/10830650},}

@INBOOK{6544996,
  author={Cambron, G. Keith},
  booktitle={Global Networks: Engineering, Operations and Design}, 
  title={Integration and Innovation}, 
  year={2013},
  volume={},
  number={},
  pages={269-296},
  abstract={This chapter contains sections titled:   Technology Integration   Lifecycle Support   Invention and Innovation   Summary   References   ]]>},
  keywords={Benchmark testing},
  doi={10.1002/9781118394519.ch13},
  ISSN={},
  publisher={IEEE},
  isbn={9781118394526},
  url={https://ieeexplore.ieee.org/document/6544996},}

@INPROCEEDINGS{10332953,
  author={Andrade, Renato and Mascarenhas, Ana Patricia Fontes Magalhaes and Simões, Marco Antonio},
  booktitle={2023 Latin American Robotics Symposium (LARS), 2023 Brazilian Symposium on Robotics (SBR), and 2023 Workshop on Robotics in Education (WRE)}, 
  title={GOBEAT: Towards a Methodology to Support MAS Test Case Definition}, 
  year={2023},
  volume={},
  number={},
  pages={218-223},
  abstract={Multi-Agent Systems (MAS) is a branch of Artificial Intelligence (AI) that works with distributed systems whose components are autonomous entities called agents. The soccer game has been used as a test bed to stimulate research in the MAS. A soccer team, i.e., a MAS, is composed of a group of players, i.e., agents, who should coordinate their actions towards a goal. The testing process in MASs has proven to be challenging. Autonomous agents are programmed to learn during their execution. So, running the same test scenario successively can lead to different results. It makes difficult the application of conventional software testing techniques. This paper proposes the Goal Behavioral Agent Testing (GoBeAT) methodology to assist in specifying the MAS test suite applied to the robot soccer domain. GoBeAT was tested in a case study by a robot soccer team of the Robotic World Cup (RoboCup) and showed positive results in the definition of a test suit.},
  keywords={Software testing;Productivity;Three-dimensional displays;Systematics;Robot kinematics;Behavioral sciences;Planning;MAS;Software Testing;Robot Soccer},
  doi={10.1109/LARS/SBR/WRE59448.2023.10332953},
  ISSN={2643-685X},
  month={Oct},}

@ARTICLE{11059240,
  author={Bruto da Costa, António A. and Irvine, Patrick and Dodoiu, Tudor and Khastgir, Siddartha and Jennings, Paul},
  journal={IEEE Access}, 
  title={Building a Robust Scenario Library for Safety Assurance of Automated Driving Systems: A Review}, 
  year={2025},
  volume={13},
  number={},
  pages={117619-117655},
  abstract={Ensuring the safety of Automated Driving Systems (ADSs) is both a critical and complex endeavor. The increasing demand for autonomous driving technologies underscores the importance of robust safety assurance, yet the intricate nature of these systems presents significant challenges. Scenario-based testing has emerged as a promising approach for ADS safety assurance, but the industry still lacks a comprehensive workflow to effectively implement this process. A pivotal element of scenario-based safety assurance is the creation of a scenario library that thoroughly encompasses the necessary conditions to test ADSs for deployment in specific regions. This paper offers an in-depth analysis of best practices and research in scenario-based safety assurance, aiming to develop a detailed scenario library tailored for testing ADSs within diverse driving conditions. The research addresses the need for scenario creation for the verification and validation (V&V) of ADSs across different environments. The diverse environmental conditions and road traffic behaviors present unique challenges that distinguish one region from another. While every region has its specificities, certain contexts pose particular difficulties in scenario development. This paper outlines the literature review methodology used and presents the current state of scenarios and scenario generation activities. The review synthesizes information from over a hundred sources, including research articles, standards, and best practices. The literature is evaluated across six key areas: Operational Design Domain (ODD), scenario description and representation, data sources and scenario generation methods, scenario selection, scenario assessment and test criteria, and general frameworks for ADS safety assurance. Key contributions, among others, include a structured classification of scenario generation and selection methods, identification of gaps in current practices, and a set of actionable recommendations for future research and regulatory alignment. The paper concludes with recommendations for future work, focusing on the use of scenarios for ADS V&V and proposing a scenario-generation framework tailored to various driving environments.},
  keywords={Safety;Standards;Libraries;Testing;Systematic literature review;Scenario generation;Taxonomy;ISO Standards;Soft sensors;Industries;Automated driving systems (ADSs);scenario;scenario-based testing;safety;safety assurance;verification and validation (V&V);scenario definition language;operational design domain (ODD);behavior;scenario generation},
  doi={10.1109/ACCESS.2025.3584206},
  ISSN={2169-3536},
  month={},}

@BOOK{10163129,
  author={Larsson, Magnus},
  booktitle={Microservices with Spring Boot and Spring Cloud: Build resilient and scalable microservices using Spring Cloud, Istio, and Kubernetes},
  year={2021},
  volume={},
  number={},
  pages={},
  abstract={Create and deploy production-quality microservices-based applications — New edition updated for the smooth running of Spring, Java, Kubernetes, and Istio, with an introduction to Helm 3 and support for Mac and Windows with WSL2Key FeaturesBuild cloud-native production-ready microservices with this comprehensively updated guideUnderstand the challenges of building large-scale microservice architecturesLearn how to get the best out of Spring Cloud, Kubernetes, and Istio in combinationBook DescriptionWant to build and deploy microservices, but don’t know where to start? Welcome to Microservices with Spring Boot and Spring Cloud. This edition features the most recent versions of Spring, Java, Kubernetes, and Istio, demonstrating faster and simpler handling of Spring Boot, local Kubernetes clusters, and Istio installation. The expanded scope includes native compilation of Spring-based microservices, support for Mac and Windows with WSL2, and an introduction to Helm 3 for packaging and deployment. A revamped security chapter now follows the OAuth 2.1 specification and makes use of the newly launched Spring Authorization Server from the Spring team. You’ll start with a set of simple cooperating microservices, then add persistence and resilience, make your microservices reactive, and document their APIs using OpenAPI. Next, you’ll learn how fundamental design patterns are applied to add important functionality, such as service discovery with Netflix Eureka and edge servers with Spring Cloud Gateway. You’ll deploy your microservices using Kubernetes and adopt Istio, then explore centralized log management using the Elasticsearch, Fluentd, and Kibana (EFK) stack, and then monitor microservices using Prometheus and Grafana. By the end of this book, you'll be building scalable and robust microservices using Spring Boot and Spring Cloud.What you will learnBuild reactive microservices using Spring BootDevelop resilient and scalable microservices using Spring CloudUse OAuth 2.1/OIDC and Spring Security to protect public APIsImplement Docker to bridge the gap between development, testing, and productionDeploy and manage microservices with KubernetesApply Istio for improved security, observability, and traffic managementWrite and run automated microservice tests with JUnit, testcontainers, Gradle, and bashWho this book is forIf you’re a Java or Spring Boot developer learning how to build microservice landscapes from scratch, then this book is for you. You don’t need any prior knowledge about microservices architecture to get started, but a solid grasp and enough experience in Java and Spring Boot to build apps autonomously is a must.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781801079150},
  url={https://ieeexplore.ieee.org/document/10163129},}

@BOOK{8824174,
  author={Ringer, Talia and Palmskog, Karl and Sergey, Ilya and Gligoric, Milos and Tatlock, Zachary},
  booktitle={QED at Large: A Survey of Engineering of Formally Verified Software},
  year={2019},
  volume={},
  number={},
  pages={},
  abstract={Development of formal proofs of correctness of programs can increase actual and perceived reliability and facilitate better understanding of program specifications and their underlying assumptions. Tools supporting such development have been available for over 40 years but have only recently seen wide practical use. Projects based on construction of machine-checked formal proofs are now reaching an unprecedented scale, comparable to large software projects, which leads to new challenges in proof development and maintenance. Despite its increasing importance, the field of proof engineering is seldom considered in its own right; related theories, techniques, and tools span many fields and venues. QED at Large covers the timeline and research literature concerning proof development for program verification, including theories, languages, and tools. It emphasizes challenges and breakthroughs at each stage in history and highlights challenges that are currently present due to the increasing scale of proof developments. This monograph is intended for use by researchers and students who are new to the field. It provides the reader with an insightful overview of the work that has led to modern-day techniques for formally verifying software. In times of increasing automation, this underpins many software systems so future trends are also highlighted.},
  keywords={},
  doi={10.1561/2500000045},
  ISSN={},
  publisher={now},
  isbn={9781680835953},
  url={https://ieeexplore.ieee.org/document/8824174},}

@ARTICLE{4068343,
  author={},
  journal={IEEE Std 1175.2-2006}, 
  title={IEEE Recommended Practice for CASE Tool Interconnection - Characterization of Interconnections}, 
  year={2007},
  volume={},
  number={},
  pages={1-45},
  abstract={This recommended practice describes interconnections that need to be understood and evaluated when buying, building, testing, or using computer-aided software engineering (CASE) tools. CASE tools are developed for use in creating computing systems. By assisting users to reach a clear understanding of the context of operation for a computing system tool, this recommended practice contributes to the effective implementation and application of computing system tools. This recommended practice does not describe the processes of evaluating, acquiring, or adopting CASE tools. This recommended practice is limited to the technical aspects of CASE tools. It does not include issues in the management, marketing, or training domains.},
  keywords={IEEE standards;Context;Computer aided software engineering;Trademarks;Standards Board;Patents;Computer-Aided Software Engineering (CASE) tools;tool communications;tool interconnections},
  doi={10.1109/IEEESTD.2007.288641},
  ISSN={},
  month={Jan},}

@BOOK{10163634,
  author={Brown, Donovan and Garverick, Joshua and McIver, Omar Dean},
  booktitle={Implementing Event-Driven Microservices Architecture in .NET 7: Develop event-based distributed apps that can scale with ever-changing business demands using C# 11 and .NET 7},
  year={2023},
  volume={},
  number={},
  pages={},
  abstract={Implement modern design patterns that leverage domain-driven data, to achieve resiliency and scalability for data-dependent applicationsKey FeaturesLearn the tenets of event-driven architecture, coupled with reliable design patterns to enhance your knowledge of distributed systems and build a foundation for professional growthUnderstand how to translate business goals and drivers into a domain model that can be used to develop an app that enables those goals and driversIdentify areas to enhance development and ensure operational support through the architectural design processBook DescriptionThis book will guide you through various hands-on practical examples for implementing event-driven microservices architecture using C# 11 and .NET 7. It has been divided into three distinct sections, each focusing on different aspects of this implementation. The first section will cover the new features of .NET 7 that will make developing applications using EDA patterns easier, the sample application that will be used throughout the book, and how the core tenets of domain-driven design (DDD) are implemented in .NET 7. The second section will review the various components of a local environment setup, the containerization of code, testing, deployment, and the observability of microservices using an EDA approach. The third section will guide you through the need for scalability and service resilience within the application, along with implementation details related to elastic and autoscale components. You’ll also cover how proper telemetry helps to automatically drive scaling events. In addition, the topic of observability is revisited using examples of service discovery and microservice inventories. By the end of this book, you’ll be able to identify and catalog domains, events, and bounded contexts to be used for the design and development of a resilient microservices architecture.What you will learnExplore .NET 7 and how it enables the development of applications using EDAUnderstand messaging protocols and producer/consumer patterns and how to implement them in .NET 7Test and deploy applications written in .NET 7 and designed using EDA principlesAccount for scaling and resiliency in microservicesCollect and learn from telemetry at the platform and application levelGet to grips with the testing and deployment of microservicesWho this book is forThis book will help .NET developers and architects looking to leverage or pivot to microservices while using a domain-driven event model.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781803230405},
  url={https://ieeexplore.ieee.org/document/10163634},}

@INPROCEEDINGS{10764818,
  author={Cao, Shaoheng and Chen, Renyi and Pan, Minxue and Yang, Wenhua and Li, Xuandong},
  booktitle={2024 39th IEEE/ACM International Conference on Automated Software Engineering (ASE)}, 
  title={Beyond Manual Modeling: Automating GUI Model Generation Using Design Documents}, 
  year={2024},
  volume={},
  number={},
  pages={91-103},
  abstract={GUI models encapsulate the desired visual appearance and interactive behaviors of applications, facilitating various downstream tasks like model-based testing (MBT). Manually constructing high-quality GUI models is not only labor-intensive and costly but also prone to errors, particularly as applications evolve and require frequent model updates. Existing automated approaches for GUI model generation heavily rely on reverse engineering, where the models are abstractions of the code. As a result, they are not suitable for MBT to test functional issues because they are consistent with the code. Meanwhile, valuable development artifacts such as UI/UX design documents, which reflect design intentions, are often overlooked. In this paper, a novel approach named DemGen is proposed to seek a unique pathway for GUI model generation. Leveraging design documents, DemGen employs computer vision pre-trained models in conjunction with a rule-based correction mechanism to identify GUI elements and their intended behaviors as defined in those documents. Subsequently, the identified content is transformed into a formal GUI model adhering to the IFML modeling language. Our evaluation, conducted in collaboration with an industry partner on commercial applications, demonstrates the effectiveness and efficiency of DemGen in GUI element recognition and GUI model generation. Moreover, we conducted a comparative analysis of manual, automated, and hybrid modeling techniques, assessing the usefulness of generated models on MBT tasks.CCS CONCEPTS• Software and its engineering → Model-driven software engineering; • Human-centered computing → Graphical user interfaces.},
  keywords={Industries;Analytical models;Visualization;Codes;Computational modeling;Collaboration;Manuals;Graphical user interfaces;Testing;Software engineering;GUI model generation;Model-driven engineering;Model-based testing},
  doi={},
  ISSN={2643-1572},
  month={Oct},}

@BOOK{9100418,
  author={Schulmeyer, G.},
  booktitle={Handbook of Software Quality Assurance, Fourth Edition},
  year={2007},
  volume={},
  number={},
  pages={},
  abstract={This thoroughly revised fourth edition of the popular book, Handbook of Software Quality Assurance, brings together the latest SQA (software quality assurance) methods, recognizing the importance of CMMI and the ISO 900-3 standard. This unique book offers you a wide spectrum of experiences and issues presented in papers from leading experts in SQA, DQA (development quality assurance), and software development and management. The fourth edition is a significant update to past editions, bringing you the very latest on current best practices in the field. You learn the role of SQA/DQA with regard to ISO 9001-2000 requirements and the criteria from the Software Engineering Institute for the various levels of CMMI. You also find an updated discussion on the American Society for Quality (ASQ) SQA certification program, covering the benefits of becoming an ASQ certified software quality engineer. This practical resource shows you how to move an organization from CMMI software quality assurance compliance to developmental quality assurance compliance. The book covers the commercial standards and modern development methods of SQA and DQA, and details how SQA can be implemented in organizations large and small. This volume also helps you better understand the requirements of the ASQ's CSQE examination. From quality management concepts for IT, teaching SQA in an industrial environment, and the inspection process, to the impact of SQA certification on the hiring process, software quality metrics recommendations, and software reliability, this invaluable book serves as your a one-stop resource for complete and current software quality assurance knowledge.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Artech},
  isbn={9781596931879},
  url={https://ieeexplore.ieee.org/document/9100418},}

@BOOK{10162687,
  author={Van Horn II, Bruce M. and Symons, Van},
  booktitle={Real-World Implementation of C# Design Patterns: Overcome daily programming challenges using elements of reusable object-oriented software},
  year={2022},
  volume={},
  number={},
  pages={},
  abstract={Build robust applications in C# easily using effective and popular design patterns and best practicesKey FeaturesRecognize solutions to common problems in software design with C#Explore real-world applications of design patterns that can be used in your everyday workGet to grips with 14 patterns and their design implementationsBook DescriptionAs a software developer, you need to learn new languages and simultaneously get familiarized with the programming paradigms and methods of leveraging patterns, as both a communications tool and an advantage when designing well-written, easy-to-maintain code. Design patterns, being a collection of best practices, provide the necessary wisdom to help you overcome common sets of challenges in object-oriented design and programming. This practical guide to design patterns helps C# developers put their programming knowledge to work. The book takes a hands-on approach to introducing patterns and anti-patterns, elaborating on 14 patterns along with their real-world implementations. Throughout the book, you'll understand the implementation of each pattern, as well as find out how to successfully implement those patterns in C# code within the context of a real-world project. By the end of this design patterns book, you’ll be able to recognize situations that tempt you to reinvent the wheel, and quickly avoid the time and cost associated with solving common and well-understood problems with battle-tested design patterns.What you will learnGet to grips with patterns, and discover how to conceive and document themExplore common patterns that may come up in your everyday workRecognize common anti-patterns early in the processUse creational patterns to create flexible and robust object structuresEnhance class designs with structural patternsSimplify object interaction and behavior with behavioral patternsWho this book is forThis book is for beginner and mid-level software developers who are looking to take their object-oriented programs or software designing skills to the next level by learning to leverage common patterns. A firm grasp of programming fundamentals and classical object-oriented programming (OOP) using languages like C#, C++, Objective-C, or Java is expected.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781803247953},
  url={https://ieeexplore.ieee.org/document/10162687},}

@INBOOK{6542353,
  author={Cambron, G. Keith},
  booktitle={Global Networks: Engineering, Operations and Design}, 
  title={Disasters and Outages}, 
  year={2013},
  volume={},
  number={},
  pages={297-316},
  abstract={This chapter contains sections titled:   Disasters   Outages   The Vicious Cycle   Summary   References   ]]>},
  keywords={Tornadoes;Earthquakes;Copper;Storms;Power cables;Bandwidth;Organizations},
  doi={10.1002/9781118394519.ch14},
  ISSN={},
  publisher={IEEE},
  isbn={9781118394526},
  url={https://ieeexplore.ieee.org/document/6542353},}

@INPROCEEDINGS{7321627,
  author={Bicevska, Zane and Bicevskis, Janis and Oditis, Ivo},
  booktitle={2015 Federated Conference on Computer Science and Information Systems (FedCSIS)}, 
  title={Smart technologies for improved software maintenance}, 
  year={2015},
  volume={},
  number={},
  pages={1533-1538},
  abstract={Steadily increasing complexity of software systems makes them difficult to configure and use without special IT knowledge. One of the solutions is to improve software systems making them “smarter”, i.e. to supplement software systems with features of self-management, at least partially. This paper describes several software components known as smart technologies, which facilitate software use and maintenance. As to date smart technologies incorporate version updating, execution environment testing, self-testing, runtime verification and business process execution. The proposed approach has been successfully applied in several software projects.},
  keywords={Software;Information systems;Business;Built-in self-test;Runtime;Complexity theory;Autonomic computing;smart technologies;self-managing systems;software maintenance},
  doi={10.15439/2015F170},
  ISSN={},
  month={Sep.},}

@INPROCEEDINGS{7878375,
  author={Fangohr, Hans and Albert, Maximilian and Franchin, Matteo},
  booktitle={2016 IEEE/ACM International Workshop on Software Engineering for Science (SE4Science)}, 
  title={Nmag Micromagnetic Simulation Tool — Software Engineering Lessons Learned}, 
  year={2016},
  volume={},
  number={},
  pages={1-7},
  abstract={We review design and development decisions and their impact for the open source code Nmag from a software engineering in computational science point of view. We summarise lessons learned and recommendations for future computational science projects. Key lessons include that encapsulating the simulation functionality in a library of a general purpose language, here Python, provides great flexibility in using the software. The choice of Python for the top-level user interface was very well received by users from the science and engineering community. The from-source installation in which required external libraries and dependencies are compiled from a tarball was remarkably robust. In places, the code is a lot more ambitious than necessary, which introduces unnecessary complexity and reduces main- tainability. Tests distributed with the package are useful, although more unit tests and continuous integration would have been desirable. The detailed documentation, together with a tutorial for the usage of the system, was perceived as one of its main strengths by the community.},
  keywords={Libraries;Software;Mathematical model;Computational modeling;Software engineering;Micromagnetics;Magnetization;Nmag;Computational Science Software Engineering;Python;Finite Elements},
  doi={},
  ISSN={},
  month={May},}

@BOOK{10769219,
  author={Chow, Dennis and Bruskin, David},
  booktitle={Automating Security Detection Engineering: A hands-on guide to implementing Detection as Code},
  year={2024},
  volume={},
  number={},
  pages={},
  abstract={Accelerate security detection development with AI-enabled technical solutions using threat-informed defenseKey FeaturesCreate automated CI/CD pipelines for testing and implementing threat detection use casesApply implementation strategies to optimize the adoption of automated work streamsUse a variety of enterprise-grade tools and APIs to bolster your detection programPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionToday's global enterprise security programs grapple with constantly evolving threats. Even though the industry has released abundant security tools, most of which are equipped with APIs for integrations, they lack a rapid detection development work stream. This book arms you with the skills you need to automate the development, testing, and monitoring of detection-based use cases. You’ll start with the technical architecture, exploring where automation is conducive throughout the detection use case lifecycle. With the help of hands-on labs, you’ll learn how to utilize threat-informed defense artifacts and then progress to creating advanced AI-powered CI/CD pipelines to bolster your Detection as Code practices. Along the way, you'll develop custom code for EDRs, WAFs, SIEMs, CSPMs, RASPs, and NIDS. The book will also guide you in developing KPIs for program monitoring and cover collaboration mechanisms to operate the team with DevSecOps principles. Finally, you'll be able to customize a Detection as Code program that fits your organization's needs. By the end of the book, you'll have gained the expertise to automate nearly the entire use case development lifecycle for any enterprise.What you will learnUnderstand the architecture of Detection as Code implementationsDevelop custom test functions using Python and TerraformLeverage common tools like GitHub and Python 3.x to create detection-focused CI/CD pipelinesIntegrate cutting-edge technology and operational patterns to further refine program efficacyApply monitoring techniques to continuously assess use case healthCreate, structure, and commit detections to a code repositoryWho this book is forThis book is for security engineers and analysts responsible for the day-to-day tasks of developing and implementing new detections at scale. If you’re working with existing programs focused on threat detection, you’ll also find this book helpful. Prior knowledge of DevSecOps, hands-on experience with any programming or scripting languages, and familiarity with common security practices and tools are recommended for an optimal learning experience.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781837631421},
  url={https://ieeexplore.ieee.org/document/10769219},}

@BOOK{10769402,
  author={Nasslahsen, Badr},
  booktitle={Spring Security: Effectively secure your web apps, RESTful services, cloud apps, and microservice architectures},
  year={2024},
  volume={},
  number={},
  pages={},
  abstract={Leverage the power of Spring Security 6 to protect your modern Java applications from hackersKey FeaturesArchitect solutions that leverage Spring Security while remaining loosely coupledImplement authentication and authorization with SAML2, OAuth 2, hashing, and encryption algorithmsIntegrate Spring Security with technologies such as microservices, Kubernetes, the cloud, and GraalVM native imagesPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionWith experienced hackers constantly targeting apps, properly securing them becomes challenging when you integrate this factor with legacy code, new technologies, and other frameworks. Written by a Lead Cloud and Security Architect as well as CISSP, this book helps you easily secure your Java apps with Spring Security, a trusted and highly customizable authentication and access control framework. The book shows you how to implement different authentication mechanisms and properly restrict access to your app. You’ll learn to integrate Spring Security with popular web frameworks like Thymeleaf and Microservice and Cloud services like Zookeeper and Eureka, along with architecting solutions that leverage its full power while staying loosely coupled. You’ll also see how Spring Security defends against session fixation, moves into concurrency control, and how you can use session management for administrative functions. This fourth edition aligns with Java 17/21 and Spring Security 6, covering advanced security scenarios for RESTful web services and microservices. This ensures you fully understand the issues surrounding stateless authentication and discover a concise approach to solving those issues. By the end of this book, you’ll be able to integrate Spring Security 6 with GraalVM native images seamlessly, from start to finish.What you will learnUnderstand common security vulnerabilities and how to resolve themImplement authentication and authorization and learn how to map users to rolesIntegrate Spring Security with LDAP, Kerberos, SAML 2, OpenID, and OAuthGet to grips with the security challenges of RESTful web services and microservicesConfigure Spring Security to use Spring Data for authenticationIntegrate Spring Security with Spring Boot, Spring Data, and web applicationsProtect against common vulnerabilities like XSS, CSRF, and ClickjackingWho this book is forIf you’re a Java web developer or an architect with fundamental knowledge of Java 17/21, web services, and the Spring Framework, this book is for you. No previous experience with Spring Security is needed to get started with this book.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781835460115},
  url={https://ieeexplore.ieee.org/document/10769402},}

@BOOK{10251263,
  author={Sayfan, Gigi and Ibryam, Bilgin},
  booktitle={Mastering Kubernetes: Dive into Kubernetes and learn how to create and operate world-class cloud-native systems},
  year={2023},
  volume={},
  number={},
  pages={},
  abstract={Go beyond the basics of Kubernetes and explore more advanced concepts, including Kubernetes in production, governance, serverless computing, and service meshes. Purchase of the print or Kindle book includes a free eBook in PDF format.Key FeaturesMaster Kubernetes architecture and design to build, deploy, and secure large-scale distributed systemsLearn advanced concepts like autoscaling, multi-cluster management, serverless computing, service meshes and policy enginesExplore Kubernetes 1.25 and its rich ecosystem of tools like Kubectl, Krew, K9s, Lens, and HelmBook DescriptionThe fourth edition of the bestseller Mastering Kubernetes includes the most recent tools and code to enable you to learn the latest features of Kubernetes 1.25. This book contains a thorough exploration of complex concepts and best practices to help you master the skills of designing and deploying large-scale distributed systems on Kubernetes clusters. You’ll learn how to run complex stateless and stateful microservices on Kubernetes, including advanced features such as horizontal pod autoscaling, rolling updates, resource quotas, and persistent storage backends. In addition, you’ll understand how to utilize serverless computing and service meshes. Further, two new chapters have been added. “Governing Kubernetes” covers the problem of policy management, how admission control addresses it, and how policy engines provide a powerful governance solution. “Running Kubernetes in Production” shows you what it takes to run Kubernetes at scale across multiple cloud providers, multiple geographical regions, and multiple clusters, and it also explains how to handle topics such as upgrades, capacity planning, dealing with cloud provider limits/quotas, and cost management. By the end of this Kubernetes book, you’ll have a strong understanding of, and hands-on experience with, a wide range of Kubernetes capabilities.What you will learnLearn how to govern Kubernetes using policy enginesLearn what it takes to run Kubernetes in production and at scaleBuild and run stateful applications and complex microservicesMaster Kubernetes networking with services, Ingress objects, load balancers, and service meshesAchieve high availability for your Kubernetes clustersImprove Kubernetes observability with tools such as Prometheus, Grafana, and JaegerExtend Kubernetes with the Kubernetes API, plugins, and webhooksWho this book is forIf you're a system administrator or cloud developer who wants to become comfortable with Kubernetes and would like to master its advanced features, then this book is for you. Sofware and DevOps engineers with a working knowledge of Kubernetes, as well as technical managers of Kubernetes-based systems, will also find this book useful. Those deciding on whether to migrate to Kubernetes and are curious about its inner workings will find plenty of answers here as well. Basic familiarity with networking concepts will prove beneficial.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781804614754},
  url={https://ieeexplore.ieee.org/document/10251263},}

@ARTICLE{4040327,
  author={},
  journal={IEEE Std P487/Draft6}, 
  title={IEEE Draft Recommended Practice for the Protection of Wire-Line Communication Facilities Serving Electric Supply Locations}, 
  year={2006},
  volume={},
  number={},
  pages={},
  abstract={This recommended practice presents engineering design practices for special high-voltage protection systems intended to protect wire-line telecommunication facilities serving electric supply locations. The following topics are included in this document: a) A description of the electric supply locations environment, i.e., ground potential rise (GPR), induced voltages, lightning, and switching transients; b)A discussion of special high-voltage protection devices; c)Definitions of service types and service performance objectives for electric supply locations telecommunication services; d)Special protection theory and philosophy; e)Special protection system design guidelines; f)Personnel safety considerations; g)Grounding; h)Cables with metallic members. Other telecommunication alternatives such as radio and optica fiber systems are excluded from this document.},
  keywords={IEEE Standards;Standards;Safety;Power transformer insulation;High-voltage techniques;Communications technology;Optical fiber cables},
  doi={},
  ISSN={},
  month={},}

@BOOK{10121010,
  author={Horner, Larry and Tutschku, Kurt and Fumagalli, Andrea and Ramanathan, ShunmugaPriya},
  booktitle={Virtualizing 5G and Beyond 5G Mobile Network},
  year={2023},
  volume={},
  number={},
  pages={},
  abstract={The fifth generation (5G) mobile network brings significant new capacity and opportunity to network operators while also creating new challenges and additional pressure to build and operate networks differently. The transformation to 5G mobile networks creates the opportunity to virtualize significant portions of the radio access (RAN) and network core, allowing operators to better compete with over-the-top and hyperscaler offerings. This book covers the business and technical areas of virtualization that enable the transformation and innovation that today’s operators are seeking. It identifies forward-looking gaps where the technology continues to develop, specifically packet acceleration and timing requirements, which today are still not fully virtualized. The book shows you the operational and support considerations, development and lifecycle management, business implications, and vendor-team dynamics involved in deploying a virtualized network. Packed with key concepts of virtualization that solve a broad array of problems, this is an essential reference for those entering this technical domain, those that are going to build and operate these networks, and those that are seeking to learn more about the telecom network. It illustrates why you just can’t do it all in the cloud today.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Artech},
  isbn={9781630819316},
  url={https://ieeexplore.ieee.org/document/10121010},}

@BOOK{10163673,
  author={Buelta, Jaime},
  booktitle={Python Architecture Patterns: Master API design, event-driven structures, and package management in Python},
  year={2022},
  volume={},
  number={},
  pages={},
  abstract={Make the best of your test suites by using cutting-edge software architecture patterns in PythonKey FeaturesLearn how to create scalable and maintainable applicationsBuild a web system for micro messaging using concepts in the bookUse profiling to find bottlenecks and improve the speed of the systemBook DescriptionDeveloping large-scale systems that continuously grow in scale and complexity requires a thorough understanding of how software projects should be implemented. Software developers, architects, and technical management teams rely on high-level software design patterns such as microservices architecture, event-driven architecture, and the strategic patterns prescribed by domain-driven design (DDD) to make their work easier. This book covers these proven architecture design patterns with a forward-looking approach to help Python developers manage application complexity—and get the most value out of their test suites. Starting with the initial stages of design, you will learn about the main blocks and mental flow to use at the start of a project. The book covers various architectural patterns like microservices, web services, and event-driven structures and how to choose the one best suited to your project. Establishing a foundation of required concepts, you will progress into development, debugging, and testing to produce high-quality code that is ready for deployment. You will learn about ongoing operations on how to continue the task after the system is deployed to end users, as the software development lifecycle is never finished. By the end of this Python book, you will have developed "architectural thinking": a different way of approaching software design, including making changes to ongoing systems.What you will learnThink like an architect, analyzing software architecture patternsExplore API design, data storage, and data representation methodsInvestigate the nuances of common architectural structuresUtilize and interoperate elements of patterns such as microservicesImplement test-driven development to perform quality code testingRecognize chunks of code that can be restructured as packagesMaintain backward compatibility and deploy iterative changesWho this book is forThis book will help software developers and architects understand the structure of large complex systems and adopt architectural patterns that are scalable. Examples in the book are implemented in Python so a fair grasp of basic Python concepts is expected. Proficiency in any programming languages such as Java or JavaScript is sufficient.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781801811774},
  url={https://ieeexplore.ieee.org/document/10163673},}

@BOOK{10162507,
  author={Evans, Jeremy},
  booktitle={Polished Ruby Programming: Build better software with more intuitive, maintainable, scalable, and high-performance Ruby code},
  year={2021},
  volume={},
  number={},
  pages={},
  abstract={Elevate your Ruby skills to an advanced level by deepening your understanding of the design principles, best practices, and trade-offs involved in implementation approaches to future-proof your Ruby applications Key FeaturesLearn Ruby web application design principles and strategies for databases, security, and testing from a Ruby committerUnderstand the design principles behind polished Ruby code and trade-offs between implementation approachesUse metaprogramming and DSLs to reduce the amount of code needed without decreasing maintainabilityBook DescriptionAnyone striving to become an expert Ruby programmer needs to be able to write maintainable applications. Polished Ruby Programming will help you get better at designing scalable and robust Ruby programs, so that no matter how big the codebase grows, maintaining it will be a breeze. This book takes you on a journey through implementation approaches for many common programming situations, the trade-offs inherent in each approach, and why you may choose to use different approaches in different situations. You'll start by refreshing Ruby fundamentals, such as correctly using core classes, class and method design, variable usage, error handling, and code formatting. Then you'll move on to higher-level programming principles, such as library design, use of metaprogramming and domain-specific languages, and refactoring. Finally, you'll learn principles specific to web application development, such as how to choose a database and web framework, and how to use advanced security features. By the end of this Ruby programming book, you’ll be a well rounded web developer with a deep understanding of Ruby. While most code examples and principles discussed in the book apply to all Ruby versions, some examples and principles are specific to Ruby 3.0, the latest release at the time of publication.What you will learnUse Ruby's core classes and design custom classes effectivelyExplore the principles behind variable usage and method argument choiceImplement advanced error handling approaches such as exponential backoffDesign extensible libraries and plugin systems in RubyUse metaprogramming and DSLs to avoid code redundancyImplement different approaches to testing and understand their trade-offsDiscover design patterns, refactoring, and optimization with RubyExplore database design principles and advanced web app securityWho this book is forThis book is for Ruby programmers who are comfortable in coding with Ruby but want to advance their skills by mastering the deeper principles and best practices behind writing maintainable, scalable, optimized, and well-structured Ruby code. This book won’t teach you the basics of Ruby – you’ll need intermediate knowledge and practical experience before you can dive in.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781801077910},
  url={https://ieeexplore.ieee.org/document/10162507},}

@BOOK{10162332,
  author={Paro, Alberto},
  booktitle={Elasticsearch 8.x Cookbook: Over 180 recipes to perform fast, scalable, and reliable searches for your enterprise},
  year={2022},
  volume={},
  number={},
  pages={},
  abstract={Search, analyze, store and manage data effectively with Elasticsearch 8.xKey FeaturesExplore the capabilities of Elasticsearch 8.x with easy-to-follow recipesExtend the Elasticsearch functionalities and learn how to deploy on Elastic CloudDeploy and manage simple Elasticsearch nodes as well as complex cluster topologiesBook DescriptionElasticsearch is a Lucene-based distributed search engine at the heart of the Elastic Stack that allows you to index and search unstructured content with petabytes of data. With this updated fifth edition, you'll cover comprehensive recipes relating to what's new in Elasticsearch 8.x and see how to create and run complex queries and analytics. The recipes will guide you through performing index mapping, aggregation, working with queries, and scripting using Elasticsearch. You'll focus on numerous solutions and quick techniques for performing both common and uncommon tasks such as deploying Elasticsearch nodes, using the ingest module, working with X-Pack, and creating different visualizations. As you advance, you'll learn how to manage various clusters, restore data, and install Kibana to monitor a cluster and extend it using a variety of plugins. Furthermore, you'll understand how to integrate your Java, Scala, Python, and big data applications such as Apache Spark and Pig with Elasticsearch and create efficient data applications powered by enhanced functionalities and custom plugins. By the end of this Elasticsearch cookbook, you'll have gained in-depth knowledge of implementing the Elasticsearch architecture and be able to manage, search, and store data efficiently and effectively using Elasticsearch.What you will learnBecome well-versed with the capabilities of X-PackOptimize search results by executing analytics aggregationsGet to grips with using text and numeric queries as well as relationship and geo queriesInstall Kibana to monitor clusters and extend it for pluginsBuild complex queries by managing indices and documentsMonitor the performance of your cluster and nodesDesign advanced mapping to take full control of index stepsIntegrate Elasticsearch in Java, Scala, Python, and big data applicationsWho this book is forIf you’re a software engineer, big data infrastructure engineer, or Elasticsearch developer, you'll find this Elasticsearch book useful. The book will also help data professionals working in e-commerce and FMCG industries who use Elastic for metrics evaluation and search analytics to gain deeper insights and make better business decisions. Prior experience with Elasticsearch will help you get the most out of this book.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781801072885},
  url={https://ieeexplore.ieee.org/document/10162332},}

@BOOK{9100206,
  author={Lehpamer, Harvey},
  booktitle={Transmission Systems Design Handbook for Wireless Networks},
  year={2002},
  volume={},
  number={},
  pages={},
  abstract={This practical new resource gives you a comprehensive understanding of the design and deployment of transmission networks for wireless applications. From principles and design, to equipment procurement, project management, testing, and operation, it's a practical, hands-on engineering guide with numerous real-life examples of turn-key operations in the wireless networking industry. This book, written for both technical and non-technical professionals, helps you deal with the costs and difficulties involved in setting up the local access with technologies that are still in the evolutionary stage. Issues involved in the deployment of various transmission technologies, and their impact on the overall wireless network topology are discussed. Strategy and approach to transmission network planning, design and deployment are explored. The book offers practical guidelines and advice derived from the author's own experience on projects worldwide. You gain a solid grounding in third generation wireless networks with increased capacity requirements, while learning about packet data architecture, and how it will impact future transmission network design and deployment.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Artech},
  isbn={9781580535540},
  url={https://ieeexplore.ieee.org/document/9100206},}

@BOOK{9100210,
  author={Hassler, Vesna},
  booktitle={Security Fundamentals for E-Commerce},
  year={2000},
  volume={},
  number={},
  pages={},
  abstract={If you're charged with maintaining the security of e-commerce sites, you need this unique book that provides an in-depth understanding of basic security problems and relevant e-commerce solutions, while helping you implement today's most advanced security technologies. From designing secure Web, e-commerce, and mobile commerce applications - to securing your internal network - to providing secure employee/user authentication, this cutting-edge book gives you a valuable security perspective you won't find in other resources. Flexibly structured to give you a comprehensive overview or to help you quickly pinpoint topics of immediate concern, the book includes sections on basic security mechanisms, the specific requirements of electronic payment systems, address communication security, and Web- and Java-related security issues. A full section is devoted to the security aspects of code and customer mobility, specifically mobile agents, mobile devices, and smart cards. Over 70 illustrations help clarify important points throughout the book.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Artech},
  isbn={9781580534062},
  url={https://ieeexplore.ieee.org/document/9100210},}

@ARTICLE{4040044,
  author={},
  journal={IEEE Std P1073.0.1.1/D01J}, 
  title={Unapproved IEEE Draft Guide for Health Informaticspoint-Of-Care Medical Device Communicationtechnical Reportguidelines for the Use of RF Wireless Technology}, 
  year={2006},
  volume={},
  number={},
  pages={},
  abstract={},
  keywords={Wireless communication;Communication system security;Standards;Radio frequency;IEEE Standards;Wireless personal area networks;Patents},
  doi={},
  ISSN={},
  month={},}

@INPROCEEDINGS{11029727,
  author={Zhang, Changjian and Kapoor, Parv and Dardik, Ian and Cui, Leyi and Meira-Góes, Rômulo and Garlan, David and Kang, Eunsuk},
  booktitle={2025 IEEE/ACM 47th International Conference on Software Engineering (ICSE)}, 
  title={Constrained LTL Specification Learning from Examples}, 
  year={2025},
  volume={},
  number={},
  pages={629-641},
  abstract={Temporal logic specifications play an important role in a wide range of software analysis tasks, such as model checking, automated synthesis, program comprehension, and runtime monitoring. Given a set of positive and negative examples, specified as traces, LTL learning is the problem of synthesizing a specification, in linear temporal logic (LTL), that evaluates to true over the positive traces and false over the negative ones. In this paper, we propose a new type of LTL learning problem called constrained LTL learning, where the user, in addition to positive and negative examples, is given an option to specify one or more constraints over the properties of the LTL formula to be learned. We demonstrate that the ability to specify these additional constraints significantly increases the range of applications for LTL learning, and also allows efficient generation of LTL formulas that satisfy certain desirable properties (such as minimality). We propose an approach for solving the constrained LTL learning problem through an encoding in first-order relational logic and reduction to an instance of the maximal satisfiability (MaxSAT) problem. An experimental evaluation demonstrates that ATLAS, an implementation of our proposed approach, is able to solve new types of learning problems while performing better than or competitively with the state-of-the-art tools in LTL learning.},
  keywords={Runtime;Model checking;Software;Encoding;Logic;Monitoring;linear temporal logic;ltl learning;formal methods;model-driven engineering;formal specifications;requirements engineering;modeling tools},
  doi={10.1109/ICSE55347.2025.00160},
  ISSN={1558-1225},
  month={April},}

@INPROCEEDINGS{7784207,
  author={Sroka, Michal and Fisch, Dominik and Nagy, Roman},
  booktitle={2016 6th International Conference on Information Communication and Management (ICICM)}, 
  title={Localised mutation in evolutionary test model learning}, 
  year={2016},
  volume={},
  number={},
  pages={13-18},
  abstract={The focus of this paper is on automation of test case design via model-based testing for automotive embedded software. A method based on an evolutionary algorithm for acquiring the necessary test model automatically from sample test cases and additional sources of information is described. In an experiment the impact of localised mutational changes on the evolutionary learning method is investigated.},
  keywords={Biological cells;Sociology;Statistics;Testing;Software;Algorithm design and analysis;Software algorithms;model-based testing;evolutionary test model learning;localised mutation;reproduction operator},
  doi={10.1109/INFOCOMAN.2016.7784207},
  ISSN={},
  month={Oct},}

@BOOK{10162621,
  author={Shrivastava, Saurabh and Srivastav, Neelanjali and Sheth, Rajesh and Karmarkar, Rohan and Arora, Kamal},
  booktitle={Solutions Architect’s Handbook: Kick-start your career as a solutions architect by learning architecture design principles and strategies},
  year={2022},
  volume={},
  number={},
  pages={},
  abstract={From fundamentals and design patterns to the different strategies for creating secure and reliable architectures in AWS cloud, learn everything you need to become a successful solutions architect. Purchase of the print or Kindle book includes a free eBook in the PDF format.Key Features(Replaced with endorsements, so these 3 bullets are not part of the actual PI)Turn business needs into end-to-end technical architectures with this practical guideAssess and overcome various challenges while updating or modernizing legacy applicationsFuture-proof your architecture with IoT, machine learning, and quantum computingBook DescriptionBecoming a solutions architect requires a hands-on approach, and this edition of the Solutions Architect's Handbook brings exactly that. This handbook will teach you how to create robust, scalable, and fault-tolerant solutions and next-generation architecture designs in a cloud environment. It will also help you build effective product strategies for your business and implement them from start to finish. This new edition features additional chapters on disruptive technologies, such as Internet of Things (IoT), quantum computing, data engineering, and machine learning. It also includes updated discussions on cloud-native architecture, blockchain data storage, and mainframe modernization with public cloud. The Solutions Architect's Handbook provides an understanding of solution architecture and how it fits into an agile enterprise environment. It will take you through the journey of solution architecture design by providing detailed knowledge of design pillars, advanced design patterns, anti-patterns, and the cloud-native aspects of modern software design. By the end of this handbook, you'll have learned the techniques needed to create efficient architecture designs that meet your business requirements.What you will learnExplore the various roles of a solutions architect in the enterprise landscapeImplement key design principles and patterns to build high-performance cost-effective solutionsChoose the best strategies to secure your architectures and increase their availabilityModernize legacy applications with the help of cloud integrationUnderstand how big data processing, machine learning, and IoT fit into modern architectureIntegrate a DevOps mindset to promote collaboration, increase operational efficiency, and streamline productionWho this book is forThis book is for software developers, system engineers, DevOps engineers, architects, and team leaders who already work in the IT industry and aspire to become solutions architect professionals. Existing solutions architects who want to expand their skillset or get a better understanding of new technologies will also learn valuable new skills. To get started, you'll need a good understanding of the real-world software development process and general programming experience in any language.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781801819060},
  url={https://ieeexplore.ieee.org/document/10162621},}

@BOOK{10460880,
  author={Sard, Patrick and Wadia, Yohan},
  booktitle={AWS Certified Solutions Architect – Professional Exam Guide (SAP-C02): Gain the practical skills, knowledge, and confidence to ace the AWS (SAP-C02) exam on your first attempt},
  year={2024},
  volume={},
  number={},
  pages={},
  abstract={Become an AWS Solutions Architect Professional with this latest AWS (SAP-C02) exam guide Purchase of this book unlocks access to web-based exam prep resources, including mock exams, flashcards, and exam tips, and the eBook PDFKey FeaturesExplore content meticulously aligned with AWS (SAP-C02) exam objectivesChallenge your knowledge through mock tests with exam-level difficultyGain expert insights and learn best practices for optimizing your cloud solutions from experienced AWS practitionersBook DescriptionKnown for its difficulty and ranking among the highest-paying IT certifications, the AWS Certified Solutions Architect Professional (SAP-C02) certification demands significant hands-on experience for success. This comprehensive guide reinforces your knowledge and enhances your skills in various solution architectures and services. Additionally, you’ll gain lifetime access to supplementary practice resources such as mock exams, flashcards, and exam tips from experts. Aligned with exam objectives, this AWS certification study guide helps you assess your knowledge through timed mock tests that simulate exam conditions. Beyond exam preparation, you’ll develop advanced skills in designing distributed systems on AWS cloud and become proficient in providing architectural recommendations for complex application implementation, and enhancing infrastructure efficiency. As you advance, you’ll gain insights into how to foster unique thinking and factor diverse considerations while architecting solutions. You’ll also get to grips with designing multi-tier applications, deploying enterprise-grade operations, and migrating complex applications to AWS. By the end of this book, you’ll be able to design and deploy innovative solutions on AWS, unlocking new opportunities and driving success in the dynamic world of cloud computing.What you will learnDesign and deploy fully secure, dynamically scalable, highly available, fault-tolerant, and reliable apps on AWSIntegrate on-premises environments seamlessly with AWS resourcesSelect appropriate architecture patterns and AWS services for designing and deploying complex applicationsContinuously improve solution architectures for security, reliability, performance, operational excellence, and cost-efficiencyPlan and execute migrations of complex applications to AWSImplement cost-control strategies to deliver cost-effective solutions on AWSWho this book is forThis book is for seasoned IT professionals adept at crafting and implementing cloud architecture on AWS. Familiarity with the AWS platform and services is essential. You'll grasp the content more effectively if you have at least 2 years of hands-on experience in AWS-based applications.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781801815079},
  url={https://ieeexplore.ieee.org/document/10460880},}

@INPROCEEDINGS{10416481,
  author={Sklyar, Vladimir and Kharchenko, Vyacheslav},
  booktitle={2023 13th International Conference on Dependable Systems, Services and Technologies (DESSERT)}, 
  title={Application of Business Analysis Techniques for Safety-Critical and Security-Critical Requirements Engineering}, 
  year={2023},
  volume={},
  number={},
  pages={1-8},
  abstract={The paper is devoted to the convergence of critical and non-critical IT in area of Business Analysis and Requirements Engineering. We compared the Safety and Security Life Cycle (SSLC) against the Business Analysis Life Cycle (BALC) to consider equivalency between specific stages. After that, Business Analysis techniques are proposed to cover a part of the SSLC stages with mature proven-in-use approaches. Our approach has found application in critical domains, such as the development of an Internet of Things (IoT) system for air quality monitoring and a safety-critical Programmable Logic Controller (PLC) employed within the Industrial Control Systems (ICS) for Nuclear Power Plants. These projects have revealed significant areas of improvement that warrant thorough research and discussion.},
  keywords={Programmable logic devices;Safety;Requirements engineering;Internet of Things;Security;Power generation;Business;Requirements Engineering;Business Analysis Techniques;Life Cycle},
  doi={10.1109/DESSERT61349.2023.10416481},
  ISSN={},
  month={Oct},}

@BOOK{9100180,
  author={Elbert, Bruce},
  booktitle={The Satellite Communication Applications Handbook, Second Edition},
  year={2003},
  volume={},
  number={},
  pages={},
  abstract={Since the publication of the best-selling first edition of The Satellite Communication Applications Handbook, the satellite industry has experienced explosive growth thanks to a flood of innovations in consumer electronics, broadcasting, the Internet, transportation, and broadband telecommunications. This second edition covers all the latest advances in satellite technology and applications and features new chapters on mobile digital audio radio and VSAT networks. It updates and expands upon the engineering and management topics that made the first edition a must-have for every satellite communications professional as well as network architects. Engineers get the latest technical details into operations, architectures, and systems components. Managers are brought up to date with the latest business applications as well as regulatory and legal decisions affecting domestic and international markets. The treatment is also of value to marketing, legal, regulatory, and financial and operations professionals who must gain a clear understanding of the capabilities and issues associated with satellite space and ground facilities and services. You get real-world, first-hand insight into: defining a satellite network architecture to meet your organization's business or operational requirements; engineering criteria and design principles for TV and radio broadcasting, mobile and fixed telephony, and VSAT data communications; and addressing business and regulatory issues to ensure a successful satellite application. Whether you are new to the satellite industry and need a quick and thorough understanding of how satellite communications operate or are a veteran professional needing a refresher on issues not encountered day to day, The Satellite Communication Applications Handbook, Second Edition is an indispensable resource to be referred to again and again.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Artech},
  isbn={9781580538084},
  url={https://ieeexplore.ieee.org/document/9100180},}

@ARTICLE{7891866,
  author={},
  journal={ISO/IEC/IEEE P24765/D3:2017}, 
  title={ISO/IEC/IEEE Approved Draft International Standard - Systems and Software Engineering - Vocabulary}, 
  year={2017},
  volume={},
  number={},
  pages={1-570},
  abstract={Consistent with ISO vocabulary standards, each technical committee is responsible for standard terminology in its area of specialization. This International Standard provides a common vocabulary applicable to all systems and software engineering work falling within the scope of ISO/IEC JTC 1/SC 7, Systems and software engineering, and the IEEE Computer Society Systems and Software Engineering Standards Committee (IEEE-CS S2ESC).The scope of each concept defined has been chosen to provide a definition that is suitable for general application. In those circumstances where a restricted application is concerned, a more specific definition might be needed.Terms have been excluded if they were considered to be parochial to one group or organization; company proprietary or trademarked; multi-word terms whose meaning could be inferred from the definitions of the component words; terms whose meaning in the information technology (IT) field could be directly inferred from their common English dictionary meaning.},
  keywords={IEEE Standards;IEC Standards;ISO Standards;Terminology;Dictionaries;Software engineering},
  doi={},
  ISSN={},
  month={Jan},}

@BOOK{9100856,
  author={Rajput, Wasim},
  booktitle={E-Commerce Systems Architecture and Applications},
  year={2000},
  volume={},
  number={},
  pages={},
  abstract={Identify all the major building blocks of an e-commerce system and understand how these building blocks interact to form an effective e-commerce system with E-Commerce Systems: Architecture, Design, and Implementation . This new book presents you with the key tools and technologies in use today, and the complete understanding you need to build an e-commerce architecture for the various types of commercial transactions - business-to-business, business-to-consumer, and intra-business. Plus, it conveys the dramatic impact e-commerce has on an enterprise's IT strategy. You get a close examination of the primary elements of the e-commerce systems architecture, including information appliances, computing networks, e-commerce services and their applications, and electronic payment systems, to help you plan, build, and maintain a vigorous e-commerce site of your own. Going beyond e-commerce via the Internet and World Wide Web, you also learn how online commercial transactions can and will take place utilizing cellular and pager networks. E-Commerce Systems: Architecture, Design, and Implementation also includes: A review of current and emergent technologies providing you with multiple options when planning and building an e-commerce system; Information on the sources for the technology you need to build your e-commerce infrastructure; A method for calculating the costs associated with building an e-commerce system suited to your organization 's needs; The technical know-how you need to understand the inherent complexities and related solutions in e-commerce system design and manufacture; Discussion of e-commerce system security issues, the popular technologies in use to control security, and the strategies you can use to assess the costs and risks associated with security implementation.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Artech},
  isbn={9781596931237},
  url={https://ieeexplore.ieee.org/document/9100856},}

@BOOK{10162695,
  author={Trabelsi, Hamida Rebai and Laniel, Marc-Andre},
  booktitle={A Developer's Guide to Cloud Apps Using Microsoft Azure: Migrate and modernize your cloud-native applications with containers on Azure using real-world case studies},
  year={2023},
  volume={},
  number={},
  pages={},
  abstract={Build and deploy modern and secure applications on Microsoft Azure by implementing best practices, patterns, and new technologies with this easy-to-follow guide Purchase of the print or Kindle book includes a free PDF eBookKey FeaturesLearn various methods to migrate legacy applications to cloud using different Azure servicesImplement continuous integration and deployment as a best practice for DevOps and agile developmentGet started with building cloud-based applications using containers and orchestrators in different scenariosBook DescriptionCompanies face several challenges during cloud adoption, with developers and architects needing to migrate legacy applications and build cloud-oriented applications using Azure-based technologies in different environments. A Developer’s Guide to Cloud Apps Using Microsoft Azure helps you learn how to migrate old apps to Azure using the Cloud Adoption Framework and presents use cases, as well as build market-ready secure and reliable applications. The book begins by introducing you to the benefits of moving legacy apps to the cloud and modernizing existing ones using a set of new technologies and approaches. You’ll then learn how to use technologies and patterns to build cloud-oriented applications. This app development book takes you on a journey through three major services in Azure, namely Azure Container Registry, Azure Container Instances, and Azure Kubernetes Service, which will help you build and deploy an application based on microservices. Finally, you’ll be able to implement continuous integration and deployment in Azure to fully automate the software delivery process, including the build and release processes. By the end of this book, you’ll be able to perform application migration assessment and planning, select the right Azure services, and create and implement a new cloud-oriented application using Azure containers and orchestrators.What you will learnGet to grips with new patterns and technologies used for cloud-native applicationsMigrate old applications and databases to Azure with easeWork with containers and orchestrators to automate app deploymentSelect the right Azure service for deployment as per the use casesSet up CI/CD pipelines to deploy apps and services on Azure DevOpsLeverage Azure App Service to deploy your first applicationBuild a containerized app using Docker and Azure Container RegistryWho this book is forThis book is for cloud developers, software architects, system administrators, developers, and computer science students looking to understand the new role of the software architect or developer in the cloud world. Professionals looking to enhance their cloud and cloud-native programming concepts will also find this book useful. A sound background in C#, ASP.NET Core, and Visual Studio (any recent version) and basic knowledge of cloud computing will be helpful.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781804613801},
  url={https://ieeexplore.ieee.org/document/10162695},}

@ARTICLE{11095692,
  author={Naimi, Lahbib and Benaddi, Lamya and Ouaddi, Charaf and Souha, Adnane and Abdelmalek, Hamza and Jakimi, Abdeslam},
  journal={IEEE Access}, 
  title={Model-Driven Architecture for Accelerating the Development of Tourism Applications}, 
  year={2025},
  volume={13},
  number={},
  pages={131696-131715},
  abstract={The tourism sector has witnessed a significant shift towards digital transformation, with mobile and web applications becoming essential tools for enhancing tourist experiences. Despite their potential, the development of these applications is often hindered by challenges such as platform heterogeneity, rapid adaptability, and integration complexity. This paper proposes a Model-Driven Architecture (MDA) approach to address these challenges and accelerate the development of tourism applications. The approach introduces the use of the 6As framework as a domain model, alongside systematic model transformations, to streamline the development process. A detailed case study demonstrates the transformation of a class diagram into a functional backend, showcasing the advantages of automation, scalability, and maintainability. Validation results reveal significant improvements in development time compared to traditional manual methods. This contribution highlights the potential of MDA to simplify the development of complex systems while ensuring quality and adaptability. Future work aims to enhance the methodology by incorporating additional architectural patterns, expanding support for diverse platforms, and integrating advanced validation techniques.},
  keywords={Unified modeling language;Adaptation models;Computational modeling;Software;Production facilities;Business;Computer architecture;Software development management;Modeling;Focusing;Model-driven architecture;tourism;software factory;model transformation;6As framework},
  doi={10.1109/ACCESS.2025.3592552},
  ISSN={2169-3536},
  month={},}

@BOOK{10162874,
  author={Feuling, Bryan},
  booktitle={Repeatability, Reliability, and Scalability through GitOps: Continuous delivery and deployment codified},
  year={2021},
  volume={},
  number={},
  pages={},
  abstract={Learn how to best use GitOps to automate manual tasks in the continuous delivery and deployment processKey FeaturesExplore the different GitOps schools of thought and understand which GitOps practices will work for you and your teamGet up and running with the fundamentals of GitOps implementationUnderstand how to effectively automate the deployment and delivery processBook DescriptionThe world of software delivery and deployment has come a long way in the last few decades. From waterfall methods to Agile practices, every company that develops its own software has to overcome various challenges in delivery and deployment to meet customer and market demands. This book will guide you through common industry practices for software delivery and deployment. Throughout the book, you'll follow the journey of a DevOps team that matures their software release process from quarterly deployments to continuous delivery using GitOps. With the help of hands-on tutorials, projects, and self-assessment questions, you'll build your knowledge of GitOps basics, different types of GitOps practices, and how to decide which GitOps practice is the best for your company. As you progress, you'll cover everything from building declarative language files to the pitfalls in performing continuous deployment with GitOps. By the end of this book, you'll be well-versed with the fundamentals of delivery and deployment, the different schools of GitOps, and how to best leverage GitOps in your teams. What you will learnExplore a variety of common industry tools for GitOpsUnderstand continuous deployment, continuous delivery, and why they are importantGain a practical understanding of using GitOps as an engineering organizationBecome well-versed with using GitOps and Kubernetes togetherLeverage Git events for automated deploymentsImplement GitOps best practices and find out how to avoid GitOps pitfallsWho this book is forThis book is for engineering leaders and anyone working in software engineering, DevOps, SRE, build/release, or cloud automation teams. A basic understanding of the DevOps software development life cycle (SDLC) will help you to get the most out of this book.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781801074315},
  url={https://ieeexplore.ieee.org/document/10162874},}

@BOOK{9101054,
  author={Sendin, Alberto and Sanchez-Fornie, Miguel and Berganza, Inigo and Simon, Javier and Urrutia, Iker},
  booktitle={Telecommunictaion Networks for the Smart Grid},
  year={2016},
  volume={},
  number={},
  pages={},
  abstract={This comprehensive new resource demonstrates how to build smart grids utilizing the latest telecommunications technologies. Readers find practical coverage of PLC and wireless for smart grid and are given concise excerpts of the different technologies, networks, and services around it. Design and planning guidelines are shown through the combination of electricity grid and telecommunications technologies that support the reliability, performance and security requirements needed in smart grid applications. This book covers a wide range of critical topics, including telecommunications for power engineers, power engineering for telecommunications engineers, utility applications projecting in smart grids, technologies for smart grid networks, and telecommunications architecture. This practical reference is supported with in-depth case studies.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Artech},
  isbn={9781630813734},
  url={https://ieeexplore.ieee.org/document/9101054},}

@BOOK{10162228,
  author={Khan, Ovais Mehboob Ahmed and Siddiqui, Nabil and Oleson, Timothy and Fussell, Mark},
  booktitle={Embracing Microservices Design: A practical guide to revealing anti-patterns and architectural pitfalls to avoid microservices fallacies},
  year={2021},
  volume={},
  number={},
  pages={},
  abstract={Develop microservice-based enterprise applications with expert guidance to avoid failures and technological debt with the help of real-world examplesKey FeaturesImplement the right microservices adoption strategy to transition from monoliths to microservicesExplore real-world use cases that explain anti-patterns and alternative practices in microservices developmentDiscover proven recommendations for avoiding architectural mistakes when designing microservicesBook DescriptionMicroservices have been widely adopted for designing distributed enterprise apps that are flexible, robust, and fine-grained into services that are independent of each other. There has been a paradigm shift where organizations are now either building new apps on microservices or transforming existing monolithic apps into microservices-based architecture. This book explores the importance of anti-patterns and the need to address flaws in them with alternative practices and patterns. You'll identify common mistakes caused by a lack of understanding when implementing microservices and cover topics such as organizational readiness to adopt microservices, domain-driven design, and resiliency and scalability of microservices. The book further demonstrates the anti-patterns involved in re-platforming brownfield apps and designing distributed data architecture. You’ll also focus on how to avoid communication and deployment pitfalls and understand cross-cutting concerns such as logging, monitoring, and security. Finally, you’ll explore testing pitfalls and establish a framework to address isolation, autonomy, and standardization. By the end of this book, you'll have understood critical mistakes to avoid while building microservices and the right practices to adopt early in the product life cycle to ensure the success of a microservices initiative.What you will learnDiscover the responsibilities of different individuals involved in a microservices initiativeAvoid the common mistakes in architecting microservices for scalability and resiliencyUnderstand the importance of domain-driven design when developing microservicesIdentify the common pitfalls involved in migrating monolithic applications to microservicesExplore communication strategies, along with their potential drawbacks and alternativesDiscover the importance of adopting governance, security, and monitoringUnderstand the role of CI/CD and testingWho this book is forThis practical microservices book is for software architects, solution architects, and developers involved in designing microservices architecture and its development, who want to gain insights into avoiding pitfalls and drawbacks in distributed applications, and save time and money that might otherwise get wasted if microservices designs fail. Working knowledge of microservices is assumed to get the most out of this book.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781801813495},
  url={https://ieeexplore.ieee.org/document/10162228},}

@INPROCEEDINGS{10912857,
  author={Ingole, Balaji Shesharao and Ramineni, Vishnu and Jayaram, Vivekananda and Pandy, Gokul and Krishnappa, Manjunatha Sughaturu and Parlapalli, Vidyasagar and Mullankandy, Sreeram and Banarse, Amey Ram},
  booktitle={2024 International Conference on Intelligent Cybernetics Technology & Applications (ICICyTA)}, 
  title={AI Chatbot Implementation on Government Websites: A Framework for Development, User Engagement, and Security for DHS Website}, 
  year={2024},
  volume={},
  number={},
  pages={377-382},
  abstract={As digital transformation reshapes public services, the integration of AI-driven chatbots on government websites has emerged as a promising solution to streamline user support and enhance accessibility. This paper presents a comprehensive framework for implementing an AI chatbot on the Department of Health service (DHS) website, targeting the efficient resolution of diverse user needs while addressing the unique security challenges inherent in government digital infrastructure. The proposed framework covers critical aspects of chatbot development, including the configuration of natural language processing (NLP) models, user intent mapping, and data protection strategies tailored to sensitive government information. Additionally, it explores strategies to ensure user engagement and accessibility, considering the diverse demographics of DHS website visitors. This paper examines potential security risks, such as data breaches and unauthorized access and mentions best practices to deal with these issues. The framework offers a pathway to balance the dual objectives of responsive service and robust cybersecurity, ultimately advancing the role of AI in enhancing the accessibility and security of government services. The deployment of AI chatbots in public service contexts requires meticulous planning to align with legal and regulatory standards, particularly in managing sensitive health data. This study emphasizes the importance of ongoing monitoring and adaptive learning mechanisms to keep the chatbot updated with evolving public policies and user needs.},
  keywords={Government;Chatbots;User experience;Planning;Security;Time factors;Public policy;Artificial intelligence;Standards;Monitoring;AI chatbot;government websites;department of health service (DHS);user engagement;natural language processing (NLP)},
  doi={10.1109/ICICYTA64807.2024.10912857},
  ISSN={},
  month={Dec},}

@INPROCEEDINGS{9172613,
  author={},
  booktitle={2020 IEEE Aerospace Conference}, 
  title={Conference Digest}, 
  year={2020},
  volume={},
  number={},
  pages={1-310},
  abstract={full conference PDF},
  keywords={Space vehicles;Mars;Moon;Robots;Conferences;Space missions;Legged locomotion},
  doi={10.1109/AERO47225.2020.9172613},
  ISSN={1095-323X},
  month={March},}

@ARTICLE{7839172,
  author={},
  journal={ISO/IEC/IEEE FDIS P24765:2016(E), January 2017}, 
  title={ISO/IEC/IEEE Draft Systems and Software Engineering - Vocabulary}, 
  year={2017},
  volume={},
  number={},
  pages={1-568},
  abstract={Consistent with ISO vocabulary standards, each technical committee is responsible for standard terminology in its area of specialization. This International Standard provides a common vocabulary applicable to all systems and software engineering work falling within the scope of ISO/IEC JTC 1/SC 7, Systems and software engineering, and the IEEE Computer Society Systems and Software Engineering Standards Committee (IEEE-CS S2ESC).The scope of each concept defined has been chosen to provide a definition that is suitable for general application. In those circumstances where a restricted application is concerned, a more specific definition might be needed.Terms have been excluded if they were considered to be parochial to one group or organization; company proprietary or trademarked; multi-word terms whose meaning could be inferred from the definitions of the component words; terms whose meaning in the information technology (IT) field could be directly inferred from their common English dictionary meaning.},
  keywords={IEEE Standards;IEC Standards;ISO Standards;Dictionaries;Terminology;Software engineering},
  doi={},
  ISSN={},
  month={Jan},}

@BOOK{10745364,
  author={Nwaiwu, Ikenna},
  booktitle={Automating API Delivery: APIOps with OpenAPI},
  year={2024},
  volume={},
  number={},
  pages={},
  abstract={Improve speed, quality, AND cost by automating your API delivery process! Automating API Delivery shows you how to strike the perfect balance between speed and usability by applying DevOps automation principles to your API design and delivery process. It lays out a clear path to making both the organizational and technical changes you need to deliver high-quality APIs both rapidly and reliably. In Automating API Delivery you’ll learn how to:  Enforce API design standards with linting Automate breaking-change checks to control design creep Ensure accuracy of API reference documents Centralize API definition consistency checks Automate API configuration deployment Conduct effective API design reviews  Author Ikenna Nwaiwu provides comprehensive guidance on implementing APIOps in your organization. He carefully walks through the technical steps and introduces the essential open-source tools, with practical advice and insights from his years of experience. You’ll benefit from his personal tips for avoiding common pitfalls and challenges of moving to automated API delivery.},
  keywords={APIOps;CI/CD;RESTful;design reviews;design creep;breaking-change checks;linting;conformance;fast;Schemathesis;Portman;secure;consistent;easy},
  doi={},
  ISSN={},
  publisher={Manning},
  isbn={9781633438781},
  url={https://ieeexplore.ieee.org/document/10745364},}

@BOOK{10162876,
  author={Madushan, Dhanushka},
  booktitle={Cloud Native Applications with Ballerina: A guide for programmers interested in developing cloud native applications using Ballerina Swan Lake},
  year={2021},
  volume={},
  number={},
  pages={},
  abstract={Learn how to build scalable cloud native applications with the new-generation Ballerina language using expert tips and best practicesKey FeaturesWork with code samples based on the Ballerina Swan Lake Beta1 versionExplore the in-built networking protocol support in Ballerina to develop secure distributed appsBuild a Ballerina app with an automated CI/CD pipeline with observability to simplify maintenance and deploymentBook DescriptionThe Ballerina programming language was created by WSO2 for the modern needs of developers where cloud native development techniques have become ubiquitous. Ballerina simplifies how programmers develop and deploy cloud native distributed apps and microservices. Cloud Native Applications with Ballerina will guide you through Ballerina essentials, including variables, types, functions, flow control, security, and more. You'll explore networking as an in-built feature in Ballerina, which makes it a first-class language for distributed computing. With this app development book, you'll learn about different networking protocols as well as different architectural patterns that you can use to implement services on the cloud. As you advance, you'll explore multiple design patterns used in microservice architecture and use serverless in Amazon Web Services (AWS) and Microsoft Azure platforms. You will also get to grips with Docker, Kubernetes, and serverless platforms to simplify maintenance and the deployment process. Later, you'll focus on the Ballerina testing framework along with deployment tools and monitoring tools to build fully automated observable cloud applications. By the end of this book, you will have learned how to apply the Ballerina language for building scalable, resilient, secured, and easy-to-maintain cloud native Ballerina projects and applications.What you will learnUnderstand the concepts and models in cloud native architectureGet to grips with the high-level concepts of building applications with the Ballerina languageUse cloud native architectural design patterns to develop cloud native Ballerina applicationsDiscover how to automate, maintain, and observe cloud native Ballerina applicationsUse a container to deploy and maintain a Ballerina application with Docker and KubernetesExplore serverless architecture and use Microsoft Azure and the AWS platform to build serverless applicationsWho this book is forThis Ballerina Swan Lake book is for cloud developers, integration developers, and microservices developers who are facing challenges with legacy tooling and are looking for the latest tools and technologies to solve them. Beginner-level programming knowledge is required before getting started with this Ballerina book.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781800206656},
  url={https://ieeexplore.ieee.org/document/10162876},}

@INPROCEEDINGS{9283630,
  author={Lahiri, Shuvendu K. and Lal, Akash and Gopinath, Sridhar and Nutz, Alexander and Levin, Vladimir and Kumar, Rahul and Deisinger, Nate and Lichtenberg, Jakob and Bansal, Chetan},
  booktitle={2020 Formal Methods in Computer Aided Design (FMCAD)}, 
  title={Angelic Checking within Static Driver Verifier: Towards high-precision defects without (modeling) cost}, 
  year={2020},
  volume={},
  number={},
  pages={169-178},
  abstract={Microsoft's Static Driver Verifier (SDV) pioneered the use of software model checking for ensuring that device drivers correctly use operating system (OS) APIs. However, the verification methodology has been difficult to extend in order to support either (a) new classes of drivers for which SDV does not already have a harness and stubs, or (b) memory-corruption properties. Any attempt to apply SDV out-of-the-box results in either false alarms due to the lack of environment modeling, or scalability issues when finding deeply nested bugs in the presence of a very large number of memory accesses. In this paper, we describe our experience designing and shipping a new class of checks known as angelic checks through SDV with the aid of angelic verification (AV) [1] technology, over a period of 4 years. AV pairs a precise inter-procedural assertion checker with automatic inference of likely specifications for the environment. AV helps compensate for the lack of environment modeling and regains scalability by making it possible to find deeply nested bugs, even for complex memory-corruption properties. These new rules have together found over a hundred confirmed defects during internal deployment at Microsoft, including several previously unknown high-impact potential security vulnerabilities. AV considerably increases the reach of SDV, both in terms of drivers as well as rules that it can support effectively.},
  keywords={Scalability;Computer bugs;Writing;Tools;Software;Space exploration;Security},
  doi={10.34727/2020/isbn.978-3-85448-042-6_24},
  ISSN={2708-7824},
  month={Sep.},}

@ARTICLE{4152660,
  author={},
  journal={IEEE Unapproved Std P11073-00101/D02J, Feb 2007}, 
  title={Unapproved IEEE Draft Guide for Health Informatics-Point-Of-Care Medical Device Communication-Technical Report-Guidelines for the Use of RF Wireless Technology}, 
  year={2007},
  volume={},
  number={},
  pages={},
  abstract={},
  keywords={Wireless communication;Communication system security;Standards;Radio frequency;Medical devices;IEEE Standards;Wireless personal area networks},
  doi={},
  ISSN={},
  month={},}

@BOOK{10251201,
  author={Simion, Adelina and Tebeka, Miki and Ryer, Mat},
  booktitle={Test-Driven Development in Go: A practical guide to writing idiomatic and efficient Go tests through real-world examples},
  year={2023},
  volume={},
  number={},
  pages={},
  abstract={Explore Go testing techniques and leverage TDD to deliver and maintain microservices architecture, including contract, end-to-end, and unit testing Purchase of the print or Kindle book includes a free PDF eBookKey FeaturesWrite Go test suites using popular mocking and testing frameworksLeverage TDD to implement testing at all levels of web applications and microservices architectureMaster the art of writing tests that cover edge cases and concurrent codeBook DescriptionExperienced developers understand the importance of designing a comprehensive testing strategy to ensure efficient shipping and maintaining services in production. This book shows you how to utilize test-driven development (TDD), a widely adopted industry practice, for testing your Go apps at different levels. You’ll also explore challenges faced in testing concurrent code, and learn how to leverage generics and write fuzz tests. The book begins by teaching you how to use TDD to tackle various problems, from simple mathematical functions to web apps. You’ll then learn how to structure and run your unit tests using Go’s standard testing library, and explore two popular testing frameworks, Testify and Ginkgo. You’ll also implement test suites using table-driven testing, a popular Go technique. As you advance, you’ll write and run behavior-driven development (BDD) tests using Ginkgo and Godog. Finally, you’ll explore the tricky aspects of implementing and testing TDD in production, such as refactoring your code and testing microservices architecture with contract testing implemented with Pact. All these techniques will be demonstrated using an example REST API, as well as smaller bespoke code examples. By the end of this book, you’ll have learned how to design and implement a comprehensive testing strategy for your Go applications and microservices architecture.What you will learnCreate practical Go unit tests using mocks and assertions with TestifyBuild table-driven test suites for HTTP web applicationsWrite BDD-style tests using the Ginkgo testing frameworkUse the Godog testing framework to reliably test web applicationsVerify microservices architecture using Pact contract testingDevelop tests that cover edge cases using property testing and fuzzingWho this book is forIf you are an intermediate-level developer or software testing professional who knows Go fundamentals and is looking to deliver projects with Go, then this book is for you. Knowledge of Go syntax, structs, functions, and interfaces will help you get the most out of this book.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781803235028},
  url={https://ieeexplore.ieee.org/document/10251201},}

@INPROCEEDINGS{7943664,
  author={Bhatt, Devesh and Hall, Brendan and Murugesan, Anitha and Oglesby, David and Bush, Eric and Engstrom, Eric and Mueller, Joseph and Pelican, Michael},
  booktitle={2017 IEEE Aerospace Conference}, 
  title={Opportunities and challenges for formal methods tools in the certification of avionics software}, 
  year={2017},
  volume={},
  number={},
  pages={1-20},
  abstract={Formal methods tools, whose underlying principles are based on mathematics and formal logic, are considered one of the most effective and rigorous means of verifying system properties and assuring the absence of undesirable system behavior. The use of such tools seem to squarely fit the needs of those aiming to develop and certify avionic software as per the DO-178C standard, the set of objectives laid out by FAA to achieve a high level of confidence on the systems. However, our recent work on a NASA-funded research project revealed that there are practical considerations and additional complexities involved in using formal method tools to provide the level of assurance as exemplified by the DO-178C. In this paper we discuss one of the key concerns with formal tools: its soundness — the characteristic of a tool to never permit the verified system property be declared true when it is actually not true. We explored two major classes of formal methods tools — namely model checkers and static analyzers — and observed several threats to their soundness such as tool fallacies and failure modes that could lead to misplaced confidence in the verified system. We present various strategies to mitigate them, including an assurance case framework to verify that potential risks are all mitigated. The intent of this paper is not to discourage but encourage scrupulous use of formal tools to certify critical avionic software by being wary of the subtle but serious issues that may be overlooked.},
  keywords={Tools;Software;Analytical models;Model checking;Aerospace electronics;Safety;Algorithm design and analysis},
  doi={10.1109/AERO.2017.7943664},
  ISSN={},
  month={March},}

@INPROCEEDINGS{10521169,
  author={Thomas, Justin and Rodriguez, Luis M. and Badger, Andrew R. and Wortman, Kristin and Wilson, Dan and Heistand, Christopher},
  booktitle={2024 IEEE Aerospace Conference}, 
  title={Flight Software for the Double Asteroid Redirection Test}, 
  year={2024},
  volume={},
  number={},
  pages={1-20},
  abstract={The NASA Double Asteroid Redirection Test (DART) mission, executed by the Johns Hopkins Applied Physics Laboratory (JHU/APL), is the first mission to successfully demonstrate the effectiveness of a kinetic impactor as a means of asteroid deflection. On September 26, 2022 at 23:14:24 UTC, the DART spacecraft collided with its intended asteroid Dimorphos, the smaller member of the (65803) Didymos binary asteroid system. The impact altered the orbital period of Dimorphos by 33 minutes, exceeding the minimum required alteration of 73 seconds.The DART Flight Software (FSW) was designed and developed to meet the needs of this unique mission. Most prominently, the DART FSW capabilities included (1) Guidance, Navigation, and Control (GNC) software for spacecraft attitude control, trajectory correction, solar array and communication antenna positioning, and autonomous optical targeting and guidance for asteroid impact, (2) the Digital Control Interface Unit (DCIU) software controller for NASA’s Evolutionary Xenon Thruster – Commercial (NEXT-C) gridded ion thruster system, and (3) software for configuring and monitoring on-board hardware systems, including the Didymos Reconnaissance and Asteroid Camera for OpNav (DRACO) optical instrument and associated hardware-based image processing and real-time image downlink data pipelines.The DART spacecraft impacted Dimorphos at a distance of approximately 11 million kilometers from Earth, requiring spacecraft operation with limited bandwidth and extended data transmission latency. The combination of several software capabilities addressed this need, including on-board data recorder management, reliable recorded data delivery, and a command system enabling real-time, time-tagged, and sequenced command execution. The communication bandwidth and latency constraints, in addition to deep space antenna resource availability, dictated the need for FSW to handle many on-board faults, including safing the spacecraft as necessary for manual diagnostics and recovery. This is done by an autonomous software-based fault protection system, referred to as Autonomy, with a flexible interpretive engine implemented in the DART FSW.Overall, DART development required a low-cost implementation in order to meet technical objectives within fiscal constraints. DART FSW addressed this objective by (1) leveraging numerous existing software components with flight heritage and (2) investing early in software development practices and tooling (DevOps) to complete FSW as-planned, minimizing late-breaking costs due to software defects or redesign efforts.This paper describes the overall DART Flight Software architecture, provides an overview of the avionics processing platform, details the specific FSW capabilities critical for mission success, explains the software development environment, discusses the approaches and results for both pre-launch testing and verification, and finally, provides results from post-launch operations and final successful asteroid impact.},
  keywords={Space vehicles;Particle beam optics;Attitude control;Fault protection;Bandwidth;Optical imaging;Software},
  doi={10.1109/AERO58975.2024.10521169},
  ISSN={1095-323X},
  month={March},}

@ARTICLE{11024036,
  author={Ebert, Christof and Nawalramka, Priyanka},
  journal={IEEE Software}, 
  title={Generative AI for Data Science}, 
  year={2025},
  volume={42},
  number={4},
  pages={20-24},
  abstract={Struggling with messy data or incomplete datasets? Trying to find patterns in your static analysis or test results? Generative AI (GAI) is transforming data science by automating data cleaning, generating high-quality synthetic data, and optimizing model training. Learn with our hands-on examples how to enhance data science with GAI to accelerate insights, enhance accuracy, and set up predictive models.—Christof Ebert},
  keywords={Training;Accuracy;Generative AI;Static analysis;Data science;Predictive models;Data models;Synthetic data;Data analysis;Software testing},
  doi={10.1109/MS.2025.3562046},
  ISSN={1937-4194},
  month={July},}

@INPROCEEDINGS{10749562,
  author={Frey, Constantin and Annighoefer, Bjoern},
  booktitle={2024 AIAA DATC/IEEE 43rd Digital Avionics Systems Conference (DASC)}, 
  title={Applying Qualifiable Model Transformations in Integrated Modular Avionics Configuration Development with Automated Tool Qualification Support}, 
  year={2024},
  volume={},
  number={},
  pages={1-10},
  abstract={Model transformations are used in the development of avionics software to make the process more efficient by automation. This paper presents a qualifiable model transformation language and engine for use in a safety-critical Integrated Modular Avionics configuration development process. Moreover, a tool qualification agent supports the tool user in the qualification of the model transformation by an automated generation of the qualification artifacts. The qualifiable model transformation language and the tool qualification agent are embedded in a domain-specific modeling environment for use in safety-critical software development. In this paper we develop a model transformation using the qualifiable model transformation language and apply it to a configuration step for Integrated Modular Avionics. Further, we generate the tool requirements for this model transformation. The aim is to validate the concepts for the qualifiable model transformation language and the tool qualification agent. First, we develop a transformation model for an exemplary model transformation that includes the information about the transformation rules and the rule sequencing. Then, the transformation generator transforms the transformation model into the executable transformation code. The transformation rules consist of search patterns and replacement patterns. Additional concepts that the model transformation uses are guard conditions and a linear rule sequencing. To ensure that the model transformation language is qualifiable, we generate its implementation in MISRA-C. We further check the transformation code with a code analysis tool. The generated requirements from the tool qualification agent consist of the specification of the transformation rules and the data and control flow. We develop templates for the tool requirements and model transformations for the generation of the qualification artifacts. These qualification model transformations transform the requirements templates together with the transformation model into tool requirements.},
  keywords={Sequential analysis;Codes;Automation;Transforms;Aerospace electronics;Software;Generators;Engines;Qualifications;Software development management;domain-specific modeling;qualifiable model transformations;software tool qualification;RTCA DO-330},
  doi={10.1109/DASC62030.2024.10749562},
  ISSN={2155-7209},
  month={Sep.},}

@INPROCEEDINGS{4599483,
  author={King, Paul and Smith, Craig},
  booktitle={Agile 2008 Conference}, 
  title={Technical lessons Learned Turning the Agile Dials to Eleven!}, 
  year={2008},
  volume={},
  number={},
  pages={233-238},
  abstract={This report outlines technical lessons learnt by about 20 of Australiapsilas most experienced agile specialists over several years across several projects within an organization which aggressively applied the agile practices with much success. In these projects the agile dials were cranked to eleven to achieve very high levels of quality. Most of the specialists involved believe that they produced the highest quality software of their careers with some of the highest productivity they have ever experienced.},
  keywords={Production;Radiation detectors;Complexity theory;Testing;Libraries;Productivity;Writing;Agile;Coverage;Mocking;TDD;Pair-programming},
  doi={10.1109/Agile.2008.15},
  ISSN={},
  month={Aug},}

@ARTICLE{4040125,
  author={},
  journal={IEEE Std P1175.2/D8.0}, 
  title={Unapproved IEEE Recommended Practice for CASE Tool Interconnection - Characterization of Interconnections (Superseded by P1175.2_D11.2)}, 
  year={2006},
  volume={},
  number={},
  pages={},
  abstract={},
  keywords={Standards;IEEE Standards;Computer aided software engineering;Behavioral sciences;Software;Syntactics;Computational modeling},
  doi={},
  ISSN={},
  month={},}

@INBOOK{6354029,
  author={},
  booktitle={A Guide to the Wireless Engineering Body of Knowledge (WEBOK)}, 
  title={Fundamental Knowledge}, 
  year={2012},
  volume={},
  number={},
  pages={265-282},
  abstract={<![CDATA[<P>This chapter contains sections titled: <UL> <LI> <P>Introduction</P> </LI> <LI> <P>Electrical and RF Engineering</P> </LI> <LI> <P>Communication Engineering</P> </LI> <LI> <P>Engineering Management</P> </LI> </UL> </P>]]>},
  keywords={Microwave filters;Radio frequency;RLC circuits;Filtering theory;Wireless communication;Band-pass filters;Microwave circuits},
  doi={10.1002/9781118444221.ch7},
  ISSN={},
  publisher={IEEE},
  isbn={9781118444245},
  url={https://ieeexplore.ieee.org/document/6354029},}

@BOOK{10251275,
  author={Cm, Safeer},
  booktitle={Architecting Cloud-Native Serverless Solutions: Design, build, and operate serverless solutions on cloud and open source platforms},
  year={2023},
  volume={},
  number={},
  pages={},
  abstract={Get up and running with serverless workloads across AWS, Azure, GCP, Kubernetes, and virtual machines with real-life examples and best practices for design, development, and security of serverless applications Purchase of the print or Kindle book includes a free PDF eBookKey FeaturesLearn with DIY projects and step-by-step instructions for different serverless technologies and vendorsExplore detailed sections on running serverless workloads across Kubernetes and virtual machinesDiscover Cloudflare Serverless Solutions to modernize your web applicationsBook DescriptionServerless computing has emerged as a mainstream paradigm in both cloud and on-premises computing, with AWS Lambda playing a pivotal role in shaping the Function-as-a-Service (FaaS) landscape. However, with the explosion of serverless technologies and vendors, it has become increasingly challenging to comprehend the foundational services and their offerings. Architecting Cloud Native Serverless Solutions lays a strong foundation for understanding the serverless landscape and technologies in a vendor-agnostic manner. You'll learn how to select the appropriate cloud vendors and technologies based on your specific needs. In addition, you'll dive deep into the serverless services across AWS, GCP, Azure, and Cloudflare followed by open source serverless tools such as Knative, OpenFaaS, and OpenWhisk, along with examples. You'll explore serverless solutions on Kubernetes that can be deployed on both cloud-hosted clusters and on-premises environments, with real-world use cases. Furthermore, you'll explore development frameworks, DevOps approaches, best practices, security considerations, and design principles associated with serverless computing. By the end of this serverless book, you'll be well equipped to solve your business problems by using the appropriate serverless vendors and technologies to build efficient and cost-effective serverless systems independently.What you will learnUnderstand the serverless landscape and its potentialBuild serverless solutions across AWS, Azure, and GCPDevelop and run serverless applications on KubernetesImplement open source FaaS with Knative, OpenFaaS, and OpenWhiskModernize web architecture with Cloudflare ServerlessDiscover popular serverless frameworks and DevOps for serverlessExplore software design and serverless architecture patternsAcquire an understanding of serverless development and security best practicesWho this book is forThis book is for DevOps, platform, cloud, site reliability engineers, or application developers looking to build serverless solutions. It’s a valuable reference for solution architects trying to modernize a legacy application or working on a greenfield project. It’s also helpful for anyone trying to solve business or operational problems without wanting to manage complicated technology infrastructure using serverless technologies. A basic understanding of cloud computing and some familiarity with at least one cloud vendor, Python programming language, and working with CLI will be helpful when reading this book.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781803235998},
  url={https://ieeexplore.ieee.org/document/10251275},}

@BOOK{9101062,
  author={Rao, Sudhakar and Shafai, Lotfollah and Sharma, Satish},
  booktitle={Handbook of Reflector Antennas and Feed Systems Volume III: Applications of Reflectors},
  year={2013},
  volume={},
  number={},
  pages={},
  abstract={This is the first truly comprehensive and most up-to-date handbook available on modern reflector antennas and feed sources for diversified space and ground applications. There has never been such an all-encompassing reflector handbook in print, and no currently available title offers coverage of such recent research developments. The Handbook consists of three volumes. Volume III focuses on the range of reflector antenna applications, including space, terrestrial, and radar. The intent of this book volume is to provide practical applications and design information on reflector antennas used for several communications systems. This book covers recent developments of reflector antennas used for satellite communications, terrestrial communications, and remote sensing applications. New subjects are introduced for the first time, including satellite antennas, Terahertz antennas, PIM, multipaction, corona, deployable mesh reflector antennas, and mechanical aspects of reflector antennas. In addition, this book contains a separate topic on integrated feed assembly for reflector antennas covering analysis, design, fabrication, and test.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Artech},
  isbn={9781608075201},
  url={https://ieeexplore.ieee.org/document/9101062},}

@INPROCEEDINGS{4813952,
  author={Borchardt, Frank},
  booktitle={2009 IET Smart Metering - Making It Happen}, 
  title={Meeting the operational and logistical challenges of smart meter roll-out - the European experience (“It's not just hanging meters on walls”)}, 
  year={2009},
  volume={},
  number={},
  pages={1-20},
  abstract={A collection of slides on meeting the operational and logistical challenges of smart meter roll-out by Frank Borchardt, was given.},
  keywords={},
  doi={},
  ISSN={0537-9989},
  month={Feb},}

@BOOK{10163685,
  author={Chou, Eric},
  booktitle={Mastering Python Networking: Utilize Python packages and frameworks for network automation, monitoring, cloud, and management},
  year={2023},
  volume={},
  number={},
  pages={},
  abstract={Get to grips with the latest container examples, Python 3 features, GitLab DevOps, network data analysis, and cloud networking to get the most out of Python for network engineering with the latest edition of this bestselling guide Purchase of the print or Kindle book includes a free eBook in PDF format.Key FeaturesExplore the power of the latest Python libraries and frameworks to tackle common and complex network problems efficiently and effectivelyUse Python and other open source tools for Network DevOps, automation, management, and monitoringUse Python 3 to implement advanced network-related featuresBook DescriptionNetworks in your infrastructure set the foundation for how your application can be deployed, maintained, and serviced. Python is the ideal language for network engineers to explore tools that were previously available to systems engineers and application developers. In Mastering Python Networking, Fourth edition, you'll embark on a Python-based journey to transition from a traditional network engineer to a network developer ready for the next generation of networks. This new edition is completely revised and updated to work with the latest Python features and DevOps frameworks. In addition to new chapters on introducing Docker containers and Python 3 Async IO for network engineers, each chapter is updated with the latest libraries with working examples to ensure compatibility and understanding of the concepts. Starting with a basic overview of Python, the book teaches you how it can interact with both legacy and API-enabled network devices. You will learn to leverage high-level Python packages and frameworks to perform network automation tasks, monitoring, management, and enhanced network security, followed by AWS and Azure cloud networking. You will use Git for code management, GitLab for continuous integration, and Python-based testing tools to verify your network.What you will learnUse Python to interact with network devicesUnderstand Docker as a tool that you can use for the development and deploymentUse Python and various other tools to obtain information from the networkLearn how to use ELK for network data analysisUtilize Flask and construct high-level API to interact with in-house applicationsDiscover the new AsyncIO feature and its concepts in Python 3Explore test-driven development concepts and use PyTest to drive code test coverageUnderstand how GitLab can be used with DevOps practices in networkingWho this book is forMastering Python Networking, Fourth edition is for network engineers, developers, and SREs who want to learn Python for network automation, programmability, monitoring, cloud, and data analysis. Network engineers who want to transition from manual to automation-based networks using the latest DevOps tools will also get a lot of useful information from this book. Basic familiarity with Python programming and networking-related concepts such as Transmission Control Protocol/Internet Protocol (TCP/IP) will be helpful in getting the most out of this book.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781803242323},
  url={https://ieeexplore.ieee.org/document/10163685},}

@BOOK{10162523,
  author={Valle, Jean-Georges},
  booktitle={Practical Hardware Pentesting: A guide to attacking embedded systems and protecting them against the most common hardware attacks},
  year={2021},
  volume={},
  number={},
  pages={},
  abstract={Learn how to pentest your hardware with the most common attract techniques and patternsKey FeaturesExplore various pentesting tools and techniques to secure your hardware infrastructureProtect your hardware by finding potential entry points like glitchesFind the best practices for securely designing your productsBook DescriptionIf you’re looking for hands-on introduction to pentesting that delivers, then Practical Hardware Pentesting is for you. This book will help you plan attacks, hack your embedded devices, and secure the hardware infrastructure. Throughout the book, you will see how a specific device works, explore the functional and security aspects, and learn how a system senses and communicates with the outside world. You’ll set up a lab from scratch and then gradually work towards an advanced hardware lab—but you’ll still be able to follow along with a basic setup. As you progress, you’ll get to grips with the global architecture of an embedded system and sniff on-board traffic, learn how to identify and formalize threats to the embedded system, and understand its relationship with its ecosystem. You’ll discover how to analyze your hardware and locate its possible system vulnerabilities before going on to explore firmware dumping, analysis, and exploitation. The reverse engineering chapter will get you thinking from an attacker point of view; you’ll understand how devices are attacked, how they are compromised, and how you can harden a device against the most common hardware attack vectors. By the end of this book, you will be well-versed with security best practices and understand how they can be implemented to secure your hardware.What you will learnPerform an embedded system test and identify security critical functionalitiesLocate critical security components and buses and learn how to attack them Discover how to dump and modify stored informationUnderstand and exploit the relationship between the firmware and hardwareIdentify and attack the security functions supported by the functional blocks of the deviceDevelop an attack lab to support advanced device analysis and attacksWho this book is forIf you’re a researcher or a security professional who wants a comprehensive introduction into hardware security assessment, then this book is for you. Electrical engineers who want to understand the vulnerabilities of their devices and design them with security in mind will also find this book useful. You won’t need any prior knowledge with hardware pentensting before you get started; everything you need is in the chapters.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781789614190},
  url={https://ieeexplore.ieee.org/document/10162523},}

@BOOK{11014632,
  author={Colombet, Quentin and Beyls, Kristof},
  booktitle={LLVM Code Generation: A deep dive into compiler backend development},
  year={2025},
  volume={},
  number={},
  pages={},
  abstract={Explore the world of code generation with the LLVM infrastructure, and learn how to extend existing backends or develop your own Get With Your Book: PDF Copy, AI Assistant, and Next-Gen Reader FreeKey FeaturesUnderstand the steps involved in generating assembly code from LLVM IRLearn the key constructs needed to leverage LLVM for your hardware or backendStrengthen your understanding with targeted exercises and practical examples in every chapterBook DescriptionThe LLVM infrastructure is a popular compiler ecosystem widely used in the tech industry and academia. This technology is crucial for both experienced and aspiring compiler developers looking to make an impact in the field. Written by Quentin Colombet, a veteran LLVM contributor and architect of the GlobalISel framework, this book provides a primer on the main aspects of LLVM, with an emphasis on its backend infrastructure; that is, everything needed to transform the intermediate representation (IR) produced by frontends like Clang into assembly code and object files. You’ll learn how to write an optimizing code generator for a toy backend in LLVM. The chapters will guide you step by step through building this backend while exploring key concepts, such as the ABI, cost model, and register allocation. You’ll also find out how to express these concepts using LLVM's existing infrastructure and how established backends address these challenges. Furthermore, the book features code snippets that demonstrate the actual APIs. By the end of this book, you’ll have gained a deeper understanding of LLVM. The concepts presented are expected to remain stable across different LLVM versions, making this book a reliable quick reference guide for understanding LLVM.What you will learnUnderstand essential compiler concepts, such as SSA, dominance, and ABIBuild and extend LLVM backends for creating custom compiler featuresOptimize code by manipulating LLVM's Intermediate RepresentationContribute effectively to LLVM open-source projects and developmentDevelop debugging skills for LLVM optimizations and passesGrasp how encoding and (dis)assembling work in the context of compilersUtilize LLVM's TableGen DSL for creating custom compiler modelsWho this book is forThis book is for both beginners to LLVM and experienced LLVM developers. If you’re new to LLVM, it offers a clear, approachable guide to compiler backends, starting with foundational concepts. For seasoned LLVM developers, it dives into less-documented areas such as TableGen, MachineIR, and MC, enabling you to solve complex problems and expand your expertise. Whether you’re starting out or looking to deepen your knowledge, this book has something for you.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781835462577},
  url={https://ieeexplore.ieee.org/document/11014632},}

@INPROCEEDINGS{10207560,
  author={Humphrey, Laura},
  booktitle={2023 IEEE International Conference on Assured Autonomy (ICAA)}, 
  title={Example Applications of Formal Methods to Aerospace and Autonomous Systems}, 
  year={2023},
  volume={},
  number={},
  pages={67-75},
  abstract={As systems become more complex, they become more difficult to verify. Testing is the most common verification approach, but testing can only cover a relatively small proportion of total system behaviors. In the aerospace domain, the time and cost required to run enough tests to adequately verify avionics is already a major issue, and it is anticipated to be an even bigger issue for autonomous systems. Formal methods, i.e. mathematically-based tools and approaches for system specification, design, and analysis-based verification, can potentially supplement test-based verification to provide better coverage of system behaviors in a more reasonable amount of time and at a more reasonable cost. However, the uptake of formal methods has been slow. Our experience interacting with several communities focused on verification and certification of aerospace and autonomous systems is that there are still frequently basic questions about what formal methods are, how they work, and what types of properties they can analyze, along with concerns that formal methods may not be realistic or mature. To help answer these questions and address these concerns, this paper provides a brief overview of formal methods and reviews a set of applications to aerospace and autonomous systems that demonstrate some types of systems and properties that formal methods are well-suited to verify, where they are mature, and where they are gaining in maturity.},
  keywords={Costs;Autonomous systems;Aerospace electronics;Aerospace testing;Behavioral sciences;Certification;formal methods;verification;certification;aerospace systems;autonomous systems},
  doi={10.1109/ICAA58325.2023.00018},
  ISSN={},
  month={June},}

@INBOOK{6671244,
  author={Jacobs, Stuart},
  booktitle={Security Management of Next Generation Telecommunications Networks and Services}, 
  title={Index}, 
  year={2014},
  volume={},
  number={},
  pages={365-373},
  abstract={No abstract.},
  keywords={},
  doi={10.1002/9781118741580.index},
  ISSN={},
  publisher={IEEE},
  isbn={9781118741665},
  url={https://ieeexplore.ieee.org/document/6671244},}

@INPROCEEDINGS{5381934,
  author={Xin, Chen},
  booktitle={2009 International Conference on Wireless Networks and Information Systems}, 
  title={Wireless Communications Trends}, 
  year={2009},
  volume={},
  number={},
  pages={278-281},
  abstract={This paper reviews the wireless communications roadmap, and discusses the trends of wireless communications, including: higher and higher data rates, ubiquity of wireless devices, smart antennas, faster, smaller, cheaper hardware, frequency congestion, and multiple-input, multiple-output systems. Finally, this paper discusses the 4G wireless evolution.},
  keywords={Wireless communication;Internet telephony;Cable TV;Communication cables;Land mobile radio;Business;Cellular phones;Web and internet services;Hardware;Wires;wireless communication;4G;telecommunication;evolution},
  doi={10.1109/WNIS.2009.53},
  ISSN={},
  month={Dec},}

@INPROCEEDINGS{10541293,
  author={Andrei, Alexandru-Tudor},
  booktitle={2024 International Conference on Development and Application Systems (DAS)}, 
  title={MW4HBI: Mobile and Wearable Human-Building Interactions with a Multi-Platform User Interface}, 
  year={2024},
  volume={},
  number={},
  pages={1-7},
  abstract={In this paper we introduce Mobile and Wearable Human-Building Interactions with a Multi-Platform User Interface (MW4HBI). We start with laying out the guidelines to design responsive user interfaces for mobile and wearable devices. Then, we provide technical specifications for deploying a multi-platform application that manages entertainment and household appliances in smart buildings via a variety of input devices. Finally, we present the results of a small-scale end-user study on the preferences for ambient parameters inside smart buildings.},
  keywords={Home appliances;Smart buildings;Input devices;Entertainment industry;User interfaces;Wearable devices;Guidelines;smart buildings;ubiquitous computing;mobile computing;wearable computing;responsive design},
  doi={10.1109/DAS61944.2024.10541293},
  ISSN={},
  month={May},}

@BOOK{10948551,
  author={Belmansour, Tidjani and Brady, Damian},
  booktitle={Building CLI Applications with C# and .NET: A step-by-step guide to developing cross-platform CLI apps—from coding and testing to deployment},
  year={2025},
  volume={},
  number={},
  pages={},
  abstract={Unlock the potential of .NET to design, test, and deploy robust CLI applications, including development, security, and monitoringKey FeaturesReceive expert guidance on building CLI applications with .NETImplement advanced techniques for creating cross-platform, modular, and robust CLI applicationsPut your knowledge into practice through hands-on exercises and real-world projectsPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionDevelopers and system administrators often face challenges like inefficient workflows, complex system operations, and the growing demand for robust automation tools. CLI applications provide a powerful solution by enhancing flexibility, efficiency, and productivity in various environments. This book will guide you through mastering the development of robust command-line tools using .NET. Written by a Microsoft Azure MVP, the book’s hands-on approach ensures practical experience with real-world projects. You’ll start with an overview of foundational principles, essential concepts, and best practices for CLI application development. From there, you’ll advance to creating interactive interfaces, integrating with external APIs and services, and implementing security measures to safeguard your applications. Each chapter will build progressively from basic to advanced topics. Beyond development, you’ll learn how to enhance application quality through testing, package for efficient distribution, and deploy effectively. The book also teaches strategies to optimize performance to ensure your applications run efficiently under heavy usage. By the end of this book, you’ll have gained a deep understanding of CLI application development with .NET to build modular, extensible, and easy-to-maintain applications.What you will learnMaster CLI application development principles to enhance productivityBuild modular and extensible CLI applications that adapt to evolving needsDevelop interactive CLI applications for engaging user experiencesIntegrate external APIs and services to extend functionalityImplement robust security measures to ensure data protectionImprove quality and reliability through comprehensive testingPackage and deploy CLI applications efficiently for smooth releasesOptimize performance to achieve high efficiency and effectivenessWho this book is forThis book is for software developers, architects, and DevOps engineers aiming to enhance their existing SaaS platforms or optimize system operations. It focuses on providing users with CLI applications that automate and streamline workflows, unlocking operational efficiency. A basic understanding of programming concepts and prior experience with .NET and C# is expected, as this book doesn't cover introductory material.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781835882757},
  url={https://ieeexplore.ieee.org/document/10948551},}

@BOOK{9100440,
  author={Miceli, Andrew},
  booktitle={Wireless Technicians Handbook, Second Edition},
  year={2003},
  volume={},
  number={},
  pages={},
  abstract={This new second edition of the Artech House classic, Wireless Technician's Handbook applies up-to-date knowledge of wireless communications formats to the real-world situations you encounter everyday. Featuring brand new material on such critical technologies as GPRS, EDGE, CDMA-2000, and WCDMA, this single, easy-to-understand volume collects the comprehensive information that is essential for your work in the field today. Covering dBm versus Watts, the differences between analog, TDMA and CDMA, as well as the evolution to 2.5 and 3G standards such as EDGE, WCDMA and CDMA-2000, and testing of handsets and base stations, this handy resource explains all first-tier wireless formats in clear, concise language, helping you to become more knowledgeable and productive. The book focuses on testing concepts and procedures, and types of testing equipment, including spectrum analyzers and power meters and their applications. Moreover, it reviews AMPS, CDMA, IS-136 and GSM, and includes important charts and screen shots from common test instruments. An invaluable reference for the field-based technician, the book also serves as an excellent guide for non-technical wireless professionals.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Artech},
  isbn={9781580537216},
  url={https://ieeexplore.ieee.org/document/9100440},}

@INPROCEEDINGS{9095732,
  author={Dou, Jinfeng and Cao, Jiabao and Li, Xin and Wang, Lijuan and Tang, Shuya},
  booktitle={2020 IEEE 5th International Conference on Cloud Computing and Big Data Analytics (ICCCBDA)}, 
  title={Big Data Oriented Light-Load Embedded Performance Modeling}, 
  year={2020},
  volume={},
  number={},
  pages={476-481},
  abstract={With increasing development of big data, the performance assessment and optimization face with a big challenge. The traditional methods widely use delivery-testing-analysis-solving (DTAS) ring. In big data area, big data environment is necessary for the testing phase in DTAS, which results in the big cost in both time and hardware. This paper proposes the big data oriented light-load embedded performance modeling. It ascertains the performance criteria to set the Capacity and Performance (C&P) factors. These factors will be embedded into the software with an on-off switch during the architecture, design and developing phases before DTAS phase. After the software coding done with embedded C&P factors, a small traffic load is run to collect the C&P data. The collected data will be used for the performance bottleneck finding, performance optimization, and forecasting the capacity and performance for various customers' scenarios. Since the data easily help locate the issue, the required running traffic is small, and the problem solving is done before the traditional DTAS, this study is more suitable for the big data application. It can save more than 50% of time, decrease the software development efforts, and reduce the lab resources occupation. Finally, the proposed method is employed in the real prototype of an Internet of Things application, obtains the better capacity and performance, and the experiment data verify its effectiveness.},
  keywords={Software;Testing;Data models;Big Data;Optimization;Telecommunication traffic;Load modeling;Big data;capacity and performance;light-load;performance modeling;performance optimization},
  doi={10.1109/ICCCBDA49378.2020.9095732},
  ISSN={},
  month={April},}

@INPROCEEDINGS{1316702,
  author={Sprinkle, J.},
  booktitle={Proceedings. 11th IEEE International Conference and Workshop on the Engineering of Computer-Based Systems, 2004.}, 
  title={Improving CBS tool development with technological spaces}, 
  year={2004},
  volume={},
  number={},
  pages={218-224},
  abstract={The complexity of computer based systems (CBSs) requires that multiple levels of abstraction be available to a designer in order to facilitate their formal specification. Generating final executable code from the model of the system is preferred to hand-coding the implementation, but this is seldom done in one step-usually there are several cascading transformations that eventually result in the executable system. We explain how the concept of the technological space (TS) can be used to define and describe the layers between cascading transformations, and the transformations themselves. TSs are also shown as a categorization that better distinguishes between a domain and the technology used to store information in a domain.},
  keywords={Space technology;Software engineering;Software systems;Systems engineering and theory;Computer architecture;Software performance;Programming;Design engineering;Object oriented modeling;Large-scale systems},
  doi={10.1109/ECBS.2004.1316702},
  ISSN={},
  month={May},}

@ARTICLE{4447435,
  author={},
  journal={IEEE Unapproved Draft Std P2600_D33b, Feb 2008}, 
  title={IEEE Standard for Information Technology: Hardcopy Device and System Security}, 
  year={2008},
  volume={},
  number={},
  pages={1-173},
  abstract={This standard defines security requirements (all aspects of security including but not limited to authentication, authorization, privacy, integrity, device management, physical security, and information security) for manufacturers, users, and others on the selection, installation, configuration, and usage of hardcopy devices (HCDs) and systems, including printers, copiers, and multifunction devices (MFDs), and the computer systems that support these devices. This standard identifies security exposures for these HCDs and systems, and instructs manufacturers and software developers on appropriate security capabilities to include in their devices and systems, and instructs users on appropriate ways to use these security capabilities.},
  keywords={Standards;Security;IEEE Standards;Best practices;Patents;Licenses;Certification;2600-2008;all-in-one;copier;facsimile;fax;hardcopy device;HCD;information security;MFD;MFP;multifunction device;multifunction product;printer;scanner},
  doi={},
  ISSN={},
  month={Dec},}

@INBOOK{5273597,
  author={Hargrave, Frank},
  booktitle={Hargrave's Communications Dictionary}, 
  title={Index}, 
  year={2001},
  volume={},
  number={},
  pages={849-915},
  abstract={},
  keywords={Dictionaries;Indexes;Communication systems},
  doi={10.1109/9780470544822.index},
  ISSN={},
  publisher={IEEE},
  isbn={9780470544822},
  url={https://ieeexplore.ieee.org/document/5273597},}

@INPROCEEDINGS{9371550,
  author={Badaroux, Marie and Pétrot, Frédéric},
  booktitle={2021 26th Asia and South Pacific Design Automation Conference (ASP-DAC)}, 
  title={Arbitrary and Variable Precision Floating-Point Arithmetic Support in Dynamic Binary Translation}, 
  year={2021},
  volume={},
  number={},
  pages={325-330},
  abstract={Floating-point hardware support has more or less been settled 35 years ago by the adoption of the IEEE 754 standard. However, many scientific applications require higher accuracy than what can be rep-resented on 64 bits, and to that end make use of dedicated arbitrary precision software libraries. To reach a good performance/accuracy trade-off, developers use variable precision, requiring e.g. more accuracy as the computation progresses. Hardware accelerators for this kind of computations do not exist yet, and independently of the actual quality of the underlying arithmetic computations, defining the right instruction set architecture, memory representations, etc, for them is a challenging task. We investigate in this paper the support for arbitrary and variable precision arithmetic in a dynamic binary translator, to help gain an insight of what such an accelerator could provide as an interface to compilers, and thus programmers. We detail our design and present an implementation in QEMU using the MPRF library for the RISC-V processor1.},
  keywords={Software libraries;Production;Hardware;Space exploration;Task analysis;Standards;Testing;System-Level Simulation;Dynamic Binary Translation;Arbitrary Precision Floating-Point},
  doi={},
  ISSN={2153-697X},
  month={Jan},}

@ARTICLE{6770564,
  author={DeDuck, Peter F. and Johnson, Steven R.},
  journal={AT&T Technical Journal}, 
  title={The FT-2000 OC-48 lightwave system}, 
  year={1992},
  volume={71},
  number={1},
  pages={14-22},
  abstract={Next generation terrestrial lightwave terminals must do more than transport digital information from one location to another. FT-2000, AT&T's newest high-capacity lightwave transmission system, is designed to meet the needs of customers into the next century. It combines a flexible hardware platform and a powerful software-based architecture. As an intelligent lightwave system, FT-2000 can operate in sophisticated self-healing networks, and is managed by an advanced control system that simplifies installing, provisioning, monitoring, and maintaining it. It is fully compliant with the American National Standards Institute (ANSI) optical interface standard, the Synchronous Optical Network (SONET). We explore the broad range of applications and customer needs that drove the specification of FT-2000, and present the architectural solution that achieves the flexibility to meet those specifications.},
  keywords={},
  doi={10.1002/j.1538-7305.1992.tb00143.x},
  ISSN={8756-2324},
  month={Jan},}

@INPROCEEDINGS{11023492,
  author={Pirwani Dar, Muhammad Daniyal and Lorch, Rob and Sadeghi, Aliakbar and Sorcigli, Vincenzo and Gollier, Héloïse and Tinelli, Cesare and Vanhoef, Mathy and Chowdhury, Omar},
  booktitle={2025 IEEE Symposium on Security and Privacy (SP)}, 
  title={Saecred: A State-Aware, Over-the-Air Protocol Testing Approach for Discovering Parsing Bugs in SAE Handshake Implementations of COTS Wi-Fi Access Points}, 
  year={2025},
  volume={},
  number={},
  pages={3691-3709},
  abstract={WPA3-Personal introduced the stateful Simultane-ous Authentication of Equals (SAE) handshake protocol to achieve forward secrecy and resistance to passphrase guessing attacks during Wi-Fi connection bootstrapping, guarantees that are lacking in WPA2-Personal. However, the initial design of WPA3-Personal with SAE was susceptible to connection downgrade and denial-of-service (DoS) attacks. The current, enhanced version introduces mechanisms to mitigate these vulnerabilities. Enabling these security-enhancing mechanisms, however, results in a variable-structured, context-sensitive packet format that can be challenging to parse and interpret correctly. Misparsing SAE handshake packets can negatively impact Wi-Fi protocol security. To uncover SAE handshake packet misparsing in commercial-off-the-shelf (COTS) Wi-Fi access points (APs), we present Saecred,a packet-structure-guided, SAE-state-aware black-box fuzzer. Saecredreduces the underlying problem of misparsing discovery to a two-dimensional search problem, where the dimensions are the packet structure and the underlying SAE protocol state. It solves this search problem by combining Iterative Deepening Search (IDS) with a context-sensitive grammar-based fuzzing approach, where the latter relies on a Syntax-Guided Synthesis (SyGuS) solver. Saecred'seffectiveness is demonstrated by evaluating it on 6 COTS APs and the widely used open-source hostapd. Our evaluation discovered several instances of 4 classes of bugs. Bugs in two of these classes violate the two fundamental guarantees SAE expects to achieve (i.e., resistance to downgrade and DoS attacks). We reported our findings to the relevant stakeholders, which resulted in patches and security advisories.},
  keywords={Resistance;Protocols;Computer bugs;Closed box;Fuzzing;Search problems;Denial-of-service attack;Security;Wireless fidelity;Testing;fuzzing;network protocol security;wifi;authentication},
  doi={10.1109/SP61157.2025.00211},
  ISSN={2375-1207},
  month={May},}

@BOOK{10460888,
  author={Lee, Pui Shing and Charm, Dr. Toa},
  booktitle={Data Stewardship in Action: A roadmap to data value realization and measurable business outcomes},
  year={2024},
  volume={},
  number={},
  pages={},
  abstract={Take your organization's data maturity to the next level by operationalizing data governanceKey FeaturesDevelop the mindset and skills essential for successful data stewardshipApply practical advice and industry best practices, spanning data governance, quality management, and compliance, to enhance data stewardshipFollow a step-by-step program to develop a data operating model and implement data stewardship effectivelyPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionIn the competitive data-centric world, mastering data stewardship is not just a requirement—it's the key to organizational success. Unlock strategic excellence with Data Stewardship in Action, your guide to exploring the intricacies of data stewardship and its implementation for maximum efficiency. From business strategy to data strategy, and then to data stewardship, this book shows you how to strategically deploy your workforce, processes, and technology for efficient data processing. You’ll gain mastery over the fundamentals of data stewardship, from understanding the different roles and responsibilities to implementing best practices for data governance. You’ll elevate your data management skills by exploring the technologies and tools for effective data handling. As you progress through the chapters, you’ll realize that this book not only helps you develop the foundational skills to become a successful data steward but also introduces innovative approaches, including leveraging AI and GPT, for enhanced data stewardship. By the end of this book, you’ll be able to build a robust data governance framework by developing policies and procedures, establishing a dedicated data governance team, and creating a data governance roadmap that ensures your organization thrives in the dynamic landscape of data management.What you will learnEnhance your job prospects by understanding the data stewardship field, roles, and responsibilitiesDiscover how to develop a data strategy and translate it into a functional data operating modelDevelop an effective and efficient data stewardship programGain practical experience of establishing a data stewardship initiativeImplement purposeful governance with measurable ROIPrioritize data use cases with the value and effort matrixWho this book is forThis book is for professionals working in the field of data management, including business analysts, data scientists, and data engineers looking to gain a deeper understanding of the data steward role. Senior executives who want to (re)establish the data governance body in their organizations will find this resource invaluable. While accessible to both beginners and professionals, basic knowledge of data management concepts, such as data modeling, data warehousing, and data quality, is a must to get started.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781837638123},
  url={https://ieeexplore.ieee.org/document/10460888},}

@BOOK{10162242,
  author={Dietrich, George and Bernal, Guilherme},
  booktitle={Crystal Programming: A project-based introduction to building efficient, safe, and readable web and CLI applications},
  year={2022},
  volume={},
  number={},
  pages={},
  abstract={The ultimate guide to Crystal programming covering all its fundamental concepts such as OOP and concurrent programming to help you write readable and safe code and build fast applicationsKey FeaturesThe book uses an example-based approach for a better demonstration of the underlying conceptsDevelop a thorough appreciation of the roles of the macro API and annotationsLeverage supportive tools – spec, documentation, deployment, and automationBook DescriptionCrystal is a programming language with a concise and user-friendly syntax, along with a seamless system and a performant core, reaching C-like speed. This book will help you gain a deep understanding of the fundamental concepts of Crystal and show you how to apply them to create various types of applications. This book comes packed with step-by-step explanations of essential concepts and practical examples. You'll learn how to use Crystal’s features to create complex and organized projects relying on OOP and its most common design patterns. As you progress, you'll gain a solid understanding of both the basic and advanced features of Crystal. This will enable you to build any application, including command-line interface (CLI) programs and web applications using IOs, concurrency and C bindings, HTTP servers, and the JSON API. By the end of this programming book, you’ll be equipped with the skills you need to use Crystal programming for building and understanding any application you come across.What you will learnExplore how Crystal combines the merits of other languagesUnderstand how to leverage existing C libraries without writing any CFocus on zero-cost abstractions with compile-time macrosUse an example-based approach to demonstrate language featuresDevelop a variety of Crystal applications, such as web and CLI appsGain an understanding of the macro API and annotationsWho this book is forDevelopers who want to learn Crystal programming or anyone looking to improve their ability to solve real-world problems using the language will find this book helpful. Experience in application development using any other programming language is expected. However, prior knowledge of Crystal is not required.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781801816311},
  url={https://ieeexplore.ieee.org/document/10162242},}

@ARTICLE{4559609,
  author={},
  journal={IEEE Unapproved Draft Std P11073-00101/D5, Jun 2008}, 
  title={IEEE Draft Health Informatics - Point-Of-Care Medical Device Communication - Technical Report - Guidelines for the Use of RF Wireless Technology}, 
  year={2008},
  volume={},
  number={},
  pages={},
  abstract={The following Guidance document addresses the use of radio frequency 1 (RF) wireless technology for the transport of medical data both to and from point-of-care (PoC) medical devices. The context of such wireless medical data transport can range from home- or mobile-based healthcare to in hospital ambulatory and stationary situations. The intent of the guidance document is to be global with respect to wireless spectrum and equipment, although working group participation and expertise have favored detail of scenarios from the US. At the time of this Guidance document several applicable RF wireless technologies exist with a range of capabilities and characteristics, and in different stages of maturity, standardization, and adoption in healthcare. It is recognized that RF technologies are rapidly evolving, and new options may become available (or sufficiently established) after the publication of this Guidance document. The recommendations, therefore, avoid being overly prescriptive and instead attempt to assist medical device manufacturers, wireless equipment manufacturers, healthcare providers, government agencies and any other end-user of this document to make reasonable judgments regarding performance and practical implementation of wireless solutions. The Guidance document defines specific use cases to estimate, compare, and contrast performance of known technologies operating on wireless personal area (WPAN), wireless local area (WLAN), wireless metropolitan area (WMAN), and wireless wide area (WWAN) networks. Major considerations are 1) the quality-of-service (QoS) requirements (reliability, latency, priority, bandwidth) associated with the data being transported, 2) the expected performance (power, link range, throughput, link establishment and maintenance) of the wireless technology, and 3) the specific needs and resources of the end user. Related issues include network architecture, EMI/EMC, coexistence with other data streams, security, cost, power consumption, and technology configurability. Performance summaries for specific wireless technologies that support defined use cases are not intended as an endorsement of optimal solution because different needs, resources, sizes, and environments cannot be comprehensively addressed. This overview document is meant to be a foundation and reference for several follow-on IEEE 11073.3.5.x standards that will profile specific classes of off-the-shelf RF wireless technologies for medical data transport. Importantly, this guidance document is not envisioned to be periodically updated, but instead will act as a source of information for the follow-on IEEE 11073-0305.x standards that will supplant it. Periodic updates will be performed on the IEEE 11073-0305.x standards only.},
  keywords={Wireless communication;Communication system security;Radio frequency;Standards;Medical devices;Performance evaluation;Patents},
  doi={},
  ISSN={},
  month={},}

@INPROCEEDINGS{10000806,
  author={Kozak, Ilona and Berko, Andrii},
  booktitle={2022 IEEE 17th International Conference on Computer Sciences and Information Technologies (CSIT)}, 
  title={Three-module framework for automated software testing}, 
  year={2022},
  volume={},
  number={},
  pages={454-457},
  abstract={Different types of automated test frameworks are used for different purposes, i.e., to investigate features of the interface, functionality, etc. Each of these frameworks has different approaches to the testing process, and therefore, own advantages and disadvantages. Repositories seems to be helpful tool while developing various types of software testing systems. Such software testing system should meet business requirements, as well as user, functional, and non-functional requirements. Such requirements can be grouped into technological, financial, ergonomic, time, etc. The designed and developed three-module framework for automated software testing was uploaded to the repository and a sub-project was prepared.},
  keywords={Software testing;Computer languages;Ergonomics;Random access memory;Software;Selenium;Information technology;automated test framework;test approaches;testing tools;remote storage},
  doi={10.1109/CSIT56902.2022.10000806},
  ISSN={2766-3639},
  month={Nov},}

@BOOK{9106123,
  author={Frey, Robert},
  booktitle={Successful Proposal Strategies for Small Businesses: Using Knowledge Management to Win Government, Private-Sector, and International Contracts, Fifth Edition},
  year={2008},
  volume={},
  number={},
  pages={},
  abstract={Winning new business presents significant challenges. The new, Fifth Edition of this perennial bestseller updates and expands upon previous editions. The result is the ultimate resource for small and mid-sized businesses, as well as non-profit organizations and public-sector agencies, looking to achieve effective, efficient, and disciplined business development, proposal development, and knowledge management (KM) processes that in turn support winning new business. This popular book and its companion CD-ROM are highly accessible, self-contained desktop references developed to be informative, highly practical, and easy to use. Among the extensive array of new material, the Fifth Edition covers how to establish an internal rapid-response task order proposal "engine" for GWACs and ID/Iqs, prepare for successful graduation from the U.S. Small Business Administration 8(a) Program, and succeed in the world of very small businesses.The CD-ROM included features useful proposal templates in Adobe Acrobat, platform-independent format; HTML pointers to Small Business Web Sites; a comprehensive, fully searchable listing Proposal and Contract Acronyms; and a sample architecture for a knowledge base or proposal library.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Artech},
  isbn={9781596932272},
  url={https://ieeexplore.ieee.org/document/9106123},}

@BOOK{10162814,
  author={Cowell, Christopher and Lotz, Nicholas and Timberlake, Chris},
  booktitle={Automating DevOps with GitLab CI/CD Pipelines: Build efficient CI/CD pipelines to verify, secure, and deploy your code using real-life examples},
  year={2023},
  volume={},
  number={},
  pages={},
  abstract={Use GitLab CI/CD pipelines for automating and deploying different steps of your software development lifecycle using best practices and troubleshooting methods. Key FeaturesReap the power of GitLab CI/CD pipelines at every stage of your software development lifecycleLearn how GitLab makes Git easier to use and more powerful when committing and reviewing codeCement your understanding using hands-on tutorials and extensive self-assessment exercisesPurchase of the print or Kindle book includes a free eBook in the PDF formatBook DescriptionDevelopers and release engineers understand the high stakes involved in building, packaging, and deploying code correctly. Ensuring that your code is functionally correct, fast, and secure is a time-consuming and complex task. Code implementation, development, and deployment can be conducted efficiently using GitLab CI/CD pipelines. Automating DevOps with GitLab CI/CD Pipelines begins with the basics of Git and GitLab, showing how to commit and review code. You’ll learn to set up GitLab Runners for executing and autoscaling CI/CD pipelines and creating and configuring pipelines for many software development lifecycle steps. You'll also discover where to find pipeline results in GitLab, and how to interpret those results. Through the course of the book, you’ll become well-equipped with deploying code to different environments, advancing CI/CD pipeline features such as connecting GitLab to a Kubernetes cluster and using GitLab with Terraform, triggering pipelines and improving pipeline performance and using best practices and troubleshooting tips for uncooperative pipelines. In-text examples, use cases, and self-assessments will reinforce the important CI/CD, GitLab, and Git concepts, and help you prepare for interviews and certification exams related to GitLab. By the end of this book, you'll be able to use GitLab to build CI/CD pipelines that automate all the DevOps steps needed to build and deploy high-quality, secure code.What you will learnGain insights into the essentials of Git, GitLab, and DevOpsUnderstand how to create, view, and run GitLab CI/CD pipelinesExplore how to verify, secure, and deploy code with GitLab CI/CD pipelinesConfigure and use GitLab Runners to execute CI/CD pipelinesExplore advanced GitLab CI/CD pipeline features like DAGs and conditional logicFollow best practices and troubleshooting methods of GitLab CI/CD pipelinesImplement end-to-end software development lifecycle workflows using examplesWho this book is forThis book is for DevOps/DevSecOps engineers, application developers, release engineers, quality assurance engineers, security engineers, SREs, and sysadmins looking to implement fast, secure and automated software development lifecycle tasks using continuous integration and continuous delivery (CI/CD) pipelines in GitLab. Basic knowledge of major stages of the software development life cycle and DevOps processes will be helpful.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781803242934},
  url={https://ieeexplore.ieee.org/document/10162814},}

@BOOK{10162599,
  author={Wade, Corey and Jiménez, Mario Corchero and Bird, Andrew and Han, Dr. Lau Cher and Lee, Graham},
  booktitle={The Python Workshop: Write Python code to solve challenging real-world problems},
  year={2022},
  volume={},
  number={},
  pages={},
  abstract={Gain proficiency, productivity, and power by working on projects and kick-starting your career in Python with this comprehensive, hands-on guide.Key FeaturesUnderstand and utilize Python syntax, objects, methods, and best practicesExplore Python’s many features and libraries through real-world problems and big dataUse your newly acquired Python skills in machine learning as well as web and software developmentBook DescriptionPython is among the most popular programming languages in the world. It’s ideal for beginners because it’s easy to read and write, and for developers, because it’s widely available with a strong support community, extensive documentation, and phenomenal libraries – both built-in and user-contributed. This project-based course has been designed by a team of expert authors to get you up and running with Python. You’ll work though engaging projects that’ll enable you to leverage your newfound Python skills efficiently in technical jobs, personal projects, and job interviews. The book will help you gain an edge in data science, web development, and software development, preparing you to tackle real-world challenges in Python and pursue advanced topics on your own. Throughout the chapters, each component has been explicitly designed to engage and stimulate different parts of the brain so that you can retain and apply what you learn in the practical context with maximum impact. By completing the course from start to finish, you’ll walk away feeling capable of tackling any real-world Python development problem.What you will learnWrite efficient and concise functions using core Python methods and librariesBuild classes to address different business needsCreate visual graphs to communicate key data insightsOrganize big data and use machine learning to make regression and classification predictionsDevelop web pages and programs with Python tools and packagesAutomate essential tasks using Python scripts in real-time executionWho this book is forThis book is for professionals, students, and hobbyists who want to learn Python and apply it to solve challenging real-world problems. Although this is a beginner’s course, you’ll learn more easily if you already have an understanding of standard programming topics like variables, if-else statements, and functions. Experience with another object-oriented program, though not essential, will also be beneficial. If Python is your first attempt at computer programming, this book will help you understand the basics with adequate detail for a motivated student.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781804615805},
  url={https://ieeexplore.ieee.org/document/10162599},}

@INPROCEEDINGS{9742049,
  author={Hu, Chi and Ma, Siyou and Yang, Wansheng and Sun, Zhe and Deng, Fei and Yang, Yonghui},
  booktitle={2021 IEEE 21st International Conference on Software Quality, Reliability and Security Companion (QRS-C)}, 
  title={Software Test Data Reuse Based on Domain Ontology Construction}, 
  year={2021},
  volume={},
  number={},
  pages={279-284},
  abstract={CPS software has more specific defects and coupling defects than traditional computer systems. The testing and validation of CPS software are seriously affected by the cognitive level of individual testers, thus the quality of testing is difficult to guarantee. We believe that the knowledge reuse technology can address this problem to some extent. To achieve that, a construction method of CPS testing knowledge graph was carried out by our team, and the domain ontology of knowledge graph was organized and illustrated. After that, the key technologies such as automatic data extraction and intelligent retrieval were researched. Finally, a test data reuse approach based on knowledge graph was designed. With this approach, we expect to achieve the goal of turning the isolated island of test data into the semantic net of knowledge. Furthermore, increasing the reusability of test knowledge and reducing the defect omissions caused by cognitive deficiency of testers. In the end, this paper concludes the effect of the knowledge reuse-based testing method.},
  keywords={Knowledge engineering;Semantics;Software quality;Ontologies;Turning;Reliability engineering;Data processing;CPS testing;test data reuse;domain ontology;knowledge graph},
  doi={10.1109/QRS-C55045.2021.00049},
  ISSN={2693-9371},
  month={Dec},}

@BOOK{9100656,
  author={Elbert, Bruce},
  booktitle={The Satellite Communication Ground Segment and Earth Station Handbook, Second Edition},
  year={2014},
  volume={},
  number={},
  pages={},
  abstract={This updated and expanded second edition reflects the state of earth station design and ground segment architecture. From international telephone network gateways to direct broadcast home receivers, today's broad range of ground systems and devices require satellite communication engineers and business managers to have a broad and sound understanding of the design and operating principles of earth stations and ground control facilities. This book explores the delivery end of the satellite link and its relationship to delivery of services. Authored by a leading authority in the field, the book provides engineers and managers with the knowledge they need to devise their own approach to implementing and managing earth stations and the overall ground segment. Readers find practical guidance in an array of critical areas, including: preparing requirements, performing preliminary analyses, reviewing hardware designs, managing the introduction of the overall ground segment, and more.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Artech},
  isbn={9781608076741},
  url={https://ieeexplore.ieee.org/document/9100656},}

@INBOOK{8671453,
  author={Farhangi, Hassan and Joos, Geza},
  booktitle={Microgrid Planning and Design: A Concise Guide}, 
  title={Front Matter}, 
  year={2019},
  volume={},
  number={},
  pages={i-xxxii},
  abstract={The prelims comprise:  Half‐Title Page Title Page Copyright Dedication Contents About the Authors Disclaimer List of Figures List of Tables Foreword Preface Acknowledgments Acronyms and Abbreviations },
  keywords={},
  doi={10.1002/9781119453550.fmatter},
  ISSN={},
  publisher={IEEE},
  isbn={9781119453536},
  url={https://ieeexplore.ieee.org/document/8671453},}

@BOOK{10981551,
  author={Estrin, Eyal},
  booktitle={Cloud Security Handbook: Effectively secure cloud environments using AWS, Azure, and GCP},
  year={2025},
  volume={},
  number={},
  pages={},
  abstract={A complete guide to securing the core components of cloud services, with practical, real-world examples using the built-in security features of Azure, AWS, and GCPKey FeaturesDiscover hands-on techniques for implementing robust cloud security implementationProtect your data and cloud infrastructure with tailored security strategies for your businessLearn how to implement DevSecOps, apply encryption, detect threats and misconfigurations, and maintain cloud compliancePurchase of the print or Kindle book includes a free PDF eBookBook DescriptionSecuring cloud resources is no easy task—each provider has its unique set of tools, processes, and challenges, demanding specialized expertise. This book cuts through the complexity, delivering practical guidance on embedding security best practices across the core infrastructure components of AWS, Azure, and GCP. It equips information security professionals and cloud engineers with the skills to identify risks and implement robust security controls throughout the design, deployment, and maintenance of public cloud environments. Starting with the shared responsibility model, cloud service models, and deployment models, this book helps you get to grips with fundamental concepts such as compute, storage, networking, identity management, and encryption. You’ll then explore common threats and compliance requirements for cloud environments. As you progress, you'll implement security strategies across deployments ranging from small-scale environments to enterprise-grade production systems, including hybrid and multi-cloud setups. This edition expands on emerging topics like GenAI service security and DevSecOps, with hands-on examples leveraging built-in security features of AWS, Azure, and GCP. By the end of this book, you'll confidently secure any cloud environment with a comprehensive understanding of cloud security principles.What you will learnGrasp the fundamental concepts of cloud servicesSecure compute, storage, and networking services across cloud platformsGet to grips with identity management in the cloudSecure Generative AI services in the cloudAudit and monitor cloud services with a security-focused approachIdentify common threats and implement encryption to safeguard cloud servicesImplement security in hybrid and multi-cloud environmentsDesign and maintain scalable security for large-scale cloud deploymentsWho this book is forThis book is for IT professionals and information security personnel taking their first steps in the public cloud or migrating existing environments to the cloud. Cloud engineers, cloud architects, and cloud security professionals responsible for maintaining production environments in the cloud will also benefit from this book. Prior experience with deploying virtual machines, using storage services, and networking will help you to get the most out of this book.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781836200000},
  url={https://ieeexplore.ieee.org/document/10981551},}

@INBOOK{10114450,
  author={Tse, T.H. and Lo, David and Gorce, Alex and Perscheid, Michael and Hirschfeld, Robert and Wong, W. Eric},
  booktitle={Handbook of Software Fault Localization: Foundations and Advances}, 
  title={Emerging Aspects of Software Fault Localization}, 
  year={2023},
  volume={},
  number={},
  pages={529-579},
  abstract={In this final chapter of the Handbook, we introduce emerging, innovative methods in software fault localization. First, we present scientific and systematic hypothesis&#x2010;testing techniques and show they may be applied in practice. Second, for fault localization in the absence of a test oracle, we present a semi&#x2010;proving methodology based on metamorphic relations and symbolic evaluation. It hinges on causes and effects instead of statistical probabilities. Third, we present an approach to predict the effectiveness of fault localization tools using machine learning. Lastly, we discuss why manually produced test cases are not ideal for fault localization and explain how to mitigate the problem by using automatically generated test cases.},
  keywords={Location awareness;Debugging;Computer bugs;Source coding;Systematics;Software;Behavioral sciences},
  doi={10.1002/9781119880929.ch13},
  ISSN={},
  publisher={IEEE},
  isbn={9781119291817},
  url={https://ieeexplore.ieee.org/document/10114450},}

@INPROCEEDINGS{6201497,
  author={Hui, Zan and Lei, Pan and Yifei, Wang},
  booktitle={2012 2nd International Conference on Consumer Electronics, Communications and Networks (CECNet)}, 
  title={Design & implementation of laboratory information management system based on agile method}, 
  year={2012},
  volume={},
  number={},
  pages={2490-2493},
  abstract={Agile software development is a new methodology of developing high quality software timely when facing significant change; it is convenient for managers to accomplish the collection, disposal, output and other work to the data. There are chart and table functions to the output data and information of laboratories, functions of the output of original and final reports of laboratories, making it affiance to precede the quality control of the data and to help managers arrange analytical plans, staff and other daily work.},
  keywords={Laboratories;Programming;Information management;Software;Quality assurance;Servers;agile Method;management system;LMIS;agile software development},
  doi={10.1109/CECNet.2012.6201497},
  ISSN={},
  month={April},}

@INPROCEEDINGS{140801,
  author={Godon, F. and Al-Khalili, D. and Inkol, R.},
  booktitle={Proceedings of the 33rd Midwest Symposium on Circuits and Systems}, 
  title={A memory controller for mapping an array of circular buffers into a RAM}, 
  year={1990},
  volume={},
  number={},
  pages={645-648 vol.2},
  abstract={A 1.5- mu m CMOS ASIC with a total complexity of over 22000 gates has been developed to generate and keep track of the offsets within 32 circular buffers. It offers a fair arbitration of interleaved read/write operations at a maximum data transfer rate of 20 MHz. Although the device is intended for a specialized electronic warfare system application, the design features incorporated make it generic and suitable for other applications such as communications interfaces in multiprocessor systems.<>},
  keywords={Random access memory;Read-write memory;Buffer storage;Radar;Very large scale integration;Military computing;Process control;Computer architecture;Counting circuits;Physics computing},
  doi={10.1109/MWSCAS.1990.140801},
  ISSN={},
  month={Aug},}

@INPROCEEDINGS{9984771,
  author={Kroculick, Joseph B.},
  booktitle={2022 IEEE AUTOTESTCON}, 
  title={An Agile Model-Based Test Strategy for Assured PNT Implementation}, 
  year={2022},
  volume={},
  number={},
  pages={1-8},
  abstract={This paper presents an agile Model-Based Testing (aMBT) strategy to guide the rapid acquisition and implemen-tation of modernized Positioning, Navigation, and Timing (PNT) capabilities. Next-generation PNT services need to be secure and assured to support of Multi-Domain Operations (MDO). With aMBT the systems that use PNT can become more responsive to mission requirements. Positioning, Navigation, and Timing (PNT) services are deliv-ered by System of Systems (SoSs) that support Multi-Domain Operations. Because, PNT services span many constituent systems and components, a holistic strategy is needed to select technology options to support end-to-end capabilities by integrating system resources. To implement Assured PNT services, a strategy is needed to determine how to ensure that PNT data is available during military operations.},
  keywords={System testing;Navigation;Systems architecture;Metadata;Software;Product development;Timing;Assured PNT;Model-Based Testing},
  doi={10.1109/AUTOTESTCON47462.2022.9984771},
  ISSN={},
  month={Aug},}

@BOOK{10162996,
  author={Bock, Lisa},
  booktitle={Learn Wireshark: A definitive guide to expertly analyzing protocols and troubleshooting networks using Wireshark},
  year={2022},
  volume={},
  number={},
  pages={},
  abstract={Expertly analyze common protocols such as TCP, IP, and ICMP, along with learning how to use display and capture filters, save and export captures, create IO and stream graphs, and troubleshoot latency issuesKey FeaturesGain a deeper understanding of common protocols so you can easily troubleshoot network issuesExplore ways to examine captures to recognize unusual traffic and possible network attacksLearn advanced techniques, create display and capture filters, and generate IO and stream graphsBook DescriptionWireshark is a popular and powerful packet analysis tool that helps network administrators investigate latency issues and potential attacks. Over the years, there have been many enhancements to Wireshark’s functionality. This book will guide you through essential features so you can capture, display, and filter data with ease. In addition to this, you’ll gain valuable tips on lesser-known configuration options, which will allow you to complete your analysis in an environment customized to suit your needs. This updated second edition of Learn Wireshark starts by outlining the benefits of traffic analysis. You’ll discover the process of installing Wireshark and become more familiar with the interface. Next, you’ll focus on the Internet Suite and then explore deep packet analysis of common protocols such as DNS, DHCP, HTTP, and ARP. The book also guides you through working with the expert system to detect network latency issues, create I/O and stream graphs, subset traffic, and save and export captures. Finally, you’ll understand how to share captures using CloudShark, a browser-based solution for analyzing packet captures. By the end of this Wireshark book, you’ll have the skills and hands-on experience you need to conduct deep packet analysis of common protocols and network troubleshooting as well as identify security issues.What you will learnMaster network analysis and troubleshoot anomalies with WiresharkDiscover the importance of baselining network trafficCorrelate the OSI model with frame formation in WiresharkNarrow in on specific traffic by using display and capture filtersConduct deep packet analysis of common protocols: IP, TCP, and ARPUnderstand the role and purpose ofICMP, DNS, HTTP, and DHCPCreate a custom configuration profile and personalize the interfaceCreate I/O and stream graphs to better visualize trafficWho this book is forIf you are a network administrator, security analyst, student, or teacher and want to learn about effective packet analysis using Wireshark, then this book is for you. In order to get the most from this book, you should have basic knowledge of network fundamentals, devices, and protocols along with an understanding of different topologies.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781803234915},
  url={https://ieeexplore.ieee.org/document/10162996},}

@INPROCEEDINGS{9789811,
  author={da Silva, Anderson Santos and Schaeffer-Filho, Alberto},
  booktitle={NOMS 2022-2022 IEEE/IFIP Network Operations and Management Symposium}, 
  title={NetWords: Enabling the Understanding of Network Property Violation Occurrences}, 
  year={2022},
  volume={},
  number={},
  pages={1-9},
  abstract={A clear trend within the context of computer networks is the use of software as an alternative to specialized hardware. The benefit of this trend include an enhancement of flexibility, modularity, and maintainability of network components. Simultaneously, it is challenging to determine if everything is happening correctly in a computer network where software, possibly with bugs, is very present. Research in this field frequently tries to improve network testing with the use of formal verification techniques or network monitoring to detect property violations, such as configuration errors or policy conflicts. However, formal verification by itself cannot detect a property violation that was not anticipated and included in the model. Similarly, network monitoring needs to wait for a property violation to occur to detect it. Consequently, both enhancement efforts fail to achieve a complete result. In this paper, we investigate the problem of guaranteeing the absence of network connectivity property violations by combining the advantages of network monitoring for detecting property violations with the advantages of formal verification to model the network. A highlight related to the success of such a combination is the use of a model based on grammars to capture the communication patterns existing on the network. Our preliminary analysis allows the evaluation of high-level properties such as "Can network component x send HTTP packets?" and the detection of property violations, such as conflicting forwarding rules, as soon they occur in the network.},
  keywords={Computer bugs;Market research;Software;Computer networks;Hardware;Grammar;Monitoring},
  doi={10.1109/NOMS54207.2022.9789811},
  ISSN={2374-9709},
  month={April},}

@INPROCEEDINGS{9432331,
  author={Sudarsanam, P. and R, Anand and Banerjee, Sumanta and R, Hemanth},
  booktitle={2021 5th International Conference on Intelligent Computing and Control Systems (ICICCS)}, 
  title={Resilience Test case Automation for LTE Femtocell Networks}, 
  year={2021},
  volume={},
  number={},
  pages={622-628},
  abstract={The automated test case generation includes the manual test cases, which are converted to automated test cases through executable test scripts. It is more essential to bring out an effective resilience testing for network gateways. The automation testing plays a major role in performing time constrained and skillful work. Moreover, the network gateways will not take part in onetime investment, even when the technology or environment evolves, the test scripts are continuously patched based on the market demands. Thus, the automation testing is remaining as a challenging task to define and test automatically in a cost-effective mode. The developed device is called as femtocell, which helps to effectively densify the network and deliver a great customer experience in small indoor environments. It is checked to meet the real world scenarios under resilience test cases. It is performed by various manual test cases. Moreover, this project automates the manual test cases.},
  keywords={Technological innovation;Automation;Manuals;Logic gates;Real-time systems;Time factors;Task analysis;Long Term Evolution;Femtocell;Gateway;SAM Client;Wireless Provisioning System},
  doi={10.1109/ICICCS51141.2021.9432331},
  ISSN={},
  month={May},}

@BOOK{9100014,
  author={Liotine, Matthew},
  booktitle={Mission-Critical Network Planning},
  year={2003},
  volume={},
  number={},
  pages={},
  abstract={Whether a terrorist attack, fiber cut, security breach, natural disaster or traffic overload, today's networks must be designed to withstand adverse conditions and provide continuous service. This comprehensive, leading-edge book reveals the techniques and strategies to help you keep enterprise data and voice networks in service under critical circumstances. You learn numerous ways to minimize single points of failure through redundancy and backups, and discover how to select the right networking technologies to improve survivability and performance. This unique and timely resource shows you how to spot vulnerabilities in a network, use the protective features of different technologies for continuity and security, and build survivable network infrastructure. Supported with over 150 illustrations, this handy reference goes far beyond typical books dealing with only disaster recovery, to offer you a complete avoidance approach that proactively implements measures to protect infrastructure and systems from unplanned events.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Artech},
  isbn={9781580535595},
  url={https://ieeexplore.ieee.org/document/9100014},}

@BOOK{10251321,
  author={Watt, Andy},
  booktitle={Building Modern SaaS Applications with C# and .NET: Build, deploy, and maintain professional SaaS applications},
  year={2023},
  volume={},
  number={},
  pages={},
  abstract={Embark on a tech-tastic adventure and build Software as a Service (SaaS) applications using the Microsoft tech stack Purchase of the print or Kindle book includes a free PDF eBookKey FeaturesUnderstand the core concepts of Software as a Service and their importance in building modern applicationsBuild a wide array of key elements for SaaS applications using practical examplesLearn to test, deploy, upgrade, and maintain a SaaS applicationBook DescriptionThere are several concepts that must be mastered to deliver functional and efficient SaaS applications. This book is perfect for developers and teams with experience in traditional application development looking to switch to SaaS and deliver slick and modern applications. You‘ll start with a general overview of SaaS as a concept and learn with the help of an example throughout the book to bring life to the technical descriptions. You’ll use the Microsoft .NET tech stack for development and C# as the programming language to develop your desired SaaS application. Delivering SaaS requires a deep understanding of all layers in the application stack. As you progress, you’ll learn how to approach the database layer, the API, and the UI to confidently approach application development using the SaaS model. Additionally, you’ll explore how to test, deploy, maintain, and upgrade each component of the application. By the end of this book, you will be well equipped to approach all aspects of delivering software using the SaaS paradigm.What you will learnExplore SaaS and understand its importance in modern application developmentDiscover multi-tenancy and its impact on design decisions for SaaSBuild, test, and deploy a database, API, and UI for a SaaS applicationApproach authentication and authorization like a proScale a SaaS applicationEmploy C# and .NET to build SaaS applicationsWho this book is forIf you are a software developer with an interest in developing apps using the ‘SaaS’ paradigm, or a tech lead, scrum master, or a director and founder - this book will help you understand how to build a SaaS application. If you are a Java developer looking to start fresh with distributed systems, this book is for you. A basic understanding of Java, Spring/Spring Boot, and Web services will help you get the most out of this book.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781803240367},
  url={https://ieeexplore.ieee.org/document/10251321},}

@BOOK{10745304,
  author={Abraham, Isaac},
  booktitle={F# in Action},
  year={2024},
  volume={},
  number={},
  pages={},
  abstract={F# is engineered to make functional programming practical and accessible. This book will get you started writing your first simple, robust, and high performing functional code. F# lets you keep your code simple even in the most complex applications—and it’s the perfect language for taking your first steps in functional programming. This practical, example-driven guide shows you how to build professional applications the F# way. In F# in Action you will learn how to:  Write performant and robust systems with succinct F# code Model domains quickly, easily and accurately with F#’s type system Design solutions using functional programming patterns Ingest and process disparate data sources Develop data-driven web applications Unit test F# code Effectively model data using a variety of techniques Use scripts to rapidly explore domains  F# in Action is based on author and Microsoft F# MVP Isaac Abraham’s years of experience working with developers as an F# consultant. It upgrades .NET development skills with the core principles of functional programming, and you’ll soon see how F#’s functional-first approach makes it easy to learn this powerful paradigm.},
  keywords={functional;.NET;data analysis;TDD;type driven development;collaborative;opinionated;Visual Studio Code;scripts;interactive;data-driven;model;domains},
  doi={},
  ISSN={},
  publisher={Manning},
  isbn={9781633439535},
  url={https://ieeexplore.ieee.org/document/10745304},}

@INBOOK{9821136,
  author={Geng, Hwaiyu},
  booktitle={Data Center Handbook}, 
  title={Index}, 
  year={2015},
  volume={},
  number={},
  pages={669-682},
  abstract={},
  keywords={},
  doi={10.1002/9781118937563.index},
  ISSN={},
  publisher={Wiley},
  isbn={9781118937587},
  url={https://ieeexplore.ieee.org/document/9821136},}

@ARTICLE{11105047,
  author={},
  journal={IEEE P3379/D4, July 2025}, 
  title={IEEE Draft Standard for Interfaces of Deep Learning Compiler on Artificial Intelligence}, 
  year={2025},
  volume={},
  number={},
  pages={1-40},
  abstract={The technical architecture and technical flow of deep learning compiler interfaces were specified in this standard. The functions and input/output parameters of interfaces were addressed in this standard.},
  keywords={IEEE Standards;Deep learning;Program processors;Input-output programs;Testing;IEEE 3379;deep learning compiler interfaces;deep learning;input/output parameters;test method},
  doi={},
  ISSN={},
  month={July},}

@INPROCEEDINGS{6716404,
  author={Kolek, Jozef and Jovanović, Zoran and Šljivić, Nenad and Narančić, Dragan},
  booktitle={2013 21st Telecommunications Forum Telfor (TELFOR)}, 
  title={Adding microMIPS backend to the LLVM compiler infrastructure}, 
  year={2013},
  volume={},
  number={},
  pages={1015-1018},
  abstract={This work describes extending of the LLVM Compiler Infrastructure with the new backend support for microMIPS, which is an architecture from MIPS family of architectures. New backend consists of 16- and 32-bit instructions, out of which 180 of 32-bit instructions are recoded MIPS32 instructions, and 14 of 32-bit instructions are new microMIPS instructions. There are the 39 highly optimized 16-bit instructions.},
  keywords={Encoding;Registers;Computer architecture;Libraries;Generators;Switches;Computers;Compilers;LLVM;microMIPS},
  doi={10.1109/TELFOR.2013.6716404},
  ISSN={},
  month={Nov},}

@INPROCEEDINGS{271291,
  author={Dai, H. and Choo, M. and Starzyk, J.A.},
  booktitle={[1992] Proceedings of the 35th Midwest Symposium on Circuits and Systems}, 
  title={Noninvasive voltage measurement through an on-chip test structure (IC testing)}, 
  year={1992},
  volume={},
  number={},
  pages={340-343 vol.1},
  abstract={A method to evaluate internal voltages through a built-in test structure is presented. Multiplexers are used to increase accessibility. The test structure does not affect normal operation of the circuit. Individual subcircuits can be tested selectively based on evaluated internal voltages.<>},
  keywords={Integrated circuit testing;Voltage measurement;Circuit testing;Multiplexing;Large-scale systems;MOSFET circuits;MOS capacitors;Built-in self-test;Performance evaluation;Analog circuits},
  doi={10.1109/MWSCAS.1992.271291},
  ISSN={},
  month={Aug},}

@INPROCEEDINGS{6188784,
  author={},
  booktitle={2012 International Conference on Devices, Circuits and Systems (ICDCS)}, 
  title={2012 International Conference on Devices, Circuits and Systems (ICDCS)}, 
  year={2012},
  volume={},
  number={},
  pages={1-748},
  abstract={Provides the entire conference content.},
  keywords={CMOS integrated circuits;CMOS technology;MESFETs;MOSFETs;Optical imaging;Random access memory;Field programmable gate arrays},
  doi={10.1109/ICDCSyst.2012.6188784},
  ISSN={},
  month={March},}

@BOOK{9100016,
  author={Muller, Nathan},
  booktitle={LANs to WANs: The Complete Management Guide},
  year={2003},
  volume={},
  number={},
  pages={},
  abstract={Empowered by today's high-performance computers interconnected over LANs and WANs, companies are faced with the daunting task of bringing workability to the diversity and complexity of today's data communications landscape. This new, comprehensive resource addresses key network management challenges, showing you how to: tie together incompatible LANs, meld legacy systems and LANs, extend the reach of LANs with wireless links, protect information assets from various disaster scenarios, and consolidate multi-protocol traffic over a single WAN backbone in a way that guarantees appropriate service levels. Moreover, this hands-on guide defines management system requirements for the enterprise, and identifies the tools and expertise companies need to manage systems and networks in wired and wireless environments. You will learn how to design and test networks before implementation, and harden systems and networks against internal and external attacks with effective security administration and the application of specific tools. Putting infrastructure issues into proper perspective, this book will help you succeed in managing advanced communications systems and networks.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Artech},
  isbn={9781580535731},
  url={https://ieeexplore.ieee.org/document/9100016},}

@BOOK{10162639,
  author={Holt, Jon},
  booktitle={Systems Engineering Demystified: A practitioner's handbook for developing complex systems using a model-based approach},
  year={2021},
  volume={},
  number={},
  pages={},
  abstract={Get to grips with systems engineering life cycles, processes, and best practices and discover techniques to successfully develop complex systemsKey FeaturesDiscover how to manage increased complexity and understand systems better via effective communicationAdopt a proven model-based approach for systems engineering in your organizationApply proven techniques for requirements, design, validation and verification, and systems engineering managementBook DescriptionSystems engineering helps us to understand, specify, and develop complex systems, and is applied across a wide set of disciplines. As systems and their associated problems become increasingly complex in this evermore connected world, the need for more rigorous, demonstrable, and repeatable techniques also increases. Written by Professor Jon Holt – an internationally recognized systems engineering expert – this book provides a blend of technical and business aspects you need to understand in order to develop successful systems. You'll start with systems engineering basics and understand the complexity, communication, and different stakeholders' views of the system. The book then covers essential aspects of model-based systems engineering, systems, life cycles, and processes, along with techniques to develop systems. Moving on, you'll explore system models and visualization techniques, focusing on the SysML, and discover how solutions can be defined by developing effective system design, verification, and validation techniques. The book concludes by taking you through key management processes and systems engineering best practices and guidelines. By the end of this systems engineering book, you'll be able to confidently apply modern model-based systems engineering techniques to your own systems and projects.What you will learnUnderstand the three evils of systems engineering - complexity, ambiguous communication, and lack of understandingRealize successful systems using model-based systems engineeringUnderstand the concept of life cycles and how they control the evolution of a systemExplore processes and related concepts such as activities, stakeholders, and resourcesDiscover how needs fit into the systems life cycle and which processes are relevant and how to comply with themFind out how design, verification, and validation fit into the life cycle and processesWho this book is forThis book is for aspiring systems engineers, engineering managers, or anyone looking to apply systems engineering practices to their systems and projects. While a well-structured, model-based approach to systems engineering is an essential skill for engineers of all disciplines, many companies are finding that new graduates have little understanding of systems engineering. This book helps you acquire this skill with the help of a simple and practical approach to developing successful systems. No prior knowledge of systems engineering or modeling is required to get started with this book.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781838985448},
  url={https://ieeexplore.ieee.org/document/10162639},}

@INBOOK{10513791,
  author={Prince, Betty},
  booktitle={Vertical 3D Memory Technologies}, 
  title={3D Stacking of RAM&#x2013;Processor Chips Using TSV}, 
  year={2014},
  volume={},
  number={},
  pages={275-344},
  abstract={Summary <p>3D chip stacking using through&#x2010;silicon vias (TSVs) can provide smaller footprints, higher performance, lower power, and higher reliability without shrinking the chip or integrating differing technologies on the same chip. TSVs are vertical holes in the chip that are filled with a conducting metal usually copper. 2D chips with embedded memory integrated with logic have developed some of the same issues from long interconnects and wide memory buses that systems previously had, such as coupling noise and ground bounce. A well&#x2010;designed 3D stacking configuration of memory and processor using TSVs can significantly shorten on&#x2010;chip buses. Early 2.5D configurations have used passive interposers with chips side by side on the interposer to connect chips. Another early configuration is a stack of identical DRAM chips connected with TSVs called a memory cube. Standards for chips reconfigured for 3D TSV packaging have been posted and new standards are being discussed. In the future, new design tools optimized for 3D can repartition chips and systems in 3D to optimize these benefits. Challenges of 3D stacking using TSVs include instability, heat dissipation, mechanical stress, and thermal stress. Remote connections between processor and memory chips are also possible and examples of work in inductive coupling or processor and memory are discussed. In the future, optical interconnects are also possible.</p>},
  keywords={Through-silicon vias;Three-dimensional displays;Random access memory;Program processors;Flash memories;Stacking;Field programmable gate arrays},
  doi={10.1002/9781118760475.ch06},
  ISSN={},
  publisher={Wiley},
  isbn={9781118760451},
  url={https://ieeexplore.ieee.org/document/10513791},}

@BOOK{10163375,
  author={Banerjee, Sinchan},
  booktitle={Scalable Data Architecture with Java: Build efficient enterprise-grade data architecting solutions using Java},
  year={2022},
  volume={},
  number={},
  pages={},
  abstract={Orchestrate data architecting solutions using Java and related technologies to evaluate, recommend and present the most suitable solution to leadership and clientsKey FeaturesLearn how to adapt to the ever-evolving data architecture technology landscapeUnderstand how to choose the best suited technology, platform, and architecture to realize effective business valueImplement effective data security and governance principlesBook DescriptionJava architectural patterns and tools help architects to build reliable, scalable, and secure data engineering solutions that collect, manipulate, and publish data. This book will help you make the most of the architecting data solutions available with clear and actionable advice from an expert. You’ll start with an overview of data architecture, exploring responsibilities of a Java data architect, and learning about various data formats, data storage, databases, and data application platforms as well as how to choose them. Next, you’ll understand how to architect a batch and real-time data processing pipeline. You’ll also get to grips with the various Java data processing patterns, before progressing to data security and governance. The later chapters will show you how to publish Data as a Service and how you can architect it. Finally, you’ll focus on how to evaluate and recommend an architecture by developing performance benchmarks, estimations, and various decision metrics. By the end of this book, you’ll be able to successfully orchestrate data architecture solutions using Java and related technologies as well as to evaluate and present the most suitable solution to your clients.What you will learnAnalyze and use the best data architecture patterns for problemsUnderstand when and how to choose Java tools for a data architectureBuild batch and real-time data engineering solutions using JavaDiscover how to apply security and governance to a solutionMeasure performance, publish benchmarks, and optimize solutionsEvaluate, choose, and present the best architectural alternativesUnderstand how to publish Data as a Service using GraphQL and a REST APIWho this book is forData architects, aspiring data architects, Java developers and anyone who wants to develop or optimize scalable data architecture solutions using Java will find this book useful. A basic understanding of data architecture and Java programming is required to get the best from this book.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781801072083},
  url={https://ieeexplore.ieee.org/document/10163375},}

@BOOK{9100768,
  author={Caballero, Jose and Guimera, Andreu and Hens, Francisco and Segura, Roger},
  booktitle={SONET/SDH, ATM and ADSL: Installation and Maintenance},
  year={2003},
  volume={},
  number={},
  pages={},
  abstract={Service level agreements guaranteeing quality of service have helped your organization to keep old customers and win new ones over. Although it may be easy for the sales department to ink a service level agreement, you have to handle the constant problems of phase fluctuations, jitter, and wander, that threaten the quality of service spelled out in these service level agreements. By showing you how to properly set up a network, test its performance, and troubleshoot any systems glitches, this book is an on-the-job companion that you can turn to time after time to ensure quality network service. This implementation, maintenance, and troubleshooting manual goes beyond overview books and standards guides to give you the technical insight and hands-on knowledge you need to know to deploy today's digital networks and keep them running 24/7. From the principles of digital technology and SDH, to network synchronization and ADSL qualification, this practical resource provides in-depth coverage of the most critical areas. Over 300 illustrations support key topics.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Artech},
  isbn={9781580536981},
  url={https://ieeexplore.ieee.org/document/9100768},}

@INPROCEEDINGS{4063813,
  author={},
  booktitle={2006 International Conference on Communications, Circuits and Systems}, 
  title={Technical Program of 2006 ICCCAS}, 
  year={2006},
  volume={1},
  number={},
  pages={25-85},
  abstract={Provides a schedule of conference events and a listing of which papers were presented in each session.},
  keywords={},
  doi={10.1109/ICCCAS.2006.284569},
  ISSN={},
  month={June},}

@INPROCEEDINGS{10553457,
  author={},
  booktitle={2024 IEEE International Systems Conference (SysCon)}, 
  title={SysCon 2024 Front Matter}, 
  year={2024},
  volume={},
  number={},
  pages={4-42},
  abstract={},
  keywords={},
  doi={10.1109/SysCon61195.2024.10553457},
  ISSN={2472-9647},
  month={April},}

@INPROCEEDINGS{4064168,
  author={},
  booktitle={2006 International Conference on Communications, Circuits and Systems}, 
  title={Technical Program of 2006 ICCCAS}, 
  year={2006},
  volume={3},
  number={},
  pages={25-85},
  abstract={Provides a schedule of conference events and a listing of which papers were presented in each session.},
  keywords={},
  doi={10.1109/ICCCAS.2006.284942},
  ISSN={},
  month={June},}

@INPROCEEDINGS{9905133,
  author={},
  booktitle={2022 International Research Conference on Smart Computing and Systems Engineering (SCSE)}, 
  title={SCSE 2022 Conference Proceedings}, 
  year={2022},
  volume={5},
  number={},
  pages={i-cdii},
  abstract={Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.},
  keywords={},
  doi={10.1109/SCSE56529.2022.9905133},
  ISSN={2613-8662},
  month={Sep.},}

@INPROCEEDINGS{4063976,
  author={},
  booktitle={2006 International Conference on Communications, Circuits and Systems}, 
  title={Technical Program of 2006 ICCCAS}, 
  year={2006},
  volume={2},
  number={},
  pages={25-85},
  abstract={Provides a schedule of conference events and a listing of which papers were presented in each session.},
  keywords={},
  doi={10.1109/ICCCAS.2006.284735},
  ISSN={},
  month={June},}

@INPROCEEDINGS{4064349,
  author={},
  booktitle={2006 International Conference on Communications, Circuits and Systems}, 
  title={Technical Program of 2006 ICCCAS}, 
  year={2006},
  volume={4},
  number={},
  pages={25-85},
  abstract={Provides a schedule of conference events and a listing of which papers were presented in each session.},
  keywords={},
  doi={10.1109/ICCCAS.2006.285102},
  ISSN={},
  month={June},}

@INPROCEEDINGS{5501146,
  author={},
  booktitle={2009 5th Central and Eastern European Software Engineering Conference in Russia (CEE-SECR)}, 
  title={[Title page]}, 
  year={2009},
  volume={},
  number={},
  pages={i-ii},
  abstract={The following topics are dealt with: crisis-time distributed systems development; regression test selection technique; agile project management; software project feasibility study; graphical processing units; industrial C/C++ software; video registration and security systems; reliable software development; industrial Java applications; parallel programs; e-government and outsourcing; program reliability; operation-friendly software development; software product management; SaaS concept; SOA testing stack; complex hardware-software systems; UML-model; Microsoft DSL technology; Microsoft.NET micro framework; WBEM/CIM & WS-MAN technology application; and agile Web development.},
  keywords={},
  doi={10.1109/CEE-SECR.2009.5501146},
  ISSN={},
  month={Oct},}

@INPROCEEDINGS{10226807,
  author={},
  booktitle={2023 International Conference on Consumer Electronics - Taiwan (ICCE-Taiwan)}, 
  title={ICCE-Taiwan 2023 Conference Proceedings}, 
  year={2023},
  volume={},
  number={},
  pages={1-891},
  abstract={},
  keywords={},
  doi={10.1109/ICCE-Taiwan58799.2023.10226807},
  ISSN={2575-8284},
  month={July},}

@INPROCEEDINGS{10633560,
  author={},
  booktitle={2024 IEEE 48th Annual Computers, Software, and Applications Conference (COMPSAC)}, 
  title={Table of Contents}, 
  year={2024},
  volume={},
  number={},
  pages={v-liii},
  abstract={},
  keywords={},
  doi={10.1109/COMPSAC61105.2024.00004},
  ISSN={2836-3795},
  month={July},}

@INPROCEEDINGS{8491782,
  author={},
  booktitle={2018 21st Euromicro Conference on Digital System Design (DSD)}, 
  title={Table of contents}, 
  year={2018},
  volume={},
  number={},
  pages={5-18},
  abstract={Presents the table of contents/splash page of the proceedings record.},
  keywords={},
  doi={10.1109/DSD.2018.00004},
  ISSN={},
  month={Aug},}

@INPROCEEDINGS{8449407,
  author={},
  booktitle={2018 IEEE/ACM 40th International Conference on Software Engineering: Companion (ICSE-Companion)}, 
  title={Table of contents}, 
  year={2018},
  volume={},
  number={},
  pages={5-26},
  abstract={Presents the table of contents/splash page of the proceedings record.},
  keywords={},
  doi={},
  ISSN={2574-1934},
  month={May},}

@INPROCEEDINGS{10069029,
  author={},
  booktitle={2022 IEEE/ACM International Conference On Computer Aided Design (ICCAD)}, 
  title={ICCAD 2022 Conference Proceedings}, 
  year={2022},
  volume={},
  number={},
  pages={1-1467},
  abstract={},
  keywords={},
  doi={},
  ISSN={1558-2434},
  month={Oct},}

@INPROCEEDINGS{8509742,
  author={},
  booktitle={2018 40th Electrical Overstress/Electrostatic Discharge Symposium (EOS/ESD)}, 
  title={EOS/ESD 2018 Bios}, 
  year={2018},
  volume={},
  number={},
  pages={i-xxiv},
  abstract={Presents an index of the authors whose articles are published in the conference proceedings record.},
  keywords={},
  doi={10.23919/EOS/ESD.2018.8509742},
  ISSN={0739-5159},
  month={Sep.},}

@INPROCEEDINGS{9241331,
  author={},
  booktitle={2020 42nd Annual EOS/ESD Symposium (EOS/ESD)}, 
  title={Bios}, 
  year={2020},
  volume={},
  number={},
  pages={1-27},
  abstract={Lists the authors included in the conference proceedings.},
  keywords={},
  doi={},
  ISSN={0739-5159},
  month={Sep.},}

@INPROCEEDINGS{10336263,
  author={},
  booktitle={2023 IEEE International Conference on Software Maintenance and Evolution (ICSME)}, 
  title={Table of Contents}, 
  year={2023},
  volume={},
  number={},
  pages={5-13},
  abstract={},
  keywords={},
  doi={10.1109/ICSME58846.2023.00004},
  ISSN={2576-3148},
  month={Oct},}

@INPROCEEDINGS{8878768,
  author={},
  booktitle={2019 IEEE International Conference on System, Computation, Automation and Networking (ICSCAN)}, 
  title={Table of contents}, 
  year={2019},
  volume={},
  number={},
  pages={1-12},
  abstract={Presents the cover/table of contents for this issue of the periodical.},
  keywords={},
  doi={10.1109/ICSCAN.2019.8878768},
  ISSN={},
  month={March},}

@INPROCEEDINGS{10740232,
  author={},
  booktitle={2024 ACM/IEEE 6th Symposium on Machine Learning for CAD (MLCAD)}, 
  title={MLCAD 2024 Cover Page}, 
  year={2024},
  volume={},
  number={},
  pages={c1-c321},
  abstract={},
  keywords={},
  doi={10.1109/MLCAD62225.2024.10740232},
  ISSN={},
  month={Sep.},}

@INPROCEEDINGS{8904836,
  author={},
  booktitle={2019 ACM/IEEE 22nd International Conference on Model Driven Engineering Languages and Systems Companion (MODELS-C)}, 
  title={Table of contents}, 
  year={2019},
  volume={},
  number={},
  pages={5-17},
  abstract={Presents the table of contents/splash page of the proceedings record.},
  keywords={},
  doi={10.1109/MODELS-C.2019.00004},
  ISSN={},
  month={Sep.},}

@INPROCEEDINGS{9169403,
  author={},
  booktitle={2020 IEEE International Conference on Blockchain and Cryptocurrency (ICBC)}, 
  title={ICBC 2020 Table of Contents}, 
  year={2020},
  volume={},
  number={},
  pages={i-xii},
  abstract={Presents the table of contents/splash page of the proceedings record.},
  keywords={},
  doi={10.1109/ICBC48266.2020.9169403},
  ISSN={},
  month={May},}

@INPROCEEDINGS{9969363,
  author={},
  booktitle={2022 IEEE 13th International Green and Sustainable Computing Conference (IGSC)}, 
  title={Contents}, 
  year={2022},
  volume={},
  number={},
  pages={1-189},
  abstract={Presents the conference table of contents.},
  keywords={},
  doi={10.1109/IGSC55832.2022.9969363},
  ISSN={},
  month={Oct},}

@INPROCEEDINGS{10764816,
  author={},
  booktitle={2024 39th IEEE/ACM International Conference on Automated Software Engineering (ASE)}, 
  title={Table of Contents}, 
  year={2024},
  volume={},
  number={},
  pages={5-38},
  abstract={},
  keywords={},
  doi={},
  ISSN={2643-1572},
  month={Oct},}

@INPROCEEDINGS{9169409,
  author={},
  booktitle={2020 IEEE International Conference on Blockchain and Cryptocurrency (ICBC)}, 
  title={ICBC 2020 Final Program}, 
  year={2020},
  volume={},
  number={},
  pages={i-l},
  abstract={Provides a schedule of conference events and a listing of which papers were presented in each session.},
  keywords={},
  doi={10.1109/ICBC48266.2020.9169409},
  ISSN={},
  month={May},}

@INPROCEEDINGS{10556022,
  author={},
  booktitle={2024 IEEE/ACM 3rd International Conference on AI Engineering – Software Engineering for AI (CAIN)}, 
  title={Table of Contents}, 
  year={2024},
  volume={},
  number={},
  pages={5-10},
  abstract={},
  keywords={},
  doi={},
  ISSN={},
  month={April},}

@INPROCEEDINGS{8715180,
  author={},
  booktitle={2019 Design, Automation & Test in Europe Conference & Exhibition (DATE)}, 
  title={[Front cover]}, 
  year={2019},
  volume={},
  number={},
  pages={c1-c126},
  abstract={The following topics are dealt with: learning (artificial intelligence); integrated circuit design; multiprocessing systems; neural nets; power aware computing; microprocessor chips; field programmable gate arrays; embedded systems; system-on-chip; optimisation.},
  keywords={},
  doi={10.23919/DATE.2019.8715180},
  ISSN={1558-1101},
  month={March},}

@INPROCEEDINGS{685775,
  author={},
  booktitle={Proceedings. Fifth International Conference on Software Reuse (Cat. No.98TB100203)}, 
  title={Subject index}, 
  year={1998},
  volume={},
  number={},
  pages={377-388},
  abstract={The index contains an entry for all items that appeared in this publication.},
  keywords={},
  doi={10.1109/ICSR.1998.685775},
  ISSN={1085-9098},
  month={June},}

@INPROCEEDINGS{8719515,
  author={},
  booktitle={2018 25th Asia-Pacific Software Engineering Conference (APSEC)}, 
  title={Table of contents}, 
  year={2018},
  volume={},
  number={},
  pages={5-15},
  abstract={Presents the table of contents/splash page of the proceedings record.},
  keywords={},
  doi={10.1109/APSEC.2018.00004},
  ISSN={2640-0715},
  month={Dec},}

@INPROCEEDINGS{9226308,
  author={},
  booktitle={2020 46th Euromicro Conference on Software Engineering and Advanced Applications (SEAA)}, 
  title={Table of Contents}, 
  year={2020},
  volume={},
  number={},
  pages={i-xi},
  abstract={Presents the table of contents/splash page of the proceedings record.},
  keywords={},
  doi={10.1109/SEAA51224.2020.00004},
  ISSN={},
  month={Aug},}
